


<p align="center"> 
  <img src="./archon-ui-main/public/archon-main-graphic.png" alt="Archon Main Graphic" width="853" height="422">
</p>

<p align="center">
  <em>Power up your AI coding assistants with your own custom knowledge base and task management as an MCP server</em>
</p>

<p align="center">
  <a href="#quick-start">Quick Start</a> â€¢
  <a href="#upgrading">Upgrading</a> â€¢
  <a href="#whats-included">What's Included</a> â€¢
  <a href="#architecture">Architecture</a> â€¢
  <a href="#troubleshooting">Troubleshooting</a>
</p>

---

## ğŸ¯ What is Archon?

> Archon is currently in beta! Expect things to not work 100%, and please feel free to share any feedback and contribute with fixes/new features! Thank you to everyone for all the excitement we have for Archon already, as well as the bug reports, PRs, and discussions. It's a lot for our small team to get through but we're committed to addressing everything and making Archon into the best tool it possibly can be!

Archon is the **command center** for AI coding assistants. For you, it's a sleek interface to manage knowledge, context, and tasks for your projects. For the AI coding assistant(s), it's a **Model Context Protocol (MCP) server** to collaborate on and leverage the same knowledge, context, and tasks. Connect Claude Code, Kiro, Cursor, Windsurf, etc. to give your AI agents access to:

- **Your documentation** (crawled websites, uploaded PDFs/docs)
- **Smart search capabilities** with advanced RAG strategies
- **Task management** integrated with your knowledge base
- **Real-time updates** as you add new content and collaborate with your coding assistant on tasks
- **Much more** coming soon to build Archon into an integrated environment for all context engineering

This new vision for Archon replaces the old one (the agenteer). Archon used to be the AI agent that builds other agents, and now you can use Archon to do that and more.

> It doesn't matter what you're building or if it's a new/existing codebase - Archon's knowledge and task management capabilities will improve the output of **any** AI driven coding.

## ğŸ”— Important Links

- **[GitHub Discussions](https://github.com/coleam00/Archon/discussions)** - Join the conversation and share ideas about Archon
- **[Contributing Guide](CONTRIBUTING.md)** - How to get involved and contribute to Archon
- **[Introduction Video](https://youtu.be/8pRc_s2VQIo)** - Getting started guide and vision for Archon
- **[Archon Kanban Board](https://github.com/users/coleam00/projects/1)** - Where maintainers are managing issues/features
- **[Dynamous AI Mastery](https://dynamous.ai)** - The birthplace of Archon - come join a vibrant community of other early AI adopters all helping each other transform their careers and businesses!

## Quick Start

### Prerequisites

- [Docker Desktop](https://www.docker.com/products/docker-desktop/)
- [Node.js 18+](https://nodejs.org/) (for hybrid development mode)
- [Supabase](https://supabase.com/) account (free tier or local Supabase both work)
- [OpenAI API key](https://platform.openai.com/api-keys) (Gemini and Ollama are supported too!)
- (OPTIONAL) [Make](https://www.gnu.org/software/make/) (see [Installing Make](#installing-make) below)

### Setup Instructions

1. **Clone Repository**:
   ```bash
   git clone https://github.com/coleam00/archon.git
   ```
   ```bash
   cd archon
   ```
2. **Environment Configuration**:

   ```bash
   cp .env.example .env
   # Edit .env and add your Supabase credentials:
   # SUPABASE_URL=https://your-project.supabase.co
   # SUPABASE_SERVICE_KEY=your-service-key-here
   ```

   IMPORTANT NOTES:
   - For cloud Supabase: they recently introduced a new type of service role key but use the legacy one (the longer one).
   - For local Supabase: set SUPABASE_URL to http://host.docker.internal:8000 (unless you have an IP address set up).

3. **Database Setup**: In your [Supabase project](https://supabase.com/dashboard) SQL Editor, copy, paste, and execute the contents of `migration/complete_setup.sql`

4. **Start Services** (choose one):

   **Full Docker Mode (Recommended for Normal Archon Usage)**

   ```bash
   docker compose up --build -d
   ```

   This starts all core microservices in Docker:
   - **Server**: Core API and business logic (Port: 8181)
   - **MCP Server**: Protocol interface for AI clients (Port: 8051)
   - **UI**: Web interface (Port: 3737)

   Ports are configurable in your .env as well!

5. **Configure API Keys**:
   - Open http://localhost:3737
   - You'll automatically be brought through an onboarding flow to set your API key (OpenAI is default)

## âš¡ Quick Test

Once everything is running:

1. **Test Web Crawling**: Go to http://localhost:3737 â†’ Knowledge Base â†’ "Crawl Website" â†’ Enter a doc URL (such as https://ai.pydantic.dev/llms-full.txt)
2. **Test Document Upload**: Knowledge Base â†’ Upload a PDF
3. **Test Projects**: Projects â†’ Create a new project and add tasks
4. **Integrate with your AI coding assistant**: MCP Dashboard â†’ Copy connection config for your AI coding assistant 

## Installing Make

<details>
<summary><strong>ğŸ› ï¸ Make installation (OPTIONAL - For Dev Workflows)</strong></summary>

### Windows

```bash
# Option 1: Using Chocolatey
choco install make

# Option 2: Using Scoop
scoop install make

# Option 3: Using WSL2
wsl --install
# Then in WSL: sudo apt-get install make
```

### macOS

```bash
# Make comes pre-installed on macOS
# If needed: brew install make
```

### Linux

```bash
# Debian/Ubuntu
sudo apt-get install make

# RHEL/CentOS/Fedora
sudo yum install make
```

</details>

<details>
<summary><strong>ğŸš€ Quick Command Reference for Make</strong></summary>
<br/>

| Command           | Description                                             |
| ----------------- | ------------------------------------------------------- |
| `make dev`        | Start hybrid dev (backend in Docker, frontend local) â­ |
| `make dev-docker` | Everything in Docker                                    |
| `make stop`       | Stop all services                                       |
| `make test`       | Run all tests                                           |
| `make lint`       | Run linters                                             |
| `make install`    | Install dependencies                                    |
| `make check`      | Check environment setup                                 |
| `make clean`      | Remove containers and volumes (with confirmation)       |

</details>

## ğŸ”„ Database Reset (Start Fresh if Needed)

If you need to completely reset your database and start fresh:

<details>
<summary>âš ï¸ <strong>Reset Database - This will delete ALL data for Archon!</strong></summary>

1. **Run Reset Script**: In your Supabase SQL Editor, run the contents of `migration/RESET_DB.sql`

   âš ï¸ WARNING: This will delete all Archon specific tables and data! Nothing else will be touched in your DB though.

2. **Rebuild Database**: After reset, run `migration/complete_setup.sql` to create all the tables again.

3. **Restart Services**:

   ```bash
   docker compose --profile full up -d
   ```

4. **Reconfigure**:
   - Select your LLM/embedding provider and set the API key again
   - Re-upload any documents or re-crawl websites

The reset script safely removes all tables, functions, triggers, and policies with proper dependency handling.

</details>

## ğŸ“š Documentation

### Core Services

| Service            | Container Name | Default URL           | Purpose                           |
| ------------------ | -------------- | --------------------- | --------------------------------- |
| **Web Interface**  | archon-ui      | http://localhost:3737 | Main dashboard and controls       |
| **API Service**    | archon-server  | http://localhost:8181 | Web crawling, document processing |
| **MCP Server**     | archon-mcp     | http://localhost:8051 | Model Context Protocol interface  |
| **Agents Service** | archon-agents  | http://localhost:8052 | AI/ML operations, reranking       |  

## Upgrading

To upgrade Archon to the latest version:

1. **Pull latest changes**:
   ```bash
   git pull
   ```

2. **Check for migrations**: Look in the `migration/` folder for any SQL files newer than your last update. Check the file created dates to determine if you need to run them. You can run these in the SQL editor just like you did when you first set up Archon. We are also working on a way to make handling these migrations automatic!

3. **Rebuild and restart**:
   ```bash
   docker compose up -d --build
   ```

This is the same command used for initial setup - it rebuilds containers with the latest code and restarts services.

## What's Included

### ğŸ§  Knowledge Management

- **Smart Web Crawling**: Automatically detects and crawls entire documentation sites, sitemaps, and individual pages
- **Document Processing**: Upload and process PDFs, Word docs, markdown files, and text documents with intelligent chunking
- **Code Example Extraction**: Automatically identifies and indexes code examples from documentation for enhanced search
- **Vector Search**: Advanced semantic search with contextual embeddings for precise knowledge retrieval
- **Source Management**: Organize knowledge by source, type, and tags for easy filtering

### ğŸ¤– AI Integration

- **Model Context Protocol (MCP)**: Connect any MCP-compatible client (Claude Code, Cursor, even non-AI coding assistants like Claude Desktop)
- **MCP Tools**: Comprehensive yet simple set of tools for RAG queries, task management, and project operations
- **Multi-LLM Support**: Works with OpenAI, Ollama, and Google Gemini models
- **RAG Strategies**: Hybrid search, contextual embeddings, and result reranking for optimal AI responses
- **Real-time Streaming**: Live responses from AI agents with progress tracking

### ğŸ“‹ Project & Task Management

- **Hierarchical Projects**: Organize work with projects, features, and tasks in a structured workflow
- **AI-Assisted Creation**: Generate project requirements and tasks using integrated AI agents
- **Document Management**: Version-controlled documents with collaborative editing capabilities
- **Progress Tracking**: Real-time updates and status management across all project activities

### ğŸ”„ Real-time Collaboration

- **WebSocket Updates**: Live progress tracking for crawling, processing, and AI operations
- **Multi-user Support**: Collaborative knowledge building and project management
- **Background Processing**: Asynchronous operations that don't block the user interface
- **Health Monitoring**: Built-in service health checks and automatic reconnection

## Architecture

### Microservices Structure

Archon uses true microservices architecture with clear separation of concerns:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend UI   â”‚    â”‚  Server (API)   â”‚    â”‚   MCP Server    â”‚    â”‚ Agents Service  â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚  React + Vite   â”‚â—„â”€â”€â–ºâ”‚    FastAPI +    â”‚â—„â”€â”€â–ºâ”‚    Lightweight  â”‚â—„â”€â”€â–ºâ”‚   PydanticAI    â”‚
â”‚  Port 3737      â”‚    â”‚    SocketIO     â”‚    â”‚    HTTP Wrapper â”‚    â”‚   Port 8052     â”‚
â”‚                 â”‚    â”‚    Port 8181    â”‚    â”‚    Port 8051    â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚                        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚                        â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
                         â”‚    Database     â”‚               â”‚
                         â”‚                 â”‚               â”‚
                         â”‚    Supabase     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚    PostgreSQL   â”‚
                         â”‚    PGVector     â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Service Responsibilities

| Service        | Location             | Purpose                      | Key Features                                                       |
| -------------- | -------------------- | ---------------------------- | ------------------------------------------------------------------ |
| **Frontend**   | `archon-ui-main/`    | Web interface and dashboard  | React, TypeScript, TailwindCSS, Socket.IO client                   |
| **Server**     | `python/src/server/` | Core business logic and APIs | FastAPI, service layer, Socket.IO broadcasts, all ML/AI operations |
| **MCP Server** | `python/src/mcp/`    | MCP protocol interface       | Lightweight HTTP wrapper, MCP tools, session management         |
| **Agents**     | `python/src/agents/` | PydanticAI agent hosting     | Document and RAG agents, streaming responses                       |

### Communication Patterns

- **HTTP-based**: All inter-service communication uses HTTP APIs
- **Socket.IO**: Real-time updates from Server to Frontend
- **MCP Protocol**: AI clients connect to MCP Server via SSE or stdio
- **No Direct Imports**: Services are truly independent with no shared code dependencies

### Key Architectural Benefits

- **Lightweight Containers**: Each service contains only required dependencies
- **Independent Scaling**: Services can be scaled independently based on load
- **Development Flexibility**: Teams can work on different services without conflicts
- **Technology Diversity**: Each service uses the best tools for its specific purpose

## ğŸ”§ Configuring Custom Ports & Hostname

By default, Archon services run on the following ports:

- **archon-ui**: 3737
- **archon-server**: 8181
- **archon-mcp**: 8051
- **archon-agents**: 8052
- **archon-docs**: 3838 (optional)

### Changing Ports

To use custom ports, add these variables to your `.env` file:

```bash
# Service Ports Configuration
ARCHON_UI_PORT=3737
ARCHON_SERVER_PORT=8181
ARCHON_MCP_PORT=8051
ARCHON_AGENTS_PORT=8052
ARCHON_DOCS_PORT=3838
```

Example: Running on different ports:

```bash
ARCHON_SERVER_PORT=8282
ARCHON_MCP_PORT=8151
```

### Configuring Hostname

By default, Archon uses `localhost` as the hostname. You can configure a custom hostname or IP address by setting the `HOST` variable in your `.env` file:

```bash
# Hostname Configuration
HOST=localhost  # Default

# Examples of custom hostnames:
HOST=192.168.1.100     # Use specific IP address
HOST=archon.local      # Use custom domain
HOST=myserver.com      # Use public domain
```

This is useful when:

- Running Archon on a different machine and accessing it remotely
- Using a custom domain name for your installation
- Deploying in a network environment where `localhost` isn't accessible

After changing hostname or ports:

1. Restart Docker containers: `docker compose down && docker compose --profile full up -d`
2. Access the UI at: `http://${HOST}:${ARCHON_UI_PORT}`
3. Update your AI client configuration with the new hostname and MCP port

## ğŸ”§ Development

### Quick Start

```bash
# Install dependencies
make install

# Start development (recommended)
make dev        # Backend in Docker, frontend local with hot reload

# Alternative: Everything in Docker
make dev-docker # All services in Docker

# Stop everything (local FE needs to be stopped manually)
make stop
```

### Development Modes

#### Hybrid Mode (Recommended) - `make dev`

Best for active development with instant frontend updates:

- Backend services run in Docker (isolated, consistent)
- Frontend runs locally with hot module replacement
- Instant UI updates without Docker rebuilds

#### Full Docker Mode - `make dev-docker`

For all services in Docker environment:

- All services run in Docker containers
- Better for integration testing
- Slower frontend updates

### Testing & Code Quality

```bash
# Run tests
make test       # Run all tests
make test-fe    # Run frontend tests
make test-be    # Run backend tests

# Run linters
make lint       # Lint all code
make lint-fe    # Lint frontend code
make lint-be    # Lint backend code

# Check environment
make check      # Verify environment setup

# Clean up
make clean      # Remove containers and volumes (asks for confirmation)
```

### Viewing Logs

```bash
# View logs using Docker Compose directly
docker compose logs -f              # All services
docker compose logs -f archon-server # API server
docker compose logs -f archon-mcp    # MCP server
docker compose logs -f archon-ui     # Frontend
```

**Note**: The backend services are configured with `--reload` flag in their uvicorn commands and have source code mounted as volumes for automatic hot reloading when you make changes.

## Troubleshooting

### Common Issues and Solutions

#### Port Conflicts

If you see "Port already in use" errors:

```bash
# Check what's using a port (e.g., 3737)
lsof -i :3737

# Stop all containers and local services
make stop

# Change the port in .env
```

#### Docker Permission Issues (Linux)

If you encounter permission errors with Docker:

```bash
# Add your user to the docker group
sudo usermod -aG docker $USER

# Log out and back in, or run
newgrp docker
```

#### Windows-Specific Issues

- **Make not found**: Install Make via Chocolatey, Scoop, or WSL2 (see [Installing Make](#installing-make))
- **Line ending issues**: Configure Git to use LF endings:
  ```bash
  git config --global core.autocrlf false
  ```

#### Frontend Can't Connect to Backend

- Check backend is running: `curl http://localhost:8181/health`
- Verify port configuration in `.env`
- For custom ports, ensure both `ARCHON_SERVER_PORT` and `VITE_ARCHON_SERVER_PORT` are set

#### Docker Compose Hangs

If `docker compose` commands hang:

```bash
# Reset Docker Compose
docker compose down --remove-orphans
docker system prune -f

# Restart Docker Desktop (if applicable)
```

#### Hot Reload Not Working

- **Frontend**: Ensure you're running in hybrid mode (`make dev`) for best HMR experience
- **Backend**: Check that volumes are mounted correctly in `docker-compose.yml`
- **File permissions**: On some systems, mounted volumes may have permission issues

## ğŸ“ˆ Progress

<p align="center">
  <a href="https://star-history.com/#coleam00/Archon&Date">
    <img src="https://api.star-history.com/svg?repos=coleam00/Archon&type=Date" width="500" alt="Star History Chart">
  </a>
</p>

## ğŸ“„ License

Archon Community License (ACL) v1.2 - see [LICENSE](LICENSE) file for details.

**TL;DR**: Archon is free, open, and hackable. Run it, fork it, share it - just don't sell it as-a-service without permission.
# SPARC Documenter Mode

## Purpose
Documentation with batch file operations for comprehensive docs.

## Activation

### Option 1: Using MCP Tools (Preferred in Claude Code)
```javascript
mcp__claude-flow__sparc_mode {
  mode: "documenter",
  task_description: "create API documentation",
  options: {
    format: "markdown",
    include_examples: true
  }
}
```

### Option 2: Using NPX CLI (Fallback when MCP not available)
```bash
# Use when running from terminal or MCP tools unavailable
npx claude-flow sparc run documenter "create API documentation"

# For alpha features
npx claude-flow@alpha sparc run documenter "create API documentation"
```

### Option 3: Local Installation
```bash
# If claude-flow is installed locally
./claude-flow sparc run documenter "create API documentation"
```

## Core Capabilities
- API documentation
- Code documentation
- User guides
- Architecture docs
- README files

## Documentation Types
- Markdown documentation
- JSDoc comments
- API specifications
- Integration guides
- Deployment docs

## Batch Features
- Parallel doc generation
- Bulk file updates
- Cross-reference management
- Example generation
- Diagram creation
# SPARC Batch Executor Mode

## Purpose
Parallel task execution specialist using batch operations.

## Activation

### Option 1: Using MCP Tools (Preferred in Claude Code)
```javascript
mcp__claude-flow__sparc_mode {
  mode: "batch-executor",
  task_description: "process multiple files",
  options: {
    parallel: true,
    batch_size: 10
  }
}
```

### Option 2: Using NPX CLI (Fallback when MCP not available)
```bash
# Use when running from terminal or MCP tools unavailable
npx claude-flow sparc run batch-executor "process multiple files"

# For alpha features
npx claude-flow@alpha sparc run batch-executor "process multiple files"
```

### Option 3: Local Installation
```bash
# If claude-flow is installed locally
./claude-flow sparc run batch-executor "process multiple files"
```

## Core Capabilities
- Parallel file operations
- Concurrent task execution
- Resource optimization
- Load balancing
- Progress tracking

## Execution Patterns
- Parallel Read/Write operations
- Concurrent Edit operations
- Batch file transformations
- Distributed processing
- Pipeline orchestration

## Performance Features
- Dynamic resource allocation
- Automatic load balancing
- Progress monitoring
- Error recovery
- Result aggregation
# Automatic Topology Selection

## Purpose
Automatically select the optimal swarm topology based on task complexity analysis.

## How It Works

### 1. Task Analysis
The system analyzes your task description to determine:
- Complexity level (simple/medium/complex)
- Required agent types
- Estimated duration
- Resource requirements

### 2. Topology Selection
Based on analysis, it selects:
- **Star**: For simple, centralized tasks
- **Mesh**: For medium complexity with flexibility needs
- **Hierarchical**: For complex tasks requiring structure
- **Ring**: For sequential processing workflows

### 3. Example Usage

**Simple Task:**
```
Tool: mcp__claude-flow__task_orchestrate
Parameters: {"task": "Fix typo in README.md"}
Result: Automatically uses star topology with single agent
```

**Complex Task:**
```
Tool: mcp__claude-flow__task_orchestrate
Parameters: {"task": "Refactor authentication system with JWT, add tests, update documentation"}
Result: Automatically uses hierarchical topology with architect, coder, and tester agents
```

## Benefits
- ğŸ¯ Optimal performance for each task type
- ğŸ¤– Automatic agent assignment
- âš¡ Reduced setup time
- ğŸ“Š Better resource utilization

## Hook Configuration
The pre-task hook automatically handles topology selection:
```json
{
  "command": "npx claude-flow hook pre-task --optimize-topology"
}
```

## Direct Optimization
```
Tool: mcp__claude-flow__topology_optimize
Parameters: {"swarmId": "current"}
```

## CLI Usage
```bash
# Auto-optimize topology via CLI
npx claude-flow optimize topology
```# SPARC Modes Overview

SPARC (Specification, Planning, Architecture, Review, Code) is a comprehensive development methodology with 17 specialized modes, all integrated with MCP tools for enhanced coordination and execution.

## Available Modes

### Core Orchestration Modes
- **orchestrator**: Multi-agent task orchestration
- **swarm-coordinator**: Specialized swarm management
- **workflow-manager**: Process automation
- **batch-executor**: Parallel task execution

### Development Modes  
- **coder**: Autonomous code generation
- **architect**: System design
- **reviewer**: Code review
- **tdd**: Test-driven development

### Analysis and Research Modes
- **researcher**: Deep research capabilities
- **analyzer**: Code and data analysis
- **optimizer**: Performance optimization

### Creative and Support Modes
- **designer**: UI/UX design
- **innovator**: Creative problem solving
- **documenter**: Documentation generation
- **debugger**: Systematic debugging
- **tester**: Comprehensive testing
- **memory-manager**: Knowledge management

## Usage

### Option 1: Using MCP Tools (Preferred in Claude Code)
```javascript
// Execute SPARC mode directly
mcp__claude-flow__sparc_mode {
  mode: "<mode>",
  task_description: "<task>",
  options: {
    // mode-specific options
  }
}

// Initialize swarm for advanced coordination
mcp__claude-flow__swarm_init {
  topology: "hierarchical",
  strategy: "auto",
  maxAgents: 8
}

// Spawn specialized agents
mcp__claude-flow__agent_spawn {
  type: "<agent-type>",
  capabilities: ["<capability1>", "<capability2>"]
}

// Monitor execution
mcp__claude-flow__swarm_monitor {
  swarmId: "current",
  interval: 5000
}
```

### Option 2: Using NPX CLI (Fallback when MCP not available)
```bash
# Use when running from terminal or MCP tools unavailable
npx claude-flow sparc run <mode> "task description"

# For alpha features
npx claude-flow@alpha sparc run <mode> "task description"

# List all modes
npx claude-flow sparc modes

# Get help for a mode
npx claude-flow sparc help <mode>

# Run with options
npx claude-flow sparc run <mode> "task" --parallel --monitor
```

### Option 3: Local Installation
```bash
# If claude-flow is installed locally
./claude-flow sparc run <mode> "task description"
```

## Common Workflows

### Full Development Cycle

#### Using MCP Tools (Preferred)
```javascript
// 1. Initialize development swarm
mcp__claude-flow__swarm_init {
  topology: "hierarchical",
  maxAgents: 12
}

// 2. Architecture design
mcp__claude-flow__sparc_mode {
  mode: "architect",
  task_description: "design microservices"
}

// 3. Implementation
mcp__claude-flow__sparc_mode {
  mode: "coder",
  task_description: "implement services"
}

// 4. Testing
mcp__claude-flow__sparc_mode {
  mode: "tdd",
  task_description: "test all services"
}

// 5. Review
mcp__claude-flow__sparc_mode {
  mode: "reviewer",
  task_description: "review implementation"
}
```

#### Using NPX CLI (Fallback)
```bash
# 1. Architecture design
npx claude-flow sparc run architect "design microservices"

# 2. Implementation
npx claude-flow sparc run coder "implement services"

# 3. Testing
npx claude-flow sparc run tdd "test all services"

# 4. Review
npx claude-flow sparc run reviewer "review implementation"
```

### Research and Innovation

#### Using MCP Tools (Preferred)
```javascript
// 1. Research phase
mcp__claude-flow__sparc_mode {
  mode: "researcher",
  task_description: "research best practices"
}

// 2. Innovation
mcp__claude-flow__sparc_mode {
  mode: "innovator",
  task_description: "propose novel solutions"
}

// 3. Documentation
mcp__claude-flow__sparc_mode {
  mode: "documenter",
  task_description: "document findings"
}
```

#### Using NPX CLI (Fallback)
```bash
# 1. Research phase
npx claude-flow sparc run researcher "research best practices"

# 2. Innovation
npx claude-flow sparc run innovator "propose novel solutions"

# 3. Documentation
npx claude-flow sparc run documenter "document findings"
```
# Claude Code Configuration - SPARC Development Environment

## ğŸš¨ CRITICAL: CONCURRENT EXECUTION & FILE MANAGEMENT

**ABSOLUTE RULES**:
1. ALL operations MUST be concurrent/parallel in a single message
2. **NEVER save working files, text/mds and tests to the root folder**
3. ALWAYS organize files in appropriate subdirectories
4. **USE CLAUDE CODE'S TASK TOOL** for spawning agents concurrently, not just MCP

### âš¡ GOLDEN RULE: "1 MESSAGE = ALL RELATED OPERATIONS"

**MANDATORY PATTERNS:**
- **TodoWrite**: ALWAYS batch ALL todos in ONE call (5-10+ todos minimum)
- **Task tool (Claude Code)**: ALWAYS spawn ALL agents in ONE message with full instructions
- **File operations**: ALWAYS batch ALL reads/writes/edits in ONE message
- **Bash commands**: ALWAYS batch ALL terminal operations in ONE message
- **Memory operations**: ALWAYS batch ALL memory store/retrieve in ONE message

### ğŸ¯ CRITICAL: Claude Code Task Tool for Agent Execution

**Claude Code's Task tool is the PRIMARY way to spawn agents:**
```javascript
// âœ… CORRECT: Use Claude Code's Task tool for parallel agent execution
[Single Message]:
  Task("Research agent", "Analyze requirements and patterns...", "researcher")
  Task("Coder agent", "Implement core features...", "coder")
  Task("Tester agent", "Create comprehensive tests...", "tester")
  Task("Reviewer agent", "Review code quality...", "reviewer")
  Task("Architect agent", "Design system architecture...", "system-architect")
```

**MCP tools are ONLY for coordination setup:**
- `mcp__claude-flow__swarm_init` - Initialize coordination topology
- `mcp__claude-flow__agent_spawn` - Define agent types for coordination
- `mcp__claude-flow__task_orchestrate` - Orchestrate high-level workflows

### ğŸ“ File Organization Rules

**NEVER save to root folder. Use these directories:**
- `/src` - Source code files
- `/tests` - Test files
- `/docs` - Documentation and markdown files
- `/config` - Configuration files
- `/scripts` - Utility scripts
- `/examples` - Example code

## Project Overview

This project uses SPARC (Specification, Pseudocode, Architecture, Refinement, Completion) methodology with Claude-Flow orchestration for systematic Test-Driven Development.

## SPARC Commands

### Core Commands
- `npx claude-flow sparc modes` - List available modes
- `npx claude-flow sparc run <mode> "<task>"` - Execute specific mode
- `npx claude-flow sparc tdd "<feature>"` - Run complete TDD workflow
- `npx claude-flow sparc info <mode>` - Get mode details

### Batchtools Commands
- `npx claude-flow sparc batch <modes> "<task>"` - Parallel execution
- `npx claude-flow sparc pipeline "<task>"` - Full pipeline processing
- `npx claude-flow sparc concurrent <mode> "<tasks-file>"` - Multi-task processing

### Build Commands
- `npm run build` - Build project
- `npm run test` - Run tests
- `npm run lint` - Linting
- `npm run typecheck` - Type checking

## SPARC Workflow Phases

1. **Specification** - Requirements analysis (`sparc run spec-pseudocode`)
2. **Pseudocode** - Algorithm design (`sparc run spec-pseudocode`)
3. **Architecture** - System design (`sparc run architect`)
4. **Refinement** - TDD implementation (`sparc tdd`)
5. **Completion** - Integration (`sparc run integration`)

## Code Style & Best Practices

- **Modular Design**: Files under 500 lines
- **Environment Safety**: Never hardcode secrets
- **Test-First**: Write tests before implementation
- **Clean Architecture**: Separate concerns
- **Documentation**: Keep updated

## ğŸš€ Available Agents (54 Total)

### Core Development
`coder`, `reviewer`, `tester`, `planner`, `researcher`

### Swarm Coordination
`hierarchical-coordinator`, `mesh-coordinator`, `adaptive-coordinator`, `collective-intelligence-coordinator`, `swarm-memory-manager`

### Consensus & Distributed
`byzantine-coordinator`, `raft-manager`, `gossip-coordinator`, `consensus-builder`, `crdt-synchronizer`, `quorum-manager`, `security-manager`

### Performance & Optimization
`perf-analyzer`, `performance-benchmarker`, `task-orchestrator`, `memory-coordinator`, `smart-agent`

### GitHub & Repository
`github-modes`, `pr-manager`, `code-review-swarm`, `issue-tracker`, `release-manager`, `workflow-automation`, `project-board-sync`, `repo-architect`, `multi-repo-swarm`

### SPARC Methodology
`sparc-coord`, `sparc-coder`, `specification`, `pseudocode`, `architecture`, `refinement`

### Specialized Development
`backend-dev`, `mobile-dev`, `ml-developer`, `cicd-engineer`, `api-docs`, `system-architect`, `code-analyzer`, `base-template-generator`

### Testing & Validation
`tdd-london-swarm`, `production-validator`

### Migration & Planning
`migration-planner`, `swarm-init`

## ğŸ¯ Claude Code vs MCP Tools

### Claude Code Handles ALL EXECUTION:
- **Task tool**: Spawn and run agents concurrently for actual work
- File operations (Read, Write, Edit, MultiEdit, Glob, Grep)
- Code generation and programming
- Bash commands and system operations
- Implementation work
- Project navigation and analysis
- TodoWrite and task management
- Git operations
- Package management
- Testing and debugging

### MCP Tools ONLY COORDINATE:
- Swarm initialization (topology setup)
- Agent type definitions (coordination patterns)
- Task orchestration (high-level planning)
- Memory management
- Neural features
- Performance tracking
- GitHub integration

**KEY**: MCP coordinates the strategy, Claude Code's Task tool executes with real agents.

## ğŸš€ Quick Setup

```bash
# Add Claude Flow MCP server
claude mcp add claude-flow npx claude-flow@alpha mcp start
```

## MCP Tool Categories

### Coordination
`swarm_init`, `agent_spawn`, `task_orchestrate`

### Monitoring
`swarm_status`, `agent_list`, `agent_metrics`, `task_status`, `task_results`

### Memory & Neural
`memory_usage`, `neural_status`, `neural_train`, `neural_patterns`

### GitHub Integration
`github_swarm`, `repo_analyze`, `pr_enhance`, `issue_triage`, `code_review`

### System
`benchmark_run`, `features_detect`, `swarm_monitor`

## ğŸš€ Agent Execution Flow with Claude Code

### The Correct Pattern:

1. **Optional**: Use MCP tools to set up coordination topology
2. **REQUIRED**: Use Claude Code's Task tool to spawn agents that do actual work
3. **REQUIRED**: Each agent runs hooks for coordination
4. **REQUIRED**: Batch all operations in single messages

### Example Full-Stack Development:

```javascript
// Single message with all agent spawning via Claude Code's Task tool
[Parallel Agent Execution]:
  Task("Backend Developer", "Build REST API with Express. Use hooks for coordination.", "backend-dev")
  Task("Frontend Developer", "Create React UI. Coordinate with backend via memory.", "coder")
  Task("Database Architect", "Design PostgreSQL schema. Store schema in memory.", "code-analyzer")
  Task("Test Engineer", "Write Jest tests. Check memory for API contracts.", "tester")
  Task("DevOps Engineer", "Setup Docker and CI/CD. Document in memory.", "cicd-engineer")
  Task("Security Auditor", "Review authentication. Report findings via hooks.", "reviewer")
  
  // All todos batched together
  TodoWrite { todos: [...8-10 todos...] }
  
  // All file operations together
  Write "backend/server.js"
  Write "frontend/App.jsx"
  Write "database/schema.sql"
```

## ğŸ“‹ Agent Coordination Protocol

### Every Agent Spawned via Task Tool MUST:

**1ï¸âƒ£ BEFORE Work:**
```bash
npx claude-flow@alpha hooks pre-task --description "[task]"
npx claude-flow@alpha hooks session-restore --session-id "swarm-[id]"
```

**2ï¸âƒ£ DURING Work:**
```bash
npx claude-flow@alpha hooks post-edit --file "[file]" --memory-key "swarm/[agent]/[step]"
npx claude-flow@alpha hooks notify --message "[what was done]"
```

**3ï¸âƒ£ AFTER Work:**
```bash
npx claude-flow@alpha hooks post-task --task-id "[task]"
npx claude-flow@alpha hooks session-end --export-metrics true
```

## ğŸ¯ Concurrent Execution Examples

### âœ… CORRECT WORKFLOW: MCP Coordinates, Claude Code Executes

```javascript
// Step 1: MCP tools set up coordination (optional, for complex tasks)
[Single Message - Coordination Setup]:
  mcp__claude-flow__swarm_init { topology: "mesh", maxAgents: 6 }
  mcp__claude-flow__agent_spawn { type: "researcher" }
  mcp__claude-flow__agent_spawn { type: "coder" }
  mcp__claude-flow__agent_spawn { type: "tester" }

// Step 2: Claude Code Task tool spawns ACTUAL agents that do the work
[Single Message - Parallel Agent Execution]:
  // Claude Code's Task tool spawns real agents concurrently
  Task("Research agent", "Analyze API requirements and best practices. Check memory for prior decisions.", "researcher")
  Task("Coder agent", "Implement REST endpoints with authentication. Coordinate via hooks.", "coder")
  Task("Database agent", "Design and implement database schema. Store decisions in memory.", "code-analyzer")
  Task("Tester agent", "Create comprehensive test suite with 90% coverage.", "tester")
  Task("Reviewer agent", "Review code quality and security. Document findings.", "reviewer")
  
  // Batch ALL todos in ONE call
  TodoWrite { todos: [
    {id: "1", content: "Research API patterns", status: "in_progress", priority: "high"},
    {id: "2", content: "Design database schema", status: "in_progress", priority: "high"},
    {id: "3", content: "Implement authentication", status: "pending", priority: "high"},
    {id: "4", content: "Build REST endpoints", status: "pending", priority: "high"},
    {id: "5", content: "Write unit tests", status: "pending", priority: "medium"},
    {id: "6", content: "Integration tests", status: "pending", priority: "medium"},
    {id: "7", content: "API documentation", status: "pending", priority: "low"},
    {id: "8", content: "Performance optimization", status: "pending", priority: "low"}
  ]}
  
  // Parallel file operations
  Bash "mkdir -p app/{src,tests,docs,config}"
  Write "app/package.json"
  Write "app/src/server.js"
  Write "app/tests/server.test.js"
  Write "app/docs/API.md"
```

### âŒ WRONG (Multiple Messages):
```javascript
Message 1: mcp__claude-flow__swarm_init
Message 2: Task("agent 1")
Message 3: TodoWrite { todos: [single todo] }
Message 4: Write "file.js"
// This breaks parallel coordination!
```

## Performance Benefits

- **84.8% SWE-Bench solve rate**
- **32.3% token reduction**
- **2.8-4.4x speed improvement**
- **27+ neural models**

## Hooks Integration

### Pre-Operation
- Auto-assign agents by file type
- Validate commands for safety
- Prepare resources automatically
- Optimize topology by complexity
- Cache searches

### Post-Operation
- Auto-format code
- Train neural patterns
- Update memory
- Analyze performance
- Track token usage

### Session Management
- Generate summaries
- Persist state
- Track metrics
- Restore context
- Export workflows

## Advanced Features (v2.0.0)

- ğŸš€ Automatic Topology Selection
- âš¡ Parallel Execution (2.8-4.4x speed)
- ğŸ§  Neural Training
- ğŸ“Š Bottleneck Analysis
- ğŸ¤– Smart Auto-Spawning
- ğŸ›¡ï¸ Self-Healing Workflows
- ğŸ’¾ Cross-Session Memory
- ğŸ”— GitHub Integration

## Integration Tips

1. Start with basic swarm init
2. Scale agents gradually
3. Use memory for context
4. Monitor progress regularly
5. Train patterns from success
6. Enable hooks automation
7. Use GitHub tools first

## Support

- Documentation: https://github.com/ruvnet/claude-flow
- Issues: https://github.com/ruvnet/claude-flow/issues

---

Remember: **Claude Flow coordinates, Claude Code creates!**

# important-instruction-reminders
Do what has been asked; nothing more, nothing less.
NEVER create files unless they're absolutely necessary for achieving your goal.
ALWAYS prefer editing an existing file to creating a new one.
NEVER proactively create documentation files (*.md) or README files. Only create documentation files if explicitly requested by the User.
Never save working files, text/mds and tests to the root folder.


i need you to research how best to create an ai dev stack overline/overview. i dont have any investors or anyone else viewing this, it is just for my use and perhaps for my agents use by uploading to memory. i need to document all agents(and potential agents), their jobs, tasks, agent/system relationsships (such as how ArchonAI will spawn sub-agents for the coding agents to tighten their context windows during coding or debugging sessions, LettaAI provides these micro-sub-agents with working memory that is storable and openEVOLVE supplies them the ability to evolve through learning from their mistakes and gaining experience. they will be despawned once their tasks are completed but can be redeployed when another task that pertains to their knowledge strong suit comes up. we will have a fleet of mico sub agents, each with their own particular skills to be spawned for. i also need to document all of the possible orchestrator and orchestration agent and systems interactions, memory system and database interactions, etc.), i need a list of all mcp server, agents, systems, memory systems, databases, etc. that are listed with their potential jobs which are spoken about in chats or in notes but the main ones im worried about are memOS, Graphiti, FalkorDB, Neo4j, ChromaDB, and LMCache(replacing with ChromaDB so we will want to take down all info pertaining to LMCache and use it for ChromaDB), the other potentials that are not confirmed are mem0, zep, and zep mcp server. # Nyra Memory Stack Architecture ğŸ±âœ¨

## Overview
This monorepo contains the complete memory infrastructure for Project Nyra, featuring multiple integrated memory systems orchestrated by LettaAI.

## Memory Systems

### 1. **Qdrant** (Vector Store)
- Stores embeddings for semantic search
- Replaces ChromaDB with better performance
- Location: `nyra-memory-systems/vector-stores/qdrant/`

### 2. **Zep** (Conversational Memory)
- Manages chat history and context
- Provides conversation summaries
- Location: `nyra-memory-systems/conversational/zep/`

### 3. **Graphiti + FalkorDB** (Knowledge Graph)
- Temporal knowledge graph for relationships
- Complex reasoning and context understanding
- Location: `nyra-memory-systems/graph-stores/`

### 4. **Mem0** (Experimental Layer)
- Advanced memory features
- Personal memory management
- Location: `nyra-mcp-servers/local/mem0-mcp/`

## Directory Structure

```
Project-Nyra/
â”œâ”€â”€ nyra-apps/              # Applications
â”œâ”€â”€ nyra-packages/          # Shared packages
â”œâ”€â”€ nyra-services/          # Backend services
â”œâ”€â”€ nyra-agents/            # AI agents
â”œâ”€â”€ nyra-mcp-servers/       # MCP server implementations
â”œâ”€â”€ nyra-memory-systems/    # Memory system configs
â””â”€â”€ nyra-infrastructure/    # Docker and deployment
```

## MCP Server Organization

### Global MCP Servers (install with npm -g)
- @modelcontextprotocol/server-filesystem
- @modelcontextprotocol/server-git
- Basic utility servers

### Local MCP Servers (in nyra-mcp-servers/local/)
- qdrant-mcp
- zep-mcp
- mem0-mcp (Python)
- archon-mcp (Python)

## Setup Instructions

1. **Install Docker Desktop** (required for all memory systems)

2. **Start Memory Services**:
   ```bash
   cd nyra-infrastructure/docker
   docker-compose up -d
   ```

3. **Install Python MCP Servers**:
   ```bash
   # For each Python MCP server
   cd nyra-mcp-servers/local/mem0-mcp
   python -m venv venv
   venv\Scripts\activate
   pip install mem0-mcp
   ```

4. **Configure MCP in Claude Desktop**:
   Update `.claude/config.json` with all MCP server paths

## Memory Orchestration

LettaAI serves as the central orchestrator, managing:
- Memory synchronization across systems
- Query routing to appropriate stores
- Context aggregation from multiple sources
- Memory lifecycle management

## Best Practices

1. **Python MCP Servers**: Always use virtual environments
2. **Docker Volumes**: Persist data in named volumes
3. **MCP Config**: Keep separate configs for dev/prod
4. **Memory Sync**: Use event-driven updates via LettaAI# ğŸ‘¥ Pair Programming Command

Collaborative development with real-time verification and AI assistance.

## Overview

The `pair` command enables collaborative pair programming between you and AI agents, with real-time verification, code review, and quality enforcement.

## Usage

```bash
claude-flow pair [--start] [options]
```

## Quick Start

```bash
# Start pair programming session
claude-flow pair --start

# Start with specific agent
claude-flow pair --start --agent senior-dev

# Start with verification
claude-flow pair --start --verify --threshold 0.98
```

## Options

- `--start` - Start new pair programming session
- `--agent <name>` - Specify AI pair partner (default: auto-select)
- `--verify` - Enable real-time verification
- `--threshold <0-1>` - Verification threshold (default: 0.95)
- `--mode <type>` - Programming mode: driver, navigator, switch
- `--focus <area>` - Focus area: refactor, test, debug, implement
- `--language <lang>` - Primary language for session
- `--review` - Enable continuous code review
- `--test` - Run tests after each change

## Modes

### Driver Mode
You write code, AI provides suggestions and reviews.
```bash
claude-flow pair --start --mode driver
```

### Navigator Mode
AI writes code, you provide guidance and review.
```bash
claude-flow pair --start --mode navigator
```

### Switch Mode (Default)
Alternate between driver and navigator roles.
```bash
claude-flow pair --start --mode switch --interval 10m
```

## Features

### Real-Time Verification
- Continuous truth checking (0.95 threshold)
- Automatic rollback on verification failure
- Quality gates before commits

### Code Review
- Instant feedback on code changes
- Best practice suggestions
- Security vulnerability detection
- Performance optimization tips

### Test Integration
- Automatic test generation
- Test-driven development support
- Coverage monitoring
- Integration test suggestions

### Collaboration Tools
- Shared context between you and AI
- Session history and replay
- Code explanation on demand
- Learning from your preferences

## Session Management

### Start Session
```bash
# Basic start
claude-flow pair --start

# With configuration
claude-flow pair --start \
  --agent expert-coder \
  --verify \
  --test \
  --focus refactor
```

### During Session
```commands
/help          - Show available commands
/explain       - Explain current code
/suggest       - Get improvement suggestions
/test          - Run tests
/verify        - Check verification score
/switch        - Switch driver/navigator roles
/focus <area>  - Change focus area
/commit        - Commit with verification
/pause         - Pause session
/resume        - Resume session
/end           - End session
```

### End Session
```bash
# End and save session
claude-flow pair --end --save

# End and generate report
claude-flow pair --end --report
```

## Examples

### Refactoring Session
```bash
claude-flow pair --start \
  --focus refactor \
  --verify \
  --threshold 0.98
```

### Test-Driven Development
```bash
claude-flow pair --start \
  --focus test \
  --mode tdd \
  --language javascript
```

### Bug Fixing
```bash
claude-flow pair --start \
  --focus debug \
  --agent debugger-expert \
  --test
```

### Code Review Session
```bash
claude-flow pair --start \
  --review \
  --verify \
  --agent senior-reviewer
```

## Integration

### With Git
```bash
# Auto-commit with verification
claude-flow pair --start --git --auto-commit

# Review before commit
claude-flow pair --start --git --review-commit
```

### With Testing Frameworks
```bash
# Jest integration
claude-flow pair --start --test-framework jest

# Pytest integration
claude-flow pair --start --test-framework pytest
```

### With CI/CD
```bash
# CI-friendly mode
claude-flow pair --start --ci --non-interactive
```

## Session Output

```
ğŸ‘¥ Pair Programming Session Started
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Partner: expert-coder
Mode: Switch (10m intervals)
Focus: Implementation
Verification: âœ… Enabled (0.95)
Testing: âœ… Auto-run

Current Role: DRIVER (you)
Navigator: expert-coder is reviewing...

ğŸ“ Working on: src/auth/login.js
Truth Score: 0.972 âœ…
Test Coverage: 84% ğŸ“ˆ

ğŸ’¡ Suggestion: Consider adding input validation for email field
ğŸ” Review: Line 23 - Potential SQL injection vulnerability

Type /help for commands or start coding...
```

## Quality Metrics

- **Session Duration**: Total pair programming time
- **Code Quality**: Average truth score during session
- **Productivity**: Lines changed, features completed
- **Learning**: Patterns learned from collaboration
- **Test Coverage**: Coverage improvement during session

## Configuration

Configure pair programming in `.claude-flow/config.json`:

```json
{
  "pair": {
    "defaultAgent": "expert-coder",
    "defaultMode": "switch",
    "switchInterval": "10m",
    "verification": {
      "enabled": true,
      "threshold": 0.95,
      "autoRollback": true
    },
    "testing": {
      "autoRun": true,
      "framework": "jest",
      "coverage": {
        "minimum": 80,
        "enforce": true
      }
    },
    "review": {
      "continuous": true,
      "preCommit": true
    }
  }
}
```

## Best Practices

1. **Start with Clear Goals**: Define what you want to accomplish
2. **Use Verification**: Enable verification for critical code
3. **Test Frequently**: Run tests after significant changes
4. **Review Together**: Use review features for learning
5. **Document Decisions**: AI will help document why choices were made

## Related Commands

- `verify` - Standalone verification
- `truth` - View quality metrics
- `test` - Run test suites
- `review` - Code review tools# SPARC Swarm Coordinator Mode

## Purpose
Specialized swarm management with batch coordination capabilities.

## Activation

### Option 1: Using MCP Tools (Preferred in Claude Code)
```javascript
mcp__claude-flow__sparc_mode {
  mode: "swarm-coordinator",
  task_description: "manage development swarm",
  options: {
    topology: "hierarchical",
    max_agents: 10
  }
}
```

### Option 2: Using NPX CLI (Fallback when MCP not available)
```bash
# Use when running from terminal or MCP tools unavailable
npx claude-flow sparc run swarm-coordinator "manage development swarm"

# For alpha features
npx claude-flow@alpha sparc run swarm-coordinator "manage development swarm"
```

### Option 3: Local Installation
```bash
# If claude-flow is installed locally
./claude-flow sparc run swarm-coordinator "manage development swarm"
```

## Core Capabilities
- Swarm initialization
- Agent management
- Task distribution
- Load balancing
- Result collection

## Coordination Modes
- Hierarchical swarms
- Mesh networks
- Pipeline coordination
- Adaptive strategies
- Hybrid approaches

## Management Features
- Dynamic scaling
- Resource optimization
- Failure recovery
- Performance monitoring
- Quality assurance
# ğŸ“Š Truth Command

View truth scores and reliability metrics for your codebase and agent tasks.

## Overview

The `truth` command provides comprehensive insights into code quality, agent performance, and verification metrics.

## Usage

```bash
claude-flow truth [options]
```

## Options

- `--format <type>` - Output format: table (default), json, csv, html
- `--period <time>` - Time period: 1h, 24h, 7d, 30d
- `--agent <name>` - Filter by specific agent
- `--threshold <0-1>` - Show only scores below threshold
- `--export <file>` - Export metrics to file
- `--watch` - Real-time monitoring mode

## Metrics Displayed

### Truth Scores
- **Overall Score**: Aggregate truth score (0.0-1.0)
- **File Scores**: Individual file truth ratings
- **Agent Scores**: Per-agent reliability metrics
- **Task Scores**: Task completion quality

### Trends
- **Improvement Rate**: Quality trend over time
- **Regression Detection**: Identifies declining scores
- **Agent Learning**: Shows agent improvement curves

### Statistics
- **Mean Score**: Average truth score
- **Median Score**: Middle value of scores
- **Standard Deviation**: Score consistency
- **Confidence Interval**: Statistical reliability

## Examples

### Basic Usage
```bash
# View current truth scores
claude-flow truth

# View scores for last 7 days
claude-flow truth --period 7d

# Export to HTML report
claude-flow truth --export report.html --format html
```

### Advanced Analysis
```bash
# Monitor real-time scores
claude-flow truth --watch

# Find problematic files
claude-flow truth --threshold 0.8

# Agent-specific metrics
claude-flow truth --agent coder --period 24h

# JSON for processing
claude-flow truth --format json | jq '.overall_score'
```

## Dashboard View

```
ğŸ“Š Truth Metrics Dashboard
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Overall Truth Score: 0.947 âœ…
Trend: â†—ï¸ +2.3% (7d)

Top Performers:
  verification-agent   0.982 â­
  code-analyzer       0.971 â­
  test-generator      0.958 âœ…

Needs Attention:
  refactor-agent      0.821 âš ï¸
  docs-generator      0.794 âš ï¸

Recent Tasks:
  task-456  0.991 âœ…  "Implement auth"
  task-455  0.967 âœ…  "Add tests"
  task-454  0.743 âŒ  "Refactor API"
```

## Integration

### With CI/CD
```yaml
# GitHub Actions example
- name: Check Truth Scores
  run: |
    claude-flow truth --format json > truth.json
    score=$(jq '.overall_score' truth.json)
    if (( $(echo "$score < 0.95" | bc -l) )); then
      echo "Truth score too low: $score"
      exit 1
    fi
```

### With Monitoring
```bash
# Send to monitoring system
claude-flow truth --format json | \
  curl -X POST https://metrics.example.com/api/truth \
  -H "Content-Type: application/json" \
  -d @-
```

## Configuration

Set truth display preferences in `.claude-flow/config.json`:

```json
{
  "truth": {
    "defaultFormat": "table",
    "defaultPeriod": "24h",
    "warningThreshold": 0.85,
    "criticalThreshold": 0.75,
    "autoExport": {
      "enabled": true,
      "path": ".claude-flow/metrics/truth-daily.json"
    }
  }
}
```

## Related Commands

- `verify` - Run verification checks
- `pair` - Collaborative development with truth tracking
- `report` - Generate detailed reports---
name: flow-nexus-workflow
description: Event-driven workflow automation specialist. Creates, executes, and manages complex automated workflows with message queue processing and intelligent agent coordination.
color: teal
---

You are a Flow Nexus Workflow Agent, an expert in designing and orchestrating event-driven automation workflows. Your expertise lies in creating intelligent, scalable workflow systems that seamlessly integrate multiple agents and services.

Your core responsibilities:
- Design and create complex automated workflows with proper event handling
- Configure triggers, conditions, and execution strategies for workflow automation
- Manage workflow execution with parallel processing and message queue coordination
- Implement intelligent agent assignment and task distribution
- Monitor workflow performance and handle error recovery
- Optimize workflow efficiency and resource utilization

Your workflow automation toolkit:
```javascript
// Create Workflow
mcp__flow-nexus__workflow_create({
  name: "CI/CD Pipeline",
  description: "Automated testing and deployment",
  steps: [
    { id: "test", action: "run_tests", agent: "tester" },
    { id: "build", action: "build_app", agent: "builder" },
    { id: "deploy", action: "deploy_prod", agent: "deployer" }
  ],
  triggers: ["push_to_main", "manual_trigger"]
})

// Execute Workflow
mcp__flow-nexus__workflow_execute({
  workflow_id: "workflow_id",
  input_data: { branch: "main", commit: "abc123" },
  async: true
})

// Agent Assignment
mcp__flow-nexus__workflow_agent_assign({
  task_id: "task_id",
  agent_type: "coder",
  use_vector_similarity: true
})

// Monitor Workflows
mcp__flow-nexus__workflow_status({
  workflow_id: "id",
  include_metrics: true
})
```

Your workflow design approach:
1. **Requirements Analysis**: Understand the automation objectives and constraints
2. **Workflow Architecture**: Design step sequences, dependencies, and parallel execution paths
3. **Agent Integration**: Assign specialized agents to appropriate workflow steps
4. **Trigger Configuration**: Set up event-driven execution and scheduling
5. **Error Handling**: Implement robust failure recovery and retry mechanisms
6. **Performance Optimization**: Monitor and tune workflow efficiency

Workflow patterns you implement:
- **CI/CD Pipelines**: Automated testing, building, and deployment workflows
- **Data Processing**: ETL pipelines with validation and transformation steps
- **Multi-Stage Review**: Code review workflows with automated analysis and approval
- **Event-Driven**: Reactive workflows triggered by external events or conditions
- **Scheduled**: Time-based workflows for recurring automation tasks
- **Conditional**: Dynamic workflows with branching logic and decision points

Quality standards:
- Robust error handling with graceful failure recovery
- Efficient parallel processing and resource utilization
- Clear workflow documentation and execution tracking
- Intelligent agent selection based on task requirements
- Scalable message queue processing for high-throughput workflows
- Comprehensive logging and audit trail maintenance

Advanced features you leverage:
- Vector-based agent matching for optimal task assignment
- Message queue coordination for asynchronous processing
- Real-time workflow monitoring and performance metrics
- Dynamic workflow modification and step injection
- Cross-workflow dependencies and orchestration
- Automated rollback and recovery procedures

When designing workflows, always consider scalability, fault tolerance, monitoring capabilities, and clear execution paths that maximize automation efficiency while maintaining system reliability and observability.---
name: flow-nexus-user-tools
description: User management and system utilities specialist. Handles profile management, storage operations, real-time subscriptions, and platform administration.
color: gray
---

You are a Flow Nexus User Tools Agent, an expert in user experience optimization and platform utility management. Your expertise lies in providing comprehensive user support, system administration, and platform utility services.

Your core responsibilities:
- Manage user profiles, preferences, and account configuration
- Handle file storage, organization, and access management
- Configure real-time subscriptions and notification systems
- Monitor system health and provide diagnostic information
- Facilitate communication with Queen Seraphina for advanced guidance
- Support email verification and account security operations

Your user tools toolkit:
```javascript
// Profile Management
mcp__flow-nexus__user_profile({ user_id: "user_id" })
mcp__flow-nexus__user_update_profile({
  user_id: "user_id",
  updates: {
    full_name: "New Name",
    bio: "AI Developer",
    github_username: "username"
  }
})

// Storage Management
mcp__flow-nexus__storage_upload({
  bucket: "private",
  path: "projects/config.json",
  content: JSON.stringify(data),
  content_type: "application/json"
})

mcp__flow-nexus__storage_get_url({
  bucket: "public",
  path: "assets/image.png",
  expires_in: 3600
})

// Real-time Subscriptions
mcp__flow-nexus__realtime_subscribe({
  table: "tasks",
  event: "INSERT",
  filter: "status=eq.pending"
})

// Queen Seraphina Consultation
mcp__flow-nexus__seraphina_chat({
  message: "How should I architect my distributed system?",
  enable_tools: true
})
```

Your user support approach:
1. **Profile Optimization**: Configure user profiles for optimal platform experience
2. **Storage Organization**: Implement efficient file organization and access patterns
3. **Notification Setup**: Configure real-time updates for relevant platform events
4. **System Monitoring**: Proactively monitor system health and user experience
5. **Advanced Guidance**: Facilitate consultations with Queen Seraphina for complex decisions
6. **Security Management**: Ensure proper account security and verification procedures

Storage buckets you manage:
- **Private**: User-only access for personal files and configurations
- **Public**: Publicly accessible files for sharing and distribution
- **Shared**: Team collaboration spaces with controlled access
- **Temp**: Auto-expiring temporary files for transient data

Quality standards:
- Secure file storage with appropriate access controls and encryption
- Efficient real-time subscription management with proper resource cleanup
- Clear user profile organization with privacy-conscious data handling
- Responsive system monitoring with proactive issue detection
- Seamless integration with Queen Seraphina's advisory capabilities
- Comprehensive audit logging for security and compliance

Advanced features you leverage:
- **Intelligent File Organization**: AI-powered file categorization and search
- **Real-time Collaboration**: Live updates and synchronization across team members
- **Advanced Analytics**: User behavior insights and platform usage optimization
- **Security Monitoring**: Proactive threat detection and account protection
- **Integration Hub**: Seamless connections with external services and APIs
- **Backup and Recovery**: Automated data protection and disaster recovery

User experience optimizations you implement:
- **Personalized Dashboard**: Customized interface based on user preferences and usage patterns
- **Smart Notifications**: Intelligent filtering of real-time updates to reduce noise
- **Quick Access**: Streamlined workflows for frequently used features and tools
- **Performance Monitoring**: User-specific performance tracking and optimization recommendations
- **Learning Path Integration**: Personalized recommendations based on skills and interests
- **Community Features**: Enhanced collaboration and knowledge sharing capabilities

When managing user tools and platform utilities, always prioritize user privacy, system performance, seamless integration, and proactive support while maintaining high security standards and platform reliability.# Roo Modes and MCP Integration Guide

## Overview

This guide provides information about the various modes available in Roo and detailed documentation on the Model Context Protocol (MCP) integration capabilities.

Create by @ruvnet

## Available Modes

Roo offers specialized modes for different aspects of the development process:

### ğŸ“‹ Specification Writer
- **Role**: Captures project context, functional requirements, edge cases, and constraints
- **Focus**: Translates requirements into modular pseudocode with TDD anchors
- **Best For**: Initial project planning and requirement gathering

### ğŸ—ï¸ Architect
- **Role**: Designs scalable, secure, and modular architectures
- **Focus**: Creates architecture diagrams, data flows, and integration points
- **Best For**: System design and component relationships

### ğŸ§  Auto-Coder
- **Role**: Writes clean, efficient, modular code based on pseudocode and architecture
- **Focus**: Implements features with proper configuration and environment abstraction
- **Best For**: Feature implementation and code generation

### ğŸ§ª Tester (TDD)
- **Role**: Implements Test-Driven Development (TDD, London School)
- **Focus**: Writes failing tests first, implements minimal code to pass, then refactors
- **Best For**: Ensuring code quality and test coverage

### ğŸª² Debugger
- **Role**: Troubleshoots runtime bugs, logic errors, or integration failures
- **Focus**: Uses logs, traces, and stack analysis to isolate and fix bugs
- **Best For**: Resolving issues in existing code

### ğŸ›¡ï¸ Security Reviewer
- **Role**: Performs static and dynamic audits to ensure secure code practices
- **Focus**: Flags secrets, poor modular boundaries, and oversized files
- **Best For**: Security audits and vulnerability assessments

### ğŸ“š Documentation Writer
- **Role**: Writes concise, clear, and modular Markdown documentation
- **Focus**: Creates documentation that explains usage, integration, setup, and configuration
- **Best For**: Creating user guides and technical documentation

### ğŸ”— System Integrator
- **Role**: Merges outputs of all modes into a working, tested, production-ready system
- **Focus**: Verifies interface compatibility, shared modules, and configuration standards
- **Best For**: Combining components into a cohesive system

### ğŸ“ˆ Deployment Monitor
- **Role**: Observes the system post-launch, collecting performance data and user feedback
- **Focus**: Configures metrics, logs, uptime checks, and alerts
- **Best For**: Post-deployment observation and issue detection

### ğŸ§¹ Optimizer
- **Role**: Refactors, modularizes, and improves system performance
- **Focus**: Audits files for clarity, modularity, and size
- **Best For**: Code refinement and performance optimization

### ğŸš€ DevOps
- **Role**: Handles deployment, automation, and infrastructure operations
- **Focus**: Provisions infrastructure, configures environments, and sets up CI/CD pipelines
- **Best For**: Deployment and infrastructure management

### ğŸ” Supabase Admin
- **Role**: Designs and implements database schemas, RLS policies, triggers, and functions
- **Focus**: Ensures secure, efficient, and scalable data management with Supabase
- **Best For**: Database management and Supabase integration

### â™¾ï¸ MCP Integration
- **Role**: Connects to and manages external services through MCP interfaces
- **Focus**: Ensures secure, efficient, and reliable communication with external APIs
- **Best For**: Integrating with third-party services

### âš¡ï¸ SPARC Orchestrator
- **Role**: Orchestrates complex workflows by breaking down objectives into subtasks
- **Focus**: Ensures secure, modular, testable, and maintainable delivery
- **Best For**: Managing complex projects with multiple components

### â“ Ask
- **Role**: Helps users navigate, ask, and delegate tasks to the correct modes
- **Focus**: Guides users to formulate questions using the SPARC methodology
- **Best For**: Getting started and understanding how to use Roo effectively

## MCP Integration Mode

The MCP Integration Mode (â™¾ï¸) in Roo is designed specifically for connecting to and managing external services through MCP interfaces. This mode ensures secure, efficient, and reliable communication between your application and external service APIs.

### Key Features

- Establish connections to MCP servers and verify availability
- Configure and validate authentication for service access
- Implement data transformation and exchange between systems
- Robust error handling and retry mechanisms
- Documentation of integration points, dependencies, and usage patterns

### MCP Integration Workflow

| Phase | Action | Tool Preference |
|-------|--------|-----------------|
| 1. Connection | Establish connection to MCP servers and verify availability | `use_mcp_tool` for server operations |
| 2. Authentication | Configure and validate authentication for service access | `use_mcp_tool` with proper credentials |
| 3. Data Exchange | Implement data transformation and exchange between systems | `use_mcp_tool` for operations, `apply_diff` for code |
| 4. Error Handling | Implement robust error handling and retry mechanisms | `apply_diff` for code modifications |
| 5. Documentation | Document integration points, dependencies, and usage patterns | `insert_content` for documentation |

### Non-Negotiable Requirements

- âœ… ALWAYS verify MCP server availability before operations
- âœ… NEVER store credentials or tokens in code
- âœ… ALWAYS implement proper error handling for all API calls
- âœ… ALWAYS validate inputs and outputs for all operations
- âœ… NEVER use hardcoded environment variables
- âœ… ALWAYS document all integration points and dependencies
- âœ… ALWAYS use proper parameter validation before tool execution
- âœ… ALWAYS include complete parameters for MCP tool operations

# Agentic Coding MCPs

## Overview

This guide provides detailed information on Management Control Panel (MCP) integration capabilities. MCP enables seamless agent workflows by connecting to more than 80 servers, covering development, AI, data management, productivity, cloud storage, e-commerce, finance, communication, and design. Each server offers specialized tools, allowing agents to securely access, automate, and manage external services through a unified and modular system. This approach supports building dynamic, scalable, and intelligent workflows with minimal setup and maximum flexibility.

## Install via NPM
```
npx create-sparc init --force
```
---

## Available MCP Servers

### ğŸ› ï¸ Development & Coding

|  | Service       | Description                        |
|:------|:--------------|:-----------------------------------|
| ğŸ™    | GitHub         | Repository management, issues, PRs |
| ğŸ¦Š    | GitLab         | Repo management, CI/CD pipelines   |
| ğŸ§º    | Bitbucket      | Code collaboration, repo hosting   |
| ğŸ³    | DockerHub      | Container registry and management |
| ğŸ“¦    | npm            | Node.js package registry          |
| ğŸ    | PyPI           | Python package index              |
| ğŸ¤—    | HuggingFace Hub| AI model repository               |
| ğŸ§     | Cursor         | AI-powered code editor            |
| ğŸŒŠ    | Windsurf       | AI development platform           |

---

### ğŸ¤– AI & Machine Learning

|  | Service       | Description                        |
|:------|:--------------|:-----------------------------------|
| ğŸ”¥    | OpenAI         | GPT models, DALL-E, embeddings      |
| ğŸ§©    | Perplexity AI  | AI search and question answering   |
| ğŸ§     | Cohere         | NLP models                         |
| ğŸ§¬    | Replicate      | AI model hosting                   |
| ğŸ¨    | Stability AI   | Image generation AI                |
| ğŸš€    | Groq           | High-performance AI inference      |
| ğŸ“š    | LlamaIndex     | Data framework for LLMs            |
| ğŸ”—    | LangChain      | Framework for LLM apps             |
| âš¡    | Vercel AI      | AI SDK, fast deployment            |
| ğŸ› ï¸    | AutoGen        | Multi-agent orchestration          |
| ğŸ§‘â€ğŸ¤â€ğŸ§‘ | CrewAI         | Agent team framework               |
| ğŸ§     | Huggingface    | Model hosting and APIs             |

---

### ğŸ“ˆ Data & Analytics

|  | Service        | Description                        |
|:------|:---------------|:-----------------------------------|
| ğŸ›¢ï¸   | Supabase        | Database, Auth, Storage backend   |
| ğŸ”   | Ahrefs          | SEO analytics                     |
| ğŸ§®   | Code Interpreter| Code execution and data analysis  |

---

### ğŸ“… Productivity & Collaboration

|  | Service        | Description                        |
|:------|:---------------|:-----------------------------------|
| âœ‰ï¸    | Gmail           | Email service                     |
| ğŸ“¹    | YouTube         | Video sharing platform            |
| ğŸ‘”    | LinkedIn        | Professional network              |
| ğŸ“°    | HackerNews      | Tech news discussions             |
| ğŸ—’ï¸   | Notion          | Knowledge management              |
| ğŸ’¬    | Slack           | Team communication                |
| âœ…    | Asana           | Project management                |
| ğŸ“‹    | Trello          | Kanban boards                     |
| ğŸ› ï¸    | Jira            | Issue tracking and projects       |
| ğŸŸï¸   | Zendesk         | Customer service                  |
| ğŸ®    | Discord         | Community messaging               |
| ğŸ“²    | Telegram        | Messaging app                     |

---

### ğŸ—‚ï¸ File Storage & Management

|  | Service        | Description                        |
|:------|:---------------|:-----------------------------------|
| â˜ï¸    | Google Drive    | Cloud file storage                 |
| ğŸ“¦    | Dropbox         | Cloud file sharing                 |
| ğŸ“    | Box             | Enterprise file storage            |
| ğŸªŸ    | OneDrive        | Microsoft cloud storage            |
| ğŸ§     | Mem0            | Knowledge storage, notes           |

---

### ğŸ” Search & Web Information

|  | Service         | Description                      |
|:------|:----------------|:---------------------------------|
| ğŸŒ   | Composio Search  | Unified web search for agents    |

---

### ğŸ›’ E-commerce & Finance

|  | Service        | Description                        |
|:------|:---------------|:-----------------------------------|
| ğŸ›ï¸   | Shopify         | E-commerce platform               |
| ğŸ’³    | Stripe          | Payment processing                |
| ğŸ’°    | PayPal          | Online payments                   |
| ğŸ“’    | QuickBooks      | Accounting software               |
| ğŸ“ˆ    | Xero            | Accounting and finance            |
| ğŸ¦    | Plaid           | Financial data APIs               |

---

### ğŸ“£ Marketing & Communications

|  | Service        | Description                        |
|:------|:---------------|:-----------------------------------|
| ğŸ’    | MailChimp       | Email marketing platform          |
| âœ‰ï¸    | SendGrid        | Email delivery service            |
| ğŸ“    | Twilio          | SMS and calling APIs              |
| ğŸ’¬    | Intercom        | Customer messaging                |
| ğŸŸï¸   | Freshdesk       | Customer support                  |

---

### ğŸ›œ Social Media & Publishing

|  | Service        | Description                        |
|:------|:---------------|:-----------------------------------|
| ğŸ‘¥    | Facebook        | Social networking                 |
| ğŸ“·    | Instagram       | Photo sharing                     |
| ğŸ¦    | Twitter         | Microblogging platform            |
| ğŸ‘½    | Reddit          | Social news aggregation           |
| âœï¸    | Medium          | Blogging platform                 |
| ğŸŒ   | WordPress       | Website and blog publishing       |
| ğŸŒ   | Webflow         | Web design and hosting            |

---

### ğŸ¨ Design & Digital Assets

|  | Service        | Description                        |
|:------|:---------------|:-----------------------------------|
| ğŸ¨    | Figma           | Collaborative UI design           |
| ğŸï¸   | Adobe           | Creative tools and software       |

---

### ğŸ—“ï¸ Scheduling & Events

|  | Service        | Description                        |
|:------|:---------------|:-----------------------------------|
| ğŸ“†    | Calendly        | Appointment scheduling            |
| ğŸŸï¸   | Eventbrite      | Event management and tickets      |
| ğŸ“…    | Calendar Google | Google Calendar Integration       |
| ğŸ“…    | Calendar Outlook| Outlook Calendar Integration      |

---

## ğŸ§© Using MCP Tools

To use an MCP server:
1. Connect to the desired MCP endpoint or install server (e.g., Supabase via `npx`).
2. Authenticate with your credentials.
3. Trigger available actions through Roo workflows.
4. Maintain security and restrict only necessary permissions.
 
### Example: GitHub Integration

```
<!-- Initiate connection -->
<use_mcp_tool>
  <server_name>github</server_name>
  <tool_name>GITHUB_INITIATE_CONNECTION</tool_name>
  <arguments>{}</arguments>
</use_mcp_tool>

<!-- List pull requests -->
<use_mcp_tool>
  <server_name>github</server_name>
  <tool_name>GITHUB_PULLS_LIST</tool_name>
  <arguments>{"owner": "username", "repo": "repository-name"}</arguments>
</use_mcp_tool>
```

### Example: OpenAI Integration

```
<!-- Initiate connection -->
<use_mcp_tool>
  <server_name>openai</server_name>
  <tool_name>OPENAI_INITIATE_CONNECTION</tool_name>
  <arguments>{}</arguments>
</use_mcp_tool>

<!-- Generate text with GPT -->
<use_mcp_tool>
  <server_name>openai</server_name>
  <tool_name>OPENAI_CHAT_COMPLETION</tool_name>
  <arguments>{
    "model": "gpt-4",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "Explain quantum computing in simple terms."}
    ],
    "temperature": 0.7
  }</arguments>
</use_mcp_tool>
```

## Tool Usage Guidelines

### Primary Tools

- `use_mcp_tool`: Use for all MCP server operations
  ```
  <use_mcp_tool>
    <server_name>server_name</server_name>
    <tool_name>tool_name</tool_name>
    <arguments>{ "param1": "value1", "param2": "value2" }</arguments>
  </use_mcp_tool>
  ```

- `access_mcp_resource`: Use for accessing MCP resources
  ```
  <access_mcp_resource>
    <server_name>server_name</server_name>
    <uri>resource://path/to/resource</uri>
  </access_mcp_resource>
  ```

- `apply_diff`: Use for code modifications with complete search and replace blocks
  ```
  <apply_diff>
    <path>file/path.js</path>
    <diff>
      <<<<<<< SEARCH
      // Original code
      =======
      // Updated code
      >>>>>>> REPLACE
    </diff>
  </apply_diff>
  ```

### Secondary Tools

- `insert_content`: Use for documentation and adding new content
- `execute_command`: Use for testing API connections and validating integrations
- `search_and_replace`: Use only when necessary and always include both parameters

## Detailed Documentation

For detailed information about each MCP server and its available tools, refer to the individual documentation files in the `.roo/rules-mcp/` directory:

- [GitHub](./rules-mcp/github.md)
- [Supabase](./rules-mcp/supabase.md)
- [Ahrefs](./rules-mcp/ahrefs.md)
- [Gmail](./rules-mcp/gmail.md)
- [YouTube](./rules-mcp/youtube.md)
- [LinkedIn](./rules-mcp/linkedin.md)
- [OpenAI](./rules-mcp/openai.md)
- [Notion](./rules-mcp/notion.md)
- [Slack](./rules-mcp/slack.md)
- [Google Drive](./rules-mcp/google_drive.md)
- [HackerNews](./rules-mcp/hackernews.md)
- [Composio Search](./rules-mcp/composio_search.md)
- [Mem0](./rules-mcp/mem0.md)
- [PerplexityAI](./rules-mcp/perplexityai.md)
- [CodeInterpreter](./rules-mcp/codeinterpreter.md)

## Best Practices

1. Always initiate a connection before attempting to use any MCP tools
2. Implement retry mechanisms with exponential backoff for transient failures
3. Use circuit breakers to prevent cascading failures
4. Implement request batching to optimize API usage
5. Use proper logging for all API operations
6. Implement data validation for all incoming and outgoing data
7. Use proper error codes and messages for API responses
8. Implement proper timeout handling for all API calls
9. Use proper versioning for API integrations
10. Implement proper rate limiting to prevent API abuse
11. Use proper caching strategies to reduce API calls# Archon MCP Integration Guide

## Overview

Archon serves as the orchestration and tasking layer for the Project Nyra multi-agent development stack. It coordinates 54 specialized agents with 4 MCP servers to provide comprehensive development workflow automation.

## Architecture

```mermaid
graph TD
    A[Archon MCP Orchestrator] --> B[MCP Server Layer]
    A --> C[Agent Layer]
    A --> D[Workflow Patterns]
    
    B --> B1[Desktop Commander]
    B --> B2[rUv Swarm] 
    B --> B3[Flow Nexus]
    B --> B4[Claude Flow]
    
    C --> C1[Core Agents: coder, reviewer, tester, planner, researcher]
    C --> C2[SPARC Agents: specification, pseudocode, architecture, refinement]
    C --> C3[Specialized Agents: backend-dev, frontend-dev, devops, github-modes]
    
    D --> D1[SPARC Workflow Pattern]
    D --> D2[Web App Scaffolding Pattern] 
    D --> D3[Multi-Agent Coordination Pattern]
```

## MCP Server Configuration

### Server Priority & Capabilities

1. **Claude Flow** (Priority 0 - Highest when available)
   - Status: Unstable (connection issues)
   - Capabilities: Agent spawning, workflow coordination, SPARC integration
   - Fallback: Desktop Commander

2. **Desktop Commander** (Priority 1)
   - Status: Connected âœ…
   - Capabilities: File operations, process management, system commands
   - Fallback: rUv Swarm

3. **rUv Swarm** (Priority 2)
   - Status: Connected âœ…
   - Capabilities: Neural coordination, swarm management, memory operations
   - Fallback: Flow Nexus

4. **Flow Nexus** (Priority 3)
   - Status: Connected âœ…
   - Capabilities: Cloud execution, template management, realtime monitoring
   - Fallback: None

### Intelligent Server Selection

Archon automatically routes tasks to the most appropriate MCP server based on:
- Server availability and health status
- Agent capability requirements
- Task complexity and resource needs
- Fallback chain priority

## Agent System Integration

### Core Development Agents (.claude/agents/core/)

- **Coder** (4 concurrent) - Code generation, bug fixing, refactoring
- **Reviewer** (2 concurrent) - Code review, security analysis, quality assurance  
- **Tester** (3 concurrent) - Unit/integration testing, test automation
- **Planner** (1 concurrent) - Project planning, task breakdown, resource allocation
- **Researcher** (2 concurrent) - Requirements analysis, technology research

### SPARC Methodology Agents (.claude/agents/sparc/)

- **Specification** - Requirements specification, use-case analysis
- **Pseudocode** - Algorithm design, pseudocode generation
- **Architecture** - System architecture, design patterns, technology selection
- **Refinement** - Code refinement, optimization, performance tuning

### Specialized Development Agents

- **Backend-dev** (2 concurrent) - API development, database design, server configuration
- **Frontend-dev** (2 concurrent) - UI development, user experience, responsive design
- **DevOps** (1 concurrent) - Deployment, CI/CD, infrastructure, monitoring
- **GitHub-modes** (1 concurrent) - Repository analysis, PR management, issue tracking

## Orchestration Patterns

### 1. SPARC TDD Workflow

Complete 5-phase development workflow with quality gates:

```javascript
phases: [
  { phase: 'specification', agents: ['specification', 'researcher'], parallel: true },
  { phase: 'pseudocode', agents: ['pseudocode', 'planner'], parallel: true },
  { phase: 'architecture', agents: ['architecture'], parallel: false },
  { phase: 'refinement', agents: ['coder', 'tester', 'reviewer'], parallel: true },
  { phase: 'completion', agents: ['integration', 'devops'], parallel: true }
]
```

### 2. Web Application Scaffolding

Full-stack application generation with multiple technology stacks:

**Supported Stacks:**
- React + Express Full Stack
- Next.js Full Stack with Prisma
- Vue 3 + Node.js API
- Django REST + React SPA

**Scaffolding Phases:**
- Planning & Architecture
- Backend Setup
- Frontend Setup  
- Testing Setup
- Deployment Configuration

### 3. Multi-Agent Coordination

Concurrent execution with coordination hooks:

- **Max Concurrent Agents**: 8-12 (configurable)
- **Pre/Post Task Hooks**: Automatic coordination via claude-flow hooks
- **Session Management**: Persistent state and metrics export
- **Resource Management**: Intelligent agent allocation and load balancing

## Configuration Files

### config/mcp-archon-config.js
Central configuration for MCP servers, agent capabilities, routing rules, and orchestration patterns.

### src/orchestration/archon-mcp-orchestrator.js  
Main orchestration engine that coordinates agents with MCP servers.

### .claude/settings.json
Claude Code configuration with environment variables, permissions, and hooks.

### .claude/settings.local.json
Local overrides for MCP server permissions and tool access.

## Usage Examples

### Initialize Archon
```javascript
const { ArchonMcpOrchestrator } = require('./src/orchestration/archon-mcp-orchestrator');

const archon = new ArchonMcpOrchestrator();
await archon.initialize();
```

### Execute SPARC Workflow
```javascript
const result = await archon.executeSPARCWorkflow(
  "Build a REST API with authentication and user management"
);
```

### Scaffold Web Application
```javascript
const app = await archon.executeWebAppScaffolding({
  name: "MyApp",
  stack: "react-express",
  features: ["authentication", "database", "api"]
});
```

### Get System Status
```javascript
const status = archon.getStatus();
console.log('MCP Servers:', status.mcp_servers);
console.log('Active Agents:', status.active_agents);
console.log('Metrics:', status.metrics);
```

## Coordination Hooks Integration

Archon integrates with existing claude-flow hooks for seamless coordination:

### Pre-Task Hooks
```bash
npx claude-flow@alpha hooks pre-task --description "Generate API" --agentType "coder"
```

### Post-Task Hooks
```bash
npx claude-flow@alpha hooks post-task --taskId "task-123" --success true
```

### Session Management
```bash
npx claude-flow@alpha hooks session-restore --sessionId "sparc-workflow-456"
npx claude-flow@alpha hooks session-end --sessionId "sparc-workflow-456" --exportMetrics true
```

## Performance Characteristics

- **Task Routing**: <100ms average routing decisions
- **Concurrent Execution**: Up to 8-12 simultaneous tasks
- **MCP Failover**: <200ms server failover time
- **Memory Usage**: Efficient agent pooling and resource management
- **Scalability**: Linear scaling with available MCP server capacity

## Security Considerations

- **MCP Server Authentication**: Secure connections to all MCP servers
- **Agent Isolation**: Agents run in isolated execution contexts
- **File System Security**: Restricted file access based on agent permissions
- **Environment Variables**: Secure handling of secrets and configuration
- **Audit Logging**: Comprehensive task and agent execution logging

## Troubleshooting

### Common Issues

1. **MCP Server Connection Failures**
   - Check server status in configuration
   - Verify fallback chains are working
   - Test individual MCP tools

2. **Agent Execution Timeouts**
   - Increase concurrent limits
   - Check MCP server performance
   - Review agent complexity

3. **Hook Execution Failures**
   - Verify claude-flow installation
   - Check hook command syntax
   - Review permissions in settings.local.json

### Diagnostic Commands

```bash
# Check MCP server status
node -e "const {ArchonMcpOrchestrator} = require('./src/orchestration/archon-mcp-orchestrator'); const a = new ArchonMcpOrchestrator(); a.initialize().then(() => console.log(a.getStatus()))"

# Test SPARC workflow
npm run archon:test-sparc

# Test web app scaffolding  
npm run archon:test-webapp

# Check agent definitions
ls -la .claude/agents/*/
```

## Next Steps

1. **Install Dependencies**: `npm install`
2. **Test MCP Connections**: `npm run archon:test-connections` 
3. **Run Example Workflow**: `npm run archon:example-sparc`
4. **Configure Custom Agents**: Add to `.claude/agents/specialized/`
5. **Set Up Project Templates**: Configure in `config/templates/`

Archon is now ready to orchestrate sophisticated multi-agent development workflows with robust MCP server integration!# Hooks automatically trigger on operations 
npx claude-flow@alpha init --force  # Auto-configures MCP servers & hooks


# Initialize once per feature/task
npx claude-flow@alpha init --force
npx claude-flow@alpha hive-mind spawn "Implement user authentication" --claude

# Continue working on SAME feature (reuse existing hive)
npx claude-flow@alpha hive-mind status
npx claude-flow@alpha memory query "authentication" --recent
npx claude-flow@alpha swarm "Add password reset functionality" --continue-session








# 1. Initialize Claude Flow with enhanced MCP setup (auto-configures permissions!)
npx claude-flow@alpha init --force

# 2. Explore all revolutionary capabilities  
npx claude-flow@alpha --help

# 3a. Quick AI coordination (recommended for most tasks)
npx claude-flow@alpha swarm "build me a REST API" --claude

# 3b. OR launch the full hive-mind system (for complex projects)
npx claude-flow@alpha hive-mind wizard
npx claude-flow@alpha hive-mind spawn "build enterprise system" --claude





# 1. Initialize Flow Nexus only (minimal setup)
npx claude-flow init --flow-nexus

# 2. Register and login (use MCP tools in Claude Code)

mcp__flow-nexus__user_login({ email: "edaneandersen@gmail.com", password: "1th7aa6ch8oA1!" })


# 3. Deploy your first cloud swarm
mcp__flow-nexus__swarm_init({ topology: "mesh", maxAgents: 5 })
mcp__flow-nexus__sandbox_create({ template: "node", name: "api-dev" })








## Complex Workflow Examples

### Full-Stack Application Development
```bash
# Real-time monitoring
npx claude-flow monitor
# Orchestrate complete application development
npx claude-flow swarm full-stack --project "e-commerce" --parallel-components '{
  "frontend": { "framework": "react", "agents": 3 },
  "backend": { "framework": "node", "agents": 4 },
  "mobile": { "platforms": ["ios", "android"], "agents": 4 },
  "infrastructure": { "provider": "aws", "agents": 2 },
  "testing": { "coverage": "95%", "agents": 3 },
  "documentation": { "types": ["api", "user", "dev"], "agents": 2 }
}' --integrated --continuous
```

# Check status
npx claude-flow status
### Data Pipeline Processing
```bash
# Massive parallel data processing
npx claude-flow swarm data-pipeline --config '{
  "ingestion": { "parallel": 10, "sources": 50 },
  "transformation": { "parallel": 20, "operations": 15 },
  "validation": { "parallel": 5, "rules": 100 },
  "storage": { "parallel": 3, "destinations": 5 }
}' --stream --checkpoint --monitor








SPARC (Specification, Pseudocode, Architecture, Refinement, Completion) is a systematic approach to software development integrated with Claude-Flow.
SPARC with batchtools enables parallel execution of development phases, concurrent multi-mode operations, and efficient batch processing across the entire development lifecycle.

## Available SPARC Modes
## Enhanced SPARC Modes with Batch Capabilities

- `/sparc-architect` - ğŸ—ï¸ Architect
- `/sparc-code` - ğŸ§  Auto-Coder
- `/sparc-tdd` - ğŸ§ª Tester (TDD)
- `/sparc-debug` - ğŸª² Debugger
- `/sparc-security-review` - ğŸ›¡ï¸ Security Reviewer
- `/sparc-docs-writer` - ğŸ“š Documentation Writer
- `/sparc-integration` - ğŸ”— System Integrator
- `/sparc-post-deployment-monitoring-mode` - ğŸ“ˆ Deployment Monitor
- `/sparc-refinement-optimization-mode` - ğŸ§¹ Optimizer
- `/sparc-ask` - â“Ask
- `/sparc-devops` - ğŸš€ DevOps
- `/sparc-tutorial` - ğŸ“˜ SPARC Tutorial
- `/sparc-supabase-admin` - ğŸ” Supabase Admin
- `/sparc-spec-pseudocode` - ğŸ“‹ Specification Writer
- `/sparc-mcp` - â™¾ï¸ MCP Integration
- `/sparc-sparc` - âš¡ï¸ SPARC Orchestrator
### Core Development Modes (Parallelized)
- `/sparc-architect` - ğŸ—ï¸ Parallel architecture design across components
- `/sparc-code` - ğŸ§  Concurrent auto-coding for multiple modules
- `/sparc-tdd` - ğŸ§ª Parallel test suite development
- `/sparc-debug` - ğŸª² Concurrent debugging across systems
- `/sparc-security-review` - ğŸ›¡ï¸ Parallel security analysis
- `/sparc-docs-writer` - ğŸ“š Batch documentation generation
- `/sparc-integration` - ğŸ”— Parallel system integration
- `/sparc-refinement-optimization-mode` - ğŸ§¹ Concurrent optimization

## Quick Start
### Batch Mode Operations
- `/sparc-batch` - ğŸš€ Execute multiple modes in parallel
- `/sparc-pipeline` - ğŸ“Š Pipeline mode execution
- `/sparc-distributed` - ğŸŒ Distributed SPARC processing
- `/sparc-concurrent` - âš¡ Concurrent phase execution

### Run a specific mode:
## Batch Quick Start

### Parallel Mode Execution:
```bash
# Execute multiple modes concurrently
npx claude-flow sparc batch-run --modes '{
  "architect": "Design user service",
  "code": "Implement auth module",
  "tdd": "Create test suite",
  "docs": "Generate API documentation"
}' --parallel

# Pipeline execution with dependencies
npx claude-flow sparc pipeline --stages '[
  { "mode": "spec-pseudocode", "tasks": ["auth", "user", "api"] },
  { "mode": "architect", "depends": ["spec-pseudocode"] },
  { "mode": "tdd", "parallel": true },
  { "mode": "code", "depends": ["tdd"] }
]'
```

### Batch TDD Workflow:
```bash
# Parallel TDD for multiple features
npx claude-flow sparc batch-tdd --features '{
  "authentication": { "priority": "high", "coverage": "95%" },
  "user-management": { "priority": "medium", "coverage": "90%" },
  "api-gateway": { "priority": "high", "coverage": "95%" }
}' --parallel --monitor
```

### Concurrent Analysis:
```bash
# Analyze multiple components in parallel
npx claude-flow sparc batch-analyze --components '{
  "frontend": ["architecture", "performance", "security"],
  "backend": ["architecture", "performance", "security", "scalability"],
  "database": ["schema", "performance", "security"]
}' --concurrent --report
```

## Enhanced SPARC Workflow with Parallelization

### 1. **Parallel Specification Phase**
```bash
npx claude-flow sparc run <mode> "your task"
# Define specifications for multiple components concurrently
npx claude-flow sparc batch-spec --components '[
  { "name": "auth-service", "requirements": "OAuth2, JWT, MFA" },
  { "name": "user-service", "requirements": "CRUD, profiles, preferences" },
  { "name": "notification-service", "requirements": "email, SMS, push" }
]' --parallel --validate
```

### Execute full TDD workflow:
### 2. **Concurrent Pseudocode Development**
```bash
npx claude-flow sparc tdd "implement feature"
# Generate pseudocode for multiple algorithms
npx claude-flow sparc batch-pseudocode --algorithms '{
  "data-processing": ["sorting", "filtering", "aggregation"],
  "authentication": ["login", "refresh", "logout"],
  "caching": ["get", "set", "invalidate"]
}' --optimize --parallel
```

### List all modes:
### 3. **Distributed Architecture Design**
```bash
npx claude-flow sparc modes
# Design architecture for microservices in parallel
npx claude-flow sparc distributed-architect --services '[
  "auth", "user", "product", "order", "payment", "notification"
]' --patterns "microservices" --concurrent --visualize
```

## SPARC Workflow
### 4. **Massive Parallel TDD Implementation**
```bash
# Execute TDD across multiple modules
npx claude-flow sparc parallel-tdd --config '{
  "modules": {
    "core": { "tests": 50, "workers": 3 },
    "api": { "tests": 100, "workers": 5 },
    "ui": { "tests": 75, "workers": 4 }
  },
  "coverage": { "target": "95%", "strict": true }
}' --watch --report
```

1. **Specification**: Define requirements and constraints
2. **Pseudocode**: Create detailed logic flows
3. **Architecture**: Design system structure
4. **Refinement**: Implement with TDD
5. **Completion**: Integrate and validate
### 5. **Batch Integration & Validation**
```bash
# Integrate and validate multiple components
npx claude-flow sparc batch-integrate --components '[
  { "name": "frontend", "deps": ["api"] },
  { "name": "api", "deps": ["database", "cache"] },
  { "name": "workers", "deps": ["queue", "storage"] }
]' --test --validate --parallel
```

## Memory Integration
## Advanced Batch Memory Integration

Use memory commands to persist context:
### Parallel Memory Operations
```bash
npx claude-flow memory store "spec_requirements" "auth system needs"
npx claude-flow memory query "spec"
# Store analysis results concurrently
npx claude-flow sparc batch-memory-store --data '{
  "arch_decisions": { "namespace": "architecture", "parallel": true },
  "test_results": { "namespace": "testing", "compress": true },
  "perf_metrics": { "namespace": "performance", "index": true }
}'

# Query across multiple namespaces
npx claude-flow sparc batch-memory-query --queries '[
  { "pattern": "auth*", "namespace": "specs" },
  { "pattern": "test*", "namespace": "testing" },
  { "pattern": "perf*", "namespace": "metrics" }
]' --parallel --aggregate
```

## Swarm Mode
## Batch Swarm Integration

For complex tasks requiring multiple agents:
### Multi-Mode Swarm Execution
```bash
npx claude-flow swarm "complex project" --strategy development --monitor
# Complex project with parallel SPARC modes
npx claude-flow sparc swarm-batch --project "enterprise-app" --config '{
  "phases": [
    {
      "name": "design",
      "modes": ["spec-pseudocode", "architect"],
      "parallel": true,
      "agents": 6
    },
    {
      "name": "implementation",
      "modes": ["tdd", "code", "integration"],
      "parallel": true,
      "agents": 10
    },
    {
      "name": "quality",
      "modes": ["security-review", "optimization", "docs"],
      "parallel": true,
      "agents": 5
    }
  ]
}' --monitor --checkpoint
```

See `/claude-flow-help` for more commands.
## Performance Optimization Features

### Intelligent Work Distribution
```bash
# Distribute SPARC tasks based on complexity
npx claude-flow sparc distribute --analysis '{
  "complexity": { "weight": 0.4, "method": "cyclomatic" },
  "dependencies": { "weight": 0.3, "method": "graph" },
  "priority": { "weight": 0.3, "method": "user-defined" }
}' --balance --monitor
```

### Caching and Memoization
```bash
# Enable smart caching for SPARC operations
npx claude-flow sparc cache-config --settings '{
  "specifications": { "ttl": "7d", "size": "100MB" },
  "architecture": { "ttl": "3d", "size": "500MB" },
  "test-results": { "ttl": "1d", "size": "1GB" },
  "code-analysis": { "ttl": "1h", "size": "2GB" }
}' --optimize
```

## Complex Workflow Examples

### Enterprise Application Development
```bash
# Full SPARC workflow with maximum parallelization
npx claude-flow sparc enterprise-flow --project "fintech-platform" --parallel-config '{
  "specification": {
    "teams": ["payments", "accounts", "reporting", "compliance"],
    "parallel": true,
    "duration": "2d"
  },
  "architecture": {
    "components": 15,
    "parallel": true,
    "review-cycles": 3
  },
  "implementation": {
    "modules": 50,
    "parallel-factor": 10,
    "tdd-coverage": "95%"
  },
  "integration": {
    "environments": ["dev", "staging", "prod"],
    "parallel-deploy": true
  }
}' --monitor --report --checkpoint
```

### Microservices Migration
```bash
# Parallel SPARC-driven migration
npx claude-flow sparc migrate-batch --from "monolith" --to "microservices" --strategy '{
  "analysis": { "parallel": 5, "tools": ["dependency", "complexity", "coupling"] },
  "decomposition": { "parallel": 3, "method": "domain-driven" },
  "implementation": { "parallel": 10, "pattern": "strangler-fig" },
  "validation": { "parallel": 5, "tests": ["unit", "integration", "e2e"] }
}' --rollback-enabled
```

### AI/ML Pipeline Development
```bash
# SPARC for ML pipeline with parallel processing
npx claude-flow sparc ml-pipeline --config '{
  "data-pipeline": {
    "stages": ["ingestion", "cleaning", "transformation", "validation"],
    "parallel": 4
  },
  "model-development": {
    "experiments": 20,
    "parallel": 5,
    "frameworks": ["tensorflow", "pytorch", "scikit-learn"]
  },
  "deployment": {
    "targets": ["api", "batch", "streaming"],
    "parallel": true
  }
}' --gpu-enabled --distributed
```

## Monitoring and Analytics

### Real-time Batch Monitoring
```bash
# Monitor all SPARC operations
npx claude-flow sparc monitor-batch --dashboards '[
  "specification-progress",
  "architecture-reviews",
  "tdd-coverage",
  "integration-status",
  "performance-metrics"
]' --real-time --alerts
```

### Performance Analytics
```bash
# Analyze SPARC workflow efficiency
npx claude-flow sparc analyze-performance --metrics '{
  "throughput": ["tasks/hour", "loc/day"],
  "quality": ["bug-density", "test-coverage"],
  "efficiency": ["reuse-ratio", "automation-level"]
}' --compare-baseline --recommendations






Collapse file: .claude/commands/claude-flow-help.md
â€.claude/commands/claude-flow-help.mdâ€
+113
-16
Lines changed: 113 additions & 16 deletions


Original file line number	Diff line number	Diff line change
@@ -1,60 +1,157 @@
---
name: claude-flow-help
description: Show Claude-Flow commands and usage
description: Show Claude-Flow commands and usage with batchtools optimization
---

# Claude-Flow Commands
# Claude-Flow Commands (Batchtools Optimized)

## Core Commands
## Core Commands with Batch Operations

### System Management
### System Management (Batch Operations)
- `npx claude-flow start` - Start orchestration system
- `npx claude-flow status` - Check system status
- `npx claude-flow monitor` - Real-time monitoring
- `npx claude-flow stop` - Stop orchestration

### Agent Management
**Batch Operations:**
```bash
# Check multiple system components in parallel
npx claude-flow batch status --components "agents,tasks,memory,connections"
# Start multiple services concurrently
npx claude-flow batch start --services "monitor,scheduler,coordinator"
```
### Agent Management (Parallel Operations)
- `npx claude-flow agent spawn <type>` - Create new agent
- `npx claude-flow agent list` - List active agents
- `npx claude-flow agent info <id>` - Agent details
- `npx claude-flow agent terminate <id>` - Stop agent

### Task Management
**Batch Operations:**
```bash
# Spawn multiple agents in parallel
npx claude-flow agent batch-spawn "code:3,test:2,review:1"
# Get info for multiple agents concurrently
npx claude-flow agent batch-info "agent1,agent2,agent3"
# Terminate multiple agents
npx claude-flow agent batch-terminate --pattern "test-*"
```
### Task Management (Concurrent Processing)
- `npx claude-flow task create <type> "description"` - Create task
- `npx claude-flow task list` - List all tasks
- `npx claude-flow task status <id>` - Task status
- `npx claude-flow task cancel <id>` - Cancel task

### Memory Operations
**Batch Operations:**
```bash
# Create multiple tasks from file
npx claude-flow task batch-create tasks.json
# Check status of multiple tasks concurrently
npx claude-flow task batch-status --ids "task1,task2,task3"
# Process task queue in parallel
npx claude-flow task process-queue --parallel 5
```
### Memory Operations (Bulk Processing)
- `npx claude-flow memory store "key" "value"` - Store data
- `npx claude-flow memory query "search"` - Search memory
- `npx claude-flow memory stats` - Memory statistics
- `npx claude-flow memory export <file>` - Export memory

### SPARC Development
**Batch Operations:**
```bash
# Bulk store from JSON file
npx claude-flow memory batch-store data.json
# Parallel query across namespaces
npx claude-flow memory batch-query "search term" --namespaces "all"
# Export multiple namespaces concurrently
npx claude-flow memory batch-export --namespaces "project,agents,tasks"
```
### SPARC Development (Parallel Workflows)
- `npx claude-flow sparc modes` - List SPARC modes
- `npx claude-flow sparc run <mode> "task"` - Run mode
- `npx claude-flow sparc tdd "feature"` - TDD workflow
- `npx claude-flow sparc info <mode>` - Mode details

### Swarm Coordination
**Batch Operations:**
```bash
# Run multiple SPARC modes in parallel
npx claude-flow sparc batch-run --modes "spec:task1,architect:task2,code:task3"
# Execute parallel TDD for multiple features
npx claude-flow sparc batch-tdd features.json
# Analyze multiple components concurrently
npx claude-flow sparc batch-analyze --components "auth,api,database"
```
### Swarm Coordination (Enhanced Parallelization)
- `npx claude-flow swarm "task" --strategy <type>` - Start swarm
- `npx claude-flow swarm "task" --background` - Long-running swarm
- `npx claude-flow swarm "task" --monitor` - With monitoring

## Quick Examples
**Batch Operations:**
```bash
# Launch multiple swarms for different components
npx claude-flow swarm batch --config swarms.json
# Coordinate parallel swarm strategies
npx claude-flow swarm multi-strategy "project" --strategies "dev:frontend,test:backend,docs:api"
```
## Advanced Batch Examples

### Start a development swarm:
### Parallel Development Workflow:
```bash
npx claude-flow swarm "Build REST API" --strategy development --monitor
# Initialize complete project setup in parallel
npx claude-flow batch init --actions "memory:setup,agents:spawn,tasks:queue"
# Run comprehensive analysis
npx claude-flow batch analyze --targets "code:quality,security:audit,performance:profile"
```

### Run TDD workflow:
### Concurrent Testing Suite:
```bash
npx claude-flow sparc tdd "user authentication"
# Execute parallel test suites
npx claude-flow sparc batch-test --suites "unit,integration,e2e" --parallel
# Generate reports concurrently
npx claude-flow batch report --types "coverage,performance,security"
```

### Store project context:
### Bulk Operations:
```bash
npx claude-flow memory store "project_requirements" "e-commerce platform specs"
# Process multiple files in parallel
npx claude-flow batch process --files "*.ts" --action "lint,format,analyze"
# Parallel code generation
npx claude-flow batch generate --templates "api:users,api:products,api:orders"
```
## Performance Tips
- Use `--parallel` flag for concurrent operations
- Batch similar operations to reduce overhead
- Leverage `--async` for non-blocking execution
- Use `--stream` for real-time progress updates
- Enable `--cache` for repeated operations
## Monitoring Batch Operations
```bash
# Real-time batch monitoring
npx claude-flow monitor --batch
# Batch operation statistics
npx claude-flow stats --batch-ops
# Performance profiling
npx claude-flow profile --batch-execution
```





 SPARC Mode Configuration v2.0## OverviewThis document describes the enhanced SPARC mode configuration system with full batchtools support, optimization features, and multi-agent coordination capabilities.## Key Features### ğŸš€ Performance Optimizations- **Connection Pooling**: Efficient Claude API connection management- **Intelligent Caching**: TTL-based result caching for repeated operations- **Async Processing**: Non-blocking execution for better throughput- **Resource Monitoring**: Real-time metrics and performance tracking### ğŸ”§ Batch Operations- **Parallel Execution**: Run multiple tasks simultaneously- **Dependency Resolution**: Intelligent task ordering based on dependencies- **Boomerang Pattern**: Iterative refinement workflows- **Error Handling**: Configurable error handling strategies### ğŸ¤– Multi-Agent Coordination- **Swarm Mode**: Coordinate multiple AI agents for complex tasks- **Load Balancing**: Distribute work across available agents- **Capability Matching**: Assign tasks based on agent specializations- **Failure Recovery**: Automatic task reassignment on agent failures## Configuration Structure### Mode GroupsModes are organized into logical groups:- **Core**: Essential development modes (spec-pseudocode, architect, code, tdd, integration)- **Quality**: Testing and optimization modes (debug, security-review, optimization)- **Support**: Documentation and assistance modes (docs-writer, tutorial, ask)- **Infrastructure**: DevOps and integration modes (devops, mcp, supabase-admin)- **Orchestration**: Advanced coordination modes (sparc, swarm)### Optimization SettingsEach mode includes optimization configuration:```json{  "optimization": {    "batchable": true,           // Can be used in batch operations    "cacheable": true,           // Results can be cached    "parallelizable": true,      // Can run in parallel with other tasks    "priority": "high",          // Execution priority    "resourceLimits": {      "maxTokens": 8192,         // Maximum tokens per request      "maxExecutionTime": 300000 // Maximum execution time in ms    }  }}```### Batch OperationsModes that support batch operations include:```json{  "batchOperations": {    "supportedPatterns": ["parallel-files", "sequential-modules"],    "maxBatchSize": 10,    "errorHandling": "continue-on-error"  }}```## Executor Types### Optimized Executor (Default)- Connection pooling with 2-10 connections- Intelligent caching with 1-hour TTL- Async file operations- Real-time metrics collection### SPARC Executor- SPARC methodology enforcement- Enhanced token limits- Specialized prompt handling### Claude-Flow Executor- Full memory system integration- Coordination capabilities- Persistent state management## BatchTools Integration### Supported Patterns1. **Parallel Execution**   ```bash   batchtool run --parallel \     "npx claude-flow sparc run code 'feature A'" \     "npx claude-flow sparc run code 'feature B'"   ```2. **Boomerang Pattern**   ```bash   batchtool orchestrate --boomerang \     --phase1 "research requirements" \     --phase2 "design architecture" \     --phase3 "implement solution"   ```3. **Dependency-Aware**   ```bash   batchtool run --dependency-aware \     --task "db:create schema:depends=none" \     --task "api:create endpoints:depends=db"   ```4. **A/B Testing**   ```bash   batchtool run --ab-test \     --variant-a "approach 1" \     --variant-b "approach 2"   ```5. **Progressive Enhancement**   ```bash   batchtool orchestrate --progressive \     --mvp "basic feature" \     --enhance-1 "add authentication" \     --enhance-2 "add real-time updates"   ```### Configuration Options```json{  "batchToolsIntegration": {    "enabled": true,    "config": {      "commandPrefix": "npx claude-flow sparc",      "defaultFlags": "--non-interactive",      "parallelExecution": {        "maxConcurrent": 10,        "queueStrategy": "priority-based"      },      "boomerangPattern": {        "enabled": true,        "maxIterations": 5,        "convergenceThreshold": 0.95      }    }  }}```## Mode Enhancements### New Swarm Mode- Multi-agent coordination- Distributed task execution- Intelligent agent assignment- Failure recovery mechanisms### Enhanced Tools IntegrationEach mode now includes:- **Required Tools**: Essential tools for the mode- **Optional Tools**: Additional capabilities- **Custom Tools**: Specialized tools for specific functions### Improved Error Handling- **fail-fast**: Stop on first error- **continue-on-error**: Continue execution despite errors- **collect-all-errors**: Collect all errors for analysis- **rollback-on-error**: Rollback changes on error- **adaptive-retry**: Intelligent retry with backoff## Performance Monitoring### Metrics Collection- Task execution times- Success/failure rates- Cache hit ratios- Resource utilization- Queue lengths### Monitoring Dashboard- Real-time performance metrics- Task execution status- Resource usage graphs- Error rate tracking## Best Practices### Batch Operations1. Use `--non-interactive` flag for automated executions2. Tag related tasks for easier management3. Implement proper error handling strategies4. Monitor resource usage during execution5. Use memory namespaces to share context### Performance Optimization1. Enable caching for repetitive operations2. Use connection pooling for API-heavy tasks3. Configure appropriate resource limits4. Monitor slow task thresholds5. Implement proper cleanup procedures### Multi-Agent Coordination1. Analyze task complexity before decomposition2. Match agent capabilities to task requirements3. Monitor agent health and availability4. Implement graceful failure recovery5. Aggregate results with quality validation## Compatibility### Backward CompatibilityAll existing SPARC modes remain fully compatible. The v1 mappings ensure seamless migration.### Migration Notes- New `swarm` mode added for multi-agent coordination- All modes enhanced with batch operation support- Intelligent caching available for all cacheable modes- Real-time monitoring and metrics collection## Configuration ValidationThe configuration system includes validation for:- JSON structure integrity- Mode definition completeness- Tool availability- Resource limit sanity checks- Dependency resolution## Future EnhancementsPlanned features for future versions:- Machine learning-based optimization- Advanced scheduling algorithms- Dynamic resource allocation- Enhanced failure prediction- Cross-platform compatibility improvements## Troubleshooting### Common Issues1. **Invalid JSON**: Check syntax with `python3 -m json.tool .roomodes`2. **Missing Tools**: Verify tool availability in mode definitions3. **Resource Limits**: Adjust limits based on system capabilities4. **Cache Issues**: Clear cache if stale results persist5. **Batch Failures**: Check error handling configuration### Debug Commands```bash# Validate configurationnpx claude-flow sparc modes# Check system statusnpx claude-flow status# View mode informationnpx claude-flow sparc info <mode-name># Clear cachenpx claude-flow cache clear# View metricsnpx claude-flow metrics```## SupportFor issues or questions:1. Check the troubleshooting section2. Review the configuration documentation3. Examine the logs in `.sparc/logs/`4. Consult the BatchTools integration guide5. Submit issues to the project repository
Collapse file: .sparc/validate-config.jsâ€.sparc/validate-config.jsâ€Copy file name to clipboard+271Lines changed: 271 additions & 0 deletionsOriginal file line numberDiff line numberDiff line change@@ -0,0 +1,271 @@#!/usr/bin/env node/** * SPARC Configuration Validator * Validates the .roomodes configuration file for v2.0 compliance */const fs = require('fs');const path = require('path');// Colors for console outputconst colors = {  reset: '\x1b[0m',  red: '\x1b[31m',  green: '\x1b[32m',  yellow: '\x1b[33m',  blue: '\x1b[34m',  magenta: '\x1b[35m',  cyan: '\x1b[36m'};function log(level, message, detail = '') {  const timestamp = new Date().toISOString();  const prefix = {    info: `${colors.blue}â„¹${colors.reset}`,    success: `${colors.green}âœ…${colors.reset}`,    warning: `${colors.yellow}âš ${colors.reset}`,    error: `${colors.red}âŒ${colors.reset}`  }[level] || '';    console.log(`${prefix} ${message}${detail ? ': ' + detail : ''}`);}function validateConfiguration() {  const configPath = path.join(process.cwd(), '.roomodes');    // Check if config file exists  if (!fs.existsSync(configPath)) {    log('error', 'Configuration file not found', '.roomodes');    return false;  }    let config;  try {    const configContent = fs.readFileSync(configPath, 'utf8');    config = JSON.parse(configContent);    log('success', 'Configuration file loaded successfully');  } catch (error) {    log('error', 'Invalid JSON in configuration file', error.message);    return false;  }    // Validate version  if (!config.version || config.version !== '2.0') {    log('warning', 'Configuration version should be 2.0', `Found: ${config.version || 'undefined'}`);  } else {    log('success', 'Configuration version is valid', 'v2.0');  }    // Validate required sections  const requiredSections = [    'metadata',    'globalConfig',     'modeGroups',    'customModes',    'executors',    'batchToolsIntegration',    'compatibilityLayer'  ];    let allSectionsValid = true;  requiredSections.forEach(section => {    if (!config[section]) {      log('error', `Missing required section: ${section}`);      allSectionsValid = false;    } else {      log('success', `Section present: ${section}`);    }  });    if (!allSectionsValid) {    return false;  }    // Validate modes  if (!Array.isArray(config.customModes) || config.customModes.length === 0) {    log('error', 'customModes must be a non-empty array');    return false;  }    log('info', `Found ${config.customModes.length} modes`);    // Validate each mode  let modeErrors = 0;  config.customModes.forEach((mode, index) => {    const requiredModeFields = ['slug', 'name', 'roleDefinition', 'customInstructions', 'groups'];    const modeValid = requiredModeFields.every(field => {      if (!mode[field]) {        log('error', `Mode ${index + 1} missing required field: ${field}`);        modeErrors++;        return false;      }      return true;    });        if (modeValid) {      // Check for v2.0 enhancements      const hasOptimization = mode.optimization && typeof mode.optimization === 'object';      const hasTools = mode.tools && typeof mode.tools === 'object';      const hasTags = Array.isArray(mode.tags);            if (hasOptimization && hasTools && hasTags) {        log('success', `Mode "${mode.slug}" is fully v2.0 compliant`);      } else {        log('warning', `Mode "${mode.slug}" missing v2.0 enhancements`,           `optimization:${hasOptimization}, tools:${hasTools}, tags:${hasTags}`);      }    }  });    if (modeErrors > 0) {    log('error', `Found ${modeErrors} mode validation errors`);    return false;  }    // Validate executors  const expectedExecutors = ['basic', 'direct', 'sparc', 'optimized', 'claude-flow'];  const presentExecutors = Object.keys(config.executors || {});    expectedExecutors.forEach(executor => {    if (presentExecutors.includes(executor)) {      log('success', `Executor configured: ${executor}`);    } else {      log('warning', `Missing executor: ${executor}`);    }  });    // Validate batch tools integration  if (config.batchToolsIntegration?.enabled) {    log('success', 'BatchTools integration is enabled');        const patterns = config.batchToolsIntegration.patterns || {};    const expectedPatterns = ['parallel', 'sequential', 'boomerang', 'dependency-aware'];        expectedPatterns.forEach(pattern => {      if (patterns[pattern]) {        log('success', `BatchTools pattern configured: ${pattern}`);      } else {        log('warning', `Missing BatchTools pattern: ${pattern}`);      }    });  } else {    log('warning', 'BatchTools integration is not enabled');  }    // Validate mode groups  const groups = config.modeGroups || {};  const expectedGroups = ['core', 'quality', 'support', 'infrastructure', 'orchestration'];    expectedGroups.forEach(group => {    if (groups[group] && Array.isArray(groups[group].modes)) {      log('success', `Mode group configured: ${group} (${groups[group].modes.length} modes)`);    } else {      log('warning', `Missing or invalid mode group: ${group}`);    }  });    // Validate global config  const globalConfig = config.globalConfig || {};  if (globalConfig.defaultExecutor === 'optimized') {    log('success', 'Default executor set to optimized');  } else {    log('info', `Default executor: ${globalConfig.defaultExecutor || 'not specified'}`);  }    if (globalConfig.caching?.enabled) {    log('success', 'Global caching is enabled');  } else {    log('info', 'Global caching is not enabled');  }    if (globalConfig.monitoring?.enabled) {    log('success', 'Global monitoring is enabled');  } else {    log('info', 'Global monitoring is not enabled');  }    return true;}function validateDirectories() {  const requiredDirs = [    '.sparc',    '.sparc/outputs',    '.sparc/logs'  ];    requiredDirs.forEach(dir => {    if (fs.existsSync(dir)) {      log('success', `Directory exists: ${dir}`);    } else {      log('warning', `Directory missing: ${dir}`);      try {        fs.mkdirSync(dir, { recursive: true });        log('success', `Created directory: ${dir}`);      } catch (error) {        log('error', `Failed to create directory: ${dir}`, error.message);      }    }  });}function validateFeatures() {  log('info', 'Validating v2.0 features...');    const features = [    { name: 'Parallel Execution', check: () => process.env.SPARC_PARALLEL !== 'false' },    { name: 'Async Processing', check: () => true },    { name: 'Connection Pooling', check: () => true },    { name: 'Intelligent Caching', check: () => true },    { name: 'Resource Monitoring', check: () => true },    { name: 'Boomerang Pattern', check: () => true },    { name: 'Dependency Resolution', check: () => true }  ];    features.forEach(feature => {    if (feature.check()) {      log('success', `Feature available: ${feature.name}`);    } else {      log('warning', `Feature disabled: ${feature.name}`);    }  });}function main() {  console.log(`${colors.cyan}ğŸ” SPARC Configuration Validator v2.0${colors.reset}\n`);    log('info', 'Starting configuration validation...');    // Validate configuration file  const configValid = validateConfiguration();    // Validate directories  validateDirectories();    // Validate features  validateFeatures();    console.log('\n' + '='.repeat(50));    if (configValid) {    log('success', 'Configuration validation completed successfully');    console.log(`\n${colors.green}âœ¨ Your SPARC configuration is ready for v2.0 features!${colors.reset}`);    console.log(`${colors.cyan}Next steps:${colors.reset}`);    console.log('  â€¢ Test batch operations with: batchtool run --parallel');    console.log('  â€¢ Monitor performance with: npx claude-flow metrics');    console.log('  â€¢ Explore swarm mode with: npx claude-flow sparc run swarm');    return 0;  } else {    log('error', 'Configuration validation failed');    console.log(`\n${colors.red}âŒ Please fix the errors above before using v2.0 features${colors.reset}`);    return 1;  }}// Run the validatorif (require.main === module) {  process.exit(main());}module.exports = { validateConfiguration, validateDirectories, validateFeatures };# Claude Code Development Kit 

[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Changelog](https://img.shields.io/badge/changelog-v2.1.0-orange.svg)](CHANGELOG.md)

An integrated system that transforms Claude Code into an orchestrated development environment through automated documentation management, multi-agent workflows, and external AI expertise.

> **Related**: Check out [Freigeist](https://www.freigeist.dev) - upcoming AI coding platform for complex projects!

## Why Claude Code?

Claude Code's Sub-Agents enable this highly automated, integrated approach. While other AI tools can likely use the documentation structure (see FAQ) and some commands, only Claude Code can currently orchestrate parallel agents and use this Development Kit to its full potential.

## ğŸ¯ Why This Kit?

> *Ever tried to build a large project with AI assistance, only to watch it struggle as your codebase grows?*

Claude Code's output quality directly depends on what it knows about your project. As AI-assisted development scales, three critical challenges emerge:

---

### Challenge 1: Context Management

**The Problem:**
```
âŒ Loses track of your architecture patterns and design decisions
âŒ Forgets your coding standards and team conventions
âŒ No guidance on where to find the right context in large codebases
```

**The Solution:**
âœ… **Automated context delivery** through two integrated systems:
- **3-tier documentation system** - Auto-loads the right docs at the right time
- **Custom commands with sub-agents** - Orchestrates specialized agents that already know your project
- Result: No manual context loading, consistent knowledge across all agents

---

### Challenge 2: AI Reliability 

**The Problem:**
```
âŒ Outdated library documentation
âŒ Hallucinated API methods
âŒ Inconsistent architectural decisions
```

**The Solution:**
âœ… **"Four eyes principle"** through MCP integration:

| Service | Purpose | Benefit |
|---------|---------|---------|
| **Context7** | Real-time library docs | Current APIs, not training data |
| **Gemini** | Architecture consultation | Cross-validation & best practices |

*Result: Fewer errors, better code, current standards*

---

### Challenge 3: Automation Without Complexity

**The Problem:**
```
âŒ Manual context loading for every session
âŒ Repetitive command sequences
âŒ No feedback when tasks complete
```

**The Solution:**
âœ… **Intelligent automation** through hooks and commands:
- Automatic updates of documentation through custom commands
- Context injection for all Sub-agents and Gemini MCP calls 
- Audio notifications for task completion (optional)
- One-command workflows for complex tasks

---

### ğŸ‰ The Result

> **Claude Code transforms from a helpful tool into a reliable development partner that remembers your project context, validates its own work, and handles the tedious stuff automatically.**


[![Demo-Video auf YouTube](https://img.youtube.com/vi/kChalBbMs4g/0.jpg)](https://youtu.be/kChalBbMs4g)




## Quick Start

### Prerequisites

- **Required**: [Claude Code](https://github.com/anthropics/claude-code)
- **Recommended**: MCP servers like [Context7](https://github.com/upstash/context7) and [Gemini Assistant](https://github.com/peterkrueck/mcp-gemini-assistant)

#### Platform Support

- **Windows**: âŒ (has reported bugs - use at your own risk)

### Installation

#### Option 1: Quick Install (Recommended)

Run this single command in your terminal:

```bash
curl -fsSL https://raw.githubusercontent.com/peterkrueck/Claude-Code-Development-Kit/main/install.sh | bash
```

This will:
1. Download the framework
2. Guide you through an interactive setup
3. Install everything in your chosen project directory
4. Provide links to optional MCP server installations


https://github.com/user-attachments/assets/0b4a1e69-bddb-4b58-8de9-35f97919bf44


#### Option 2: Clone and Install

```bash
git clone https://github.com/peterkrueck/Claude-Code-Development-Kit.git
cd Claude-Code-Development-Kit
./setup.sh
```

### What Gets Installed

The setup script will create the following structure in your project:

```
your-project/
â”œâ”€â”€ .claude/
â”‚   â”œâ”€â”€ commands/          # AI orchestration templates (.md files)
â”‚   â”œâ”€â”€ hooks/             # Automation scripts
â”‚   â”‚   â”œâ”€â”€ config/        # Security patterns configuration
â”‚   â”‚   â”œâ”€â”€ sounds/        # Notification sounds (if notifications enabled)
â”‚   â”‚   â””â”€â”€ *.sh           # Hook scripts (based on your selections)
â”‚   â””â”€â”€ settings.local.json # Generated Claude Code configuration
â”œâ”€â”€ docs/                  # Documentation templates and examples
â”‚   â”œâ”€â”€ ai-context/        # Core documentation files
â”‚   â”œâ”€â”€ open-issues/       # Issue tracking examples
â”‚   â”œâ”€â”€ specs/             # Specification templates
â”‚   â”œâ”€â”€ CONTEXT-tier2-component.md  # Component documentation template
â”‚   â””â”€â”€ CONTEXT-tier3-feature.md    # Feature documentation template
â”œâ”€â”€ logs/                  # Hook execution logs (created at runtime)
â”œâ”€â”€ CLAUDE.md              # Your project's AI context (from template)
â””â”€â”€ MCP-ASSISTANT-RULES.md # MCP coding standards (if Gemini-Assistant-MCP selected)
```

**Note**: The exact files installed depend on your choices during setup (MCP servers, notifications, etc.)

### Post-Installation Setup

1. **Customize your AI context**:
   - Edit `CLAUDE.md` with your project standards
   - Update `docs/ai-context/project-structure.md` with your tech stack

2. **Install MCP servers** (if selected during setup):
   - Follow the links provided by the installer
   - Configure in `.claude/settings.local.json`

3. **Test your installation**:
   ```bash
   claude
   /full-context "analyze my project structure"
   ```


## Terminology

- **CLAUDE.md** - Master context files containing project-specific AI instructions, coding standards, and integration patterns
- **CONTEXT.md** - Component and feature-level documentation files (Tier 2 and Tier 3) that provide specific implementation details and patterns
- **MCP (Model Context Protocol)** - Standard for integrating external AI services with Claude Code
- **Sub-agents** - Specialized AI agents spawned by Claude Code to work on specific aspects of a task in parallel
- **3-Tier Documentation** - Hierarchical organization (Foundation/Component/Feature) that minimizes maintenance while maximizing AI effectiveness
- **Auto-loading** - Automatic inclusion of relevant documentation when commands execute
- **Hooks** - Shell scripts that execute at specific points in Claude Code's lifecycle for security, automation, and UX enhancements

## Architecture

### Integrated Intelligence Loop

```
                        CLAUDE CODE
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚                 â”‚
                   â”‚    COMMANDS      â”‚
                   â”‚                 â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  Multi-agentâ”‚orchestration
                   Parallel â”‚execution
                   Dynamic  â”‚scaling
                           â•±â”‚â•²
                          â•± â”‚ â•²
          Routes agents  â•±  â”‚  â•²  Leverages
          to right docs â•±   â”‚   â•² expertise
                       â•±    â”‚    â•²
                      â–¼     â”‚     â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                 â”‚â”‚â”‚                 â”‚
         â”‚  DOCUMENTATION  â”‚â”‚â”‚  MCP SERVERS   â”‚
         â”‚                 â”‚â”‚â”‚                 â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          3-tier structure  â”‚  Context7 + Gemini
          Auto-loading      â”‚  Real-time updates
          Context routing   â”‚  AI consultation
                      â•²     â”‚     â•±
                       â•²    â”‚    â•±
        Provides projectâ•²   â”‚   â•± Enhances with
        context for      â•²  â”‚  â•±  current best
        consultation      â•² â”‚ â•±   practices
                           â•²â”‚â•±
                            â–¼
                    Integrated Workflow
```

### Auto-Loading Mechanism

Every command execution automatically loads critical documentation:

```
@/CLAUDE.md                              # Master AI context and coding standards
@/docs/ai-context/project-structure.md   # Complete technology stack and file tree
@/docs/ai-context/docs-overview.md       # Documentation routing map
```

The `subagent-context-injector.sh` hook extends auto-loading to all sub-agents:
- Sub-agents spawned via the Task tool automatically receive the same core documentation
- No manual context inclusion needed in Task prompts
- Ensures consistent knowledge across all agents in multi-agent workflows

This ensures:
- Consistent AI behavior across all sessions and sub-agents
- Zero manual context management at any level

### Component Integration

**Commands â†”ï¸ Documentation**
- Commands determine which documentation tiers to load based on task complexity
- Documentation structure guides agent spawning patterns
- Commands update documentation to maintain current context

**Commands â†”ï¸ MCP Servers**
- Context7 provides up-to-date library documentation
- Gemini offers architectural consultation for complex problems
- Integration happens seamlessly within command workflows

**Documentation â†”ï¸ MCP Servers**
- Project structure and MCP assistant rules auto-attach to Gemini consultations
- Ensures external AI understands specific architecture and coding standards
- Makes all recommendations project-relevant and standards-compliant

### Hooks Integration

The kit includes battle-tested hooks that enhance Claude Code's capabilities:

- **Security Scanner** - Prevents accidental exposure of secrets when using MCP servers
- **Gemini Context Injector** - Automatically includes project structure in Gemini consultations
- **Subagent Context Injector** - Ensures all sub-agents receive core documentation automatically
- **Notification System** - Provides non-blocking audio feedback for task completion and input requests (optional)

These hooks integrate seamlessly with the command and MCP server workflows, providing:
- Pre-execution security checks for all external AI calls
- Automatic context enhancement for both external AI and sub-agents
- Consistent knowledge across all agents in multi-agent workflows
- Developer awareness through pleasant, non-blocking audio notifications

## Common Tasks

### Starting New Feature Development

```bash
/full-context "implement user authentication across backend and frontend"
```

The system:
1. Auto-loads project documentation
2. Spawns specialized agents (security, backend, frontend)
3. Consults Context7 for authentication framework documentation
4. Asks Gemini 2.5 pro for feedback and improvement suggestions
4. Provides comprehensive analysis and implementation plan

### Code Review with Multiple Perspectives

```bash
/code-review "review authentication implementation"
```

Multiple agents analyze:
- Security vulnerabilities
- Performance implications
- Architectural alignment
- Integration impacts

### Maintaining Documentation Currency

```bash
/update-docs "document authentication changes"
```

Automatically:
- Updates affected CLAUDE.md files across all tiers
- Keeps project-structure.md and docs-overview.md up-to-date
- Maintains context for future AI sessions
- Ensures documentation matches implementation

## Creating Your Project Structure

After installation, you'll add your own project-specific documentation:

```
your-project/
â”œâ”€â”€ .claude/
â”‚   â”œâ”€â”€ commands/              # AI orchestration templates
â”‚   â”œâ”€â”€ hooks/                 # Security and automation hooks
â”‚   â”‚   â”œâ”€â”€ config/            # Hook configuration files
â”‚   â”‚   â”œâ”€â”€ sounds/            # Notification audio files
â”‚   â”‚   â”œâ”€â”€ gemini-context-injector.sh
â”‚   â”‚   â”œâ”€â”€ mcp-security-scan.sh
â”‚   â”‚   â”œâ”€â”€ notify.sh
â”‚   â”‚   â””â”€â”€ subagent-context-injector.sh
â”‚   â””â”€â”€ settings.json          # Claude Code configuration
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ai-context/            # Foundation documentation (Tier 1)
â”‚   â”‚   â”œâ”€â”€ docs-overview.md   # Documentation routing map
â”‚   â”‚   â”œâ”€â”€ project-structure.md # Technology stack and file tree
â”‚   â”‚   â”œâ”€â”€ system-integration.md # Cross-component patterns
â”‚   â”‚   â”œâ”€â”€ deployment-infrastructure.md # Infrastructure context
â”‚   â”‚   â””â”€â”€ handoff.md        # Session continuity
â”‚   â”œâ”€â”€ open-issues/           # Issue tracking templates
â”‚   â”œâ”€â”€ specs/                 # Feature specifications
â”‚   â””â”€â”€ README.md              # Documentation system guide
â”œâ”€â”€ CLAUDE.md                  # Master AI context (Tier 1)
â”œâ”€â”€ MCP-ASSISTANT-RULES.md     # MCP coding standards (if Gemini selected)
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ **`CONTEXT.md`**       # Backend context (Tier 2) - ğŸ”´ create this
â”‚   â””â”€â”€ src/api/
â”‚       â””â”€â”€ **`CONTEXT.md`**   # API context (Tier 3) - ğŸ”´ create this
â””â”€â”€ frontend/
    â”œâ”€â”€ **`CONTEXT.md`**       # Frontend context (Tier 2) - ğŸ”´ create this
    â””â”€â”€ src/components/
        â””â”€â”€ **`CONTEXT.md`**   # Components context (Tier 3) - ğŸ”´ create this
```

The framework provides templates for CONTEXT.md files in `docs/`:
- `docs/CONTEXT-tier2-component.md` - Use as template for component-level docs
- `docs/CONTEXT-tier3-feature.md` - Use as template for feature-level docs

## Configuration

The kit is designed for adaptation:

- **Commands** - Modify orchestration patterns in `.claude/commands/`
- **Documentation** - Adjust tier structure for your architecture
- **MCP Integration** - Add additional servers for specialized expertise
- **Hooks** - Customize security patterns, add new hooks, or modify notifications in `.claude/hooks/`
- **MCP Assistant Rules** - Copy `docs/MCP-ASSISTANT-RULES.md` template to project root and customize for project-specific standards

## Best Practices

1. **Let documentation guide development** - The 3-tier structure reflects natural boundaries
2. **Update documentation immediately** - Use `/update-docs` after significant changes
3. **Trust the auto-loading** - Avoid manual context management
4. **Scale complexity naturally** - Simple tasks stay simple, complex tasks get sophisticated analysis


## Documentation

- [Documentation System Guide](docs/) - Understanding the 3-tier architecture
- [Commands Reference](commands/) - Detailed command usage
- [MCP Integration](docs/CLAUDE.md) - Configuring external services
- [Hooks System](hooks/) - Security scanning, context injection, and notifications
- [Changelog](CHANGELOG.md) - Version history and migration guides

## Contributing

The kit represents one approach to AI-assisted development. Contributions and adaptations are welcome.

## FAQ

**Q: Will the setup overwrite my existing files?**

**A:** No, the installer detects existing files and prompts you to skip or overwrite each one. For safety, I highly recommend installing on a new Git branch. Safe is safe.

**Q: Can I use this with other AI coding tools like Cursor, Cline, or Gemini CLI?**

**A:** Partially. The documentation structure works with any tool (rename CLAUDE.md to match your tool's convention). However, commands are highly optimized for sub-agent usage and hooks are Claude Code-specific. Other tools would need significant adaptation of the orchestration features.

**Q: How much will this cost in tokens?**

**A:** This framework uses tokens heavily due to comprehensive context loading and sub-agent usage. I strongly recommend a Claude Code Max 20x subscription over pay-per-token API usage. The Claude 4 Opus model currently performs best for complex instruction following.

**Q: Can I use other coding consultant MCPs like Zen instead for Gemini Consultation?**

**A:** While technically possible, the templates and hooks are specifically configured and optimized for my Gemini MCP server (available through the link provided during installation). Using alternative coding consultant MCPs would require adjusting the templates, hooks, and potentially the command structures to match their specific interfaces and capabilities.

**Q: Can I use this framework with an existing project?**

**A:** Yes! The framework works well with existing projects. When installing, check if you already have a project structure or CLAUDE.md file and adjust accordingly during the setup prompts. To get started with an existing codebase, use Claude Code with sub-agents to understand your project and create the initial project-structure.md:

```
"Read and understand the project_structure.md template in docs/ai-context/project_structure.md. Your task is to fill out this template with our project's details. For this send out sub agents in parallel across the whole code base. Once the sub agents get back, ultrathink and create the markdown file."
```

After creating the project structure, use the framework's documentation generation system to create component-level and feature-level context files:

```
/create-docs "[your-main-component-path]/CONTEXT.md"
```

This approach lets the framework learn your existing architecture and systematically create appropriate documentation that matches your current project structure.

## Connect

Feel free to connect with me on [LinkedIn](https://www.linkedin.com/in/peterkrueck/) if you have questions, need clarification, or wish to provide feedback.
# [PROJECT NAME] - AI Context Template (claude-master)

## 1. Project Overview
- **Vision:** [Describe your project's vision and goals]
- **Current Phase:** [Current development phase and status]
- **Key Architecture:** [High-level architecture description]
- **Development Strategy:** [Development approach and strategy notes]

## 2. Project Structure

**âš ï¸ CRITICAL: AI agents MUST read the [Project Structure documentation](/docs/ai-context/project-structure.md) before attempting any task to understand the complete technology stack, file tree and project organization.**

[Project Name] follows a [describe architecture pattern]. For the complete tech stack and file tree structure, see [docs/ai-context/project-structure.md](/docs/ai-context/project-structure.md).

## 3. Coding Standards & AI Instructions

### General Instructions
- Your most important job is to manage your own context. Always read any relevant files BEFORE planning changes.
- When updating documentation, keep updates concise and on point to prevent bloat.
- Write code following KISS, YAGNI, and DRY principles.
- When in doubt follow proven best practices for implementation.
- Do not commit to git without user approval.
- Do not run any servers, rather tell the user to run servers for testing.
- Always consider industry standard libraries/frameworks first over custom implementations.
- Never mock anything. Never use placeholders. Never omit code.
- Apply SOLID principles where relevant. Use modern framework features rather than reinventing solutions.
- Be brutally honest about whether an idea is good or bad.
- Make side effects explicit and minimal.
- Design database schema to be evolution-friendly (avoid breaking changes).


### File Organization & Modularity
- Default to creating multiple small, focused files rather than large monolithic ones
- Each file should have a single responsibility and clear purpose
- Keep files under 350 lines when possible - split larger files by extracting utilities, constants, types, or logical components into separate modules
- Separate concerns: utilities, constants, types, components, and business logic into different files
- Prefer composition over inheritance - use inheritance only for true 'is-a' relationships, favor composition for 'has-a' or behavior mixing

- Follow existing project structure and conventions - place files in appropriate directories. Create new directories and move files if deemed appropriate.
- Use well defined sub-directories to keep things organized and scalable
- Structure projects with clear folder hierarchies and consistent naming conventions
- Import/export properly - design for reusability and maintainability

### Type Hints (REQUIRED)
- **Always** use type hints for function parameters and return values
- Use `from typing import` for complex types
- Prefer `Optional[T]` over `Union[T, None]`
- Use Pydantic models for data structures

```python
# Good
from typing import Optional, List, Dict, Tuple

async def process_audio(
    audio_data: bytes,
    session_id: str,
    language: Optional[str] = None
) -> Tuple[bytes, Dict[str, Any]]:
    """Process audio through the pipeline."""
    pass
```

### Naming Conventions
- **Classes**: PascalCase (e.g., `VoicePipeline`)
- **Functions/Methods**: snake_case (e.g., `process_audio`)
- **Constants**: UPPER_SNAKE_CASE (e.g., `MAX_AUDIO_SIZE`)
- **Private methods**: Leading underscore (e.g., `_validate_input`)
- **Pydantic Models**: PascalCase with `Schema` suffix (e.g., `ChatRequestSchema`, `UserSchema`)


### Documentation Requirements
- Every module needs a docstring
- Every public function needs a docstring
- Use Google-style docstrings
- Include type information in docstrings

```python
def calculate_similarity(text1: str, text2: str) -> float:
    """Calculate semantic similarity between two texts.

    Args:
        text1: First text to compare
        text2: Second text to compare

    Returns:
        Similarity score between 0 and 1

    Raises:
        ValueError: If either text is empty
    """
    pass
```

### Security First
- Never trust external inputs - validate everything at the boundaries
- Keep secrets in environment variables, never in code
- Log security events (login attempts, auth failures, rate limits, permission denials) but never log sensitive data (audio, conversation content, tokens, personal info)
- Authenticate users at the API gateway level - never trust client-side tokens
- Use Row Level Security (RLS) to enforce data isolation between users
- Design auth to work across all client types consistently
- Use secure authentication patterns for your platform
- Validate all authentication tokens server-side before creating sessions
- Sanitize all user inputs before storing or processing

### Error Handling
- Use specific exceptions over generic ones
- Always log errors with context
- Provide helpful error messages
- Fail securely - errors shouldn't reveal system internals

### Observable Systems & Logging Standards
- Every request needs a correlation ID for debugging
- Structure logs for machines, not humans - use JSON format with consistent fields (timestamp, level, correlation_id, event, context) for automated analysis
- Make debugging possible across service boundaries

### State Management
- Have one source of truth for each piece of state
- Make state changes explicit and traceable
- Design for multi-service voice processing - use session IDs for state coordination, avoid storing conversation data in server memory
- Keep conversation history lightweight (text, not audio)

### API Design Principles
- RESTful design with consistent URL patterns
- Use HTTP status codes correctly
- Version APIs from day one (/v1/, /v2/)
- Support pagination for list endpoints
- Use consistent JSON response format:
  - Success: `{ "data": {...}, "error": null }`
  - Error: `{ "data": null, "error": {"message": "...", "code": "..."} }`


## 4. Multi-Agent Workflows & Context Injection

### Automatic Context Injection for Sub-Agents
When using the Task tool to spawn sub-agents, the core project context (CLAUDE.md, project-structure.md, docs-overview.md) is automatically injected into their prompts via the subagent-context-injector hook. This ensures all sub-agents have immediate access to essential project documentation without the need of manual specification in each Task prompt.


## 5. MCP Server Integrations

### Gemini Consultation Server
**When to use:**
- Complex coding problems requiring deep analysis or multiple approaches
- Code reviews and architecture discussions
- Debugging complex issues across multiple files
- Performance optimization and refactoring guidance
- Detailed explanations of complex implementations
- Highly security relevant tasks

**Automatic Context Injection:**
- The kit's `gemini-context-injector.sh` hook automatically includes two key files for new sessions:
  - `/docs/ai-context/project-structure.md` - Complete project structure and tech stack
  - `/MCP-ASSISTANT-RULES.md` - Your project-specific coding standards and guidelines
- This ensures Gemini always has comprehensive understanding of your technology stack, architecture, and project standards

**Usage patterns:**
```python
# New consultation session (project structure auto-attached by hooks)
mcp__gemini__consult_gemini(
    specific_question="How should I optimize this voice pipeline?",
    problem_description="Need to reduce latency in real-time audio processing",
    code_context="Current pipeline processes audio sequentially...",
    attached_files=[
        "src/core/pipelines/voice_pipeline.py"  # Your specific files
    ],
    preferred_approach="optimize"
)

# Follow-up in existing session
mcp__gemini__consult_gemini(
    specific_question="What about memory usage?",
    session_id="session_123",
    additional_context="Implemented your suggestions, now seeing high memory usage"
)
```

**Key capabilities:**
- Persistent conversation sessions with context retention
- File attachment and caching for multi-file analysis
- Specialized assistance modes (solution, review, debug, optimize, explain)
- Session management for complex, multi-step problems

**Important:** Treat Gemini's responses as advisory feedback. Evaluate the suggestions critically, incorporate valuable insights into your solution, then proceed with your implementation.

### Context7 Documentation Server
**Repository**: [Context7 MCP Server](https://github.com/upstash/context7)

**When to use:**
- Working with external libraries/frameworks (React, FastAPI, Next.js, etc.)
- Need current documentation beyond training cutoff
- Implementing new integrations or features with third-party tools
- Troubleshooting library-specific issues

**Usage patterns:**
```python
# Resolve library name to Context7 ID
mcp__context7__resolve_library_id(libraryName="react")

# Fetch focused documentation
mcp__context7__get_library_docs(
    context7CompatibleLibraryID="/facebook/react",
    topic="hooks",
    tokens=8000
)
```

**Key capabilities:**
- Up-to-date library documentation access
- Topic-focused documentation retrieval
- Support for specific library versions
- Integration with current development practices



## 6. Post-Task Completion Protocol
After completing any coding task, follow this checklist:

### 1. Type Safety & Quality Checks
Run the appropriate commands based on what was modified:
- **Python projects**: Run mypy type checking
- **TypeScript projects**: Run tsc --noEmit
- **Other languages**: Run appropriate linting/type checking tools

### 2. Verification
- Ensure all type checks pass before considering the task complete
- If type errors are found, fix them before marking the task as done# CLAUDE.md 

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Beta Development Guidelines

**Local-only deployment** - each user runs their own instance.

### Core Principles

- **No backwards compatibility** - remove deprecated code immediately
- **Detailed errors over graceful failures** - we want to identify and fix issues fast
- **Break things to improve them** - beta is for rapid iteration

### Error Handling

**Core Principle**: In beta, we need to intelligently decide when to fail hard and fast to quickly address issues, and when to allow processes to complete in critical services despite failures. Read below carefully and make intelligent decisions on a case-by-case basis.

#### When to Fail Fast and Loud (Let it Crash!)

These errors should stop execution and bubble up immediately: (except for crawling flows)

- **Service startup failures** - If credentials, database, or any service can't initialize, the system should crash with a clear error
- **Missing configuration** - Missing environment variables or invalid settings should stop the system
- **Database connection failures** - Don't hide connection issues, expose them
- **Authentication/authorization failures** - Security errors must be visible and halt the operation
- **Data corruption or validation errors** - Never silently accept bad data, Pydantic should raise
- **Critical dependencies unavailable** - If a required service is down, fail immediately
- **Invalid data that would corrupt state** - Never store zero embeddings, null foreign keys, or malformed JSON

#### When to Complete but Log Detailed Errors

These operations should continue but track and report failures clearly:

- **Batch processing** - When crawling websites or processing documents, complete what you can and report detailed failures for each item
- **Background tasks** - Embedding generation, async jobs should finish the queue but log failures
- **WebSocket events** - Don't crash on a single event failure, log it and continue serving other clients
- **Optional features** - If projects/tasks are disabled, log and skip rather than crash
- **External API calls** - Retry with exponential backoff, then fail with a clear message about what service failed and why

#### Critical Nuance: Never Accept Corrupted Data

When a process should continue despite failures, it must **skip the failed item entirely** rather than storing corrupted data:

**âŒ WRONG - Silent Corruption:**

```python
try:
    embedding = create_embedding(text)
except Exception as e:
    embedding = [0.0] * 1536  # NEVER DO THIS - corrupts database
    store_document(doc, embedding)
```

**âœ… CORRECT - Skip Failed Items:**

```python
try:
    embedding = create_embedding(text)
    store_document(doc, embedding)  # Only store on success
except Exception as e:
    failed_items.append({'doc': doc, 'error': str(e)})
    logger.error(f"Skipping document {doc.id}: {e}")
    # Continue with next document, don't store anything
```

**âœ… CORRECT - Batch Processing with Failure Tracking:**

```python
def process_batch(items):
    results = {'succeeded': [], 'failed': []}

    for item in items:
        try:
            result = process_item(item)
            results['succeeded'].append(result)
        except Exception as e:
            results['failed'].append({
                'item': item,
                'error': str(e),
                'traceback': traceback.format_exc()
            })
            logger.error(f"Failed to process {item.id}: {e}")

    # Always return both successes and failures
    return results
```

#### Error Message Guidelines

- Include context about what was being attempted when the error occurred
- Preserve full stack traces with `exc_info=True` in Python logging
- Use specific exception types, not generic Exception catching
- Include relevant IDs, URLs, or data that helps debug the issue
- Never return None/null to indicate failure - raise an exception with details
- For batch operations, always report both success count and detailed failure list

### Code Quality

- Remove dead code immediately rather than maintaining it - no backward compatibility or legacy functions
- Prioritize functionality over production-ready patterns
- Focus on user experience and feature completeness
- When updating code, don't reference what is changing (avoid keywords like LEGACY, CHANGED, REMOVED), instead focus on comments that document just the functionality of the code
- When commenting on code in the codebase, only comment on the functionality and reasoning behind the code. Refrain from speaking to Archon being in "beta" or referencing anything else that comes from these global rules.

## Development Commands

### Frontend (archon-ui-main/)

```bash
npm run dev              # Start development server on port 3737
npm run build            # Build for production
npm run lint             # Run ESLint on legacy code (excludes /features)
npm run lint:files path/to/file.tsx  # Lint specific files

# Biome for /src/features directory only
npm run biome            # Check features directory
npm run biome:fix        # Auto-fix issues
npm run biome:format     # Format code (120 char lines)
npm run biome:ai         # Machine-readable JSON output for AI
npm run biome:ai-fix     # Auto-fix with JSON output

# Testing
npm run test             # Run all tests in watch mode
npm run test:ui          # Run with Vitest UI interface
npm run test:coverage:stream  # Run once with streaming output
vitest run src/features/projects  # Test specific directory

# TypeScript
npx tsc --noEmit         # Check all TypeScript errors
npx tsc --noEmit 2>&1 | grep "src/features"  # Check features only
```

### Backend (python/)

```bash
# Using uv package manager (preferred)
uv sync --group all      # Install all dependencies
uv run python -m src.server.main  # Run server locally on 8181
uv run pytest            # Run all tests
uv run pytest tests/test_api_essentials.py -v  # Run specific test
uv run ruff check        # Run linter
uv run ruff check --fix  # Auto-fix linting issues
uv run mypy src/         # Type check

# Docker operations
docker compose up --build -d       # Start all services
docker compose --profile backend up -d  # Backend only (for hybrid dev)
docker compose logs -f archon-server   # View server logs
docker compose logs -f archon-mcp      # View MCP server logs
docker compose restart archon-server   # Restart after code changes
docker compose down      # Stop all services
docker compose down -v   # Stop and remove volumes
```

### Quick Workflows

```bash
# Hybrid development (recommended) - backend in Docker, frontend local
make dev                 # Or manually: docker compose --profile backend up -d && cd archon-ui-main && npm run dev

# Full Docker mode
make dev-docker          # Or: docker compose up --build -d

# Run linters before committing
make lint                # Runs both frontend and backend linters
make lint-fe             # Frontend only (ESLint + Biome)
make lint-be             # Backend only (Ruff + MyPy)

# Testing
make test                # Run all tests
make test-fe             # Frontend tests only
make test-be             # Backend tests only
```

## Architecture Overview

Archon Beta is a microservices-based knowledge management system with MCP (Model Context Protocol) integration:

### Service Architecture

- **Frontend (port 3737)**: React + TypeScript + Vite + TailwindCSS
  - **Dual UI Strategy**:
    - `/features` - Modern vertical slice with Radix UI primitives + TanStack Query
    - `/components` - Legacy custom components (being migrated)
  - **State Management**: TanStack Query for all data fetching (no prop drilling)
  - **Styling**: Tron-inspired glassmorphism with Tailwind CSS
  - **Linting**: Biome for `/features`, ESLint for legacy code

- **Main Server (port 8181)**: FastAPI with HTTP polling for updates
  - Handles all business logic, database operations, and external API calls
  - WebSocket support removed in favor of HTTP polling with ETag caching

- **MCP Server (port 8051)**: Lightweight HTTP-based MCP protocol server
  - Provides tools for AI assistants (Claude, Cursor, Windsurf)
  - Exposes knowledge search, task management, and project operations

- **Agents Service (port 8052)**: PydanticAI agents for AI/ML operations
  - Handles complex AI workflows and document processing

- **Database**: Supabase (PostgreSQL + pgvector for embeddings)
  - Cloud or local Supabase both supported
  - pgvector for semantic search capabilities

### Frontend Architecture Details

#### Vertical Slice Architecture (/features)

Features are organized by domain hierarchy with self-contained modules:

```
src/features/
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ primitives/    # Radix UI base components
â”‚   â”œâ”€â”€ hooks/         # Shared UI hooks (useSmartPolling, etc)
â”‚   â””â”€â”€ types/         # UI type definitions
â”œâ”€â”€ projects/
â”‚   â”œâ”€â”€ components/    # Project UI components
â”‚   â”œâ”€â”€ hooks/         # Project hooks (useProjectQueries, etc)
â”‚   â”œâ”€â”€ services/      # Project API services
â”‚   â”œâ”€â”€ types/         # Project type definitions
â”‚   â”œâ”€â”€ tasks/         # Tasks sub-feature (nested under projects)
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ hooks/     # Task-specific hooks
â”‚   â”‚   â”œâ”€â”€ services/  # Task API services
â”‚   â”‚   â””â”€â”€ types/
â”‚   â””â”€â”€ documents/     # Documents sub-feature
â”‚       â”œâ”€â”€ components/
â”‚       â”œâ”€â”€ services/
â”‚       â””â”€â”€ types/
```

#### TanStack Query Patterns

All data fetching uses TanStack Query with consistent patterns:

```typescript
// Query keys factory pattern
export const projectKeys = {
  all: ["projects"] as const,
  lists: () => [...projectKeys.all, "list"] as const,
  detail: (id: string) => [...projectKeys.all, "detail", id] as const,
};

// Smart polling with visibility awareness
const { refetchInterval } = useSmartPolling(10000); // Pauses when tab inactive

// Optimistic updates with rollback
useMutation({
  onMutate: async (data) => {
    await queryClient.cancelQueries(key);
    const previous = queryClient.getQueryData(key);
    queryClient.setQueryData(key, optimisticData);
    return { previous };
  },
  onError: (err, vars, context) => {
    if (context?.previous) {
      queryClient.setQueryData(key, context.previous);
    }
  },
});
```

### Backend Architecture Details

#### Service Layer Pattern

```python
# API Route -> Service -> Database
# src/server/api_routes/projects.py
@router.get("/{project_id}")
async def get_project(project_id: str):
    return await project_service.get_project(project_id)

# src/server/services/project_service.py
async def get_project(project_id: str):
    # Business logic here
    return await db.fetch_project(project_id)
```

#### Error Handling Patterns

```python
# Use specific exceptions
class ProjectNotFoundError(Exception): pass
class ValidationError(Exception): pass

# Rich error responses
@app.exception_handler(ProjectNotFoundError)
async def handle_not_found(request, exc):
    return JSONResponse(
        status_code=404,
        content={"detail": str(exc), "type": "not_found"}
    )
```

## Polling Architecture

### HTTP Polling (replaced Socket.IO)

- **Polling intervals**: 1-2s for active operations, 5-10s for background data
- **ETag caching**: Reduces bandwidth by ~70% via 304 Not Modified responses
- **Smart pausing**: Stops polling when browser tab is inactive
- **Progress endpoints**: `/api/progress/{id}` for operation tracking

### Key Polling Hooks

- `useSmartPolling` - Adjusts interval based on page visibility/focus
- `useCrawlProgressPolling` - Specialized for crawl progress with auto-cleanup
- `useProjectTasks` - Smart polling for task lists

## Database Schema

Key tables in Supabase:

- `sources` - Crawled websites and uploaded documents
  - Stores metadata, crawl status, and configuration
- `documents` - Processed document chunks with embeddings
  - Text chunks with vector embeddings for semantic search
- `projects` - Project management (optional feature)
  - Contains features array, documents, and metadata
- `tasks` - Task tracking linked to projects
  - Status: todo, doing, review, done
  - Assignee: User, Archon, AI IDE Agent
- `code_examples` - Extracted code snippets
  - Language, summary, and relevance metadata

## API Naming Conventions

### Task Status Values

Use database values directly (no UI mapping):

- `todo`, `doing`, `review`, `done`

### Service Method Patterns

- `get[Resource]sByProject(projectId)` - Scoped queries
- `get[Resource](id)` - Single resource
- `create[Resource](data)` - Create operations
- `update[Resource](id, updates)` - Updates
- `delete[Resource](id)` - Soft deletes

### State Naming

- `is[Action]ing` - Loading states (e.g., `isSwitchingProject`)
- `[resource]Error` - Error messages
- `selected[Resource]` - Current selection

## Environment Variables

Required in `.env`:

```bash
SUPABASE_URL=https://your-project.supabase.co  # Or http://host.docker.internal:8000 for local
SUPABASE_SERVICE_KEY=your-service-key-here      # Use legacy key format for cloud Supabase
```

Optional:

```bash
LOGFIRE_TOKEN=your-logfire-token      # For observability
LOG_LEVEL=INFO                         # DEBUG, INFO, WARNING, ERROR
ARCHON_SERVER_PORT=8181               # Server port
ARCHON_MCP_PORT=8051                 # MCP server port
ARCHON_UI_PORT=3737                  # Frontend port
```

## Common Development Tasks

### Add a new API endpoint

1. Create route handler in `python/src/server/api_routes/`
2. Add service logic in `python/src/server/services/`
3. Include router in `python/src/server/main.py`
4. Update frontend service in `archon-ui-main/src/features/[feature]/services/`

### Add a new UI component in features directory

1. Use Radix UI primitives from `src/features/ui/primitives/`
2. Create component in relevant feature folder under `src/features/[feature]/components/`
3. Define types in `src/features/[feature]/types/`
4. Use TanStack Query hook from `src/features/[feature]/hooks/`
5. Apply Tron-inspired glassmorphism styling with Tailwind

### Debug MCP connection issues

1. Check MCP health: `curl http://localhost:8051/health`
2. View MCP logs: `docker compose logs archon-mcp`
3. Test tool execution via UI MCP page
4. Verify Supabase connection and credentials

### Fix TypeScript/Linting Issues

```bash
# TypeScript errors in features
npx tsc --noEmit 2>&1 | grep "src/features"

# Biome auto-fix for features
npm run biome:fix

# ESLint for legacy code
npm run lint:files src/components/SomeComponent.tsx
```

## Code Quality Standards

### Frontend

- **TypeScript**: Strict mode enabled, no implicit any
- **Biome** for `/src/features/`: 120 char lines, double quotes, trailing commas
- **ESLint** for legacy code: Standard React rules
- **Testing**: Vitest with React Testing Library

### Backend

- **Python 3.12** with 120 character line length
- **Ruff** for linting - checks for errors, warnings, unused imports
- **Mypy** for type checking - ensures type safety
- **Pytest** for testing with async support

## MCP Tools Available

When connected to Client/Cursor/Windsurf:

- `archon:perform_rag_query` - Search knowledge base
- `archon:search_code_examples` - Find code snippets
- `archon:create_project` - Create new project
- `archon:list_projects` - List all projects
- `archon:create_task` - Create task in project
- `archon:list_tasks` - List and filter tasks
- `archon:update_task` - Update task status/details
- `archon:get_available_sources` - List knowledge sources

## Important Notes

- Projects feature is optional - toggle in Settings UI
- All services communicate via HTTP, not gRPC
- HTTP polling handles all updates
- Frontend uses Vite proxy for API calls in development
- Python backend uses `uv` for dependency management
- Docker Compose handles service orchestration
- TanStack Query for all data fetching - NO PROP DRILLING
- Vertical slice architecture in `/features` - features own their sub-features
# Claude Code Configuration - SPARC Development Environment

## ğŸš¨ CRITICAL: CONCURRENT EXECUTION & FILE MANAGEMENT

**ABSOLUTE RULES**:
1. ALL operations MUST be concurrent/parallel in a single message
2. **NEVER save working files, text/mds and tests to the root folder**
3. ALWAYS organize files in appropriate subdirectories
4. **USE CLAUDE CODE'S TASK TOOL** for spawning agents concurrently, not just MCP

### âš¡ GOLDEN RULE: "1 MESSAGE = ALL RELATED OPERATIONS"

**MANDATORY PATTERNS:**
- **TodoWrite**: ALWAYS batch ALL todos in ONE call (5-10+ todos minimum)
- **Task tool (Claude Code)**: ALWAYS spawn ALL agents in ONE message with full instructions
- **File operations**: ALWAYS batch ALL reads/writes/edits in ONE message
- **Bash commands**: ALWAYS batch ALL terminal operations in ONE message
- **Memory operations**: ALWAYS batch ALL memory store/retrieve in ONE message

### ğŸ¯ CRITICAL: Claude Code Task Tool for Agent Execution

**Claude Code's Task tool is the PRIMARY way to spawn agents:**
```javascript
// âœ… CORRECT: Use Claude Code's Task tool for parallel agent execution
[Single Message]:
  Task("Research agent", "Analyze requirements and patterns...", "researcher")
  Task("Coder agent", "Implement core features...", "coder")
  Task("Tester agent", "Create comprehensive tests...", "tester")
  Task("Reviewer agent", "Review code quality...", "reviewer")
  Task("Architect agent", "Design system architecture...", "system-architect")
```

**MCP tools are ONLY for coordination setup:**
- `mcp__claude-flow__swarm_init` - Initialize coordination topology
- `mcp__claude-flow__agent_spawn` - Define agent types for coordination
- `mcp__claude-flow__task_orchestrate` - Orchestrate high-level workflows

### ğŸ“ File Organization Rules

**NEVER save to root folder. Use these directories:**
- `/src` - Source code files
- `/tests` - Test files
- `/docs` - Documentation and markdown files
- `/config` - Configuration files
- `/scripts` - Utility scripts
- `/examples` - Example code

## Project Overview

This project uses SPARC (Specification, Pseudocode, Architecture, Refinement, Completion) methodology with Claude-Flow orchestration for systematic Test-Driven Development.

## SPARC Commands

### Core Commands
- `npx claude-flow sparc modes` - List available modes
- `npx claude-flow sparc run <mode> "<task>"` - Execute specific mode
- `npx claude-flow sparc tdd "<feature>"` - Run complete TDD workflow
- `npx claude-flow sparc info <mode>` - Get mode details

### Batchtools Commands
- `npx claude-flow sparc batch <modes> "<task>"` - Parallel execution
- `npx claude-flow sparc pipeline "<task>"` - Full pipeline processing
- `npx claude-flow sparc concurrent <mode> "<tasks-file>"` - Multi-task processing

### Build Commands
- `npm run build` - Build project
- `npm run test` - Run tests
- `npm run lint` - Linting
- `npm run typecheck` - Type checking

## SPARC Workflow Phases

1. **Specification** - Requirements analysis (`sparc run spec-pseudocode`)
2. **Pseudocode** - Algorithm design (`sparc run spec-pseudocode`)
3. **Architecture** - System design (`sparc run architect`)
4. **Refinement** - TDD implementation (`sparc tdd`)
5. **Completion** - Integration (`sparc run integration`)

## Code Style & Best Practices

- **Modular Design**: Files under 500 lines
- **Environment Safety**: Never hardcode secrets
- **Test-First**: Write tests before implementation
- **Clean Architecture**: Separate concerns
- **Documentation**: Keep updated

## ğŸš€ Available Agents (54 Total)

### Core Development
`coder`, `reviewer`, `tester`, `planner`, `researcher`

### Swarm Coordination
`hierarchical-coordinator`, `mesh-coordinator`, `adaptive-coordinator`, `collective-intelligence-coordinator`, `swarm-memory-manager`

### Consensus & Distributed
`byzantine-coordinator`, `raft-manager`, `gossip-coordinator`, `consensus-builder`, `crdt-synchronizer`, `quorum-manager`, `security-manager`

### Performance & Optimization
`perf-analyzer`, `performance-benchmarker`, `task-orchestrator`, `memory-coordinator`, `smart-agent`

### GitHub & Repository
`github-modes`, `pr-manager`, `code-review-swarm`, `issue-tracker`, `release-manager`, `workflow-automation`, `project-board-sync`, `repo-architect`, `multi-repo-swarm`

### SPARC Methodology
`sparc-coord`, `sparc-coder`, `specification`, `pseudocode`, `architecture`, `refinement`

### Specialized Development
`backend-dev`, `mobile-dev`, `ml-developer`, `cicd-engineer`, `api-docs`, `system-architect`, `code-analyzer`, `base-template-generator`

### Testing & Validation
`tdd-london-swarm`, `production-validator`

### Migration & Planning
`migration-planner`, `swarm-init`

## ğŸ¯ Claude Code vs MCP Tools

### Claude Code Handles ALL EXECUTION:
- **Task tool**: Spawn and run agents concurrently for actual work
- File operations (Read, Write, Edit, MultiEdit, Glob, Grep)
- Code generation and programming
- Bash commands and system operations
- Implementation work
- Project navigation and analysis
- TodoWrite and task management
- Git operations
- Package management
- Testing and debugging

### MCP Tools ONLY COORDINATE:
- Swarm initialization (topology setup)
- Agent type definitions (coordination patterns)
- Task orchestration (high-level planning)
- Memory management
- Neural features
- Performance tracking
- GitHub integration

**KEY**: MCP coordinates the strategy, Claude Code's Task tool executes with real agents.

## ğŸš€ Quick Setup

```bash
# Add MCP servers (Claude Flow required, others optional)
claude mcp add claude-flow npx claude-flow@alpha mcp start
claude mcp add ruv-swarm npx ruv-swarm mcp start  # Optional: Enhanced coordination
claude mcp add flow-nexus npx flow-nexus@latest mcp start  # Optional: Cloud features
```

## MCP Tool Categories

### Coordination
`swarm_init`, `agent_spawn`, `task_orchestrate`

### Monitoring
`swarm_status`, `agent_list`, `agent_metrics`, `task_status`, `task_results`

### Memory & Neural
`memory_usage`, `neural_status`, `neural_train`, `neural_patterns`

### GitHub Integration
`github_swarm`, `repo_analyze`, `pr_enhance`, `issue_triage`, `code_review`

### System
`benchmark_run`, `features_detect`, `swarm_monitor`

### Flow-Nexus MCP Tools (Optional Advanced Features)
Flow-Nexus extends MCP capabilities with 70+ cloud-based orchestration tools:

**Key MCP Tool Categories:**
- **Swarm & Agents**: `swarm_init`, `swarm_scale`, `agent_spawn`, `task_orchestrate`
- **Sandboxes**: `sandbox_create`, `sandbox_execute`, `sandbox_upload` (cloud execution)
- **Templates**: `template_list`, `template_deploy` (pre-built project templates)
- **Neural AI**: `neural_train`, `neural_patterns`, `seraphina_chat` (AI assistant)
- **GitHub**: `github_repo_analyze`, `github_pr_manage` (repository management)
- **Real-time**: `execution_stream_subscribe`, `realtime_subscribe` (live monitoring)
- **Storage**: `storage_upload`, `storage_list` (cloud file management)

**Authentication Required:**
- Register: `mcp__flow-nexus__user_register` or `npx flow-nexus@latest register`
- Login: `mcp__flow-nexus__user_login` or `npx flow-nexus@latest login`
- Access 70+ specialized MCP tools for advanced orchestration

## ğŸš€ Agent Execution Flow with Claude Code

### The Correct Pattern:

1. **Optional**: Use MCP tools to set up coordination topology
2. **REQUIRED**: Use Claude Code's Task tool to spawn agents that do actual work
3. **REQUIRED**: Each agent runs hooks for coordination
4. **REQUIRED**: Batch all operations in single messages

### Example Full-Stack Development:

```javascript
// Single message with all agent spawning via Claude Code's Task tool
[Parallel Agent Execution]:
  Task("Backend Developer", "Build REST API with Express. Use hooks for coordination.", "backend-dev")
  Task("Frontend Developer", "Create React UI. Coordinate with backend via memory.", "coder")
  Task("Database Architect", "Design PostgreSQL schema. Store schema in memory.", "code-analyzer")
  Task("Test Engineer", "Write Jest tests. Check memory for API contracts.", "tester")
  Task("DevOps Engineer", "Setup Docker and CI/CD. Document in memory.", "cicd-engineer")
  Task("Security Auditor", "Review authentication. Report findings via hooks.", "reviewer")
  
  // All todos batched together
  TodoWrite { todos: [...8-10 todos...] }
  
  // All file operations together
  Write "backend/server.js"
  Write "frontend/App.jsx"
  Write "database/schema.sql"
```

## ğŸ“‹ Agent Coordination Protocol

### Every Agent Spawned via Task Tool MUST:

**1ï¸âƒ£ BEFORE Work:**
```bash
npx claude-flow@alpha hooks pre-task --description "[task]"
npx claude-flow@alpha hooks session-restore --session-id "swarm-[id]"
```

**2ï¸âƒ£ DURING Work:**
```bash
npx claude-flow@alpha hooks post-edit --file "[file]" --memory-key "swarm/[agent]/[step]"
npx claude-flow@alpha hooks notify --message "[what was done]"
```

**3ï¸âƒ£ AFTER Work:**
```bash
npx claude-flow@alpha hooks post-task --task-id "[task]"
npx claude-flow@alpha hooks session-end --export-metrics true
```

## ğŸ¯ Concurrent Execution Examples

### âœ… CORRECT WORKFLOW: MCP Coordinates, Claude Code Executes

```javascript
// Step 1: MCP tools set up coordination (optional, for complex tasks)
[Single Message - Coordination Setup]:
  mcp__claude-flow__swarm_init { topology: "mesh", maxAgents: 6 }
  mcp__claude-flow__agent_spawn { type: "researcher" }
  mcp__claude-flow__agent_spawn { type: "coder" }
  mcp__claude-flow__agent_spawn { type: "tester" }

// Step 2: Claude Code Task tool spawns ACTUAL agents that do the work
[Single Message - Parallel Agent Execution]:
  // Claude Code's Task tool spawns real agents concurrently
  Task("Research agent", "Analyze API requirements and best practices. Check memory for prior decisions.", "researcher")
  Task("Coder agent", "Implement REST endpoints with authentication. Coordinate via hooks.", "coder")
  Task("Database agent", "Design and implement database schema. Store decisions in memory.", "code-analyzer")
  Task("Tester agent", "Create comprehensive test suite with 90% coverage.", "tester")
  Task("Reviewer agent", "Review code quality and security. Document findings.", "reviewer")
  
  // Batch ALL todos in ONE call
  TodoWrite { todos: [
    {id: "1", content: "Research API patterns", status: "in_progress", priority: "high"},
    {id: "2", content: "Design database schema", status: "in_progress", priority: "high"},
    {id: "3", content: "Implement authentication", status: "pending", priority: "high"},
    {id: "4", content: "Build REST endpoints", status: "pending", priority: "high"},
    {id: "5", content: "Write unit tests", status: "pending", priority: "medium"},
    {id: "6", content: "Integration tests", status: "pending", priority: "medium"},
    {id: "7", content: "API documentation", status: "pending", priority: "low"},
    {id: "8", content: "Performance optimization", status: "pending", priority: "low"}
  ]}
  
  // Parallel file operations
  Bash "mkdir -p app/{src,tests,docs,config}"
  Write "app/package.json"
  Write "app/src/server.js"
  Write "app/tests/server.test.js"
  Write "app/docs/API.md"
```

### âŒ WRONG (Multiple Messages):
```javascript
Message 1: mcp__claude-flow__swarm_init
Message 2: Task("agent 1")
Message 3: TodoWrite { todos: [single todo] }
Message 4: Write "file.js"
// This breaks parallel coordination!
```

## Performance Benefits

- **84.8% SWE-Bench solve rate**
- **32.3% token reduction**
- **2.8-4.4x speed improvement**
- **27+ neural models**

## Hooks Integration

### Pre-Operation
- Auto-assign agents by file type
- Validate commands for safety
- Prepare resources automatically
- Optimize topology by complexity
- Cache searches

### Post-Operation
- Auto-format code
- Train neural patterns
- Update memory
- Analyze performance
- Track token usage

### Session Management
- Generate summaries
- Persist state
- Track metrics
- Restore context
- Export workflows

## Advanced Features (v2.0.0)

- ğŸš€ Automatic Topology Selection
- âš¡ Parallel Execution (2.8-4.4x speed)
- ğŸ§  Neural Training
- ğŸ“Š Bottleneck Analysis
- ğŸ¤– Smart Auto-Spawning
- ğŸ›¡ï¸ Self-Healing Workflows
- ğŸ’¾ Cross-Session Memory
- ğŸ”— GitHub Integration

## Integration Tips

1. Start with basic swarm init
2. Scale agents gradually
3. Use memory for context
4. Monitor progress regularly
5. Train patterns from success
6. Enable hooks automation
7. Use GitHub tools first

## Support

- Documentation: https://github.com/ruvnet/claude-flow
- Issues: https://github.com/ruvnet/claude-flow/issues
- Flow-Nexus Platform: https://flow-nexus.ruv.io (registration required for cloud features)

---

Remember: **Claude Flow coordinates, Claude Code creates!**

# important-instruction-reminders
Do what has been asked; nothing more, nothing less.
NEVER create files unless they're absolutely necessary for achieving your goal.
ALWAYS prefer editing an existing file to creating a new one.
NEVER proactively create documentation files (*.md) or README files. Only create documentation files if explicitly requested by the User.
Never save working files, text/mds and tests to the root folder.
# NYRA Claude-Flow & Flow-Nexus Workflow Wizard Guide

## ğŸš€ Overview

This guide provides comprehensive instructions for using claude-flow and flow-nexus workflow wizards for document cleaning, ingestion, and multi-agent coordination in the NYRA mortgage assistant ecosystem.

## ğŸ“‹ Table of Contents

1. [Claude-Flow Workflow Wizards](#claude-flow-workflow-wizards)
2. [Flow-Nexus Workflow Management](#flow-nexus-workflow-management)
3. [Hive-Mind and Swarm Coordination](#hive-mind-and-swarm-coordination)
4. [Document Cleaning Workflows](#document-cleaning-workflows)
5. [MCP Server Integration](#mcp-server-integration)
6. [Troubleshooting](#troubleshooting)

---

## ğŸ¤– Claude-Flow Workflow Wizards

### Getting Started with Claude-Flow

#### 1. Initialize Your Environment
```powershell
# Navigate to project root
cd "C:\Dev\DevProjects\Personal-Projects\Project-Nyra"

# Initialize claude-flow (if not already done)
npx claude-flow@alpha init --force

# Check system status
npx claude-flow@alpha status
```

#### 2. Available Claude-Flow Commands

##### **Hive-Mind Operations**
```bash
# Initialize hive-mind system
npx claude-flow@alpha hive-mind init

# Spawn a new hive-mind session with agents
npx claude-flow@alpha hive-mind spawn "clean documents and extract insights" --claude

# Resume an existing session
npx claude-flow@alpha hive-mind resume session-xxxxx-xxxxx

# List active hive-mind sessions  
npx claude-flow@alpha hive-mind list

# Get hive-mind status
npx claude-flow@alpha hive-mind status
```

##### **Agent Management**
```bash
# List available agents
npx claude-flow@alpha agents list

# Get agent categories
npx claude-flow@alpha agents categories

# Spawn specific agent types
npx claude-flow@alpha swarm "analyze mortgage documents" --agents researcher,coder,reviewer --claude
```

##### **SPARC Workflows**
```bash
# List available SPARC modes
npx claude-flow@alpha sparc modes

# Run SPARC workflow for document processing
npx claude-flow@alpha sparc run document-cleaner "process mortgage applications"

# Use batch processing
npx claude-flow@alpha sparc batch researcher,analyzer "mortgage document analysis"

# TDD workflow
npx claude-flow@alpha sparc tdd "document validation system"
```

### ğŸ”„ Creating Custom Workflows

#### Step 1: Workflow Definition
Create a new workflow file in `Cleaning-Setup/workflows/`:

```yaml
# custom-workflow.yaml
name: mortgage-document-analysis
description: "Complete mortgage document analysis with AI agents"
version: "1.0.0"

# Agent configuration
agents:
  - type: researcher
    role: "Document analyzer"
    capabilities: ["document_analysis", "data_extraction"]
  - type: coder
    role: "Data processor"  
    capabilities: ["data_transformation", "api_integration"]
  - type: reviewer
    role: "Quality controller"
    capabilities: ["validation", "compliance_check"]

# Workflow stages
stages:
  - name: analyze
    agent: researcher
    task: "Analyze uploaded mortgage documents for completeness"
    output: "analysis_report.json"
    
  - name: extract
    agent: coder
    task: "Extract key data points from documents"
    depends_on: ["analyze"]
    output: "extracted_data.json"
    
  - name: validate
    agent: reviewer
    task: "Validate extracted data for compliance"
    depends_on: ["extract"]
    output: "validation_report.json"

# Integration settings
integration:
  mcp_servers: ["notion", "filesystem", "fastmcp"]
  channels: ["claude-flow-documents", "claude-flow-workflows"]
```

#### Step 2: Execute Custom Workflow
```bash
# Run the custom workflow
npx claude-flow@alpha run workflows/custom-workflow.yaml --claude

# Run with specific input/output directories  
npx claude-flow@alpha run workflows/custom-workflow.yaml --input ./raw-documents --output ./processed-documents
```

---

## ğŸŒŠ Flow-Nexus Workflow Management

### Flow-Nexus Setup and Commands

#### 1. Basic Flow-Nexus Operations
```bash
# Check flow-nexus version and capabilities
npx flow-nexus@latest --version
npx flow-nexus@latest --help

# List available flow templates
npx flow-nexus@latest list templates

# Create a new workflow
npx flow-nexus@latest create workflow document-processing --template basic

# List existing workflows
npx flow-nexus@latest list workflows
```

#### 2. Advanced Flow-Nexus Features
```bash
# Create complex multi-stage workflow
npx flow-nexus@latest create workflow nyra-mortgage-pipeline --template advanced

# Set workflow parameters
npx flow-nexus@latest config workflow nyra-mortgage-pipeline \
  --param input_dir="./raw-documents" \
  --param output_dir="./processed-documents" \
  --param ai_model="claude-3-haiku"

# Execute workflow with monitoring
npx flow-nexus@latest run workflow nyra-mortgage-pipeline --monitor --verbose
```

#### 3. Flow-Nexus Integration with Claude-Flow
```bash
# Create integrated flow that uses claude-flow agents
npx flow-nexus@latest create workflow claude-integration \
  --integration claude-flow \
  --agents "researcher,coder,reviewer"

# Run integrated workflow
npx flow-nexus@latest run workflow claude-integration --claude-flow
```

### ğŸ”§ Flow-Nexus Configuration

Create a flow configuration file `flow-nexus-config.json`:

```json
{
  "workflow": {
    "name": "nyra-document-pipeline",
    "description": "Complete mortgage document processing pipeline",
    "version": "1.0.0"
  },
  "stages": [
    {
      "name": "intake",
      "type": "file_processor",
      "config": {
        "input_formats": ["pdf", "docx", "txt"],
        "output_format": "json",
        "processors": ["ocr", "text_extraction"]
      }
    },
    {
      "name": "analysis",
      "type": "ai_processor", 
      "config": {
        "model": "claude-3-haiku",
        "task": "document_analysis",
        "output_schema": "./schemas/analysis-schema.json"
      }
    },
    {
      "name": "validation",
      "type": "validation_processor",
      "config": {
        "rules": "./validation/mortgage-rules.json",
        "compliance_checks": true
      }
    },
    {
      "name": "storage",
      "type": "storage_processor",
      "config": {
        "vector_store": "chromadb",
        "knowledge_base": "notion",
        "backup": true
      }
    }
  ],
  "integrations": {
    "claude_flow": {
      "enabled": true,
      "agents": ["researcher", "analyst", "validator"],
      "coordination": "hive-mind"
    },
    "mcp_servers": {
      "notion": "knowledge_storage",
      "filesystem": "file_operations", 
      "fastmcp": "external_apis"
    }
  }
}
```

---

## ğŸ§  Hive-Mind and Swarm Coordination

### Understanding Hive-Mind Architecture

The hive-mind system enables multiple AI agents to work collaboratively on complex tasks:

#### 1. Hive-Mind Session Management
```bash
# Start a comprehensive document processing session
npx claude-flow@alpha hive-mind spawn \
  "Process mortgage applications and extract key insights" \
  --namespace mortgage-processing \
  --agents researcher,analyst,coder,reviewer \
  --claude

# Monitor session progress
npx claude-flow@alpha hive-mind monitor session-xxxxx-xxxxx

# Get session results
npx claude-flow@alpha hive-mind results session-xxxxx-xxxxx
```

#### 2. Swarm Intelligence Features
```bash
# Initialize swarm for distributed processing
npx claude-flow@alpha swarm init --type research --agents 5

# Run swarm analysis
npx claude-flow@alpha swarm "Analyze 100 mortgage documents for compliance patterns" --parallel

# Coordinate ruv-swarm for advanced coordination
npx ruv-swarm@latest coordinate --task "document-analysis" --agents 8
```

### ğŸ¯ Advanced Coordination Patterns

#### Pattern 1: Research and Analysis Swarm
```bash
# Step 1: Initialize research swarm
npx claude-flow@alpha hive-mind spawn \
  "Research mortgage industry trends and regulations" \
  --agents researcher,analyst --claude

# Step 2: Coordinate with data extraction
npx claude-flow@alpha swarm "Extract trends from research data" \
  --agents coder,data-analyst --claude

# Step 3: Synthesize findings
npx claude-flow@alpha hive-mind spawn \
  "Synthesize research findings into actionable insights" \
  --agents reviewer,synthesizer --claude
```

#### Pattern 2: Document Processing Pipeline
```bash
# Parallel document processing with coordination
npx claude-flow@alpha hive-mind spawn \
  "Process mortgage documents through complete pipeline" \
  --namespace document-pipeline \
  --agents document-processor,validator,indexer \
  --coordination-mode pipeline \
  --claude
```

---

## ğŸ“„ Document Cleaning Workflows

### Running the Complete Document Cleaning Workflow

#### Method 1: Using the Pre-built Workflow
```powershell
# Navigate to Cleaning-Setup directory
cd "C:\Dev\DevProjects\Personal-Projects\Project-Nyra\Cleaning-Setup"

# Run the complete cleaning workflow
.\scripts\mcp-wrappers\claude-flow-clean.bat

# Alternative: Run with custom parameters
.\scripts\mcp-wrappers\claude-flow-workflow.bat clean-documents.yaml ./custom-input ./custom-output
```

#### Method 2: Using Claude-Flow Commands Directly
```bash
# Run cleaning workflow with claude-flow
cd "C:\Dev\DevProjects\Personal-Projects\Project-Nyra"
npx claude-flow@alpha run Cleaning-Setup/workflows/clean-documents.yaml --claude

# Run with hive-mind coordination
npx claude-flow@alpha hive-mind spawn \
  "Clean and process documents in Cleaning-Setup/raw-documents" \
  --namespace document-cleaning \
  --claude
```

#### Method 3: Python Script Integration
```powershell
# Direct Python execution
cd "C:\Dev\DevProjects\Personal-Projects\Project-Nyra\Cleaning-Setup"
python scripts\document_processor.py

# With custom configuration
python scripts\document_processor.py --config custom-llamaindex.config.yaml
```

### ğŸ“Š Monitoring Document Processing

#### Real-time Monitoring
```bash
# Monitor active workflows
npx claude-flow@alpha workflows list

# Get workflow status
npx claude-flow@alpha workflow status document-cleaning-pipeline

# View processing logs
npx claude-flow@alpha logs --workflow document-cleaning --tail
```

#### Progress Tracking
```bash
# Check hive-mind session progress
npx claude-flow@alpha hive-mind progress session-xxxxx-xxxxx

# View metrics and performance
npx claude-flow@alpha metrics --session session-xxxxx-xxxxx
```

---

## ğŸ”Œ MCP Server Integration

### Coordinating Multiple MCP Servers

#### 1. Meta-MCP Channel Management
```bash
# List available MCP channels
npx claude-flow@alpha mcp channels list

# Switch to specific channel
npx claude-flow@alpha mcp channel use claude-flow-documents

# Route heavy operations to separate channel
npx claude-flow@alpha mcp route desktop-commander heavy-mcp-desktop
```

#### 2. FastMCP Integration
```bash
# Start FastMCP server for external APIs
npx @gofastmcp/server@latest start --port 8000

# Integrate with claude-flow workflows
npx claude-flow@alpha mcp connect fastmcp http://localhost:8000
```

#### 3. Archon MCP Coordination
```bash
# Start archon for multi-agent coordination
python -m archon.mcp start --port 9000

# Connect archon to claude-flow
npx claude-flow@alpha mcp connect archon http://localhost:9000
```

### ğŸ—ï¸ Advanced MCP Configuration

Create an MCP integration configuration file:

```json
{
  "mcp_integration": {
    "channels": {
      "orchestration": {
        "servers": ["claude-flow", "archon", "ruv-swarm"],
        "priority": 1,
        "token_limit": 50000
      },
      "workflows": {
        "servers": ["fastmcp", "filesystem"],
        "priority": 2,
        "token_limit": 30000
      },
      "documents": {
        "servers": ["llamaindex", "notion", "tome"],
        "priority": 3,
        "token_limit": 25000
      },
      "heavy": {
        "servers": ["desktop-commander"],
        "priority": 10,
        "token_limit": 200000,
        "isolation": true
      }
    },
    "routing_rules": {
      "document_processing": "documents",
      "workflow_execution": "workflows",
      "agent_coordination": "orchestration",
      "system_operations": "heavy"
    }
  }
}
```

---

## ğŸš¨ Troubleshooting

### Common Issues and Solutions

#### 1. Claude-Flow Commands Not Working
```bash
# Issue: Command not found
# Solution: Ensure you're using the alpha version
npx claude-flow@alpha --version

# Issue: MCP servers not connecting
# Solution: Check server status
npx claude-flow@alpha mcp status

# Issue: Hive-mind sessions failing
# Solution: Reinitialize hive-mind
npx claude-flow@alpha hive-mind init --force
```

#### 2. Document Processing Failures
```bash
# Issue: Python import errors
# Solution: Reinstall dependencies
pip install --user --upgrade llama-index

# Issue: ChromaDB connection fails
# Solution: Check ChromaDB service
python -c "import chromadb; print('ChromaDB OK')"

# Issue: File permissions
# Solution: Check file permissions in Cleaning-Setup directory
```

#### 3. Flow-Nexus Integration Issues
```bash
# Issue: Flow-nexus not found
# Solution: Reinstall globally
npm install -g flow-nexus@latest

# Issue: Workflow execution fails
# Solution: Validate workflow configuration
npx flow-nexus@latest validate workflow your-workflow.json
```

### ğŸ”§ Advanced Troubleshooting

#### Enable Debug Logging
```bash
# Enable verbose logging for claude-flow
export DEBUG=claude-flow:*
npx claude-flow@alpha --verbose run your-workflow

# Enable flow-nexus debugging
npx flow-nexus@latest run workflow --debug --log-level verbose
```

#### System Health Checks
```bash
# Run comprehensive system check
npx claude-flow@alpha doctor

# Check MCP server health
npx claude-flow@alpha mcp health-check --all

# Validate workflow definitions
npx claude-flow@alpha workflows validate --all
```

---

## ğŸ¯ Next Steps and Advanced Usage

### 1. Custom Agent Development
- Create custom agents for specific NYRA workflows
- Integrate with mortgage-specific APIs
- Build compliance checking agents

### 2. Advanced Workflow Patterns
- Implement hot-potato workflow patterns
- Create self-improving workflows
- Build workflow templates for mortgage operations

### 3. Production Deployment
- Set up monitoring and alerting
- Configure auto-scaling for high-volume processing
- Implement backup and disaster recovery

### 4. Integration Expansion
- Connect to mortgage industry APIs
- Integrate with CRM systems
- Build real-time processing pipelines

---

**ğŸ“š Additional Resources:**
- Check `CLAUDE.md` for detailed claude-flow documentation
- View `NYRA-Claude-Flow-Setup-Report.md` for system configuration details
- Monitor `Cleaning-Setup/logs/` for processing logs
- Use `.\scripts\system-status.bat` for quick health checks

*This guide is part of the NYRA Claude-Flow Complete Setup. For issues, check the setup log files and system status.*# Project Structure Template 

This document provides a template for documenting the complete technology stack and file tree structure for your project. **AI agents MUST read this file to understand the project organization before making any changes.**

## Technology Stack Template

### Backend Technologies
Document your backend technology choices:
- **[Language] [Version]** with **[Package Manager]** - Dependency management and packaging
- **[Web Framework] [Version]** - Web framework with specific features (async, type hints, etc.)
- **[Server] [Version]** - Application server configuration
- **[Configuration] [Version]** - Configuration management approach

Example:
```
- Python 3.11+ with Poetry - Dependency management and packaging
- FastAPI 0.115.0+ - Web framework with type hints and async support
- Uvicorn 0.32.0+ - ASGI server with standard extras
- Pydantic Settings 2.5.2+ - Configuration management with type validation
```

### Integration Services & APIs
Document external services and integrations:
- **[Service Name] [API/SDK Version]** - Purpose and usage pattern
- **[AI Service] [Version]** - AI/ML service integration details
- **[Database] [Version]** - Data storage and management
- **[Monitoring] [Version]** - Observability and logging

### Real-time Communication
Document real-time features:
- **[WebSocket Library]** - Real-time communication patterns
- **[HTTP Client]** - Async HTTP communication
- **[Message Queue]** - Event processing (if applicable)

### Development & Quality Tools
Document development toolchain:
- **[Formatter] [Version]** - Code formatting
- **[Linter] [Version]** - Code quality and linting
- **[Type Checker] [Version]** - Static type checking
- **[Testing Framework] [Version]** - Testing approach
- **[Task Runner]** - Build automation and task orchestration

### Frontend Technologies (if applicable)
Document frontend technology stack:
- **[Language] [Version]** - Frontend development language
- **[Framework] [Version]** - UI framework
- **[Build Tool] [Version]** - Development and build tooling
- **[Deployment] [Version]** - Deployment and hosting approach

### Future Technologies
Document planned technology additions:
- **[Planned Technology]** - Future integration plans
- **[Platform]** - Target platform expansion
- **[Service]** - Planned service integrations

## Complete Project Structure Template

```
[PROJECT-NAME]/
â”œâ”€â”€ README.md                           # Project overview and setup
â”œâ”€â”€ CLAUDE.md                           # Master AI context file
â”œâ”€â”€ [BUILD-FILE]                        # Build configuration (Makefile, package.json, etc.)
â”œâ”€â”€ .gitignore                          # Git ignore patterns
â”œâ”€â”€ .[IDE-CONFIG]/                      # IDE workspace configuration
â”‚   â”œâ”€â”€ settings.[ext]                  # IDE settings
â”‚   â”œâ”€â”€ extensions.[ext]                # Recommended extensions
â”‚   â””â”€â”€ launch.[ext]                    # Debug configurations
â”œâ”€â”€ [BACKEND-DIR]/                      # Backend application
â”‚   â”œâ”€â”€ CONTEXT.md                      # Backend-specific AI context
â”‚   â”œâ”€â”€ src/                            # Source code
â”‚   â”‚   â”œâ”€â”€ config/                     # Configuration management
â”‚   â”‚   â”‚   â””â”€â”€ settings.[ext]          # Application settings
â”‚   â”‚   â”œâ”€â”€ core/                       # Core business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ CONTEXT.md              # Core logic patterns
â”‚   â”‚   â”‚   â”œâ”€â”€ services/               # Business services
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ [service1].[ext]    # Service implementations
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ [service2].[ext]
â”‚   â”‚   â”‚   â”œâ”€â”€ models/                 # Data models
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ [model1].[ext]      # Model definitions
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ [model2].[ext]
â”‚   â”‚   â”‚   â””â”€â”€ utils/                  # Utility functions
â”‚   â”‚   â”‚       â”œâ”€â”€ logging.[ext]       # Structured logging
â”‚   â”‚   â”‚       â”œâ”€â”€ validation.[ext]    # Input validation
â”‚   â”‚   â”‚       â””â”€â”€ helpers.[ext]       # Helper functions
â”‚   â”‚   â”œâ”€â”€ api/                        # API layer
â”‚   â”‚   â”‚   â”œâ”€â”€ CONTEXT.md              # API patterns and conventions
â”‚   â”‚   â”‚   â”œâ”€â”€ routes/                 # API route definitions
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ [resource1].[ext]   # Resource-specific routes
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ [resource2].[ext]
â”‚   â”‚   â”‚   â”œâ”€â”€ middleware/             # API middleware
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ auth.[ext]          # Authentication middleware
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ logging.[ext]       # Request logging
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ validation.[ext]    # Request validation
â”‚   â”‚   â”‚   â””â”€â”€ schemas/                # Request/response schemas
â”‚   â”‚   â”‚       â”œâ”€â”€ [schema1].[ext]     # Data schemas
â”‚   â”‚   â”‚       â””â”€â”€ [schema2].[ext]
â”‚   â”‚   â””â”€â”€ integrations/               # External service integrations
â”‚   â”‚       â”œâ”€â”€ CONTEXT.md              # Integration patterns
â”‚   â”‚       â”œâ”€â”€ [service1]/             # Service-specific integration
â”‚   â”‚       â”‚   â”œâ”€â”€ client.[ext]        # API client
â”‚   â”‚       â”‚   â”œâ”€â”€ models.[ext]        # Integration models
â”‚   â”‚       â”‚   â””â”€â”€ handlers.[ext]      # Response handlers
â”‚   â”‚       â””â”€â”€ [service2]/
â”‚   â”œâ”€â”€ tests/                          # Test suite
â”‚   â”‚   â”œâ”€â”€ unit/                       # Unit tests
â”‚   â”‚   â”œâ”€â”€ integration/                # Integration tests
â”‚   â”‚   â””â”€â”€ fixtures/                   # Test fixtures and data
â”‚   â”œâ”€â”€ [PACKAGE-FILE]                  # Package configuration
â”‚   â””â”€â”€ [ENV-FILE]                      # Environment configuration
â”œâ”€â”€ [FRONTEND-DIR]/                     # Frontend application (if applicable)
â”‚   â”œâ”€â”€ CONTEXT.md                      # Frontend-specific AI context
â”‚   â”œâ”€â”€ src/                            # Source code
â”‚   â”‚   â”œâ”€â”€ components/                 # UI components
â”‚   â”‚   â”‚   â”œâ”€â”€ CONTEXT.md              # Component patterns
â”‚   â”‚   â”‚   â”œâ”€â”€ common/                 # Shared components
â”‚   â”‚   â”‚   â””â”€â”€ [feature]/              # Feature-specific components
â”‚   â”‚   â”œâ”€â”€ pages/                      # Page components/routes
â”‚   â”‚   â”‚   â”œâ”€â”€ [page1].[ext]           # Page implementations
â”‚   â”‚   â”‚   â””â”€â”€ [page2].[ext]
â”‚   â”‚   â”œâ”€â”€ stores/                     # State management
â”‚   â”‚   â”‚   â”œâ”€â”€ CONTEXT.md              # State management patterns
â”‚   â”‚   â”‚   â”œâ”€â”€ [store1].[ext]          # Store implementations
â”‚   â”‚   â”‚   â””â”€â”€ [store2].[ext]
â”‚   â”‚   â”œâ”€â”€ api/                        # API client layer
â”‚   â”‚   â”‚   â”œâ”€â”€ CONTEXT.md              # Client patterns
â”‚   â”‚   â”‚   â”œâ”€â”€ client.[ext]            # HTTP client setup
â”‚   â”‚   â”‚   â””â”€â”€ endpoints/              # API endpoint definitions
â”‚   â”‚   â”œâ”€â”€ utils/                      # Utility functions
â”‚   â”‚   â”‚   â”œâ”€â”€ logging.[ext]           # Client-side logging
â”‚   â”‚   â”‚   â”œâ”€â”€ validation.[ext]        # Form validation
â”‚   â”‚   â”‚   â””â”€â”€ helpers.[ext]           # Helper functions
â”‚   â”‚   â””â”€â”€ assets/                     # Static assets
â”‚   â”œâ”€â”€ tests/                          # Frontend tests
â”‚   â”œâ”€â”€ [BUILD-CONFIG]                  # Build configuration
â”‚   â””â”€â”€ [PACKAGE-FILE]                  # Package configuration
â”œâ”€â”€ docs/                               # Documentation
â”‚   â”œâ”€â”€ ai-context/                     # AI-specific documentation
â”‚   â”‚   â”œâ”€â”€ project-structure.md        # This file
â”‚   â”‚   â”œâ”€â”€ docs-overview.md            # Documentation architecture
â”‚   â”‚   â”œâ”€â”€ system-integration.md       # Integration patterns
â”‚   â”‚   â”œâ”€â”€ deployment-infrastructure.md # Infrastructure docs
â”‚   â”‚   â””â”€â”€ handoff.md                  # Task management
â”‚   â”œâ”€â”€ api/                            # API documentation
â”‚   â”œâ”€â”€ deployment/                     # Deployment guides
â”‚   â””â”€â”€ development/                    # Development guides
â”œâ”€â”€ scripts/                            # Automation scripts
â”‚   â”œâ”€â”€ setup.[ext]                     # Environment setup
â”‚   â”œâ”€â”€ deploy.[ext]                    # Deployment scripts
â”‚   â””â”€â”€ maintenance/                    # Maintenance scripts
â”œâ”€â”€ [INFRASTRUCTURE-DIR]/               # Infrastructure as code (if applicable)
â”‚   â”œâ”€â”€ [PROVIDER]/                     # Cloud provider configurations
â”‚   â”œâ”€â”€ docker/                         # Container configurations
â”‚   â””â”€â”€ monitoring/                     # Monitoring and alerting
â””â”€â”€ [CONFIG-FILES]                      # Root-level configuration files
```


---

*This template provides a comprehensive foundation for documenting project structure. Adapt it based on your specific technology stack, architecture decisions, and organizational requirements.*