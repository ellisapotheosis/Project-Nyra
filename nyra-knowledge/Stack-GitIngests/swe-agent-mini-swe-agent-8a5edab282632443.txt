Directory structure:
└── swe-agent-mini-swe-agent/
    ├── README.md
    ├── LICENSE.md
    ├── mkdocs.yml
    ├── pyproject.toml
    ├── .pre-commit-config.yaml
    ├── docs/
    │   ├── _footer.md
    │   ├── contributing.md
    │   ├── faq.md
    │   ├── index.md
    │   ├── quickstart.md
    │   ├── SECURITY.md
    │   ├── advanced/
    │   │   ├── configuration.md
    │   │   ├── control_flow.md
    │   │   ├── cookbook.md
    │   │   └── local_models.md
    │   ├── assets/
    │   │   ├── bubbles.css
    │   │   ├── custom.css
    │   │   ├── gif-controls.css
    │   │   ├── image_theme.css
    │   │   ├── mini-flow.excalidraw
    │   │   ├── mkdocstrings.css
    │   │   ├── navigation_bubbles.css
    │   │   ├── navigation_cards.css
    │   │   └── js/
    │   │       ├── gif-controls.js
    │   │       └── sh-annotation.js
    │   ├── overrides/
    │   │   ├── 404.html
    │   │   └── main.html
    │   ├── reference/
    │   │   ├── index.md
    │   │   ├── agents/
    │   │   │   ├── default.md
    │   │   │   ├── interactive.md
    │   │   │   └── textual.md
    │   │   ├── environments/
    │   │   │   ├── docker.md
    │   │   │   ├── local.md
    │   │   │   └── swerex_docker.md
    │   │   ├── models/
    │   │   │   ├── anthropic.md
    │   │   │   ├── litellm.md
    │   │   │   ├── test_models.md
    │   │   │   └── utils.md
    │   │   └── run/
    │   │       ├── github_issue.md
    │   │       ├── hello_world.md
    │   │       ├── mini.md
    │   │       ├── swebench.md
    │   │       └── swebench_single.md
    │   └── usage/
    │       ├── inspector.md
    │       ├── mini.md
    │       ├── mini_v.md
    │       └── swebench.md
    ├── src/
    │   └── minisweagent/
    │       ├── __init__.py
    │       ├── __main__.py
    │       ├── py.typed
    │       ├── test_output.json
    │       ├── agents/
    │       │   ├── README.md
    │       │   ├── __init__.py
    │       │   ├── default.py
    │       │   ├── interactive.py
    │       │   └── interactive_textual.py
    │       ├── config/
    │       │   ├── README.md
    │       │   ├── __init__.py
    │       │   ├── default.yaml
    │       │   ├── github_issue.yaml
    │       │   ├── mini.tcss
    │       │   ├── mini.yaml
    │       │   ├── mini_no_temp.yaml
    │       │   └── extra/
    │       │       ├── __init__.py
    │       │       ├── swebench.yaml
    │       │       └── swebench_roulette.yaml
    │       ├── environments/
    │       │   ├── README.md
    │       │   ├── __init__.py
    │       │   ├── docker.py
    │       │   ├── local.py
    │       │   ├── singularity.py
    │       │   └── extra/
    │       │       ├── __init__.py
    │       │       └── swerex_docker.py
    │       ├── models/
    │       │   ├── README.md
    │       │   ├── __init__.py
    │       │   ├── anthropic.py
    │       │   ├── litellm_model.py
    │       │   ├── test_models.py
    │       │   ├── extra/
    │       │   │   ├── __init__.py
    │       │   │   └── roulette.py
    │       │   └── utils/
    │       │       ├── __init__.py
    │       │       ├── cache_control.py
    │       │       └── key_per_thread.py
    │       ├── run/
    │       │   ├── README.md
    │       │   ├── __init__.py
    │       │   ├── github_issue.py
    │       │   ├── hello_world.py
    │       │   ├── inspector.py
    │       │   ├── mini.py
    │       │   ├── mini_extra.py
    │       │   ├── extra/
    │       │   │   ├── __init__.py
    │       │   │   ├── config.py
    │       │   │   ├── swebench.py
    │       │   │   ├── swebench_single.py
    │       │   │   └── utils/
    │       │   │       ├── __init__.py
    │       │   │       └── batch_progress.py
    │       │   └── utils/
    │       │       ├── __init__.py
    │       │       └── save.py
    │       └── utils/
    │           ├── __init__.py
    │           └── log.py
    ├── tests/
    │   ├── __init__.py
    │   ├── conftest.py
    │   ├── agents/
    │   │   ├── __init__.py
    │   │   ├── test_default.py
    │   │   ├── test_interactive.py
    │   │   └── test_interactive_textual.py
    │   ├── config/
    │   │   ├── __init__.py
    │   │   └── test_swebench_template.py
    │   ├── environments/
    │   │   ├── __init__.py
    │   │   ├── test_docker.py
    │   │   ├── test_init.py
    │   │   ├── test_local.py
    │   │   ├── test_singularity.py
    │   │   └── extra/
    │   │       ├── __init__.py
    │   │       └── test_swerex_docker.py
    │   ├── models/
    │   │   ├── __init__.py
    │   │   ├── test_anthropic_model.py
    │   │   ├── test_cache_control.py
    │   │   ├── test_init.py
    │   │   ├── test_litellm_model.py
    │   │   └── test_test_models.py
    │   ├── run/
    │   │   ├── __init__.py
    │   │   ├── test_batch_progress.py
    │   │   ├── test_cli_integration.py
    │   │   ├── test_extra_config.py
    │   │   ├── test_github_issue.py
    │   │   ├── test_inspector.py
    │   │   ├── test_local.py
    │   │   ├── test_run_hello_world.py
    │   │   ├── test_save.py
    │   │   ├── test_swebench.py
    │   │   └── test_swebench_single.py
    │   └── test_data/
    │       ├── __init__.py
    │       ├── github_issue.traj.json
    │       ├── local.traj.json
    │       ├── results.json
    │       ├── update_outputs_github_issue.py
    │       └── update_outputs_local.py
    ├── .cursor/
    │   └── rules/
    │       ├── project.mdc
    │       └── style.mdc
    └── .github/
        ├── CODE_OF_CONDUCT.md
        ├── copilot-instructions.md
        ├── dependabot.yml
        ├── mlc_config.json
        ├── PULL_REQUEST_TEMPLATE.md
        ├── .codecov.yml
        ├── ISSUE_TEMPLATE/
        │   ├── bug.yml
        │   ├── config.yml
        │   └── question.yml
        └── workflows/
            ├── build-docs.yaml
            ├── check-links-periodic.yaml
            ├── check-links-pr.yaml
            ├── fire.yaml
            ├── pylint.yaml
            ├── pytest.yaml
            └── release.yaml

================================================
FILE: README.md
================================================
<div align="center">
<a href="https://mini-swe-agent.com/latest/"><img src="https://github.com/SWE-agent/mini-swe-agent/raw/main/docs/assets/mini-swe-agent-banner.svg" alt="mini-swe-agent banner" style="height: 7em"/></a>
</div>

# The 100 line AI agent that solves GitHub issues & more

📣 [New blogpost: Randomly switching between GPT-5 and Sonnet 4 boosts performance](https://www.swebench.com/SWE-bench/blog/2025/08/19/mini-roulette/)

[![Docs](https://img.shields.io/badge/Docs-green?style=for-the-badge&logo=materialformkdocs&logoColor=white)](https://mini-swe-agent.com/latest/)
[![Slack](https://img.shields.io/badge/Slack-4A154B?style=for-the-badge&logo=slack&logoColor=white)](https://join.slack.com/t/swe-bench/shared_invite/zt-36pj9bu5s-o3_yXPZbaH2wVnxnss1EkQ)
[![PyPI - Version](https://img.shields.io/pypi/v/mini-swe-agent?style=for-the-badge&logo=python&logoColor=white&labelColor=black&color=deeppink)](https://pypi.org/project/mini-swe-agent/)

In 2024, [SWE-bench](https://github.com/swe-bench/SWE-bench) & [SWE-agent](https://github.com/swe-agent/swe-agent) helped kickstart the coding agent revolution.

We now ask: **What if SWE-agent was 100x smaller, and still worked nearly as well?**

`mini` is for

- **Researchers** who want to **[benchmark](https://swe-bench.com), [fine-tune](https://swesmith.com/) or RL** without assumptions, bloat, or surprises
- **Developers** who like their tools like their scripts: **short, sharp, and readable**
- **Engineers** who want something **trivial to sandbox & to deploy anywhere**

Here's some details:

- **Minimal**: Just [100 lines of python](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/agents/default.py) (+100 total for [env](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/environments/local.py),
[model](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/models/litellm_model.py), [script](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/run/hello_world.py)) — no fancy dependencies!
- **Powerful:** Resolves 68% of GitHub issues in the [SWE-bench verified benchmark](https://www.swebench.com/) ([leaderboard](https://swe-bench.com/)).
- **Convenient:** Comes with UIs that turn this into your daily dev swiss army knife!
- **Deployable:** In addition to local envs, you can use **docker**, **podman**, **singularity**, **apptainer**, and more
- **Tested:** [![Codecov](https://img.shields.io/codecov/c/github/swe-agent/mini-swe-agent?style=flat-square)](https://codecov.io/gh/SWE-agent/mini-swe-agent)
- **Cutting edge:** Built by the Princeton & Stanford team behind [SWE-bench](https://swebench.com) and [SWE-agent](https://swe-agent.com).

<details>

<summary>More motivation (for research)</summary>

[SWE-agent](https://swe-agent.com/latest/) jump-started the development of AI agents in 2024. Back then, we placed a lot of emphasis on tools and special interfaces for the agent.
However, one year later, as LMs have become more capable, a lot of this is not needed at all to build a useful agent!
In fact, mini-SWE-agent

- **Does not have any tools other than bash** — it doesn't even use the tool-calling interface of the LMs.
  This means that you can run it with literally any model. When running in sandboxed environments you also don't need to take care
  of installing a single package — all it needs is bash.
- **Has a completely linear history** — every step of the agent just appends to the messages and that's it.
  So there's no difference between the trajectory and the messages that you pass on to the LM.
  Great for debugging & fine-tuning.
- **Executes actions with `subprocess.run`** — every action is completely independent (as opposed to keeping a stateful shell session running).
  This makes it trivial to execute the actions in sandboxes (literally just switch out `subprocess.run` with `docker exec`) and to
  scale up effortlessly. Seriously, this is [a big deal](https://mini-swe-agent.com/latest/faq/#why-no-shell-session), trust me.

This makes it perfect as a baseline system and for a system that puts the language model (rather than
the agent scaffold) in the middle of our attention.
You can see the result on the [SWE-bench (bash only)](https://www.swebench.com/) leaderboard, that evaluates the performance of different LMs with `mini`.

</details>

<details>
<summary>More motivation (as a tool)</summary>

Some agents are overfitted research artifacts. Others are UI-heavy frontend monsters.

`mini` wants to be a hackable tool, not a black box.

- **Simple** enough to understand at a glance
- **Convenient** enough to use in daily workflows
- **Flexible** to extend

Unlike other agents (including our own [swe-agent](https://swe-agent.com/latest/)), it is radically simpler, because it:

- **Does not have any tools other than bash** — it doesn't even use the tool-calling interface of the LMs.
  Instead of implementing custom tools for every specific thing the agent might want to do, the focus is fully on the LM utilizing the shell to its full potential.
  Want it to do something specific like opening a PR?
  Just tell the LM to figure it out rather than spending time to implement it in the agent.
- **Executes actions with `subprocess.run`** — every action is completely independent (as opposed to keeping a stateful shell session running).
  This is [a big deal](https://mini-swe-agent.com/latest/faq/#why-no-shell-session) for the stability of the agent, trust me.
- **Has a completely linear history** — every step of the agent just appends to the messages that are passed to the LM in the next step and that's it.
  This is great for debugging and understanding what the LM is prompted with.

</details>

<details>
<summary>Should I use SWE-agent or mini-SWE-agent?</summary>

You should use `mini-swe-agent` if

- You want a quick command line tool that works locally
- You want an agent with a very simple control flow
- You want even faster, simpler & more stable sandboxing & benchmark evaluations
- You are doing FT or RL and don't want to overfit to a specific agent scaffold

You should use `swe-agent` if

- You need specific tools or want to experiment with different tools
- You want to experiment with different history processors
- You want very powerful yaml configuration without touching code

What you get with both

- Excellent performance on SWE-Bench
- A trajectory browser

</details>

<table>
<tr>
<td width="50%">
<a href="https://mini-swe-agent.com/latest/usage/mini/"><strong>Simple UI</strong></a> (<code>mini</code>)
</td>
<td>
<a href="https://mini-swe-agent.com/latest/usage/mini_v/"><strong>Visual UI</strong></a> (<code>mini -v</code>)
</td>
</tr>
<tr>
<td width="50%">

  ![mini](https://github.com/SWE-agent/swe-agent-media/blob/main/media/mini/gif/mini.gif?raw=true)

</td>
<td>

  ![miniv](https://github.com/SWE-agent/swe-agent-media/blob/main/media/mini/gif/mini2.gif?raw=true)

</td>
</tr>
<tr>
  <td>
    <a href="https://mini-swe-agent.com/latest/usage/swebench/"><strong>Batch inference</strong></a>
  </td>
  <td>
    <a href="https://mini-swe-agent.com/latest/usage/inspector/"><strong>Trajectory browser</strong></a>
  </td>
<tr>
<tr>

<td>

![swebench](https://github.com/SWE-agent/swe-agent-media/blob/main/media/mini/gif/swebench.gif?raw=true)

</td>

<td>

![inspector](https://github.com/SWE-agent/swe-agent-media/blob/main/media/mini/gif/inspector.gif?raw=true)

</td>

</tr>
<td>
<a href="https://mini-swe-agent.com/latest/advanced/cookbook/"><strong>Python bindings</strong></a>
</td>
<td>
<a href="https://mini-swe-agent.com"><strong>More in the docs</strong></a>
</td>
</tr>
<tr>
<td>

```python
agent = DefaultAgent(
    LitellmModel(model_name=...),
    LocalEnvironment(),
)
agent.run("Write a sudoku game")
```
</td>
<td>

* [Quick start](https://mini-swe-agent.com/latest/quickstart/)
* [`mini`](https://mini-swe-agent.com/latest/usage/mini/)
* [FAQ](https://mini-swe-agent.com/latest/faq/)
* [Configuration](https://mini-swe-agent.com/latest/advanced/configuration/)
* [Power up](https://mini-swe-agent.com/latest/advanced/cookbook/)

</td>
</tr>
</table>

## Let's get started!

Option 1: Install + run in virtual environment

```bash
pip install uv && uvx mini-swe-agent [-v]
# or
pip install pipx && pipx ensurepath && pipx run mini-swe-agent [-v]
```

Option 2: Install in current environment

```bash
pip install mini-swe-agent && mini [-v]
```

Option 3: Install from source

```bash
git clone https://github.com/SWE-agent/mini-swe-agent.git
cd mini-swe-agent
pip install -e .
mini [-v]
```

Read more in our [documentation](https://mini-swe-agent.com/latest/):

* [Quick start guide](https://mini-swe-agent.com/latest/quickstart/)
* More on [`mini`](https://mini-swe-agent.com/latest/usage/mini/) and [`mini -v`](https://mini-swe-agent.com/latest/usage/mini_v/)
* [Configuration](https://mini-swe-agent.com/latest/advanced/configuration/)
* [Power up with the cookbook](https://mini-swe-agent.com/latest/advanced/cookbook/)
* [FAQ](https://mini-swe-agent.com/latest/faq/)
* [Contribute!](https://mini-swe-agent.com/latest/contributing/)

## Attribution

If you found this work helpful, please consider citing the [SWE-agent paper](https://arxiv.org/abs/2405.15793) in your work:

```bibtex
@inproceedings{yang2024sweagent,
  title={{SWE}-agent: Agent-Computer Interfaces Enable Automated Software Engineering},
  author={John Yang and Carlos E Jimenez and Alexander Wettig and Kilian Lieret and Shunyu Yao and Karthik R Narasimhan and Ofir Press},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  url={https://arxiv.org/abs/2405.15793}
}
```

Our other projects:

<div align="center">
  <a href="https://github.com/SWE-agent/SWE-agent"><img src="https://github.com/SWE-agent/mini-swe-agent/raw/main/docs/assets/sweagent_logo_text_below.svg" alt="SWE-agent" height="120px"></a>
   &nbsp;&nbsp;
  <a href="https://github.com/SWE-agent/SWE-ReX"><img src="https://github.com/SWE-agent/mini-swe-agent/raw/main/docs/assets/swerex_logo_text_below.svg" alt="SWE-ReX" height="120px"></a>
   &nbsp;&nbsp;
  <a href="https://github.com/SWE-bench/SWE-bench"><img src="https://github.com/SWE-agent/mini-swe-agent/raw/main/docs/assets/swebench_logo_text_below.svg" alt="SWE-bench" height="120px"></a>
  &nbsp;&nbsp;
  <a href="https://github.com/SWE-bench/SWE-smith"><img src="https://github.com/SWE-agent/mini-swe-agent/raw/main/docs/assets/swesmith_logo_text_below.svg" alt="SWE-smith" height="120px"></a>
  &nbsp;&nbsp;
  <a href="https://github.com/SWE-bench/sb-cli"><img src="https://github.com/SWE-agent/mini-swe-agent/raw/main/docs/assets/sbcli_logo_text_below.svg" alt="sb-cli" height="120px"></a>
</div>




================================================
FILE: LICENSE.md
================================================
MIT License

Copyright (c) 2025 Kilian A. Lieret and Carlos E. Jimenez

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


================================================
FILE: mkdocs.yml
================================================
site_name: mini-SWE-agent documentation
site_url: https://mini-swe-agent.com
theme:
  name: material
  icon:
    repo: fontawesome/brands/github
    annotation: material/chevron-right-circle
  custom_dir: docs/overrides
  logo: assets/mini_square.svg
  favicon: assets/mini_square.svg
  palette:
    - media: "(prefers-color-scheme)"
      toggle:
        icon: material/brightness-auto
        name: Switch to light mode
    - scheme: default
      # primary: black  # override in custom.css
      accent: deep orange
      media: "(prefers-color-scheme: light)"
      toggle:
        icon: material/weather-night
        name: Switch to dark mode
    - scheme: slate
      # primary: black  # override in custom.css
      accent: deep orange
      media: "(prefers-color-scheme: dark)"
      toggle:
        icon: material/weather-sunny
        name: Switch to light mode
  features:
    - navigation.indexes
    - navigation.top
    - content.action.edit
    - navigation.footer
    - content.code.copy
    - content.footnote.tooltips
    - header.autohide
    - announce.dismiss
    - content.code.annotate
    - navigation.tabs
    - navigation.tabs.sticky
markdown_extensions:
  - admonition
  - pymdownx.details
  - pymdownx.tabbed:
      alternate_style: true
  - pymdownx.superfences
  - pymdownx.magiclink
  - footnotes
  - attr_list
  - md_in_html
  - pymdownx.snippets:
      check_paths: true
  - pymdownx.emoji:
      emoji_index: !!python/name:material.extensions.emoji.twemoji
      emoji_generator: !!python/name:material.extensions.emoji.to_svg
nav:
  - Getting Started:
    - "Overview": index.md
    - "Quick start": quickstart.md
    - "Usage":
      - "<code>mini</code>": usage/mini.md
      - "<code>mini -v</code>": usage/mini_v.md
      - "SWE-bench": usage/swebench.md
      - "Inspector": usage/inspector.md
    - "Advanced":
      - "Configuration": advanced/configuration.md
      - "Local models": advanced/local_models.md
      - "Control flow": advanced/control_flow.md
      - "Cookbook": advanced/cookbook.md
    - "FAQ": faq.md
    - "Contributing": contributing.md
  - API Reference:
    - "reference/index.md"
    - Agents:
      - "DefaultAgent": "reference/agents/default.md"
      - "InteractiveAgent": "reference/agents/interactive.md"
      - "TextualAgent": "reference/agents/textual.md"
    - Models:
      - "LitellmModel": "reference/models/litellm.md"
      - "AnthropicModel": "reference/models/anthropic.md"
      - "DeterministicModel": "reference/models/test_models.md"
      - "Model Utilities": "reference/models/utils.md"
    - Environments:
      - "LocalEnvironment": "reference/environments/local.md"
      - "DockerEnvironment": "reference/environments/docker.md"
      - "SwerexDockerEnvironment": "reference/environments/swerex_docker.md"
    - Run Scripts:
      - "Hello World": "reference/run/hello_world.md"
      - "mini": "reference/run/mini.md"
      - "GitHub Issue": "reference/run/github_issue.md"
      - "SWE-bench (batch)": "reference/run/swebench.md"
      - "SWE-bench (single)": "reference/run/swebench_single.md"
  - Blog: https://www.swebench.com/SWE-bench/blog/category/mini-swe-agent/
  - Leaderboard: https://swebench.com
plugins:
  - glightbox
  - search
  - include-markdown
  - mike:
      canonical_version: latest
      version_selector: true
  - mkdocstrings:
      default_handler: python
      handlers:
        python:
          paths: [src]
          options:
            merge_init_into_class: true
            summary: false
            show_root_heading: true
            heading_level: 2
            docstring_style: google
            show_if_no_docstring: true
            show_signature: true
            show_signature_annotations: true
            signature_crossrefs: true
            separate_signature: true
            show_symbol_type_heading: true
            show_symbol_type_toc: true
            members_order: source
            filters:
              - "!^_"
              - "!^model_config"
              - "!^model_post_init"
repo_url: https://github.com/SWE-agent/mini-SWE-agent
repo_name: SWE-agent/mini-SWE-agent
edit_uri: edit/main/docs/
extra_javascript:
  - assets/js/sh-annotation.js
  - assets/js/gif-controls.js
extra_css:
  - assets/custom.css
  - assets/gif-controls.css
  - assets/mkdocstrings.css
  - assets/image_theme.css
  - assets/bubbles.css
  - assets/navigation_cards.css
  - https://fonts.googleapis.com/icon?family=Material+Icons
extra:
  version:
    provider: mike
  analytics:
    provider: google
    property: G-SG2NC1GDRD



================================================
FILE: pyproject.toml
================================================
[build-system]
requires = ["setuptools"]
build-backend = "setuptools.build_meta"


[project]
name = "mini-swe-agent"
dynamic = ["version"]
description = "Nano SWE Agent - A simple AI software engineering agent"
readme = "README.md"
requires-python = ">=3.10"
license = {file = "LICENSE.md"}
keywords = ["nlp", "agents", "code"]
authors = [
    {name = "Kilian Lieret", email = "kilian.lieret@posteo.de" },
    {name = "Carlos E. Jimenez", email = "carlosej@princeton.edu" },
]

# Classifiers help users find your project by categorizing it.
classifiers = [
  # How mature is this project? Common values are
  #   3 - Alpha, 4 - Beta, 5 - Production/Stable
  "Development Status :: 3 - Alpha",
  "Operating System :: OS Independent",
  # Indicate who your project is intended for
  "Intended Audience :: Developers",
  # Pick your license as you wish
  "License :: OSI Approved :: MIT License",
  "Programming Language :: Python :: 3.10",
  "Programming Language :: Python :: 3 :: Only",
]

dependencies = [
    "pyyaml",
    "requests",
    "jinja2",
    "litellm >= 1.75.5",  # want to have gpt-5 support
    "tenacity",
    "rich",
    "python-dotenv",
    "typer",
    "platformdirs",
    "textual",
    "prompt_toolkit",
    "openai <= 1.99.5",  # https://github.com/SWE-agent/mini-swe-agent/issues/446
]

[project.optional-dependencies]
full = [
    "mini-swe-agent[dev]",
    "swe-rex>=1.4.0",
]

dev = [
    "datasets",
    "pytest",
    "pytest-cov",
    "pytest-asyncio",
    "pytest-xdist",
    "pre-commit",
    "ruff",
    "mkdocs-include-markdown-plugin",
    "mkdocstrings[python]>=0.18",
    "mike",
    "mkdocs-material",
    "mkdocs-glightbox",
]

[project.urls]
Documentation = "https://mini-swe-agent.com/latest/"
Repository = "https://github.com/SWE-agent/mini-SWE-agent"
"Bug Tracker" = "https://github.com/SWE-agent/mini-SWE-agent/issues"

[project.scripts]
mini = "minisweagent.run.mini:app"
mini-swe-agent = "minisweagent.run.mini:app"
mini-extra = "minisweagent.run.mini_extra:main"
mini-e= "minisweagent.run.mini_extra:main"

[tool.setuptools]
include-package-data = true

[tool.setuptools.dynamic]
version = {attr = "minisweagent.__version__"}

[tool.setuptools.packages.find]
where = ["src"]
include = ["minisweagent*"]

[tool.setuptools.package-data]
minisweagent = ["config/**/*"]

[tool.ruff]
# Exclude a variety of commonly ignored directories.
exclude = [
    ".bzr",
    ".direnv",
    ".eggs",
    ".git",
    ".git-rewrite",
    ".hg",
    ".ipynb_checkpoints",
    ".mypy_cache",
    ".nox",
    ".pants.d",
    ".pyenv",
    ".pytest_cache",
    ".pytype",
    ".ruff_cache",
    ".svn",
    ".tox",
    ".venv",
    ".vscode",
    "__pypackages__",
    "_build",
    "buck-out",
    "build",
    "dist",
    "node_modules",
    "site-packages",
    "venv",
    # ---- project specific ----
    "tests/test_data",
    # Exclude commands so they don't get the __future__ imports
    "config/commands",
]

line-length = 120
indent-width = 4

target-version = "py310"

[tool.ruff.lint]
# Enable Pyflakes (`F`) and a subset of the pycodestyle (`E`)  codes by default.
# Unlike Flake8, Ruff doesn't enable pycodestyle warnings (`W`) or
# McCabe complexity (`C901`) by default.
# I001: Isort, I002: required import
select = [
    # Error (E)
    "E",
    # Error (PLE)
    "PLE",
    # pycodestyle
    "E713",  # not in
    "E714",  # is not
    "E711",  # comparison with None
    # pyflakes
    "F821",
    "F822",
    "F401",  # unused-import
    "F841",  # unused var
    "F541",  # f-string without args
    "F901",  # raise NotImplemented should be raise NotImplementedError
    # isort
    "I001",  # isort
    "I002",  # required import
    # pyupgrade and related
    "UP",    # pyupgrade
    "C401",  # flake8-comprehensions: unnecessary-generator-set
    "C402",  # flake8-comprehensions: unnecessary-generator-dict
    "C403",  # flake8-comprehensions: unnecessary-list-comprehension-set
    "C404",  # flake8-comprehensions: unnecessary-list-comprehension-dict
    "C405",  # flake8-comprehensions: unnecessary-literal-set
    "F632",  # pyflakes: is-literal
    "W605",  # pycodestyle: invalid-escape-sequence
    # bugbear
    "B006",  # mutable default
    "B007",  # unused loop var
    "B009",  # getattr with constant
    # flake8-errmsg
    "EM",
    # flake8-return
    "RET",
    # RUF
    "RUF019",  # unneded key in dict check
    # pytest
    "PT",
    # flake8-simplify (SIM)
    "SIM201",
    # flake8-use-pathlib
    "PTH100",
    "PTH110",
    "PTH111",
    "PTH112",
    "PTH113",
    "PTH114",
    "PTH117",
    "PTH118",
    "PTH119",
    "PTH120",
    "PTH121",
    "PTH122",
    "PTH202",
    "PTH203",
    "PTH204",
    "PTH205",
]
ignore = [
    # flake8-return
    "RET505",  # can't autofix
    "RET506",  # can't autofix
    "RET507",  # can't autofix
    # error (E)
    "E501",    # line too long
    "E402",    # import not on top of file
    "E722",    # bare except
    "E741",    # ambiguous symbol
    # pytest
    "PT011",
    "PT018",
    # flake8-errmsg
    "EM101",   # exception must not use a string literal
    "EM102",   # exception must not use an f-string literal
    "EM103",   # exception must not use a .format(...) string directly
]

# Allow fix for all enabled rules (when `--fix`) is provided.
fixable = ["ALL"]
unfixable = []

# Allow unused variables when underscore-prefixed.
dummy-variable-rgx = "^(_+|(_+[a-zA-Z0-9_]*[a-zA-Z0-9]+?))$"

[tool.ruff.format]
# Like Black, use double quotes for strings.
quote-style = "double"

# Like Black, indent with spaces, rather than tabs.
indent-style = "space"

# Like Black, respect magic trailing commas.
skip-magic-trailing-comma = false

# Like Black, automatically detect the appropriate line ending.
line-ending = "auto"

[tool.typos.default.extend-identifiers]
# *sigh* this just isn't worth the cost of fixing
ACI = "ACI"

[tool.typos.default.extend-words]
# Don't correct the surname "Teh"
aci = "aci"
ba = "ba"

[tool.pylint.messages_control]
disable = [
    "line-too-long",           # C0301 - disable line length checks
    "missing-docstring",       # C0111 - disable docstring requirements
    "missing-class-docstring", # C0115 - disable class docstring requirements
    "missing-function-docstring", # C0116 - disable function docstring requirements
    "missing-module-docstring", # C0114 - disable module docstring requirements
    "unspecified-encoding",    # W1514 - disable unspecified encoding warnings
    "duplicate-code",          # R0801 - disable code duplication checks
    "too-few-public-methods",  # R0903 - disable too few public methods warnings
]

[tool.pytest.ini_options]
asyncio_mode = "auto"
asyncio_default_fixture_loop_scope = "function"
markers = [
    "slow: marks tests as slow (deselect with '-k \"not slow\"')",
]



================================================
FILE: .pre-commit-config.yaml
================================================
ci:
  autoupdate_commit_msg: "chore: update pre-commit hooks"

repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: "v6.0.0"
    hooks:
      - id: check-added-large-files
      - id: check-case-conflict
      - id: check-merge-conflict
      - id: check-symlinks
      - id: mixed-line-ending
      - id: detect-private-key
      # - id: check-ast
      - id: trailing-whitespace

  - repo: https://github.com/crate-ci/typos
    rev: v1
    hooks:
      - id: typos
        files: \.(py|md|rst|yaml|toml)
        exclude: pyproject.toml

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.12.9
    hooks:
      # Run the linter.
      - id: ruff
        args: ["--fix"]
      # Run the formatter.
      - id: ruff-format

  - repo: https://github.com/pre-commit/mirrors-prettier
    rev: "v4.0.0-alpha.8" # Use the sha or tag you want to point at
    hooks:
      - id: prettier
        types_or: ["javascript", "css"]



================================================
FILE: docs/_footer.md
================================================
<div class="grid cards">
  <a href="https://github.com/SWE-agent/SWE-agent/issues/new" class="nav-card-link">
    <div class="nav-card">
      <div class="nav-card-header">
        <span class="material-icons nav-card-icon">bug_report</span>
        <span class="nav-card-title">Something broken/unclear?</span>
      </div>
      <p class="nav-card-description">Open an issue on GitHub!</p>
    </div>
  </a>

  <a href="https://join.slack.com/t/swe-bench/shared_invite/zt-36pj9bu5s-o3_yXPZbaH2wVnxnss1EkQ" class="nav-card-link">
    <div class="nav-card">
      <div class="nav-card-header">
        <span class="material-icons nav-card-icon">help</span>
        <span class="nav-card-title">Open-ended discussions</span>
      </div>
      <p class="nav-card-description">Join our Slack!</p>
    </div>
  </a>
</div>



================================================
FILE: docs/contributing.md
================================================
# ❤️ Contributing

We happily accept contributions!

## Areas of help

- Feedback on the `mini` and `mini -v` interfaces at [this github issue](https://github.com/swe-agent/mini-swe-agent/issues/161) or in our [Slack channel](https://join.slack.com/t/swe-bench/shared_invite/zt-36pj9bu5s-o3_yXPZbaH2wVnxnss1EkQ).
- Documentation, examples, tutorials, etc. In particular, we're looking for
    - examples of how this library is used in the wild
    - additional examples for the [cookbook](advanced/cookbook.md)
- Support for more models (anything where `litellm` doesn't work out of the box)
- Support for more environments & deployments (e.g., run it as a github action, etc.)
- Take a look at the [issues](https://github.com/SWE-agent/mini-swe-agent/issues) and look for issues marked `good-first-issue` or `help-wanted` (please read the guidelines below first)

## Design & Architecture

- `mini-swe-agent` aims to stay minimalistic, hackable, and of high quality code.
- To extend features, we prefer to add a new version of the one of the four components (see [cookbook](advanced/cookbook.md)), rather than making the existing components more complex.
- Components should be relatively self-contained, but if there are utilities that might be shared, add a `utils` folder (like [this one](https://github.com/SWE-agent/mini-swe-agent/tree/main/src/minisweagent/models/utils)). But keep it simple!
- If your component is a bit more specific, add it into an `extra` folder (like [this one](https://github.com/SWE-agent/mini-swe-agent/tree/main/src/minisweagent/run/extra))
- Our target audience is anyone who doesn't shy away from modifying a bit of code (especially a run script) to get what they want.
- Therefore, not everything needs to be configurable with the config files, but it should be easy to create a run script that makes use of it.
- Many LMs write very verbose code -- please clean it up! Same goes for the tests. They should still be concise and readable.
- Please install `pre-commit` (`pip install pre-commit && pre-commit install`) and run it before committing. This will enforce our style guide.

## Development setup

Make sure to follow the dev setup instructions in [quickstart.md](quickstart.md).

After that you can run `pytest` with `pytest -n auto` (this parallelizes the tests across all cores for speedup).

{% include-markdown "_footer.md" %}



================================================
FILE: docs/faq.md
================================================
# FAQ

## General

!!! question "Does mini-SWE-agent work on my system?"

    mini-SWE-agent should work on any system that has a bash shell or uses a container runtime to emulate one.

??? question "Should I use mini-SWE-agent or swe-agent?"

    You should use `mini-swe-agent` if

    - You want a quick command line tool that works locally
    - You want an agent with a very simple control flow
    - You want even faster, simpler & more stable sandboxing & benchmark evaluations
    - You are doing FT or RL and don't want to overfit to a specific agent scaffold

    You should use `swe-agent` if

    - You need specific tools or want to experiment with different tools
    - You want to experiment with different history processors
    - You want very powerful yaml configuration without touching code

    What you get with both

    - Excellent performance on SWE-Bench
    - A trajectory browser

??? question "How is `mini` simpler than `swe-agent`?"

    `mini` is simpler than `swe-agent` because it:

    - Does not have any tools other than bash — it doesn't even use the tool-calling interface of the LMs.
      This means you don't have to install anything in any environment you're running in. `bash` is all you need.
    - Has a completely linear history — every step of the agent just appends to the messages and that's it.
    - Executes actions with `subprocess.run` — every action is completely independent (as opposed to keeping a stateful shell session running).
      This [avoids so many issues](#why-no-shell-session), trust me.

??? question "What are the limitations of mini-SWE-agent?"

    mini-SWE-agent can be extended trivially in various ways, the following assumes the default setup.
    As reflected in the high SWE-bench scores, none of the following limitations are a problem in practice.

    - No tools other than bash
    - Actions are parsed from triple-backtick blocks (rather than assuming a function calling/tool calling format)
    - By default, actions are executed as `subprocess.run`, i.e., every action is independent of the previous ones.
      (meaning that the agent cannot change directories or export environment variables; however environment variables
      can be set per-action). This [avoids so many issues](#why-no-shell-session), trust me.

    If you want more flexibility with these items, you can use [SWE-agent](https://swe-agent.com/) instead.

??? question "Where is global configuration stored?"

    The global configuration is stored in the `.env` file in the config directory.
    The location is printed when you run `mini --help`.

    The `.env` file is a simple key-value file that is read by the `dotenv` library.


## Models

!!! question "What models do you support?"

    Currently, mini-SWE-agent supports all models that are supported by [litellm](https://github.com/BerriAI/litellm)
    and we're open to extend the `models/` directory with more models should `litellm` not support them.

!!! question "How do I set the API key for a model?"

    The API key can be stored either as an environment variable (note that enviroinment variables are not persistent
    unless you set them in your `~/.bashrc` or similar), or as a permanent key in the config file.

    To temporarily set the API key as an environment variable, you can use the following command:

    ```bash
    export OPENAI_API_KEY=sk-test123
    ```

    To permanently set the API key in the config file, you can use the following command:

    ```bash
    mini-extra config set OPENAI_API_KEY sk-test123
    ```

    Alternatively, you can directly edit the `.env` file in the config directory
    (the location is printed when you run `mini --help`).

!!! question "How can I set the default model?"

    The default model is stored in the config/environment as `MSWEA_MODEL_NAME`.
    To permanently change it:

    ```bash
    mini-extra config set MSWEA_MODEL_NAME claude-sonnet-4-20250514
    ```

    Alternatively, you can directly edit the `.env` file in the config directory
    (the location is printed when you run `mini --help`).

## Minutia

??? question "Why is not needed a running shell session such a big deal?"
    <a name="why-no-shell-session"></a>

    Most agents so far kept a running shell session. Every action from the agent was executed in this session.
    However, this is far from trivial:

    1. It's not obvious when a command has terminated. Essentially you're just pasting input into the shell session, and press enter—but when do you stop reading output?
       We've experimented with various heuristics (watching PIDs, watching for the shell to go back to the prompt, etc.) but all of them were flaky.
       The `mini` agent doesn't need any of this!
    2. Particularly bad commands from the LM can kill the shell session. Then what?
    3. Interrupting a command running in a shell session can also mess up the shell itself and can in particular interfere with all the following outputs you want to extract.

    `mini` is different: There is no running shell session. Every action is executed as a subprocess, that means
    every action is independent of the previous ones (it is literally a `subprocess.run`/`os.system`/`docker exec` call).

    This means that the agent cannot even change directories or export environment variables.
    But you don't need this! You can always prefix `cd /path/to/project` or `export FOO=bar` to every action
    (and in fact some LMs like Claude will do that even if you don't ask them to).

{% include-markdown "_footer.md" %}



================================================
FILE: docs/index.md
================================================
<div align="center">
<img src="assets/mini-swe-agent-banner.svg" alt="mini-swe-agent banner" style="height: 7em"/>

<h1 style="margin-bottom: 1ex;">The 100 line AI agent that's actually useful</h1>

<a href="https://join.slack.com/t/swe-bench/shared_invite/zt-36pj9bu5s-o3_yXPZbaH2wVnxnss1EkQ">
    <img src="https://img.shields.io/badge/Slack-4A154B?style=for-the-badge&logo=slack&logoColor=white" alt="Slack">
</a>
<a href="https://github.com/SWE-agent/mini-swe-agent">
    <img alt="GitHub Release" src="https://img.shields.io/github/v/release/swe-agent/mini-swe-agent?style=for-the-badge&logo=github&label=GitHub&labelColor=black&color=green" alt="GitHub Release">
</a>
<a href="https://pypi.org/project/mini-swe-agent/">
    <img src="https://img.shields.io/pypi/v/mini-swe-agent?style=for-the-badge&logo=python&logoColor=white&labelColor=black&color=deeppink" alt="PyPI - Version">
</a>

</div>


In 2024, [SWE-bench](https://swebench.com) & [SWE-agent](https://swe-agent.com) helped kickstart the coding agent revolution.

We now ask: **What if the agent was 100x smaller, and still worked nearly as well?**

`mini` is for

- **Researchers** who want to **[benchmark](https://swe-bench.com), [fine-tune](https://swesmith.com/) or RL** without assumptions, bloat, or surprises
- **Developers** who like their tools like their scripts: **short, sharp, and readable**
- **Engineers** who want something **trivial to sandbox & to deploy anywhere**

Here's some details:

- **Minimal**: Just [100 lines of python](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/agents/default.py) (+100 total for [env](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/environments/local.py),
[model](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/models/litellm_model.py), [script](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/run/hello_world.py)) — no fancy dependencies!
- **Powerful:** Resolves 68% of GitHub issues in the [SWE-bench verified benchmark](https://www.swebench.com/) ([leaderboard](https://swe-bench.com/)).
- **Convenient:** Comes with UIs that turn this into your daily dev swiss army knife!
- **Deployable:** In addition to local envs, you can use **docker**, **podman**, **singularity**, **apptainer**, and more
- **Cutting edge:** Built by the Princeton & Stanford team behind [SWE-bench](https://swebench.com) and [SWE-agent](https://swe-agent.com).

??? note "Why use mini-SWE-agent for research?"

    [SWE-agent](https://swe-agent.com/latest/) jump-started the development of AI agents in 2024. Back then, we placed a lot of emphasis on tools and special interfaces for the agent. However, one year later, a lot of this is not needed at all to build a useful agent!

    In fact, mini-SWE-agent:

    - **Does not have any tools other than bash** — it doesn't even use the tool-calling interface of the LMs.
      This means that you can run it with literally any model.
      When running in sandboxed environments you also don't need to take care of installing a single package — all it needs is bash.
    - **Has a completely linear history** — every step of the agent just appends to the messages and that's it.
      So there's no difference between the trajectory and the messages that you pass on to the LM.
      Great for debugging & fine-tuning.
    - **Executes actions with `subprocess.run`** — every action is completely independent (as opposed to keeping a stateful shell session running). This makes it trivial to execute the actions in sandboxes (literally just switch out `subprocess.run` with `docker exec`) and to scale up effortlessly.
      Seriously, this is [a big deal](faq.md#why-no-shell-session), trust me.

    This makes it perfect as a baseline system and for a system that puts the language model (rather than the agent scaffold) in the middle of our attention.
    You can see the result on the [SWE-bench (bash only)](https://www.swebench.com/) leaderboard, that evaluates the performance of different LMs with `mini`.

??? note "Why use mini-SWE-agent as a tool?"

    Some agents are overfitted research artifacts. Others are UI-heavy frontend monsters.

    `mini` wants to be a hackable tool, not a black box.

    - **Simple** enough to understand at a glance
    - **Convenient** enough to use in daily workflows
    - **Flexible** to extend

    Unlike other agents (including our own [swe-agent](https://swe-agent.com/latest/)), it is radically simpler, because it:

    - **Does not have any tools other than bash** — it doesn't even use the tool-calling interface of the LMs.
      Instead of implementing custom tools for every specific thing the agent might want to do, the focus is fully on the LM utilizing the shell to its full potential.
      Want it to do something specific like opening a PR?
      Just tell the LM to figure it out rather than spending time to implement it in the agent.
    - **Executes actions with `subprocess.run`** — every action is completely independent (as opposed to keeping a stateful shell session running).
      This is [a big deal](https://mini-swe-agent.com/latest/faq/#why-no-shell-session) for the stability of the agent, trust me.
    - **Has a completely linear history** — every step of the agent just appends to the messages that are passed to the LM in the next step and that's it.
      This is great for debugging and understanding what the LM is prompted with.

??? note "Should I use mini-SWE-agent or swe-agent?"

    You should use `mini-swe-agent` if

    - You want a quick command line tool that works locally
    - You want an agent with a very simple control flow
    - You want even faster, simpler & more stable sandboxing & benchmark evaluations
    - You are doing FT or RL and don't want to overfit to a specific agent scaffold

    You should use `swe-agent` if

    - You need specific tools or want to experiment with different tools
    - You want to experiment with different history processors
    - You want very powerful yaml configuration without touching code

    What you get with both

    - Excellent performance on SWE-Bench
    - A trajectory browser

</details>
<table>
<tr>
<td width="50%">
<a href="usage/mini"><strong>Simple UI</strong></a> (<code>mini</code>)
</td>
<td>
<a href="usage/mini_v"><strong>Visual UI</strong></a> (<code>mini -v</code>)
</td>
</tr>
<tr>
<td width="50%">
  <div class="gif-container" data-glightbox-disabled>
    <img src="https://github.com/SWE-agent/swe-agent-media/blob/main/media/mini/png/mini.png?raw=true"
         data-gif="https://github.com/SWE-agent/swe-agent-media/blob/main/media/mini/gif/mini.gif?raw=true"
         alt="mini" data-glightbox="false" />
  </div>
</td>
<td>
  <div class="gif-container" data-glightbox-disabled>
    <img src="https://github.com/SWE-agent/swe-agent-media/blob/main/media/mini/png/mini2.png?raw=true"
         data-gif="https://github.com/SWE-agent/swe-agent-media/blob/main/media/mini/gif/mini2.gif?raw=true"
         alt="miniv" data-glightbox="false" />
  </div>
</td>
</tr>
<tr>
<td>
<a href="usage/swebench/"><strong>Batch inference</strong></a>
</td>
<td>
<a href="usage/inspector/"><strong>Trajectory browser</strong></a>
</td>
</tr>
<tr>
<td>
<div class="gif-container" data-glightbox-disabled>
  <img src="https://github.com/SWE-agent/swe-agent-media/blob/main/media/mini/png/swebench.png?raw=true"
       data-gif="https://github.com/SWE-agent/swe-agent-media/blob/main/media/mini/gif/swebench.gif?raw=true"
       alt="swebench" data-glightbox="false" />
</div>
</td>
<td>
<div class="gif-container" data-glightbox-disabled>
  <img src="https://github.com/SWE-agent/swe-agent-media/blob/main/media/mini/png/inspector.png?raw=true"
       data-gif="https://github.com/SWE-agent/swe-agent-media/blob/main/media/mini/gif/inspector.gif?raw=true"
       alt="inspector" data-glightbox="false" />
</div>
</td>
</tr>
<tr>
<td>
<a href="advanced/cookbook/"><strong>Python bindings</strong></a>
</td>
<td>
<a href="https://mini-swe-agent.com"><strong>More in the docs</strong></a>
</td>
</tr>
<tr>
<td>
<pre><code class="language-python">agent = DefaultAgent(
    LitellmModel(model_name=...),
    LocalEnvironment(),
)
agent.run("Write a sudoku game")</code></pre>
</td>
<td>
<ul>
<li><a href="quickstart">Quick start</a></li>
<li><a href="usage/mini"><code>mini</code></a></li>
<li><a href="faq">FAQ</a></li>
<li><a href="advanced/configuration">Configuration</a></li>
<li><a href="advanced/cookbook">Power up</a></li>
</ul>
</td>
</tr>
</table>


## Continue reading:

<div class="grid cards">
  <a href="quickstart/" class="nav-card-link">
    <div class="nav-card">
      <div class="nav-card-header">
        <span class="material-icons nav-card-icon">launch</span>
        <span class="nav-card-title">Installation & Quick Start</span>
      </div>
      <p class="nav-card-description">Get started with mini-SWE-agent</p>
    </div>
  </a>

  <a href="usage/mini/" class="nav-card-link">
    <div class="nav-card">
      <div class="nav-card-header">
        <span class="material-icons nav-card-icon">flash_on</span>
        <span class="nav-card-title">Usage: Simple UI</span>
      </div>
      <p class="nav-card-description">Learn to use the <code>mini</code> command</p>
    </div>
  </a>

  <a href="usage/mini_v/" class="nav-card-link">
    <div class="nav-card">
      <div class="nav-card-header">
        <span class="material-icons nav-card-icon">visibility</span>
        <span class="nav-card-title">Usage: Visual UI</span>
      </div>
      <p class="nav-card-description">Try the visual interface with <code>mini -v</code></p>
    </div>
  </a>

  <a href="faq/" class="nav-card-link">
    <div class="nav-card">
      <div class="nav-card-header">
        <span class="material-icons nav-card-icon">help</span>
        <span class="nav-card-title">FAQ</span>
      </div>
      <p class="nav-card-description">Common questions and answers</p>
    </div>
  </a>

  <a href="advanced/configuration/" class="nav-card-link">
    <div class="nav-card">
      <div class="nav-card-header">
        <span class="material-icons nav-card-icon">settings</span>
        <span class="nav-card-title">Configuration</span>
      </div>
      <p class="nav-card-description">Setup and customize your agent</p>
    </div>
  </a>

  <a href="advanced/cookbook/" class="nav-card-link">
    <div class="nav-card">
      <div class="nav-card-header">
        <span class="material-icons nav-card-icon">fitness_center</span>
        <span class="nav-card-title">Power up</span>
      </div>
      <p class="nav-card-description">Start hacking the agent!</p>
    </div>
  </a>
</div>

## 📣 New features

Please check the [github release notes](https://github.com/SWE-agent/mini-swe-agent/releases) for the latest updates.

## 📣 Documentation updates

* Jul 27: More notes on [local models](advanced/local_models.md)

{% include-markdown "_footer.md" %}



================================================
FILE: docs/quickstart.md
================================================
# Quick start

!!! tip "Installation Options"

    === "pip"

        Use pip to install `mini` in your current environment:

        ```bash
        pip install mini-swe-agent
        ```

        And try our command line interface

        ```bash
        mini  # simple UI
        mini -v  # visual UI
        mini-extra  # extra utilities
        ```

    === "uv (isolated)"

        Use `uv`/`uvx` ([installation](https://docs.astral.sh/uv/getting-started/installation/)) to install & run the `mini` agent in an isolated environment.

        Quickly install + run:

        ```bash
        uvx mini-swe-agent  # simple UI
        uvx mini-swe-agent -v  # visual UI
        uvx --from mini-swe-agent mini-extra  # extra utilities
        ```

        Permanently install

        ```bash
        uv tool install mini-swe-agent
        # then
        mini  # simple UI
        mini -v  # visual UI
        mini-extra  # extra utilities
        ```

    === "pipx (isolated)"

        Use pipx ([installation](https://pipx.pypa.io/stable/installation/)) to install & run `mini` in an isolated environment.

        Quick install + run:

        ```bash
        # Simple UI
        pipx run mini-swe-agent
        # Textual UI
        pipx run mini-swe-agent -v
        # Extra utilities
        pipx run --spec mini-swe-agent mini-extra
        ```

        or for a persistent installation (recommended):

        ```bash
        pipx install mini-swe-agent
        # then
        mini  # simple UI
        mini -v  # visual UI
        mini-extra  # extra utilities
        ```

        If the invocation doesn't immediately work, you might need to run `pipx ensurepath`.

    === "From source/dev"

        For development or if you want to customize the agent:

        ```bash
        git clone https://github.com/SWE-agent/mini-swe-agent.git
        cd mini-swe-agent
        pip install -e .
        ```

        Then run:

        ```bash
        mini  # simple UI
        mini -v  # visual UI
        mini-extra  # extra utilities
        ```

        Or pick a [run script](https://github.com/SWE-agent/mini-swe-agent/tree/main/src/minisweagent/run):

        ```bash
        python src/minisweagent/run/hello_world.py
        ```

        If you are planning to contribute, please also install the dev dependencies
        and `pre-commit` hooks:

        ```bash
        pip install -e '.[dev]'
        pip install pre-commit && pre-commit install
        ```

        To check your installation, you can run `pytest -n auto` in the root folder.
        This should run all tests in parallel (should take ~3min to run).

        Note that there are still some extra dependencies that are not installed by default
        (basically anything that is in an `.../extra/...` folder).
        If you truly want to get the maximal package, you can run `pip install -e '.[full]'`

!!! note "Changelog"

    Please see the [github release notes](https://github.com/SWE-agent/mini-swe-agent/releases) for recent changes.

!!! example "Example Prompts"

    Try mini-SWE-agent with these example prompts:

    - Implement a Sudoku solver in python in the `sudoku` folder. Make sure the codebase is modular and well tested with pytest.
    - Please run pytest on the current project, discover failing unittests and help me fix them. Always make sure to test the final solution.
    - Help me document & type my codebase by adding short docstrings and type hints.

## Models

!!! note "Models should be set up the first time you run `mini`"

    If you missed the setup wizard, just run `mini-extra config setup`, or take a look at the following section.
    If you want to use local models, please check this [guide](advanced/local_models.md).

!!! success "Which model to use?"

    We recommend using `claude-sonnet-4-20250514` for most tasks.
    For openai models, we recommend using `openai/gpt-5` or `openai/gpt-5-mini`.
    You can check scores of different models at our [SWE-bench (bash-only)](https://swebench.com) leaderboard.

### Setting API keys

There are several ways to set your API keys:

* **Recommended**: Run our setup script: `mini-extra config setup`. This should also run automatically the first time you run `mini`.
* Use `mini-extra config set ANTHROPIC_API_KEY <your-api-key>` to put the key in the `mini` [config file](advanced/configuration.md).
* Export your key as an environment variable: `export ANTHROPIC_API_KEY=<your-api-key>` (this is not persistent if you restart your shell, unless you add it to your shell config, like `~/.bashrc` or `~/.zshrc`).
* If you only use a single model, you can also set `MSWEA_MODEL_API_KEY` (as environment variable or in the config file). This takes precedence over all other keys.
* If you run several agents in parallel, see our note about rotating anthropic keys [here](advanced/configuration.md).

??? note "All the API key names"

    Here's a list of all the API key names available in `litellm`:

    ```
    ALEPH_ALPHA_API_KEY
    ALEPHALPHA_API_KEY
    ANTHROPIC_API_KEY
    ANYSCALE_API_KEY
    AZURE_AI_API_KEY
    AZURE_API_KEY
    AZURE_OPENAI_API_KEY
    BASETEN_API_KEY
    CEREBRAS_API_KEY
    CLARIFAI_API_KEY
    CLOUDFLARE_API_KEY
    CO_API_KEY
    CODESTRAL_API_KEY
    COHERE_API_KEY
    DATABRICKS_API_KEY
    DEEPINFRA_API_KEY
    DEEPSEEK_API_KEY
    FEATHERLESS_AI_API_KEY
    FIREWORKS_AI_API_KEY
    FIREWORKS_API_KEY
    FIREWORKSAI_API_KEY
    GEMINI_API_KEY
    GROQ_API_KEY
    HUGGINGFACE_API_KEY
    INFINITY_API_KEY
    MARITALK_API_KEY
    MISTRAL_API_KEY
    NEBIUS_API_KEY
    NLP_CLOUD_API_KEY
    NOVITA_API_KEY
    NVIDIA_NIM_API_KEY
    OLLAMA_API_KEY
    OPENAI_API_KEY
    OPENAI_LIKE_API_KEY
    OPENROUTER_API_KEY
    OR_API_KEY
    PALM_API_KEY
    PERPLEXITYAI_API_KEY
    PREDIBASE_API_KEY
    PROVIDER_API_KEY
    REPLICATE_API_KEY
    TOGETHERAI_API_KEY
    VOLCENGINE_API_KEY
    VOYAGE_API_KEY
    WATSONX_API_KEY
    WX_API_KEY
    XAI_API_KEY
    XINFERENCE_API_KEY
    ```

### Selecting a model

* **Recommended**: `mini-extra config setup` (should be run the first time you run `mini`) can set the default model for you
* All command line interfaces allow you to set the model name with `-m` or `--model`.
* In addition, you can set the default model with `mini-extra config set MSWEA_MODEL_NAME <model-name>`, by editing the global [config file](advanced/configuration.md) (shortcut: `mini-extra config edit`), or by setting the `MSWEA_MODEL_NAME` environment variable.
* You can also set your model in a config file (key `model_name` under `model`).
* If you want to use local models, please check this [guide](advanced/local_models.md).


### GPT-5 and friends <a name="gpt-5"></a>

!!! tip "litellm versions"

    If you upgrade `litellm` (`pip install -U litellm`), you probably don't need to register the models
    and don't need to follow the instructions below.

`gpt-5` and friends are not included in `litellm` yet, so we need to register them manually.
For this, first create the following file:

??? note "model_registry.json"

    ```json
    {
        "gpt-5": {
            "max_tokens": 128000,
            "max_input_tokens": 400000,
            "max_output_tokens": 128000,
            "input_cost_per_token": 1.25e-06,
            "output_cost_per_token": 1e-05,
            "cache_read_input_token_cost": 1.25e-07,
            "litellm_provider": "openai",
            "mode": "chat",
            "supported_endpoints": [
                "/v1/chat/completions",
                "/v1/batch",
                "/v1/responses"
            ],
            "supported_modalities": [
                "text",
                "image"
            ],
            "supported_output_modalities": [
                "text"
            ],
            "supports_pdf_input": true,
            "supports_function_calling": true,
            "supports_parallel_function_calling": true,
            "supports_response_schema": true,
            "supports_vision": true,
            "supports_prompt_caching": true,
            "supports_system_messages": true,
            "supports_tool_choice": true,
            "supports_native_streaming": true,
            "supports_reasoning": true
        },
        "gpt-5-mini": {
            "max_tokens": 128000,
            "max_input_tokens": 400000,
            "max_output_tokens": 128000,
            "input_cost_per_token": 2.5e-07,
            "output_cost_per_token": 2e-06,
            "cache_read_input_token_cost": 2.5e-08,
            "litellm_provider": "openai",
            "mode": "chat",
            "supported_endpoints": [
                "/v1/chat/completions",
                "/v1/batch",
                "/v1/responses"
            ],
            "supported_modalities": [
                "text",
                "image"
            ],
            "supported_output_modalities": [
                "text"
            ],
            "supports_pdf_input": true,
            "supports_function_calling": true,
            "supports_parallel_function_calling": true,
            "supports_response_schema": true,
            "supports_vision": true,
            "supports_prompt_caching": true,
            "supports_system_messages": true,
            "supports_tool_choice": true,
            "supports_native_streaming": true,
            "supports_reasoning": true
        },
        "gpt-5-nano": {
            "max_tokens": 128000,
            "max_input_tokens": 400000,
            "max_output_tokens": 128000,
            "input_cost_per_token": 5e-08,
            "output_cost_per_token": 4e-07,
            "cache_read_input_token_cost": 5e-09,
            "litellm_provider": "openai",
            "mode": "chat",
            "supported_endpoints": [
                "/v1/chat/completions",
                "/v1/batch",
                "/v1/responses"
            ],
            "supported_modalities": [
                "text",
                "image"
            ],
            "supported_output_modalities": [
                "text"
            ],
            "supports_pdf_input": true,
            "supports_function_calling": true,
            "supports_parallel_function_calling": true,
            "supports_response_schema": true,
            "supports_vision": true,
            "supports_prompt_caching": true,
            "supports_system_messages": true,
            "supports_tool_choice": true,
            "supports_native_streaming": true,
            "supports_reasoning": true
        },
        "gpt-5-chat": {
            "max_tokens": 32768,
            "max_input_tokens": 1047576,
            "max_output_tokens": 32768,
            "input_cost_per_token": 5e-06,
            "output_cost_per_token": 2e-05,
            "input_cost_per_token_batches": 2.5e-06,
            "output_cost_per_token_batches": 1e-05,
            "cache_read_input_token_cost": 1.25e-06,
            "litellm_provider": "openai",
            "mode": "chat",
            "supported_endpoints": [
                "/v1/chat/completions",
                "/v1/batch",
                "/v1/responses"
            ],
            "supported_modalities": [
                "text",
                "image"
            ],
            "supported_output_modalities": [
                "text"
            ],
            "supports_pdf_input": true,
            "supports_function_calling": true,
            "supports_parallel_function_calling": true,
            "supports_response_schema": true,
            "supports_vision": true,
            "supports_prompt_caching": true,
            "supports_system_messages": true,
            "supports_tool_choice": true,
            "supports_native_streaming": true
        },
        "gpt-5-chat-latest": {
            "max_tokens": 128000,
            "max_input_tokens": 400000,
            "max_output_tokens": 128000,
            "input_cost_per_token": 1.25e-06,
            "output_cost_per_token": 1e-05,
            "cache_read_input_token_cost": 1.25e-07,
            "litellm_provider": "openai",
            "mode": "chat",
            "supported_endpoints": [
                "/v1/chat/completions",
                "/v1/batch",
                "/v1/responses"
            ],
            "supported_modalities": [
                "text",
                "image"
            ],
            "supported_output_modalities": [
                "text"
            ],
            "supports_pdf_input": true,
            "supports_function_calling": true,
            "supports_parallel_function_calling": true,
            "supports_response_schema": true,
            "supports_vision": true,
            "supports_prompt_caching": true,
            "supports_system_messages": true,
            "supports_tool_choice": true,
            "supports_native_streaming": true,
            "supports_reasoning": true
        },
        "gpt-5-2025-08-07": {
            "max_tokens": 128000,
            "max_input_tokens": 400000,
            "max_output_tokens": 128000,
            "input_cost_per_token": 1.25e-06,
            "output_cost_per_token": 1e-05,
            "cache_read_input_token_cost": 1.25e-07,
            "litellm_provider": "openai",
            "mode": "chat",
            "supported_endpoints": [
                "/v1/chat/completions",
                "/v1/batch",
                "/v1/responses"
            ],
            "supported_modalities": [
                "text",
                "image"
            ],
            "supported_output_modalities": [
                "text"
            ],
            "supports_pdf_input": true,
            "supports_function_calling": true,
            "supports_parallel_function_calling": true,
            "supports_response_schema": true,
            "supports_vision": true,
            "supports_prompt_caching": true,
            "supports_system_messages": true,
            "supports_tool_choice": true,
            "supports_native_streaming": true,
            "supports_reasoning": true
        },
        "gpt-5-mini-2025-08-07": {
            "max_tokens": 128000,
            "max_input_tokens": 400000,
            "max_output_tokens": 128000,
            "input_cost_per_token": 2.5e-07,
            "output_cost_per_token": 2e-06,
            "cache_read_input_token_cost": 2.5e-08,
            "litellm_provider": "openai",
            "mode": "chat",
            "supported_endpoints": [
                "/v1/chat/completions",
                "/v1/batch",
                "/v1/responses"
            ],
            "supported_modalities": [
                "text",
                "image"
            ],
            "supported_output_modalities": [
                "text"
            ],
            "supports_pdf_input": true,
            "supports_function_calling": true,
            "supports_parallel_function_calling": true,
            "supports_response_schema": true,
            "supports_vision": true,
            "supports_prompt_caching": true,
            "supports_system_messages": true,
            "supports_tool_choice": true,
            "supports_native_streaming": true,
            "supports_reasoning": true
        },
        "gpt-5-nano-2025-08-07": {
            "max_tokens": 128000,
            "max_input_tokens": 400000,
            "max_output_tokens": 128000,
            "input_cost_per_token": 5e-08,
            "output_cost_per_token": 4e-07,
            "cache_read_input_token_cost": 5e-09,
            "litellm_provider": "openai",
            "mode": "chat",
            "supported_endpoints": [
                "/v1/chat/completions",
                "/v1/batch",
                "/v1/responses"
            ],
            "supported_modalities": [
                "text",
                "image"
            ],
            "supported_output_modalities": [
                "text"
            ],
            "supports_pdf_input": true,
            "supports_function_calling": true,
            "supports_parallel_function_calling": true,
            "supports_response_schema": true,
            "supports_vision": true,
            "supports_prompt_caching": true,
            "supports_system_messages": true,
            "supports_tool_choice": true,
            "supports_native_streaming": true,
            "supports_reasoning": true
        }
    }
    ```

Now tell `mini` where to find the file, e.g.,

```bash
mini-extra config set LITELLM_MODEL_REGISTRY_PATH $HOME/model_registry.json
```

Now you're good to go! The only thing to keep in mind is to

1. Reference the model together with the provider, e.g., `openai/gpt-5` (rather than just `gpt-5`)
2. Select a config file without temperature setting, e.g., [`mini_no_temp.yaml`](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/config/mini_no_temp.yaml)

Here's a few examples:

=== "GPT-5"

    ```bash
    mini -v -m openai/gpt-5 -c mini_no_temp
    ```

=== "GPT-5-mini"

    ```bash
    mini -v -m openai/gpt-5-mini -c mini_no_temp
    ```

=== "GPT-5-nano"

    ```bash
    mini -v -m openai/gpt-5-nano -c mini_no_temp
    ```

Or with the [visual UI](usage/mini_v.md):

=== "GPT-5"

    ```bash
    mini -v -m openai/gpt-5 -c mini_no_temp
    ```

=== "GPT-5-mini"

    ```bash
    mini -v -m openai/gpt-5-mini -c mini_no_temp
    ```

=== "GPT-5-nano"

    ```bash
    mini -v -m openai/gpt-5-nano -c mini_no_temp
    ```

{% include-markdown "_footer.md" %}



================================================
FILE: docs/SECURITY.md
================================================
# Security Policy

## Reporting a Vulnerability

Please contact Kilian Lieret (kl5675@princeton.edu), John Yang (johnby@stanford.edu), Carlos E. Jimenez (carlosej@princeton.edu), and Ofir Press (ofirp@princeton.edu).


================================================
FILE: docs/advanced/configuration.md
================================================
# Configuration

!!! abstract "Configuring mini"

    * This guide shows how to configure the `mini` agent.
    * You should already be familiar with the [quickstart guide](../quickstart.md).
    * Want more? See the [cookbook](cookbook.md) for subclassing & developing your own agent.

## Environment variables and global configuration

!!! tip "Setting up models"

    Setting up models is also covered in the [quickstart guide](../quickstart.md).

All global configuration can be either set as environment variables, or in the `.env` file (the exact location is printed when you run `mini`).
Environment variables take precedence over variables set in the `.env` file.

We provide several helper functions to update the global configuration.

For example, to set the default model and API keys, you can run:

```bash
mini-extra config setup
```

or to update specific settings:

```
mini-extra config set KEY VALUE
# e.g.,
mini-extra config set MSWEA_MODEL_NAME "claude-sonnet-4-20250514"
mini-extra config set MSWEA_MODEL_API_KEY "sk-..."
```

or to unset a key:

```bash
mini-extra config unset KEY
# e.g.,
mini-extra config unset MSWEA_MODEL_API_KEY
```

You can also edit the `.env` file directly and we provide a helper function for that:

```bash
mini-extra config edit
```

To set environment variables (recommended for temporary experiemntation or API keys):

```bash
export KEY="value"
# windows:
setx KEY "value"
```

### Models and keys

!!! tip "See also"

    Read the [quickstart guide](../quickstart.md) first—it already covers most of this.

```bash
# Default model name
# (default: not set)
MSWEA_MODEL_NAME="claude-sonnet-4-20250514"

# Default API key
# (default: not set)
MSWEA_MODEL_API_KEY="sk-..."
```

To register extra models to litellm (see [local models](local_models.md) for more details), you can either specify the path in the agent file, or set

```bash
LITELLM_MODEL_REGISTRY_PATH="/path/to/your/model/registry.json"
```

For Anthropic models, you can also use `ANTHROPIC_API_KEYS` for advanced parallel execution:

```bash
# Multiple Anthropic keys for parallel execution (separated by "::")
ANTHROPIC_API_KEYS="key1::key2::key3"
```

This allows different threads to use different API keys to avoid prompt caching conflicts when running multiple agents in parallel.

Global cost limits:

```bash
# Global limit on number of model calls (0 = no limit)
# (default: 0)
MSWEA_GLOBAL_CALL_LIMIT="100"

# Global cost limit in dollars (0 = no limit)
# (default: 0)
MSWEA_GLOBAL_COST_LIMIT="10.00"
```

### Default config files

```bash
# Set a custom directory for agent config files in addition to the builtin ones
# This allows to specify them by names
MSWEA_CONFIG_DIR="/path/to/your/own/config/dir"

# Config path for mini run script
# (default: package_dir / "config" / "mini.yaml")
MSWEA_MINI_CONFIG_PATH="/path/to/your/own/config"

# Config path for GitHub issue script
# (default: package_dir / "config" / "github_issue.yaml")
MSWEA_GITHUB_CONFIG_PATH="/path/to/your/github/config.yaml"

# Custom style path for trajectory inspector
# (default: package_dir / "config" / "mini.tcss")
MSWEA_INSPECTOR_STYLE_PATH="/path/to/your/inspector/style.tcss"

# Custom style path for mini textual interface
# (default: package_dir / "config" / "mini.tcss")
MSWEA_MINI_STYLE_PATH="/path/to/your/mini/style.tcss"
```

### Settings for environments

```bash
# Path/name to the singularity/apptainer executable
# (default: "singularity")
MSWEA_SINGULARITY_EXECUTABLE="singularity"

# Path/name to the docker executable
# (default: "docker")
MSWEA_DOCKER_EXECUTABLE="docker"
```

### Default run files

```bash
# Default run script entry point for the main CLI
# (default: "minisweagent.run.mini")
MSWEA_DEFAULT_RUN="minisweagent.run.mini"

# Set to true to use visual mode by default for the main CLI
# (default: false)
MSWEA_VISUAL_MODE_DEFAULT="false"
```

## Agent configuration files

Configuration files look like this:

??? note "Configuration file"

    ```yaml
    --8<-- "src/minisweagent/config/mini.yaml"
    ```

We use [Jinja2](https://jinja.palletsprojects.com/) to render templates (e.g., the instance template).
TL;DR: You include variables with double curly braces, e.g. `{{task}}`, but you can also do fairly complicated logic like this:

??? note "Example: Dealing with long observations"

    ```jinja
    <returncode>{{output.returncode}}</returncode>
    {% if output.output | length < 10000 -%}
        <output>
            {{ output.output -}}
        </output>
    {%- else -%}
        <warning>
            The output of your last command was too long.
            Please try a different command that produces less output.
            If you're looking at a file you can try use head, tail or sed to view a smaller number of lines selectively.
            If you're using grep or find and it produced too much output, you can use a more selective search pattern.
            If you really need to see something from the full command's output, you can redirect output to a file and then search in that file.
        </warning>

        {%- set elided_chars = output.output | length - 10000 -%}

        <output_head>
            {{ output.output[:5000] }}
        </output_head>

        <elided_chars>
            {{ elided_chars }} characters elided
        </elided_chars>

        <output_tail>
            {{ output.output[-5000:] }}
        </output_tail>
    {%- endif -%}
    ```

In all builtin agents, you can use the following variables:

- Environment variables (`LocalEnvironment` only, see discussion [here](https://github.com/SWE-agent/mini-swe-agent/pull/425))
- Agent config variables
- Environment config variables
- Explicitly passed variables (`observation`, `task` etc.) depending on the template

{% include-markdown "_footer.md" %}


================================================
FILE: docs/advanced/control_flow.md
================================================
# Agent control flow

!!! abstract "Understanding the default agent"

    * This guide shows the control flow of the default agent.
    * After this, you're ready to [remix & extend mini](cookbook.md)

The following diagram shows the control flow of the mini agent:

<div align="center">
    <img src="../../assets/mini_control_flow.svg" alt="Agent control flow" style="max-width: 600px;" />
</div>

And here is the code that implements it:

??? note "Default agent class"

    - [Read on GitHub](https://github.com/swe-agent/mini-swe-agent/blob/main/src/minisweagent/agents/default.py)
    - [API reference](../reference/agents/default.md)

    ```python
    --8<-- "src/minisweagent/agents/default.py"
    ```

Essentially, `DefaultAgent.run` calls `DefaultAgent.step` in a loop until the agent has finished its task.

The `step` method is the core of the agent. It does the following:

1. Queries the model for a response based on the current messages (`DefaultAgent.query`, calling `Model.query`)
2. Parses the response to get the action, i.e., the shell command to execute (`DefaultAgent.parse_action`)
3. Executes the action in the environment (`DefaultAgent.execute_action`, calling `Environment.execute`)
4. Renders the observation message with `DefaultAgent.render_template`
5. Adds the observation to the messages

The interesting bit is how we handle error conditions and the finish condition:
This uses exceptions of two types: `TerminatingException` and `NonTerminatingException`.

- `TerminatingException` is raised when the agent has finished its task or we hit a limit (cost, step limit, etc.)
- `NonTerminatingException` is raised when the agent has not finished its task, but we want to continue the loop.
   In this case, all we need to do is to add a new message to the messages list, so that the LM can see the new state.
   There are two typical cases that we handle this way:

    1. `TimeoutError`: the action took too long to execute (we show partial output)
    2. `FormatError`: the output from the LM contained zero or multiple actions (we show the error message)

The `DefaultAgent.run` method catches these exceptions and handles them by adding the corresponding message to the messages list and continuing the loop.

```python
while True:
    try:
        self.step()
    except NonTerminatingException as e:
        self.add_message("user", str(e))
    except TerminatingException as e:
        self.add_message("user", str(e))
        return type(e).__name__, str(e)
```

Using exceptions for the control flow is a lot easier than passing around flags and states, especially when extending or subclassing the agent.


================================================
FILE: docs/advanced/cookbook.md
================================================
# Cookbook

!!! abstract "Remixing & extending mini"

    * This guide shows how to mix the different components of the `mini` agent to create your own custom version.
    * You might want to first take a look at the [control flow of the default agent](control_flow.md) first


!!! note "Development setup"

    Make sure to follow the dev setup instructions in [quickstart.md](../quickstart.md).

We provide several different entry points to the agent,
for example [hello world](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/run/hello_world.py),
or the [default when calling `mini`](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/run/mini.py).

Want to cook up your custom version and the config is not enough?
Just follow the recipe below:

1. What's the control flow you need? Pick an [agent class](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/agents) (e.g., [simplest example](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/agents/default.py), [with human in the loop](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/agents/interactive.py))
2. How should actions be executed? Pick an [environment class](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/environments) (e.g., [local](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/environments/local.py), or [docker](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/environments/docker.py))
3. How is the LM queried? Pick a [model class](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/models) (e.g., [litellm](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/models/litellm_model.py))
4. How to invoke the agent? Bind them all together in a [run script](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/run), possibly reading from a [config](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/config) (e.g., [hello world](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/run/hello_world.py), or [`mini` entry point](https://github.com/SWE-agent/mini-swe-agent/blob/main/src/minisweagent/run/mini.py))

We aim to keep all of these components very simple, but offer lots of choice between them -- enough to cover a broad range of
things that you might want to do.

You can override the default entry point by setting the `MSWEA_DEFAULT_RUN` environment variable to the import path of your run script.

## Mix & match

### Models

=== "Hello world (use automatic model selection)"

    ```python
    from minisweagent.agents.default import DefaultAgent
    from minisweagent.models import get_model
    from minisweagent.environments.local import LocalEnvironment

    model_name = "claude-sonnet-4-20250514"

    agent = DefaultAgent(
        get_model(model_name=model_name),
        LocalEnvironment(),
    )
    agent.run(task)
    ```

=== "Hello world (Anthropic)"

    ```python
    from minisweagent.agents.default import DefaultAgent
    from minisweagent.models.anthropic_model import AnthropicModel
    from minisweagent.environments.local import LocalEnvironment

    model_name = "claude-sonnet-4-20250514"

    agent = DefaultAgent(
        AnthropicModel(model_name=model_name),
        LocalEnvironment(),
    )
    agent.run(task)
    ```

=== "Hello world (Litellm)"

    ```python
    from minisweagent.agents.default import DefaultAgent
    from minisweagent.models.litellm_model import LitellmModel
    from minisweagent.environments.local import LocalEnvironment

    model_name = "gpt-4o"

    agent = DefaultAgent(
        LitellmModel(model_name=model_name),
        LocalEnvironment(),
    )
    agent.run(task)
    ```

### Environments

=== "Hello world with local execution"

    ```python
    from minisweagent.environments.local import LocalEnvironment

    agent = DefaultAgent(
        LitellmModel(model_name=model_name),
        LocalEnvironment(),
    )
    ```

=== "Hello world with docker execution"

    ```python
    from minisweagent.environments.docker import DockerEnvironment

    agent = DefaultAgent(
        LitellmModel(model_name=model_name),
        DockerEnvironment(),
    )
    ```

### Agents

=== "Default agent"

    ```python
    from minisweagent.agents.default import DefaultAgent
    from minisweagent.models import get_model
    from minisweagent.environments.local import LocalEnvironment

    agent = DefaultAgent(
        get_model(model_name=model_name),
        LocalEnvironment(),
    )
    ```

=== "Human in the loop"

    ```python
    from minisweagent.agents.interactive import InteractiveAgent
    from minisweagent.models import get_model
    from minisweagent.environments.local import LocalEnvironment

    agent = InteractiveAgent(
        LitellmModel(model_name=model_name),
        LocalEnvironment(),
    )
    ```

=== "Human in the loop (textual)"

    ```python
    from minisweagent.agents.interactive_textual import TextualAgent
    from minisweagent.models import get_model
    from minisweagent.environments.local import LocalEnvironment

    agent = TextualAgent(
        LitellmModel(model_name=model_name),
        LocalEnvironment(),
    )
    ```

## Advanced

### Customizing execution

An agent that uses python function for some actions:


=== "Subclassing the agent"

    ```python
    from minisweagent.agents.default import DefaultAgent
    import shlex

    def python_function(*args) -> dict:
        ...
        return {"output": "..."}

    class AgentWithPythonFunctions(DefaultAgent):
        def execute_action(self, action: dict) -> dict:
            if action["action"].startswith("python_function"):
                args = shlex.split(action["action"].removeprefix("python_function").strip())
                return python_function(*args)
            return super().execute_action(action)
    ```


=== "Subclassing the environment"

    ```python
    from minisweagent.agents.default import DefaultAgent
    import shlex

    def python_function(*args) -> dict:
        ...
        return {"output": "..."}

    class EnvironmentWithPythonFunctions(LocalEnvironment):
        def execute(self, command: str, cwd: str = "") -> dict:
            if command.startswith("python_function"):
                args = shlex.split(command.removeprefix("python_function").strip())
                return python_function(*args)
            return super().execute(command, cwd)

    agent = DefaultAgent(
        LitellmModel(model_name=model_name),
        EnvironmentWithPythonFunctions(),
    )
    ```

An agent that exits when the `submit` command is issued:

=== "Subclassing the agent"

    ```python
    from minisweagent.agents.default import DefaultAgent, Submitted

    class AgentQuitsOnSubmit(DefaultAgent):
        def execute_action(self, action: dict) -> dict:
            if action["action"] == "submit":
                # The `Submitted` exception will be caught by the agent and
                # the final output will be printed.
                raise Submitted("The agent has finished its task.")
            return super().execute_action(action)
    ```

=== "Subclassing the environment"

    ```python
    from minisweagent.agents.default import DefaultAgent, Submitted
    from minisweagent.environments.local import LocalEnvironment

    class EnvironmentQuitsOnSubmit(LocalEnvironment):
        def execute(self, command: str, cwd: str = "") -> dict:
            if command == "submit":
                raise Submitted("The agent has finished its task.")
            return super().execute(command, cwd)

    agent = DefaultAgent(
        LitellmModel(model_name=model_name),
        EnvironmentQuitsOnSubmit(),
    )
    ```


An agent that validates actions before execution (also an example of how to use an extended config class):

=== "Subclassing the agent"

    ```python
    import re
    from dataclasses import dataclass
    from minisweagent.agents.default import (
        DefaultAgent, NonTerminatingException, DefaultAgentConfig
    )

    @dataclass
    class ValidatingAgentConfig(DefaultAgentConfig):
        forbidden_patterns: list[str] = [
            r"rm -rf /",
            r"sudo.*passwd",
            r"mkfs\.",
        ]

    class ValidatingAgent(DefaultAgent):
        def __init__(self, *args, **kwargs):
            super().__init__(*args, **kwargs, config_class=ValidatingAgentConfig)

        def execute_action(self, action: dict) -> dict:
            for pattern in self.config.forbidden_patterns:
                if re.search(pattern, action["action"], re.IGNORECASE):
                    raise NonTerminatingException("Action blocked")
            return super().execute_action(action)
    ```

=== "Subclassing the environment"

    ```python
    import re
    from dataclasses import dataclass
    from minisweagent.agents.default import (
        DefaultAgent, NonTerminatingException, DefaultAgentConfig
    )
    from minisweagent.environments.local import LocalEnvironment

    @dataclass
    class EnvironmentWithForbiddenPatternsConfig(LocalEnvironmentConfig):
        forbidden_patterns: list[str] = [
            r"rm -rf /",
            r"sudo.*passwd",
            r"mkfs\.",
        ]

    class EnvironmentWithForbiddenPatterns(LocalEnvironment):
        def __init__(self, *args, **kwargs):
            super().__init__(*args, **kwargs, config_class=EnvironmentWithForbiddenPatternsConfig)

        def execute(self, command: str, cwd: str = "") -> dict:
            for pattern in self.config.forbidden_patterns:
                if re.search(pattern, command, re.IGNORECASE):
                    raise NonTerminatingException("Action blocked")
            return super().execute(command, cwd)

    agent = DefaultAgent(
        LitellmModel(model_name=model_name),
        EnvironmentWithForbiddenPatterns(),
    )
    ```

{% include-markdown "_footer.md" %}



================================================
FILE: docs/advanced/local_models.md
================================================
!!! abstract "Local models"

    * This guide shows how to set up local models.
    * You should already be familiar with the [quickstart guide](../quickstart.md).
    * You should also quickly skim the [configuration guide](configuration.md) to understand
      the global configuration and configuration files.


!!! tip "Examples"

    * [Issue #303](https://github.com/SWE-agent/mini-swe-agent/issues/303) has several examples of how to use local models.
    * We also welcome concrete examples of how to use local models per pull request into this guide.

## Using litellm

Currently, all models are supported via [`litellm`](https://www.litellm.ai/)
(but if you have specific needs, we're open to add more specific model classes in the [`models`](https://github.com/SWE-agent/mini-swe-agent/tree/main/src/minisweagent/models) submodule).

If you use local models, you most likely need to add some extra keywords to the `litellm` call.
This is done with the `model_kwargs` dictionary which is directly passed to `litellm.completion`.

In other words, this is how we invoke litellm:

```python
litellm.completion(
    model=model_name,
    messages=messages,
    **model_kwargs
)
```

You can set `model_kwargs` in an agent config file like the following one:

??? note "Default configuration file"

    ```yaml
    --8<-- "src/minisweagent/config/mini.yaml"
    ```

In the last section, you can add

```yaml
model:
  model_name: "my-local-model"
  model_kwargs:
    custom_llm_provider: "openai"
    api_base="https://..."
    ...
  ...
```

!!! tip "Updating the default `mini` configuration file"

    You can set the `MSWEA_MINI_CONFIG_PATH` setting to set path to the default `mini` configuration file.
    This will allow you to override the default configuration file with your own.
    See the [configuration guide](configuration.md) for more details.

If this is not enough, our model class should be simple to modify:

??? note "Complete model class"

    - [Read on GitHub](https://github.com/swe-agent/mini-swe-agent/blob/main/src/minisweagent/models/litellm_model.py)
    - [API reference](../reference/models/litellm.md)

    ```python
    --8<-- "src/minisweagent/models/litellm_model.py"
    ```

The other part that you most likely need to figure out are costs.
There are two ways to do this with `litellm`:

1. You set up a litellm proxy server (which gives you a lot of control over all the LM calls)
2. You update the model registry (next section)

### Updating the model registry

LiteLLM get its cost and model metadata from [this file](https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json). You can override or add data from this file if it's outdated or missing your desired model by including a custom registry file.

The model registry JSON file should follow LiteLLM's format:

```json
{
  "my-custom-model": {
    "max_tokens": 4096,
    "input_cost_per_token": 0.0001,
    "output_cost_per_token": 0.0002,
    "litellm_provider": "openai",
    "mode": "chat"
  },
  "my-local-model": {
    "max_tokens": 8192,
    "input_cost_per_token": 0.0,
    "output_cost_per_token": 0.0,
    "litellm_provider": "ollama",
    "mode": "chat"
  }
}
```

There are two ways of setting the path to the model registry:

1. Set `LITELLM_MODEL_REGISTRY_PATH` (e.g., `mini-extra config set LITELLM_MODEL_REGISTRY_PATH /path/to/model_registry.json`)
2. Set `litellm_model_registry` in the agent config file

```yaml
model:
  litellm_model_registry: "/path/to/model_registry.json"
  ...
...
```

## Concrete examples

!!! success "Help us fill this section!"

    We welcome concrete examples of how to use local models per pull request into this guide.
    Please add your example here.



================================================
FILE: docs/assets/bubbles.css
================================================
/* Floating bubbles styles */
.floating-bubbles {
  position: fixed;
  bottom: 20px;
  right: 20px;
  display: flex;
  flex-direction: column;
  gap: 10px;
  z-index: 1000;
}

.floating-bubbles-title {
  position: absolute;
  top: -30px;
  right: 0;
  font-size: 12px;
  color: #777;
  text-align: right;
  font-weight: bold;
  opacity: 0;
  visibility: hidden;
  transition:
    opacity 0.3s ease,
    visibility 0.3s ease;
  white-space: nowrap;
}

.floating-bubbles:hover .floating-bubbles-title {
  opacity: 1;
  visibility: visible;
}

.bubble {
  width: 40px;
  height: 40px;
  display: flex;
  justify-content: center;
  align-items: center;
  position: relative;
  transition: transform 0.3s ease;
}

.bubble:hover {
  transform: scale(1.1);
}

.bubble img {
  width: 40px;
  height: 40px;
}

.bubble-tooltip {
  position: absolute;
  right: 60px;
  background-color: #333;
  color: white;
  padding: 5px 10px;
  border-radius: 4px;
  font-size: 14px;
  white-space: nowrap;
  opacity: 0;
  visibility: hidden;
  transition:
    opacity 0.3s ease,
    visibility 0.3s ease;
}

.bubble:hover .bubble-tooltip {
  opacity: 1;
  visibility: visible;
}

.floating-bubbles:hover .bubble-tooltip {
  opacity: 1;
  visibility: visible;
}

/* Hide on mobile */
@media (max-width: 768px) {
  .floating-bubbles {
    display: none;
  }
}



================================================
FILE: docs/assets/custom.css
================================================
[data-md-color-scheme="default"] {
  --md-default-bg-color: #fff7ec;
  --md-primary-fg-color: #000000;
  --md-typeset-a-color: #0099ff;
  --md-code-bg-color: #e7e7e7;
}

[data-md-color-scheme="slate"] {
  --md-primary-fg-color: #000000;
  --md-default-fg-color: #fff7ec;
  --md-default-bg-color: #111111;
}

.clickable-banner {
  color: #000000;
}
/*
.md-main__inner.md-grid,
.md-grid {
  max-width: 64rem;
} */

@media screen and (min-width: 1220px) {
  .md-main__inner.md-grid,
  .md-grid {
    max-width: 64rem;
  }
}

.md-typeset h1,
.md-typeset h2,
.md-typeset h3 {
  font-weight: 400;
  color: var(
    --md-primary-fg-color-dark
  ); /* this actually works for both light and dark themes */
}

.gif-container-styled {
  max-width: 600px;
  border: 1px solid #ccc;
  border-radius: 10px;
  padding: 10px;
}



================================================
FILE: docs/assets/gif-controls.css
================================================
/* GIF Container Styles */
.gif-container {
  position: relative;
  display: inline-block;
  width: 100%;
}

.gif-container img {
  width: 100%;
  height: auto;
  display: block;
  cursor: pointer;
  transition: opacity 0.2s ease;
}

.gif-container img:hover {
  opacity: 0.9;
}

/* Play Button Styles */
.gif-play-button {
  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  background: rgba(0, 0, 0, 0.7);
  border: none;
  border-radius: 50%;
  width: 60px;
  height: 60px;
  cursor: pointer;
  display: flex;
  align-items: center;
  justify-content: center;
  transition: all 0.3s ease;
  z-index: 10;
}

.gif-play-button:hover {
  background: rgba(0, 0, 0, 0.9);
  transform: translate(-50%, -50%) scale(1.1);
}

.gif-play-button::before {
  content: "";
  width: 0;
  height: 0;
  border-left: 20px solid white;
  border-top: 12px solid transparent;
  border-bottom: 12px solid transparent;
  margin-left: 4px;
}

/* Hide play button when GIF is playing */
.gif-container.playing .gif-play-button {
  display: none;
}

/* Disable glightbox for GIF containers */
.gif-container[data-glightbox-disabled] img {
  pointer-events: auto;
}



================================================
FILE: docs/assets/image_theme.css
================================================
.dark-mode-only {
  display: none !important;
}

[data-md-color-scheme="slate"] .dark-mode-only {
  display: inline-block !important;
}

[data-md-color-scheme="slate"] .light-mode-only {
  display: none !important;
}



================================================
FILE: docs/assets/mini-flow.excalidraw
================================================
{
  "type": "excalidraw",
  "version": 2,
  "source": "https://excalidraw.com",
  "elements": [
    {
      "id": "kZqOIU8ASFz1aPjPoOGT_",
      "type": "rectangle",
      "x": 271,
      "y": 299,
      "width": 591.9999999999999,
      "height": 449,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b0i",
      "roundness": {
        "type": 3
      },
      "seed": 1388336696,
      "version": 407,
      "versionNonce": 1547150236,
      "isDeleted": false,
      "boundElements": [
        {
          "id": "NVq54PuJIVl9Yolxt9X8F",
          "type": "text"
        }
      ],
      "updated": 1753229610154,
      "link": null,
      "locked": false
    },
    {
      "id": "NVq54PuJIVl9Yolxt9X8F",
      "type": "text",
      "x": 499.9333343505859,
      "y": 304,
      "width": 134.13333129882812,
      "height": 25,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b0j",
      "roundness": null,
      "seed": 1708323128,
      "version": 307,
      "versionNonce": 2100875292,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1753229610154,
      "link": null,
      "locked": false,
      "text": "run(task: str)",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "top",
      "containerId": "kZqOIU8ASFz1aPjPoOGT_",
      "originalText": "run(task: str)",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "HTA2cFsA6TD3HOxYjeglc",
      "type": "rectangle",
      "x": 333,
      "y": 364,
      "width": 290,
      "height": 309.00000000000006,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b0m",
      "roundness": {
        "type": 3
      },
      "seed": 809412664,
      "version": 481,
      "versionNonce": 873965128,
      "isDeleted": false,
      "boundElements": [
        {
          "type": "text",
          "id": "T_1nTAECG1Rt8O9yQq5yq"
        },
        {
          "id": "u_nHrHddhp0p36EUuscWc",
          "type": "arrow"
        },
        {
          "id": "NBmg1AFHEn5ceGWk2xDKM",
          "type": "arrow"
        },
        {
          "id": "zzMMshCB5YSRPeO0dNmOl",
          "type": "arrow"
        }
      ],
      "updated": 1752083366149,
      "link": null,
      "locked": false
    },
    {
      "id": "T_1nTAECG1Rt8O9yQq5yq",
      "type": "text",
      "x": 447.875,
      "y": 369,
      "width": 60.25,
      "height": 25,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b0n",
      "roundness": null,
      "seed": 1672274232,
      "version": 419,
      "versionNonce": 1267875384,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1752083057527,
      "link": null,
      "locked": false,
      "text": "step()",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "top",
      "containerId": "HTA2cFsA6TD3HOxYjeglc",
      "originalText": "step()",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "3L8DBqrtxQkUhGu3BBIYz",
      "type": "rectangle",
      "x": 362,
      "y": 405,
      "width": 228.99999999999997,
      "height": 46,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b0o",
      "roundness": {
        "type": 3
      },
      "seed": 219617096,
      "version": 135,
      "versionNonce": 1885754184,
      "isDeleted": false,
      "boundElements": [
        {
          "type": "text",
          "id": "Fl5rTU0pyWRYkUf2kyunQ"
        },
        {
          "id": "DQmBitsbagROz_TuQqL2n",
          "type": "arrow"
        }
      ],
      "updated": 1752083583428,
      "link": null,
      "locked": false
    },
    {
      "id": "Fl5rTU0pyWRYkUf2kyunQ",
      "type": "text",
      "x": 442.4166679382324,
      "y": 415.5,
      "width": 68.16666412353516,
      "height": 25,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b0p",
      "roundness": null,
      "seed": 1137297464,
      "version": 100,
      "versionNonce": 139190584,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1752079774055,
      "link": null,
      "locked": false,
      "text": "query()",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "3L8DBqrtxQkUhGu3BBIYz",
      "originalText": "query()",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "x05r-LEq6aSuQXuVGn_8U",
      "type": "rectangle",
      "x": 363,
      "y": 465,
      "width": 232,
      "height": 194.99999999999997,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b0q",
      "roundness": {
        "type": 3
      },
      "seed": 913305416,
      "version": 283,
      "versionNonce": 1913479736,
      "isDeleted": false,
      "boundElements": [
        {
          "type": "text",
          "id": "SppjRVQSpbJuAKoZaH68-"
        }
      ],
      "updated": 1752083298174,
      "link": null,
      "locked": false
    },
    {
      "id": "SppjRVQSpbJuAKoZaH68-",
      "type": "text",
      "x": 391.0166702270508,
      "y": 470,
      "width": 175.96665954589844,
      "height": 25,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b0r",
      "roundness": null,
      "seed": 1883424328,
      "version": 265,
      "versionNonce": 798251832,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1752083298174,
      "link": null,
      "locked": false,
      "text": "get_observation()",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "top",
      "containerId": "x05r-LEq6aSuQXuVGn_8U",
      "originalText": "get_observation()",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "bYq6dzFvm_TNw1-f7jMj4",
      "type": "rectangle",
      "x": 375,
      "y": 506,
      "width": 203,
      "height": 40,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b0s",
      "roundness": {
        "type": 3
      },
      "seed": 1859737928,
      "version": 103,
      "versionNonce": 1943286840,
      "isDeleted": false,
      "boundElements": [
        {
          "type": "text",
          "id": "3FWMRLuVAqkWD07ml7QOF"
        },
        {
          "id": "u6kc0pbufILeAHmUgx8cF",
          "type": "arrow"
        }
      ],
      "updated": 1752083298174,
      "link": null,
      "locked": false
    },
    {
      "id": "3FWMRLuVAqkWD07ml7QOF",
      "type": "text",
      "x": 405.3000030517578,
      "y": 513.5,
      "width": 142.39999389648438,
      "height": 25,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b0t",
      "roundness": null,
      "seed": 1836850744,
      "version": 93,
      "versionNonce": 884010296,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1752083298174,
      "link": null,
      "locked": false,
      "text": "parse_action()",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "bYq6dzFvm_TNw1-f7jMj4",
      "originalText": "parse_action()",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "dRxkIh_02VGAqSvAW-QmL",
      "type": "rectangle",
      "x": 374.5,
      "y": 553,
      "width": 203,
      "height": 40,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b0u",
      "roundness": {
        "type": 3
      },
      "seed": 352567864,
      "version": 129,
      "versionNonce": 1378594360,
      "isDeleted": false,
      "boundElements": [
        {
          "type": "text",
          "id": "TLclLRL9gmt8N3ICMM6nn"
        },
        {
          "id": "mEBWdMtIg-7cEMeByQpsX",
          "type": "arrow"
        }
      ],
      "updated": 1752083353063,
      "link": null,
      "locked": false
    },
    {
      "id": "TLclLRL9gmt8N3ICMM6nn",
      "type": "text",
      "x": 392.7750015258789,
      "y": 560.5,
      "width": 166.4499969482422,
      "height": 25,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b0v",
      "roundness": null,
      "seed": 1690221368,
      "version": 133,
      "versionNonce": 1918654520,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1752083298175,
      "link": null,
      "locked": false,
      "text": "execute_action()",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "dRxkIh_02VGAqSvAW-QmL",
      "originalText": "execute_action()",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "Q5uCucZa7PyaoLoFRD3bK",
      "type": "rectangle",
      "x": 375.5,
      "y": 602,
      "width": 203,
      "height": 40,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b0w",
      "roundness": {
        "type": 3
      },
      "seed": 589377848,
      "version": 179,
      "versionNonce": 1371115336,
      "isDeleted": false,
      "boundElements": [
        {
          "type": "text",
          "id": "KIKnFJ35Hj9qZ7Z16eyj3"
        },
        {
          "id": "IDm-KdgCAnkQVwTA2MCjN",
          "type": "arrow"
        }
      ],
      "updated": 1752083471576,
      "link": null,
      "locked": false
    },
    {
      "id": "KIKnFJ35Hj9qZ7Z16eyj3",
      "type": "text",
      "x": 407.3916702270508,
      "y": 609.5,
      "width": 139.21665954589844,
      "height": 25,
      "angle": 0,
      "strokeColor": "#1e1e1e",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b0x",
      "roundness": null,
      "seed": 1263946296,
      "version": 197,
      "versionNonce": 425568824,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1752083298175,
      "link": null,
      "locked": false,
      "text": "has_finished()",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "Q5uCucZa7PyaoLoFRD3bK",
      "originalText": "has_finished()",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "u_nHrHddhp0p36EUuscWc",
      "type": "arrow",
      "x": 477.9194444444444,
      "y": 678,
      "width": 173.91944444444442,
      "height": 370,
      "angle": 0,
      "strokeColor": "#4dabf7",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b17",
      "roundness": null,
      "seed": 1871456568,
      "version": 631,
      "versionNonce": 146840860,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1753229616456,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          0,
          37
        ],
        [
          -173.91944444444442,
          37
        ],
        [
          -173.91944444444442,
          -333
        ],
        [
          0,
          -333
        ],
        [
          0,
          -319
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "HTA2cFsA6TD3HOxYjeglc",
        "focus": 0.0005555555555555322,
        "gap": 5,
        "fixedPoint": [
          0.4997222222222222,
          1.0121359223300974
        ]
      },
      "endBinding": {
        "elementId": "HTA2cFsA6TD3HOxYjeglc",
        "focus": -0.0005555555555555322,
        "gap": 5,
        "fixedPoint": [
          0.4997222222222222,
          -0.012135922330096947
        ]
      },
      "startArrowhead": null,
      "endArrowhead": "triangle_outline",
      "elbowed": true,
      "fixedSegments": [
        {
          "index": 2,
          "start": [
            0,
            37
          ],
          "end": [
            -173.91944444444442,
            37
          ]
        },
        {
          "index": 3,
          "start": [
            -173.91944444444442,
            37
          ],
          "end": [
            -173.91944444444442,
            -333
          ]
        },
        {
          "index": 4,
          "start": [
            -173.91944444444442,
            -333
          ],
          "end": [
            0,
            -333
          ]
        }
      ],
      "startIsSpecial": false,
      "endIsSpecial": false
    },
    {
      "id": "NilPTEGsdFvqEOCEr-qv2",
      "type": "rectangle",
      "x": 652,
      "y": 508,
      "width": 138,
      "height": 35,
      "angle": 0,
      "strokeColor": "#f08c00",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "dashed",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b19",
      "roundness": {
        "type": 3
      },
      "seed": 1768366392,
      "version": 200,
      "versionNonce": 1342678072,
      "isDeleted": false,
      "boundElements": [
        {
          "type": "text",
          "id": "9hmMn0_iYsWEsGA8XbKXr"
        },
        {
          "id": "u6kc0pbufILeAHmUgx8cF",
          "type": "arrow"
        },
        {
          "id": "NBmg1AFHEn5ceGWk2xDKM",
          "type": "arrow"
        }
      ],
      "updated": 1752083627190,
      "link": null,
      "locked": false
    },
    {
      "id": "9hmMn0_iYsWEsGA8XbKXr",
      "type": "text",
      "x": 661.2333335876465,
      "y": 513,
      "width": 119.53333282470703,
      "height": 25,
      "angle": 0,
      "strokeColor": "#f08c00",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b1A",
      "roundness": null,
      "seed": 14917960,
      "version": 152,
      "versionNonce": 1356716872,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1752083627190,
      "link": null,
      "locked": false,
      "text": "FormatError",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "NilPTEGsdFvqEOCEr-qv2",
      "originalText": "FormatError",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "u6kc0pbufILeAHmUgx8cF",
      "type": "arrow",
      "x": 583,
      "y": 525.9,
      "width": 64,
      "height": 0.5,
      "angle": 0,
      "strokeColor": "#f08c00",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "dashed",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b1F",
      "roundness": null,
      "seed": 846760248,
      "version": 140,
      "versionNonce": 418798904,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1752083627190,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          64,
          -0.5
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "bYq6dzFvm_TNw1-f7jMj4",
        "focus": -0.0050000000000010566,
        "gap": 5,
        "fixedPoint": [
          1.0246305418719213,
          0.49749999999999944
        ]
      },
      "endBinding": {
        "elementId": "NilPTEGsdFvqEOCEr-qv2",
        "focus": 0.005714285714289093,
        "gap": 5,
        "fixedPoint": [
          -0.036231884057971016,
          0.4971428571428565
        ]
      },
      "startArrowhead": null,
      "endArrowhead": "triangle_outline",
      "elbowed": true,
      "fixedSegments": null,
      "startIsSpecial": null,
      "endIsSpecial": null
    },
    {
      "id": "NBmg1AFHEn5ceGWk2xDKM",
      "type": "arrow",
      "x": 794.9832005770699,
      "y": 534.9999992630947,
      "width": 329,
      "height": 189.99999926309465,
      "angle": 0,
      "strokeColor": "#f08c00",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "dashed",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b1G",
      "roundness": null,
      "seed": 670561352,
      "version": 119,
      "versionNonce": 1578316360,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1752083627190,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          35.01679942293015,
          0
        ],
        [
          35.01679942293015,
          -189.99999926309465
        ],
        [
          -293.98320057706985,
          -189.99999926309465
        ],
        [
          -293.98320057706985,
          -175.99999926309465
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "NilPTEGsdFvqEOCEr-qv2",
        "focus": 0.5428571007482621,
        "gap": 4.9960403067158445,
        "fixedPoint": [
          1.0361101491092017,
          0.7714285503741328
        ]
      },
      "endBinding": {
        "elementId": "HTA2cFsA6TD3HOxYjeglc",
        "focus": -1.0323624595469256,
        "gap": 5,
        "fixedPoint": [
          0.5793103448275863,
          -0.016181229773462782
        ]
      },
      "startArrowhead": null,
      "endArrowhead": "triangle_outline",
      "elbowed": true,
      "fixedSegments": [
        {
          "index": 3,
          "start": [
            35.01679942293015,
            -189.99999926309465
          ],
          "end": [
            -293.98320057706985,
            -189.99999926309465
          ]
        }
      ],
      "startIsSpecial": false,
      "endIsSpecial": false
    },
    {
      "id": "AX2aiK31VK7_1UiSLfdc_",
      "type": "rectangle",
      "x": 647,
      "y": 555.5,
      "width": 150.00000000000003,
      "height": 35,
      "angle": 0,
      "strokeColor": "#f08c00",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "dashed",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b1H",
      "roundness": {
        "type": 3
      },
      "seed": 1824429368,
      "version": 271,
      "versionNonce": 631633464,
      "isDeleted": false,
      "boundElements": [
        {
          "type": "text",
          "id": "53J6W7696f9YZjvOZJAmA"
        },
        {
          "id": "mEBWdMtIg-7cEMeByQpsX",
          "type": "arrow"
        },
        {
          "id": "zzMMshCB5YSRPeO0dNmOl",
          "type": "arrow"
        }
      ],
      "updated": 1752083627190,
      "link": null,
      "locked": false
    },
    {
      "id": "53J6W7696f9YZjvOZJAmA",
      "type": "text",
      "x": 656.2583312988281,
      "y": 560.5,
      "width": 131.48333740234375,
      "height": 25,
      "angle": 0,
      "strokeColor": "#f08c00",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b1I",
      "roundness": null,
      "seed": 1135029816,
      "version": 233,
      "versionNonce": 1753590088,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1752083627190,
      "link": null,
      "locked": false,
      "text": "TimeoutError",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "AX2aiK31VK7_1UiSLfdc_",
      "originalText": "TimeoutError",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "mEBWdMtIg-7cEMeByQpsX",
      "type": "arrow",
      "x": 582.5,
      "y": 572.9,
      "width": 59.5,
      "height": 0,
      "angle": 0,
      "strokeColor": "#f08c00",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "dashed",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b1J",
      "roundness": null,
      "seed": 3487800,
      "version": 30,
      "versionNonce": 184184632,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1752083627190,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          59.5,
          0
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "dRxkIh_02VGAqSvAW-QmL",
        "focus": -0.0050000000000010566,
        "gap": 5,
        "fixedPoint": [
          1.0246305418719213,
          0.49749999999999944
        ]
      },
      "endBinding": {
        "elementId": "AX2aiK31VK7_1UiSLfdc_",
        "focus": 0.005714285714285987,
        "gap": 5,
        "fixedPoint": [
          -0.033333333333333326,
          0.4971428571428565
        ]
      },
      "startArrowhead": null,
      "endArrowhead": "triangle_outline",
      "elbowed": true,
      "fixedSegments": null,
      "startIsSpecial": null,
      "endIsSpecial": null
    },
    {
      "id": "zzMMshCB5YSRPeO0dNmOl",
      "type": "arrow",
      "x": 802,
      "y": 572.9,
      "width": 328,
      "height": 227.89999999999998,
      "angle": 0,
      "strokeColor": "#f08c00",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "dashed",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b1K",
      "roundness": null,
      "seed": 874959416,
      "version": 267,
      "versionNonce": 194068552,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1752083627190,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          28,
          0
        ],
        [
          28,
          -227.89999999999998
        ],
        [
          -300,
          -227.89999999999998
        ],
        [
          -300,
          -213.89999999999998
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "AX2aiK31VK7_1UiSLfdc_",
        "focus": -0.005714285714283111,
        "gap": 5,
        "fixedPoint": [
          1.0333333333333332,
          0.4971428571428565
        ]
      },
      "endBinding": {
        "elementId": "HTA2cFsA6TD3HOxYjeglc",
        "focus": -1.0323624595469256,
        "gap": 5,
        "fixedPoint": [
          0.5827586206896552,
          -0.016181229773462782
        ]
      },
      "startArrowhead": null,
      "endArrowhead": "triangle_outline",
      "elbowed": true,
      "fixedSegments": [
        {
          "index": 2,
          "start": [
            28,
            0
          ],
          "end": [
            28,
            -227.89999999999998
          ]
        },
        {
          "index": 3,
          "start": [
            28,
            -227.89999999999998
          ],
          "end": [
            -300,
            -227.89999999999998
          ]
        }
      ],
      "startIsSpecial": false,
      "endIsSpecial": false
    },
    {
      "id": "9vVptv0pJ21kzm7o-XTcN",
      "type": "rectangle",
      "x": 653,
      "y": 603.5,
      "width": 138,
      "height": 35,
      "angle": 0,
      "strokeColor": "#2f9e44",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "dashed",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b1L",
      "roundness": {
        "type": 3
      },
      "seed": 441098808,
      "version": 516,
      "versionNonce": 1694774600,
      "isDeleted": false,
      "boundElements": [
        {
          "type": "text",
          "id": "2rZO3--9cLORMhdVOkJpH"
        },
        {
          "id": "IDm-KdgCAnkQVwTA2MCjN",
          "type": "arrow"
        },
        {
          "id": "02-7Dab-6KXsD5OQu81Xi",
          "type": "arrow"
        }
      ],
      "updated": 1752083523092,
      "link": null,
      "locked": false
    },
    {
      "id": "2rZO3--9cLORMhdVOkJpH",
      "type": "text",
      "x": 673.1916656494141,
      "y": 608.5,
      "width": 97.61666870117188,
      "height": 25,
      "angle": 0,
      "strokeColor": "#2f9e44",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b1M",
      "roundness": null,
      "seed": 1579785016,
      "version": 475,
      "versionNonce": 612916280,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1752083517920,
      "link": null,
      "locked": false,
      "text": "Submitted",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "9vVptv0pJ21kzm7o-XTcN",
      "originalText": "Submitted",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "IDm-KdgCAnkQVwTA2MCjN",
      "type": "arrow",
      "x": 583.5,
      "y": 621.9,
      "width": 64.5,
      "height": 1,
      "angle": 0,
      "strokeColor": "#2f9e44",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "dashed",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b1N",
      "roundness": null,
      "seed": 950205752,
      "version": 303,
      "versionNonce": 211817784,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1752083517921,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          64.5,
          -1
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "Q5uCucZa7PyaoLoFRD3bK",
        "focus": -0.08228217280349966,
        "gap": 5,
        "fixedPoint": [
          1.0246305418719213,
          0.49749999999999944
        ]
      },
      "endBinding": {
        "elementId": "9vVptv0pJ21kzm7o-XTcN",
        "focus": -0.057318364753231495,
        "gap": 5,
        "fixedPoint": [
          -0.036231884057971016,
          0.4971428571428565
        ]
      },
      "startArrowhead": null,
      "endArrowhead": "triangle_outline",
      "elbowed": true,
      "fixedSegments": null,
      "startIsSpecial": null,
      "endIsSpecial": null
    },
    {
      "id": "02-7Dab-6KXsD5OQu81Xi",
      "type": "arrow",
      "x": 796,
      "y": 620.9,
      "width": 110,
      "height": 0.10000000000002274,
      "angle": 0,
      "strokeColor": "#2f9e44",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "dashed",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b1O",
      "roundness": null,
      "seed": 377123144,
      "version": 53,
      "versionNonce": 1249066568,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1752083523092,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          110,
          0.10000000000002274
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "9vVptv0pJ21kzm7o-XTcN",
        "focus": -0.009524302500131027,
        "gap": 5,
        "fixedPoint": [
          1.036231884057971,
          0.4971428571428565
        ]
      },
      "endBinding": null,
      "startArrowhead": null,
      "endArrowhead": "triangle_outline",
      "elbowed": true,
      "fixedSegments": null,
      "startIsSpecial": null,
      "endIsSpecial": null
    },
    {
      "id": "uUDo0oDqlR1HXrPiCYErN",
      "type": "rectangle",
      "x": 649,
      "y": 410.5,
      "width": 161.99999999999997,
      "height": 35,
      "angle": 0,
      "strokeColor": "#e03131",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "dashed",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b1P",
      "roundness": {
        "type": 3
      },
      "seed": 777119816,
      "version": 275,
      "versionNonce": 284113720,
      "isDeleted": false,
      "boundElements": [
        {
          "type": "text",
          "id": "nymwb05elsKUPgcCq5Dq4"
        },
        {
          "id": "DQmBitsbagROz_TuQqL2n",
          "type": "arrow"
        },
        {
          "id": "0S7h-NbiMJuO20wa0jMMV",
          "type": "arrow"
        }
      ],
      "updated": 1752083599890,
      "link": null,
      "locked": false
    },
    {
      "id": "nymwb05elsKUPgcCq5Dq4",
      "type": "text",
      "x": 655.8666687011719,
      "y": 415.5,
      "width": 148.26666259765625,
      "height": 25,
      "angle": 0,
      "strokeColor": "#e03131",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 4,
      "strokeStyle": "solid",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b1Q",
      "roundness": null,
      "seed": 1803394888,
      "version": 240,
      "versionNonce": 622801992,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1752083599890,
      "link": null,
      "locked": false,
      "text": "LimitsExceeded",
      "fontSize": 20,
      "fontFamily": 5,
      "textAlign": "center",
      "verticalAlign": "middle",
      "containerId": "uUDo0oDqlR1HXrPiCYErN",
      "originalText": "LimitsExceeded",
      "autoResize": true,
      "lineHeight": 1.25
    },
    {
      "id": "DQmBitsbagROz_TuQqL2n",
      "type": "arrow",
      "x": 596,
      "y": 427.9,
      "width": 48,
      "height": 0,
      "angle": 0,
      "strokeColor": "#e03131",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "dashed",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b1R",
      "roundness": null,
      "seed": 229383480,
      "version": 14,
      "versionNonce": 651367480,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1752083599890,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          48,
          0
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "3L8DBqrtxQkUhGu3BBIYz",
        "focus": -0.004347826086956204,
        "gap": 5,
        "fixedPoint": [
          1.0218340611353713,
          0.49782608695652125
        ]
      },
      "endBinding": {
        "elementId": "uUDo0oDqlR1HXrPiCYErN",
        "focus": 0.005714285714283298,
        "gap": 5,
        "fixedPoint": [
          -0.030864197530864203,
          0.4971428571428565
        ]
      },
      "startArrowhead": null,
      "endArrowhead": "triangle_outline",
      "elbowed": true,
      "fixedSegments": null,
      "startIsSpecial": null,
      "endIsSpecial": null
    },
    {
      "id": "0S7h-NbiMJuO20wa0jMMV",
      "type": "arrow",
      "x": 816,
      "y": 427.9,
      "width": 87,
      "height": 0.10000000000002274,
      "angle": 0,
      "strokeColor": "#e03131",
      "backgroundColor": "transparent",
      "fillStyle": "solid",
      "strokeWidth": 2,
      "strokeStyle": "dashed",
      "roughness": 1,
      "opacity": 100,
      "groupIds": [],
      "frameId": null,
      "index": "b1S",
      "roundness": null,
      "seed": 892325944,
      "version": 53,
      "versionNonce": 851536712,
      "isDeleted": false,
      "boundElements": [],
      "updated": 1752083599890,
      "link": null,
      "locked": false,
      "points": [
        [
          0,
          0
        ],
        [
          87,
          0.10000000000002274
        ]
      ],
      "lastCommittedPoint": null,
      "startBinding": {
        "elementId": "uUDo0oDqlR1HXrPiCYErN",
        "focus": -0.011302757088728667,
        "gap": 5,
        "fixedPoint": [
          1.0308641975308643,
          0.4971428571428565
        ]
      },
      "endBinding": null,
      "startArrowhead": null,
      "endArrowhead": "triangle_outline",
      "elbowed": true,
      "fixedSegments": null,
      "startIsSpecial": null,
      "endIsSpecial": null
    }
  ],
  "appState": {
    "gridSize": 20,
    "gridStep": 5,
    "gridModeEnabled": false,
    "viewBackgroundColor": "#ffffff",
    "lockedMultiSelections": {}
  },
  "files": {}
}


================================================
FILE: docs/assets/mkdocstrings.css
================================================
/* From https://mkdocstrings.github.io/python/usage/customization/#symbol-types */
[data-md-color-scheme="default"] {
  --doc-symbol-parameter-fg-color: #df50af;
  --doc-symbol-attribute-fg-color: #0079ff;
  --doc-symbol-function-fg-color: #00dfa2;
  --doc-symbol-method-fg-color: #00dfa2;
  --doc-symbol-class-fg-color: #d1b619;
  --doc-symbol-module-fg-color: #ff0060;

  --doc-symbol-parameter-bg-color: #df50af1a;
  --doc-symbol-attribute-bg-color: #0079ff1a;
  --doc-symbol-function-bg-color: #00dfa21a;
  --doc-symbol-method-bg-color: #00dfa21a;
  --doc-symbol-class-bg-color: #d1b6191a;
  --doc-symbol-module-bg-color: #ff00601a;
}



================================================
FILE: docs/assets/navigation_bubbles.css
================================================
/* Navigation bubble cards styling */
.nav-card-link {
  text-decoration: none;
  color: inherit;
  display: block;
}

.nav-card {
  padding: 1rem;
  border: 1px solid var(--md-default-fg-color--lighter);
  border-radius: 0.5rem;
  transition:
    box-shadow 0.2s,
    transform 0.2s;
}

.nav-card:hover {
  box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
  transform: translateY(-2px);
}

.nav-card-header {
  display: flex;
  align-items: center;
  margin-bottom: 0.5rem;
}

.nav-card-icon {
  margin-right: 0.5rem;
  font-size: 1.2rem;
  color: var(--md-default-fg-color);
}

.nav-card-description {
  margin: 0;
  color: var(--md-default-fg-color--light);
}

.nav-card-title {
  margin: 0;
  font-weight: bold;
  color: var(--md-default-fg-color);
}



================================================
FILE: docs/assets/navigation_cards.css
================================================
/* Navigation cards styling */
.nav-card-link {
  text-decoration: none;
  color: inherit;
  display: block;
}

.nav-card {
  padding: 1rem;
  border: 1px solid var(--md-default-fg-color--lighter);
  border-radius: 0.5rem;
  transition:
    box-shadow 0.2s,
    transform 0.2s;
}

.nav-card:hover {
  box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
  transform: translateY(-2px);
}

.nav-card-header {
  display: flex;
  align-items: center;
  margin-bottom: 0.5rem;
}

.nav-card-icon {
  margin-right: 0.5rem;
  font-size: 1.2rem;
  color: var(--md-default-fg-color);
}

.nav-card-description {
  margin: 0;
  color: var(--md-default-fg-color--light);
}

.nav-card-title {
  margin: 0;
  font-weight: bold;
  color: var(--md-default-fg-color);
}



================================================
FILE: docs/assets/js/gif-controls.js
================================================
document.addEventListener("DOMContentLoaded", function () {
  const gifContainers = document.querySelectorAll(".gif-container");

  gifContainers.forEach((container) => {
    const img = container.querySelector("img[data-gif]");

    if (img) {
      const pngSrc = img.src;
      const gifSrc = img.getAttribute("data-gif");
      let isGif = false;

      // Create play button dynamically
      const playButton = document.createElement("button");
      playButton.className = "gif-play-button";
      playButton.setAttribute("aria-label", "Play demo");
      container.appendChild(playButton);

      function toggleGif() {
        if (!isGif) {
          // Switch to GIF
          img.src = gifSrc;
          isGif = true;
          container.classList.add("playing");
        } else {
          // Switch back to PNG
          img.src = pngSrc;
          isGif = false;
          container.classList.remove("playing");
        }
      }

      // Play button click
      playButton.addEventListener("click", function (e) {
        e.preventDefault();
        e.stopPropagation();
        toggleGif();
      });

      // Image click (play or pause)
      img.addEventListener("click", function (e) {
        e.preventDefault();
        e.stopPropagation();
        toggleGif();
      });
    }
  });
});



================================================
FILE: docs/assets/js/sh-annotation.js
================================================
// Source: https://github.com/srl-labs/learn-srlinux/pull/133/files
// this script is used to remove extra leading space when annotating shell code blocks ending with `\`
// character. See https://github.com/squidfunk/mkdocs-material/issues/3846 for more info.
document$.subscribe(() => {
  const tags = document.querySelectorAll("code .se");
  tags.forEach((tag) => {
    if (tag.innerText.startsWith("\\")) {
      tag.innerText = "\\";
    }
  });
});



================================================
FILE: docs/overrides/404.html
================================================
 {% extends "main.html" %}

 <!-- Content -->
 {% block content %}
   <h1>404 - Not found</h1>

   Please help us fix this by <a href="https://github.com/SWE-agent/SWE-agent/issues/new">opening a bug report</a>.

 {% endblock %}



================================================
FILE: docs/overrides/main.html
================================================
{% extends "base.html" %}

{% block content %}
{{ super() }}

<!-- Floating Bubbles -->
<div class="floating-bubbles">
<div class="floating-bubbles-title">Our projects</div>
  <a href="https://swe-agent.com/" class="bubble" title="SWE-agent">
    <img src="{{ 'assets/sweagent_logo.svg' | url }}" alt="SWE-agent">
    <span class="bubble-tooltip">SWE-agent</span>
  </a>
  <a href="https://swe-rex.com/" class="bubble" title="SWE-rex">
    <img src="{{ 'assets/swerex_logo.svg' | url }}" alt="SWE-rex">
    <span class="bubble-tooltip">SWE-ReX</span>
  </a>
  <a href="https://swesmith.com" class="bubble" title="SWE-smith">
    <img src="{{ 'assets/swesmith_logo.png' | url }}" alt="SWE-smith">
    <span class="bubble-tooltip">SWE-smith</span>
  </a>
  <a href="https://swebench.com" class="bubble" title="SWE-bench">
    <img src="{{ 'assets/swebench_logo.png' | url }}" alt="SWE-bench">
    <span class="bubble-tooltip">SWE-bench</span>
  </a>
  <a href="https://www.swebench.com/sb-cli/" class="bubble" title="sb-cli">
    <img src="{{ 'assets/sbcli_logo.svg' | url }}" alt="sb-cli">
    <span class="bubble-tooltip">sb-cli</span>
  </a>
</div>
{% endblock %}



================================================
FILE: docs/reference/index.md
================================================
# API Reference

This section provides detailed documentation for all classes and modules in mini-SWE-agent.

## Agents

- **[DefaultAgent](agents/default.md)** - The minimal default agent implementation
- **[InteractiveAgent](agents/interactive.md)** - Agent with human-in-the-loop functionality
- **[TextualAgent](agents/textual.md)** - Agent with interactive TUI using Textual

## Models

- **[LitellmModel](models/litellm.md)** - Wrapper for LiteLLM models (supports most LLM providers)
- **[AnthropicModel](models/anthropic.md)** - Specialized interface for Anthropic models
- **[DeterministicModel](models/test_models.md)** - Deterministic models for testing
- **[Model Utilities](models/utils.md)** - Convenience functions for model selection and configuration

## Environments

- **[LocalEnvironment](environments/local.md)** - Execute commands in the local environment
- **[DockerEnvironment](environments/docker.md)** - Execute commands in Docker containers
- **[SwerexDockerEnvironment](environments/swerex_docker.md)** - Extended Docker environment with SWE-Rex integration

## Run Scripts

Entry points and command-line interfaces:

- **[Hello World](run/hello_world.md)** - Simple example usage
- **[mini](run/mini.md)** - Interactive local execution
- **[GitHub Issue](run/github_issue.md)** - GitHub issue solver
- **[SWE-bench](run/swebench.md)** - SWE-bench evaluation script

{% include-markdown "../_footer.md" %}


================================================
FILE: docs/reference/agents/default.md
================================================
# DefaultAgent

!!! tip "Understanding the control flow"

    Check out the [control flow guide](../../advanced/control_flow.md) for a visual explanation of the agent's control flow following this picture:

    <div align="center">
        <img src="../../../assets/mini_control_flow.svg" alt="Agent control flow" style="max-width: 500px;" />
    </div>

::: minisweagent.agents.default.AgentConfig

::: minisweagent.agents.default.DefaultAgent

::: minisweagent.agents.default.NonTerminatingException

::: minisweagent.agents.default.TerminatingException

{% include-markdown "../../_footer.md" %}


================================================
FILE: docs/reference/agents/interactive.md
================================================
# Interactive

!!! tip "See also"

    * This agent subclass builds on top of the [default agent](default.md), make sure to read that first.
    * This class powers the `mini` command line tool, see [usage](../../usage/mini.md) for more details.

::: minisweagent.agents.interactive

{% include-markdown "../../_footer.md" %}


================================================
FILE: docs/reference/agents/textual.md
================================================
# TextualAgent

!!! tip "See also"

    * This agent subclass builds on top of the [default agent](default.md), make sure to read that first.
    * This class powers the `mini` command line tool, see [usage](../../usage/mini.md) for more details.


::: minisweagent.agents.interactive_textual.TextualAgent

::: minisweagent.agents.interactive_textual.TextualAgentConfig

::: minisweagent.agents.interactive_textual.SmartInputContainer

::: minisweagent.agents.interactive_textual.AddLogEmitCallback

{% include-markdown "../../_footer.md" %}


================================================
FILE: docs/reference/environments/docker.md
================================================
# Docker

::: minisweagent.environments.docker

{% include-markdown "../../_footer.md" %}


================================================
FILE: docs/reference/environments/local.md
================================================
# Local

::: minisweagent.environments.local

{% include-markdown "../../_footer.md" %}


================================================
FILE: docs/reference/environments/swerex_docker.md
================================================
# SWE-rex Docker

::: minisweagent.environments.extra.swerex_docker

{% include-markdown "../../_footer.md" %}


================================================
FILE: docs/reference/models/anthropic.md
================================================
# Anthropic

::: minisweagent.models.anthropic

{% include-markdown "../../_footer.md" %}


================================================
FILE: docs/reference/models/litellm.md
================================================
# Litellm Model

!!! tip "Guides"

    * Setting up most models is covered in the [quickstart guide](../../quickstart.md).
    * If you want to use local models, please check this [guide](../../advanced/local_models.md).

::: minisweagent.models.litellm_model

{% include-markdown "../../_footer.md" %}


================================================
FILE: docs/reference/models/test_models.md
================================================
# Test Models

::: minisweagent.models.test_models

{% include-markdown "../../_footer.md" %}


================================================
FILE: docs/reference/models/utils.md
================================================
# Model Utilities

Convenience functions for selecting and configuring models.

::: minisweagent.models.get_model

::: minisweagent.models.get_model_name

::: minisweagent.models.get_model_class

{% include-markdown "../../_footer.md" %}


================================================
FILE: docs/reference/run/github_issue.md
================================================
# GitHub Issue

::: minisweagent.run.github_issue

{% include-markdown "../../_footer.md" %}


================================================
FILE: docs/reference/run/hello_world.md
================================================
# Hello World

::: minisweagent.run.hello_world

{% include-markdown "../../_footer.md" %}


================================================
FILE: docs/reference/run/mini.md
================================================
# Local

::: minisweagent.run.mini

{% include-markdown "../../_footer.md" %}


================================================
FILE: docs/reference/run/swebench.md
================================================
# SWE-bench

::: minisweagent.run.extra.swebench

{% include-markdown "../../_footer.md" %}


================================================
FILE: docs/reference/run/swebench_single.md
================================================
# SWE-bench Single

::: minisweagent.run.extra.swebench_single

{% include-markdown "../../_footer.md" %}


================================================
FILE: docs/usage/inspector.md
================================================
# Inspector: Browse agent trajectories

!!! abstract "Overview"

    * The `inspector` is a tool that allows you to browse `.traj.json` files that show the history of a mini-SWE-agent run.
    * Quickly start it with `mini-e i` or `mini-extra inspector`.

<figure markdown="span">
  <div class="gif-container gif-container-styled" data-glightbox-disabled>
    <img src="https://github.com/SWE-agent/swe-agent-media/blob/main/media/mini/png/inspector.png?raw=true"
         data-gif="https://github.com/SWE-agent/swe-agent-media/blob/main/media/mini/gif/inspector.gif?raw=true"
         alt="inspector" data-glightbox="false" width="600" />
  </div>
</figure>

## Usage

```bash
# Find all .traj.json files recursively from current directory
mini-extra inspector
# or shorter
mini-e i
# Open the inspector for a specific file
mini-e i <path_to_traj.json>
# Search for trajectory files in a specific directory
mini-e i <path_to_directory>
```

## Key bindings

- `q`: Quit the inspector
- `h`/`LEFT`: Previous step
- `l`/`RIGHT`: Next step
- `j`/`DOWN`: Scroll down
- `k`/`UP`: Scroll up
- `H`: Previous trajectory
- `L`: Next trajectory

### FAQ

> How can I select/copy text on the screen?

Hold down the `Alt`/`Option` key and use the mouse to select the text.

## Implementation

The inspector is implemented with [textual](https://textual.textualize.io/).

??? note "Implementation"

    - [Read on GitHub](https://github.com/swe-agent/mini-swe-agent/blob/main/src/minisweagent//run/inspector.py)

    ```python linenums="1"
    --8<-- "src/minisweagent/run/inspector.py"
    ```

{% include-markdown "../_footer.md" %}


================================================
FILE: docs/usage/mini.md
================================================
# `mini`

!!! abstract "Overview"

    * `mini` is a REPL-style interactive command line interface for using mini-SWE-agent in the local requirement (as opposed for workflows that require sandboxing or large scale batch processing).
    * Compared to [`mini -v`](mini_v.md), `mini` is more lightweight and does not require threading.

!!! tip "Feedback wanted!"
    Give feedback on the `mini` and `mini -v` interfaces at [this github issue](https://github.com/swe-agent/mini-swe-agent/issues/161)
    or in our [Slack channel](https://join.slack.com/t/swe-bench/shared_invite/zt-36pj9bu5s-o3_yXPZbaH2wVnxnss1EkQ).

<figure markdown="span">
  <div class="gif-container gif-container-styled" data-glightbox-disabled>
    <img src="https://github.com/SWE-agent/swe-agent-media/blob/main/media/mini/png/mini.png?raw=true"
         data-gif="https://github.com/SWE-agent/swe-agent-media/blob/main/media/mini/gif/mini.gif?raw=true"
         alt="mini" data-glightbox="false" width="600" />
  </div>
</figure>


## Command line options

Useful switches:

- `-h`/`--help`: Show help
- `-t`/`--task`: Specify a task to run (else you will be prompted)
- `-c`/`--config`: Specify a config file to use, else we will use [`mini.yaml`](https://github.com/swe-agent/mini-swe-agent/blob/main/src/minisweagent/config/mini.yaml) or the config `MSWEA_MINI_CONFIG_PATH` environment variable (see [configuration](../advanced/configuration.md)).
  It's enough to specify the name of the config file, e.g., `-c mini.yaml` (see [configuration](../advanced/configuration.md) for how it is resolved).
- `-m`/`--model`: Specify a model to use, else we will use the model `MSWEA_MODEL_NAME` environment variable (see [configuration](../advanced/configuration.md))
- `-y`/`--yolo`: Start in `yolo` mode (see below)

## Modes of operation

`mini` provides three different modes of operation

- `confirm` (`/c`): The LM proposes an action and the user is prompted to confirm (press Enter) or reject (enter a rejection message)
- `yolo` (`/y`): The action from the LM is executed immediately without confirmation
- `human` (`/u`): The user takes over to type and execute commands

You can switch between the modes with the `/c`, `/y`, and `/u` commands that you can enter any time the agent is waiting for input.
You can also press `Ctrl+C` to interrupt the agent at any time, allowing you to switch between modes.

`mini` starts in `confirm` mode by default. To start in `yolo` mode, you can add `-y`/`--yolo` to the command line.

## Miscellaneous tips

- `mini` saves the full history of your last run to your global config directory.
  The path to the directory is printed when you start `mini`.

## Implementation

??? note "Default config"

    - [Read on GitHub](https://github.com/swe-agent/mini-swe-agent/blob/main/src/minisweagent/config/mini.yaml)

    ```yaml
    --8<-- "src/minisweagent/config/mini.yaml"
    ```

??? note "Run script"

    - [Read on GitHub](https://github.com/swe-agent/mini-swe-agent/blob/main/src/minisweagent/run/mini.py)
    - [API reference](../reference/run/mini.md)

    ```python
    --8<-- "src/minisweagent/run/mini.py"
    ```

??? note "Agent class"

    - [Read on GitHub](https://github.com/swe-agent/mini-swe-agent/blob/main/src/minisweagent/agents/interactive.py)
    - [API reference](../reference/agents/interactive.md)

    ```python
    --8<-- "src/minisweagent/agents/interactive.py"
    ```

{% include-markdown "../_footer.md" %}


================================================
FILE: docs/usage/mini_v.md
================================================
# `mini -v`

!!! abstract "Overview"

    * `mini -v` is a pager-style interactive command line interface for using mini-SWE-agent in the local requirement (as opposed for workflows that require sandboxing or large scale batch processing).
    * Compared to [`mini`](mini.md), `mini -v` offers a more advanced UI based on [Textual](https://textual.textualize.io/).

!!! tip "Feedback wanted!"
    Give feedback on the `mini` and `mini -v` interfaces at [this github issue](https://github.com/swe-agent/mini-swe-agent/issues/161)
    or in our [Slack channel](https://join.slack.com/t/swe-bench/shared_invite/zt-36pj9bu5s-o3_yXPZbaH2wVnxnss1EkQ).


<figure markdown="span">
  <div class="gif-container gif-container-styled" data-glightbox-disabled>
    <img src="https://github.com/SWE-agent/swe-agent-media/blob/main/media/mini/png/mini2.png?raw=true"
         data-gif="https://github.com/SWE-agent/swe-agent-media/blob/main/media/mini/gif/mini2.gif?raw=true"
         alt="miniv" data-glightbox="false" width="600" />
  </div>
</figure>

## Command line options

Invocation

```bash
mini -v [other options]
```

!!! tip "Default visual mode"

    If you want to use the visual mode by default, you can set the `MSWEA_VISUAL_MODE_DEFAULT` environment variable to `true`
    (`mini-extra config set MSWEA_VISUAL_MODE_DEFAULT true`).

Useful switches:

- `-h`/`--help`: Show help
- `-t`/`--task`: Specify a task to run (else you will be prompted)
- `-c`/`--config`: Specify a config file to use, else we will use [`mini.yaml`](https://github.com/swe-agent/mini-swe-agent/blob/main/src/minisweagent/config/mini.yaml) or the config `MSWEA_MINI_CONFIG_PATH` environment variable (see [configuration](../advanced/configuration.md))
  It's enough to specify the name of the config file, e.g., `-c mini.yaml` (see [configuration](../advanced/configuration.md) for how it is resolved).
- `-m`/`--model`: Specify a model to use, else we will use the model `MSWEA_MODEL_NAME` environment variable (see [configuration](../advanced/configuration.md))
- `-y`/`--yolo`: Start in `yolo` mode (see below)

## Key bindings

!!! tip "Focused input fields"

    Whenever you are prompted to enter text, the input field will be focused.
    You can use `Tab` or `Esc` to switch between the input field controls and the general controls below.

- `f1` or `?`: Show keybinding help
- `q` (or `ctrl+q`): Quit the agent
- `c`: Switch to `confirm` mode
- `y` (or `ctrl+y`): Switch to `yolo` mode
- `h` or `LEFT`: Go to previous step of the agent
- `l` or `RIGHT`: Go to next step of the agent
- `0`: Go to first step of the agent
- `$`: Go to last step of the agent
- `j` or `DOWN`: Scroll down
- `k` or `UP`: Scroll up

## Modes of operation

`mini -v` provides two different modes of operation

- `confirm` (`c`): The LM proposes an action and the user is prompted to confirm (press Enter)) or reject (enter a rejection message))
- `yolo` (`y`): The action from the LM is executed immediately without confirmation
- `human` (`u`): The user is prompted to enter a command directly

You can switch between the modes at any time by pressing the `c`, `y`, or `u` keys.

`mini -v` starts in `confirm` mode by default. To start in `yolo` mode, you can add `-y`/`--yolo` to the command line.

### FAQ

> How can I select/copy text on the screen?

Hold down the `Alt`/`Option` key and use the mouse to select the text.

## Miscellaneous tips

- `mini` saves the full history of your last run to your global config directory.
  The path to the directory is printed when you start `mini`.

## Implementation

??? note "Default config"

    - [Read on GitHub](https://github.com/swe-agent/mini-swe-agent/blob/main/src/minisweagent/config/mini.yaml)

    ```yaml
    --8<-- "src/minisweagent/config/mini.yaml"
    ```

??? note "Run script"

    - [Read on GitHub](https://github.com/swe-agent/mini-swe-agent/blob/main/src/minisweagent/run/mini.py)
    - [API reference](../reference/run/mini.md)

    ```python
    --8<-- "src/minisweagent/run/mini.py"
    ```

??? note "Agent class"

    - [Read on GitHub](https://github.com/swe-agent/mini-swe-agent/blob/main/src/minisweagent/agents/interactive.py)
    - [API reference](../reference/agents/interactive.md)

    ```python
    --8<-- "src/minisweagent/agents/interactive.py"
    ```

{% include-markdown "../_footer.md" %}



================================================
FILE: docs/usage/swebench.md
================================================
# SWE-bench

!!! abstract "Overview"

    * We provide two scripts to run on the [SWE-bench](https://www.swebench.com/) benchmark.
    * `mini-extra swebench` runs on all task instances in batch mode.
    * `mini-extra swebench-single` runs on a single task instance with interactivity (useful for debugging).
    * You can also take a look at the runscripts to figure out how to build your own batch processing pipeline.

<figure markdown="span">
  <div class="gif-container gif-container-styled" data-glightbox-disabled>
    <img src="https://github.com/SWE-agent/swe-agent-media/blob/main/media/mini/png/swebench.png?raw=true"
         data-gif="https://github.com/SWE-agent/swe-agent-media/blob/main/media/mini/gif/swebench.gif?raw=true"
         alt="swebench" data-glightbox="false" width="600" />
  </div>
</figure>

## Usage

!!! warning "Docker container availability"

    The docker containers for Linux assume an x86 Linux architecture;
    you might not be able to run them on other architectures.


!!! tip "Quickstart"

    === "Batch mode"

        Batch mode runs on all task instances in parallel.

        ```bash
        mini-extra swebench --help
        # or
        python src/minisweagent/run/extra/swebench.py --help
        # Example:
        mini-extra swebench \
            --model claude-sonnet-4-20250514 \
            --subset verified \
            --split test \
            --workers 4
        ```

        Basic flags:

        - `-o`, `--output` - Output directory
        - `-m`, `--model` - Model to use
        - `-c`, `--config` - Path to a config file (default: `swebench.yaml` in the `config` directory)
        - `-w`, `--workers` - Number of worker threads for parallel processing (default: `1`)

        Data selection flags:

        - `--subset` - SWEBench subset to use or path to a dataset (default: `lite`)
        - `--split` - Dataset split (default: `dev`)
        - `--slice` - Slice specification (e.g., '0:5' for first 5 instances)
        - `--filter` - Filter instance IDs by regex
        - `--shuffle` - Shuffle instances (default: `False`)
        - `--redo-existing` - Redo existing instances (default: `False`)

        Advanced flags:

        - `--environment-class` - Environment type to use (recommended: `docker` or `singularity`)

    === "Single instance (for debugging)"

        Single instance mode runs on a single task instance with interactivity (useful for debugging).

        ```bash
        mini-extra swebench-single --help
        # or
        python src/minisweagent/run/extra/swebench_single.py --help
        # Example:
        mini-extra swebench-single \
            --subset verified \
            --split test \
            --model claude-sonnet-4-20250514 \
            -i sympy__sympy-15599
        # or
        mini-extra swebench-single \
            --subset verified \
            --split test \
            -m claude-sonnet-4-20250514 \
            -i 0  # instance index
        ```

        Note: If you want to run the script without prompting for confirmation at exit,
        add the `--exit-immediately` flag.

        Basic flags:

        - `-m`, `--model` - Model to use
        - `-c`, `--config` - Path to a config file (default: `swebench.yaml` in the `config` directory)
        - `-o`, `--output` - Output trajectory file (default: saves to global config directory)

        Data selection flags:

        - `--subset` - SWEBench subset to use or path to a dataset (default: `lite`)
        - `--split` - Dataset split (default: `dev`)
        - `-i`, `--instance` - SWE-Bench instance ID (default: `0`)

        Advanced flags:

        - `--environment-class` - Environment type to use (recommended: `docker` or `singularity`)
        - `--exit-immediately` - Exit immediately when the agent wants to finish instead of prompting (default: `False`)

!!! tip "Evaluating on SWE-bench"

    You can use the [sb-cli](https://www.swebench.com/sb-cli/) for extremely fast, cloud-based evaluations
    (and it's free!). After installing it and getting a token, simply run:

    ```bash
    sb-cli submit swe-bench_verified test --predictions_path preds.json --run_id some-id-for-your-run
    ```

    Typically you will have results within 20 minutes (this is not limited by how many instances you run,
    but by the slowest-to-evaluate instance in SWE-bench).


## FAQ

> Can I set global cost limits?

Yes, you can set global cost limits with the `MSWEA_GLOBAL_CALL_LIMIT` and `MSWEA_GLOBAL_COST_LIMIT` environment variables/global config.
See [configuration](../advanced/configuration.md) for more details.

> What happens to uncompleted tasks when I abort with KeyboardInterrupt?

Trajectories are only saved upon completion, so most likely, you can just rerun the script to complete the tasks next time.
However, you should still check for `KeyboardInterrupt` in `preds.json` in case some tasks were aborted but saved.

> Certain tasks are being stuck even though I deleted the trajectories.

The completed instances are inferred from `preds.json`. Remove the corresponding items from the file.

> How can I run on a different dataset?

As long as it follows the SWE-bench format, you can use `--subset /path/to/your/dataset` to run on a custom dataset.
The dataset needs to be loadable as `datasets.load_dataset(path, split=split)`.

> Some progress runners are stuck at 'initializing task' for a very long time

They might be pulling docker containers -- the run sshould start immediately the next time.

> I have some docker issues

Try running the docker command manually to see what's going on (it should be printed out in the console).
Confirm that it's running with `docker ps`, and that you can use `docker exec -it <container-id> ls` to get some output.

## Implementation

??? note "Default config"

    - [Read on GitHub](https://github.com/swe-agent/mini-swe-agent/blob/main/src/minisweagent/config/extra/swebench.yaml)

    ```yaml
    --8<-- "src/minisweagent/config/extra/swebench.yaml"
    ```

??? note "`swebench.py` run script"

    - [Read on GitHub](https://github.com/swe-agent/mini-swe-agent/blob/main/src/minisweagent/run/extra/swebench.py)
    - [API reference](../reference/run/swebench.md)

    ```python
    --8<-- "src/minisweagent/run/extra/swebench.py"
    ```

??? note "`swebench_single.py` run script"

    - [Read on GitHub](https://github.com/swe-agent/mini-swe-agent/blob/main/src/minisweagent/run/extra/swebench_single.py)
    - [API reference](../reference/run/swebench_single.md)

    ```python
    --8<-- "src/minisweagent/run/extra/swebench_single.py"
    ```

{% include-markdown "../_footer.md" %}


================================================
FILE: src/minisweagent/__init__.py
================================================
"""
This file provides:

- Path settings for global config file & relative directories
- Version numbering
- Protocols for the core components of mini-swe-agent.
  By the magic of protocols & duck typing, you can pretty much ignore them,
  unless you want the static type checking.
"""

__version__ = "1.9.1"

import os
from pathlib import Path
from typing import Any, Protocol

import dotenv
from platformdirs import user_config_dir
from rich.console import Console

from minisweagent.utils.log import logger

package_dir = Path(__file__).resolve().parent

global_config_dir = Path(os.getenv("MSWEA_GLOBAL_CONFIG_DIR") or user_config_dir("mini-swe-agent"))
global_config_dir.mkdir(parents=True, exist_ok=True)
global_config_file = Path(global_config_dir) / ".env"

if not os.getenv("MSWEA_SILENT_STARTUP"):
    Console().print(
        f"👋 This is [bold green]mini-swe-agent[/bold green] version [bold green]{__version__}[/bold green].\n"
        f"Your config is stored in [bold green]'{global_config_file}'[/bold green]"
    )
dotenv.load_dotenv(dotenv_path=global_config_file)


# === Protocols ===
# You can ignore them unless you want static type checking.


class Model(Protocol):
    """Protocol for language models."""

    config: Any
    cost: float
    n_calls: int

    def query(self, messages: list[dict[str, str]], **kwargs) -> dict: ...

    def get_template_vars(self) -> dict[str, Any]: ...


class Environment(Protocol):
    """Protocol for execution environments."""

    config: Any

    def execute(self, command: str, cwd: str = "") -> dict[str, str]: ...

    def get_template_vars(self) -> dict[str, Any]: ...


class Agent(Protocol):
    """Protocol for agents."""

    model: Model
    env: Environment
    messages: list[dict[str, str]]
    config: Any

    def run(self, task: str, **kwargs) -> tuple[str, str]: ...


__all__ = [
    "Agent",
    "Model",
    "Environment",
    "package_dir",
    "__version__",
    "global_config_file",
    "global_config_dir",
    "logger",
]



================================================
FILE: src/minisweagent/__main__.py
================================================
#!/usr/bin/env python3
"""This is what happens when you do `python -m minisweagent` or `pipx run mini-swe-agent`."""

from minisweagent.run.mini import app

if __name__ == "__main__":
    app()



================================================
FILE: src/minisweagent/py.typed
================================================
[Empty file]


================================================
FILE: src/minisweagent/test_output.json
================================================
{
  "info": {
    "exit_status": "Submitted",
    "submission": "diff --git a/tests/missing_colon.py b/tests/missing_colon.py\nindex 20edef5..f55e657 100755\n--- a/tests/missing_colon.py\n+++ b/tests/missing_colon.py\n@@ -1,10 +1,11 @@\n #!/usr/bin/env python3\n \n \n-def division(a: float, b: float) -> float\n+def division(a: float, b: float) -> float:\n+    if b == 0:\n+        raise ValueError(\"Cannot divide by zero\")\n     return a/b\n \n \n if __name__ == \"__main__\":\n     print(division(123, 15))\n-\n",
    "model_stats": {
      "instance_cost": 0.9999999999999999,
      "api_calls": 10
    },
    "mini_version": "1.9.1"
  },
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant that can interact multiple times with a computer shell to solve programming tasks.\nYour response must contain exactly ONE bash code block with ONE command (or commands connected with && or ||).\n\nInclude a THOUGHT section before your command where you explain your reasoning process.\nFormat your response as shown in <format_example>.\n\n<format_example>\nTHOUGHT: Your reasoning and analysis here\n\n```bash\nyour_command_here\n```\n</format_example>\n\nFailure to follow these rules will cause your response to be rejected."
    },
    {
      "role": "user",
      "content": "<pr_description>\nConsider the following PR description:\nI'm running missing_colon.py as follows:\n\ndivision(23, 0)\n\nbut I get the following error:\n\n  File \"/Users/fuchur/Documents/24/git_sync/swe-agent-test-repo/tests/./missing_colon.py\", line 4\n    def division(a: float, b: float) -> float\n                                             ^\nSyntaxError: invalid syntax\n\n</pr_description>\n\n<instructions>\n# Task Instructions\n\n## Overview\nYou're a software engineer interacting continuously with a computer by submitting commands.\nYou'll be helping implement necessary changes to meet requirements in the PR description.\nYour task is specifically to make changes to non-test files in the current directory in order to fix the issue described in the PR description in a way that is general and consistent with the codebase.\n\nIMPORTANT: This is an interactive process where you will think and issue ONE command, see its result, then think and issue your next command.\n\nFor each response:\n1. Include a THOUGHT section explaining your reasoning and what you're trying to accomplish\n2. Provide exactly ONE bash command to execute\n\n## Important Boundaries\n- MODIFY: Regular source code files in \n- DO NOT MODIFY: Tests, configuration files (pyproject.toml, setup.cfg, etc.)\n\n## Recommended Workflow\n1. Analyze the codebase by finding and reading relevant files\n2. Create a script to reproduce the issue\n3. Edit the source code to resolve the issue\n4. Verify your fix works by running your script again\n5. Test edge cases to ensure your fix is robust\n\n## Command Execution Rules\nYou are operating in an environment where\n1. You write a single command\n2. The system executes that command in a subshell\n3. You see the result\n4. You write your next command\n\nEach response should include:\n1. A **THOUGHT** section where you explain your reasoning and plan\n2. A single bash code block with your command\n\nFormat your responses like this:\n\n<format_example>\nTHOUGHT: Here I explain my reasoning process, analysis of the current situation,\nand what I'm trying to accomplish with the command below.\n\n```bash\nyour_command_here\n```\n</format_example>\n\nCommands must be specified in a single bash code block:\n\n```bash\nyour_command_here\n```\n\n**CRITICAL REQUIREMENTS:**\n- Your response SHOULD include a THOUGHT section explaining your reasoning\n- Your response MUST include EXACTLY ONE bash code block\n- This bash block MUST contain EXACTLY ONE command (or a set of commands connected with && or ||)\n- If you include zero or multiple bash blocks, or no command at all, YOUR RESPONSE WILL FAIL\n- Do NOT try to run multiple independent commands in separate blocks in one response\n- Directory or environment variable changes are not persistent. Every action is executed in a new subshell.\n- However, you can prefix any action with `MY_ENV_VAR=MY_VALUE cd /path/to/working/dir && ...` or write/load environment variables from files\n\nExample of a CORRECT response:\n<example_response>\nTHOUGHT: I need to understand the structure of the repository first. Let me check what files are in the current directory to get a better understanding of the codebase.\n\n```bash\nls -la\n```\n</example_response>\n\nExample of an INCORRECT response:\n<example_response>\nTHOUGHT: I need to examine the codebase and then look at a specific file. I'll run multiple commands to do this.\n\n```bash\nls -la\n```\n\nNow I'll read the file:\n\n```bash\ncat file.txt\n```\n</example_response>\n\nIf you need to run multiple commands, either:\n1. Combine them in one block using && or ||\n```bash\ncommand1 && command2 || echo \"Error occurred\"\n```\n\n2. Wait for the first command to complete, see its output, then issue the next command in your following response.\n\n## Environment Details\n- You have a full Linux shell environment\n- Always use non-interactive flags (-y, -f) for commands\n- Avoid interactive tools like vi, nano, or any that require user input\n- If a command isn't available, you can install it\n\n## Useful Command Examples\n\n### Create a new file:\n```bash\ncat <<'EOF' > newfile.py\nimport numpy as np\nhello = \"world\"\nprint(hello)\nEOF\n```\n\n### Edit files with sed:\n```bash\n# Replace all occurrences\nsed -i 's/old_string/new_string/g' filename.py\n\n# Replace only first occurrence\nsed -i 's/old_string/new_string/' filename.py\n\n# Replace first occurrence on line 1\nsed -i '1s/old_string/new_string/' filename.py\n\n# Replace all occurrences in lines 1-10\nsed -i '1,10s/old_string/new_string/g' filename.py\n```\n\n### View file content:\n```bash\n# View specific lines with numbers\nnl -ba filename.py | sed -n '10,20p'\n```\n\n### Any other command you want to run\n```bash\nanything\n```\n\n## Submission\nWhen you've completed your work (reading, editing, testing), and cannot make further progress\nissue exactly the following command:\n\n```bash\necho COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT && git add -A && git diff --cached\n```\n\nThis command will submit your work.\nYou cannot continue working (reading, editing, testing) in any way on this task after submitting.\n</instructions>"
    },
    {
      "role": "assistant",
      "content": "I'll help you solve this issue. Let me first examine the content of the file that's causing the syntax error.\n\n```bash\ncat /Users/fuchur/Documents/24/git_sync/swe-agent-test-repo/tests/./missing_colon.py\n```"
    },
    {
      "role": "user",
      "content": "<returncode>1</returncode>\n<output>\ncat: /Users/fuchur/Documents/24/git_sync/swe-agent-test-repo/tests/./missing_colon.py: No such file or directory\n</output>"
    },
    {
      "role": "assistant",
      "content": "I need to find the correct path to the file. Let me check the current directory structure:\n\n```bash\nls -la\n```"
    },
    {
      "role": "user",
      "content": "<returncode>0</returncode>\n<output>\ntotal 48\ndrwxr-xr-x 1 root root 4096 Jul  1 18:43 .\ndrwxr-xr-x 1 root root 4096 Aug 20 14:18 ..\ndrwxr-xr-x 7 root root 4096 Jul  1 18:43 .git\n-rw-r--r-- 1 root root 3495 May 20  2024 .gitignore\ndrwxr-xr-x 3 root root 4096 Jul  1 18:36 .pytest_cache\n-rw-r--r-- 1 root root   61 Jul  1 18:36 Dockerfile\n-rw-r--r-- 1 root root  183 Apr 11  2024 README.md\n-rwxr-xr-x 1 root root  404 Jul  1 18:31 build_and_push_docker.sh\ndrwxr-xr-x 2 root root 4096 Jun 18  2024 problem_statements\n-rw-r--r-- 1 root root  503 Jun  5  2024 pyproject.toml\ndrwxr-xr-x 3 root root 4096 Jun  4 18:17 src\ndrwxr-xr-x 3 root root 4096 Jul  1 18:24 tests\n</output>"
    },
    {
      "role": "assistant",
      "content": "Let me check the tests directory where the missing_colon.py file should be:\n\n```bash\nls -la tests/\n```"
    },
    {
      "role": "user",
      "content": "<returncode>0</returncode>\n<output>\ntotal 28\ndrwxr-xr-x 3 root root 4096 Jul  1 18:24 .\ndrwxr-xr-x 1 root root 4096 Jul  1 18:43 ..\ndrwxr-xr-x 2 root root 4096 Jul  1 18:43 __pycache__\n-rwxr-xr-x 1 root root  368 Jan 17  2025 existing_lint_error.py\n-rw-r--r-- 1 root root   65 Jun 24  2024 filetoread.txt\n-rwxr-xr-x 1 root root  141 Jul  1 18:24 missing_colon.py\n-rw-r--r-- 1 root root  128 May 20  2024 test_tribonaccy.py\n</output>"
    },
    {
      "role": "assistant",
      "content": "Great! I found the missing_colon.py file in the tests directory. Let me examine its content:\n\n```bash\ncat tests/missing_colon.py\n```"
    },
    {
      "role": "user",
      "content": "<returncode>0</returncode>\n<output>\n#!/usr/bin/env python3\n\n\ndef division(a: float, b: float) -> float\n    return a/b\n\n\nif __name__ == \"__main__\":\n    print(division(123, 15))\n\n</output>"
    },
    {
      "role": "assistant",
      "content": "I found the issue! In Python, a function definition needs to end with a colon `:` before the function body. The error message is pointing to line 4 where the colon is missing after the function return type annotation.\n\nLet me fix the file:\n\n```bash\nsed -i 's/def division(a: float, b: float) -> float/def division(a: float, b: float) -> float:/' tests/missing_colon.py\n```"
    },
    {
      "role": "user",
      "content": "<returncode>0</returncode>\n<output>\n</output>"
    },
    {
      "role": "assistant",
      "content": "Now let's verify that the file has been fixed correctly:\n\n```bash\ncat tests/missing_colon.py\n```"
    },
    {
      "role": "user",
      "content": "<returncode>0</returncode>\n<output>\n#!/usr/bin/env python3\n\n\ndef division(a: float, b: float) -> float:\n    return a/b\n\n\nif __name__ == \"__main__\":\n    print(division(123, 15))\n\n</output>"
    },
    {
      "role": "assistant",
      "content": "Let's test if our fix resolved the syntax error by running the script:\n\n```bash\npython3 tests/missing_colon.py\n```"
    },
    {
      "role": "user",
      "content": "<returncode>0</returncode>\n<output>\n8.2\n</output>"
    },
    {
      "role": "assistant",
      "content": "Great! The script now runs correctly without syntax errors and correctly outputs the result of 123 divided by 15, which is 8.2.\n\nLet me now check if running the division with the originally mentioned values (23, 0) would work:\n\n```bash\npython3 -c \"from tests.missing_colon import division; print(division(23, 0))\"\n```"
    },
    {
      "role": "user",
      "content": "<returncode>1</returncode>\n<output>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/testbed/tests/missing_colon.py\", line 5, in division\n    return a/b\n           ~^~\nZeroDivisionError: division by zero\n</output>"
    },
    {
      "role": "assistant",
      "content": "As expected, when trying to divide by zero (23, 0), we get a ZeroDivisionError. While our syntax fix was successful, the function doesn't handle division by zero gracefully.\n\nLet me update the division function to handle the zero division case properly:\n\n```bash\ncat > tests/missing_colon.py << 'EOF'\n#!/usr/bin/env python3\n\n\ndef division(a: float, b: float) -> float:\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a/b\n\n\nif __name__ == \"__main__\":\n    print(division(123, 15))\nEOF\n```"
    },
    {
      "role": "user",
      "content": "<returncode>0</returncode>\n<output>\n</output>"
    },
    {
      "role": "assistant",
      "content": "Now let's verify that the updated file works correctly both for valid division and for the division by zero case:\n\n```bash\necho MINI_SWE_AGENT_FINAL_OUTPUT && git add -A && git diff --cached\n```"
    },
    {
      "role": "user",
      "content": "diff --git a/tests/missing_colon.py b/tests/missing_colon.py\nindex 20edef5..f55e657 100755\n--- a/tests/missing_colon.py\n+++ b/tests/missing_colon.py\n@@ -1,10 +1,11 @@\n #!/usr/bin/env python3\n \n \n-def division(a: float, b: float) -> float\n+def division(a: float, b: float) -> float:\n+    if b == 0:\n+        raise ValueError(\"Cannot divide by zero\")\n     return a/b\n \n \n if __name__ == \"__main__\":\n     print(division(123, 15))\n-\n"
    }
  ],
  "trajectory_format": "mini-swe-agent-1"
}


================================================
FILE: src/minisweagent/agents/README.md
================================================
# Agent implementations

* `default.py` - Minimal default agent implementation.
* `interactive.py` - Extends `default.py` with some minimal human-in-the-loop functionality (confirm actions, etc.).
* `interactive_textual.py` - Extends `default.py` with [Textual](https://textual.textualize.io/) for an interactive TUI.
   (this is a more complicated UI).


================================================
FILE: src/minisweagent/agents/__init__.py
================================================
"""Agent implementations for mini-SWE-agent."""



================================================
FILE: src/minisweagent/agents/default.py
================================================
"""Basic agent class. See https://mini-swe-agent.com/latest/advanced/control_flow/ for visual explanation."""

import re
import subprocess
from collections.abc import Callable
from dataclasses import asdict, dataclass

from jinja2 import Template

from minisweagent import Environment, Model


@dataclass
class AgentConfig:
    # The default settings are the bare minimum to run the agent. Take a look at the config files for improved settings.
    system_template: str = "You are a helpful assistant that can do anything."
    instance_template: str = (
        "Your task: {{task}}. Please reply with a single shell command in triple backticks. "
        "To finish, the first line of the output of the shell command must be 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'."
    )
    timeout_template: str = (
        "The last command <command>{{action['action']}}</command> timed out and has been killed.\n"
        "The output of the command was:\n <output>\n{{output}}\n</output>\n"
        "Please try another command and make sure to avoid those requiring interactive input."
    )
    format_error_template: str = "Please always provide EXACTLY ONE action in triple backticks."
    action_observation_template: str = "Observation: {{output}}"
    step_limit: int = 0
    cost_limit: float = 3.0


class NonTerminatingException(Exception):
    """Raised for conditions that can be handled by the agent."""


class FormatError(NonTerminatingException):
    """Raised when the LM's output is not in the expected format."""


class ExecutionTimeoutError(NonTerminatingException):
    """Raised when the action execution timed out."""


class TerminatingException(Exception):
    """Raised for conditions that terminate the agent."""


class Submitted(TerminatingException):
    """Raised when the LM declares that the agent has finished its task."""


class LimitsExceeded(TerminatingException):
    """Raised when the agent has reached its cost or step limit."""


class DefaultAgent:
    def __init__(self, model: Model, env: Environment, *, config_class: Callable = AgentConfig, **kwargs):
        self.config = config_class(**kwargs)
        self.messages: list[dict] = []
        self.model = model
        self.env = env
        self.extra_template_vars = {}

    def render_template(self, template: str, **kwargs) -> str:
        template_vars = asdict(self.config) | self.env.get_template_vars() | self.model.get_template_vars()
        return Template(template).render(**kwargs, **template_vars, **self.extra_template_vars)

    def add_message(self, role: str, content: str, **kwargs):
        self.messages.append({"role": role, "content": content, **kwargs})

    def run(self, task: str, **kwargs) -> tuple[str, str]:
        """Run step() until agent is finished. Return exit status & message"""
        self.extra_template_vars |= {"task": task, **kwargs}
        self.messages = []
        self.add_message("system", self.render_template(self.config.system_template))
        self.add_message("user", self.render_template(self.config.instance_template))
        while True:
            try:
                self.step()
            except NonTerminatingException as e:
                self.add_message("user", str(e))
            except TerminatingException as e:
                self.add_message("user", str(e))
                return type(e).__name__, str(e)

    def step(self) -> dict:
        """Query the LM, execute the action, return the observation."""
        return self.get_observation(self.query())

    def query(self) -> dict:
        """Query the model and return the response."""
        if 0 < self.config.step_limit <= self.model.n_calls or 0 < self.config.cost_limit <= self.model.cost:
            raise LimitsExceeded()
        response = self.model.query(self.messages)
        self.add_message("assistant", **response)
        return response

    def get_observation(self, response: dict) -> dict:
        """Execute the action and return the observation."""
        output = self.execute_action(self.parse_action(response))
        observation = self.render_template(self.config.action_observation_template, output=output)
        self.add_message("user", observation)
        return output

    def parse_action(self, response: dict) -> dict:
        """Parse the action from the message. Returns the action."""
        actions = re.findall(r"```bash\n(.*?)\n```", response["content"], re.DOTALL)
        if len(actions) == 1:
            return {"action": actions[0].strip(), **response}
        raise FormatError(self.render_template(self.config.format_error_template, actions=actions))

    def execute_action(self, action: dict) -> dict:
        try:
            output = self.env.execute(action["action"])
        except subprocess.TimeoutExpired as e:
            output = e.output.decode("utf-8", errors="replace") if e.output else ""
            raise ExecutionTimeoutError(
                self.render_template(self.config.timeout_template, action=action, output=output)
            )
        except TimeoutError:
            raise ExecutionTimeoutError(self.render_template(self.config.timeout_template, action=action, output=""))
        self.has_finished(output)
        return output

    def has_finished(self, output: dict[str, str]):
        """Raises Submitted exception with final output if the agent has finished its task."""
        lines = output.get("output", "").lstrip().splitlines(keepends=True)
        if lines and lines[0].strip() in ["MINI_SWE_AGENT_FINAL_OUTPUT", "COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT"]:
            raise Submitted("".join(lines[1:]))



================================================
FILE: src/minisweagent/agents/interactive.py
================================================
"""A small generalization of the default agent that puts the user in the loop.

There are three modes:
- human: commands issued by the user are executed immediately
- confirm: commands issued by the LM but not whitelisted are confirmed by the user
- yolo: commands issued by the LM are executed immediately without confirmation
"""

import re
from dataclasses import dataclass, field
from typing import Literal

from prompt_toolkit.history import FileHistory
from prompt_toolkit.shortcuts import PromptSession
from rich.console import Console
from rich.rule import Rule

from minisweagent import global_config_dir
from minisweagent.agents.default import AgentConfig, DefaultAgent, LimitsExceeded, NonTerminatingException, Submitted

console = Console(highlight=False)
prompt_session = PromptSession(history=FileHistory(global_config_dir / "interactive_history.txt"))


@dataclass
class InteractiveAgentConfig(AgentConfig):
    mode: Literal["human", "confirm", "yolo"] = "confirm"
    """Whether to confirm actions."""
    whitelist_actions: list[str] = field(default_factory=list)
    """Never confirm actions that match these regular expressions."""
    confirm_exit: bool = True
    """If the agent wants to finish, do we ask for confirmation from user?"""


class InteractiveAgent(DefaultAgent):
    _MODE_COMMANDS_MAPPING = {"/u": "human", "/c": "confirm", "/y": "yolo"}

    def __init__(self, *args, config_class=InteractiveAgentConfig, **kwargs):
        super().__init__(*args, config_class=config_class, **kwargs)
        self.cost_last_confirmed = 0.0

    def add_message(self, role: str, content: str, **kwargs):
        # Extend supermethod to print messages
        super().add_message(role, content, **kwargs)
        if role == "assistant":
            console.print(
                f"\n[red][bold]mini-swe-agent[/bold] (step [bold]{self.model.n_calls}[/bold], [bold]${self.model.cost:.2f}[/bold]):[/red]\n",
                end="",
                highlight=False,
            )
        else:
            console.print(f"\n[bold green]{role.capitalize()}[/bold green]:\n", end="", highlight=False)
        console.print(content, highlight=False, markup=False)

    def query(self) -> dict:
        # Extend supermethod to handle human mode
        if self.config.mode == "human":
            match command := self._prompt_and_handle_special("[bold yellow]>[/bold yellow] "):
                case "/y" | "/c":  # Just go to the super query, which queries the LM for the next action
                    pass
                case _:
                    msg = {"content": f"\n```bash\n{command}\n```"}
                    self.add_message("assistant", msg["content"])
                    return msg
        try:
            with console.status("Waiting for the LM to respond..."):
                return super().query()
        except LimitsExceeded:
            console.print(
                f"Limits exceeded. Limits: {self.config.step_limit} steps, ${self.config.cost_limit}.\n"
                f"Current spend: {self.model.n_calls} steps, ${self.model.cost:.2f}."
            )
            self.config.step_limit = int(input("New step limit: "))
            self.config.cost_limit = float(input("New cost limit: "))
            return super().query()

    def step(self) -> dict:
        # Override the step method to handle user interruption
        try:
            console.print(Rule())
            return super().step()
        except KeyboardInterrupt:
            # We always add a message about the interrupt and then just proceed to the next step
            interruption_message = self._prompt_and_handle_special(
                "\n\n[bold yellow]Interrupted.[/bold yellow] "
                "[green]Type a comment/command[/green] (/h for available commands)"
                "\n[bold yellow]>[/bold yellow] "
            ).strip()
            if not interruption_message or interruption_message in self._MODE_COMMANDS_MAPPING:
                interruption_message = "Temporary interruption caught."
            raise NonTerminatingException(f"Interrupted by user: {interruption_message}")

    def execute_action(self, action: dict) -> dict:
        # Override the execute_action method to handle user confirmation
        if self.should_ask_confirmation(action["action"]):
            self.ask_confirmation()
        return super().execute_action(action)

    def should_ask_confirmation(self, action: str) -> bool:
        return self.config.mode == "confirm" and not any(re.match(r, action) for r in self.config.whitelist_actions)

    def ask_confirmation(self) -> None:
        prompt = (
            "[bold yellow]Execute?[/bold yellow] [green][bold]Enter[/bold] to confirm[/green], "
            "or [green]Type a comment/command[/green] (/h for available commands)\n"
            "[bold yellow]>[/bold yellow] "
        )
        match user_input := self._prompt_and_handle_special(prompt).strip():
            case "" | "/y":
                pass  # confirmed, do nothing
            case "/u":  # Skip execution action and get back to query
                raise NonTerminatingException("Command not executed. Switching to human mode")
            case _:
                raise NonTerminatingException(
                    f"Command not executed. The user rejected your command with the following message: {user_input}"
                )

    def _prompt_and_handle_special(self, prompt: str) -> str:
        """Prompts the user, takes care of /h (followed by requery) and sets the mode. Returns the user input."""
        console.print(prompt, end="")
        user_input = prompt_session.prompt("")
        if user_input == "/h":
            console.print(
                f"Current mode: [bold green]{self.config.mode}[/bold green]\n"
                f"[bold green]/y[/bold green] to switch to [bold yellow]yolo[/bold yellow] mode (execute LM commands without confirmation)\n"
                f"[bold green]/c[/bold green] to switch to [bold yellow]confirmation[/bold yellow] mode (ask for confirmation before executing LM commands)\n"
                f"[bold green]/u[/bold green] to switch to [bold yellow]human[/bold yellow] mode (execute commands issued by the user)\n"
            )
            return self._prompt_and_handle_special(prompt)
        if user_input in self._MODE_COMMANDS_MAPPING:
            if self.config.mode == self._MODE_COMMANDS_MAPPING[user_input]:
                return self._prompt_and_handle_special(
                    f"[bold red]Already in {self.config.mode} mode.[/bold red]\n{prompt}"
                )
            self.config.mode = self._MODE_COMMANDS_MAPPING[user_input]
            console.print(f"Switched to [bold green]{self.config.mode}[/bold green] mode.")
            return user_input
        return user_input

    def has_finished(self, output: dict[str, str]):
        try:
            return super().has_finished(output)
        except Submitted as e:
            if self.config.confirm_exit:
                console.print(
                    "[bold green]Agent wants to finish.[/bold green] "
                    "[green]Type a comment to give it a new task or press enter to quit.\n"
                    "[bold yellow]>[/bold yellow] ",
                    end="",
                )
                if new_task := self._prompt_and_handle_special("").strip():
                    raise NonTerminatingException(f"The user added a new task: {new_task}")
            raise e



================================================
FILE: src/minisweagent/agents/interactive_textual.py
================================================
"""
Extension of the `default.py` agent that uses Textual for an interactive TUI.
For a simpler version of an interactive UI that does not require threading and more, see `interactive.py`.
"""

import logging
import os
import re
import threading
import time
import traceback
from collections.abc import Iterable
from dataclasses import dataclass, field
from pathlib import Path
from typing import Literal

from rich.spinner import Spinner
from rich.text import Text
from textual.app import App, ComposeResult, SystemCommand
from textual.binding import Binding
from textual.containers import Container, Vertical, VerticalScroll
from textual.css.query import NoMatches
from textual.events import Key
from textual.screen import Screen
from textual.widgets import Footer, Header, Input, Static, TextArea

from minisweagent.agents.default import AgentConfig, DefaultAgent, NonTerminatingException, Submitted


@dataclass
class TextualAgentConfig(AgentConfig):
    mode: Literal["confirm", "yolo"] = "confirm"
    """Mode for action execution: 'confirm' requires user confirmation, 'yolo' executes immediately."""
    whitelist_actions: list[str] = field(default_factory=list)
    """Never confirm actions that match these regular expressions."""
    confirm_exit: bool = True
    """If the agent wants to finish, do we ask for confirmation from user?"""


class _TextualAgent(DefaultAgent):
    def __init__(self, app: "TextualAgent", *args, **kwargs):
        """Connects the DefaultAgent to the TextualApp."""
        self.app = app
        super().__init__(*args, config_class=TextualAgentConfig, **kwargs)
        self._current_action_from_human = False

    def add_message(self, role: str, content: str, **kwargs):
        super().add_message(role, content, **kwargs)
        if self.app.agent_state != "UNINITIALIZED":
            self.app.call_from_thread(self.app.on_message_added)

    def query(self) -> dict:
        if self.config.mode == "human":
            human_input = self.app.input_container.request_input("Enter your command:")
            self._current_action_from_human = True
            msg = {"content": f"\n```bash\n{human_input}\n```"}
            self.add_message("assistant", msg["content"])
            return msg
        self._current_action_from_human = False
        return super().query()

    def run(self, task: str, **kwargs) -> tuple[str, str]:
        try:
            exit_status, result = super().run(task, **kwargs)
        except Exception as e:
            result = str(e)
            self.app.call_from_thread(self.app.action_quit)
            print(traceback.format_exc())
            return "ERROR", result
        else:
            self.app.call_from_thread(self.app.on_agent_finished, exit_status, result)
        self.app.call_from_thread(self.app.action_quit)
        return exit_status, result

    def execute_action(self, action: dict) -> dict:
        if self.config.mode == "human" and not self._current_action_from_human:  # threading, grrrrr
            raise NonTerminatingException("Command not executed because user switched to manual mode.")
        if (
            self.config.mode == "confirm"
            and action["action"].strip()
            and not any(re.match(r, action["action"]) for r in self.config.whitelist_actions)
        ):
            result = self.app.input_container.request_input("Press ENTER to confirm or provide rejection reason")
            if result:  # Non-empty string means rejection
                raise NonTerminatingException(f"Command not executed: {result}")
        return super().execute_action(action)

    def has_finished(self, output: dict[str, str]):
        try:
            return super().has_finished(output)
        except Submitted as e:
            if self.config.confirm_exit:
                if new_task := self.app.input_container.request_input(
                    "[bold green]Agent wants to finish.[/bold green] "
                    "[green]Type a comment to give it a new task or press enter to quit.\n"
                ).strip():
                    raise NonTerminatingException(f"The user added a new task: {new_task}")
            raise e


class AddLogEmitCallback(logging.Handler):
    def __init__(self, callback):
        """Custom log handler that forwards messages via callback."""
        super().__init__()
        self.callback = callback

    def emit(self, record: logging.LogRecord):
        self.callback(record)  # type: ignore[attr-defined]


def _messages_to_steps(messages: list[dict]) -> list[list[dict]]:
    """Group messages into "pages" as shown by the UI."""
    steps = []
    current_step = []
    for message in messages:
        current_step.append(message)
        if message["role"] == "user":
            steps.append(current_step)
            current_step = []
    if current_step:
        steps.append(current_step)
    return steps


class SmartInputContainer(Container):
    def __init__(self, app: "TextualAgent"):
        """Smart input container supporting single-line and multi-line input modes."""
        super().__init__(classes="smart-input-container")
        self._app = app
        self._multiline_mode = False
        self.can_focus = True
        self.display = False

        self.pending_prompt: str | None = None
        self._input_event = threading.Event()
        self._input_result: str | None = None

        self._header_display = Static(id="input-header-display", classes="message-header input-request-header")
        self._hint_text = Static(classes="hint-text")
        self._single_input = Input(placeholder="Type your input...")
        self._multi_input = TextArea(show_line_numbers=False, classes="multi-input")
        self._input_elements_container = Vertical(
            self._header_display,
            self._hint_text,
            self._single_input,
            self._multi_input,
            classes="message-container",
        )

    def compose(self) -> ComposeResult:
        yield self._input_elements_container

    def on_mount(self) -> None:
        """Initialize the widget state."""
        self._multi_input.display = False
        self._update_mode_display()

    def on_focus(self) -> None:
        """Called when the container gains focus."""
        if self._multiline_mode:
            self._multi_input.focus()
        else:
            self._single_input.focus()

    def request_input(self, prompt: str) -> str:
        """Request input from user. Returns input text (empty string if confirmed without reason)."""
        self._input_event.clear()
        self._input_result = None
        self.pending_prompt = prompt
        self._header_display.update(prompt)
        self._update_mode_display()
        self._app.call_from_thread(self._app.update_content)
        self._input_event.wait()
        return self._input_result or ""

    def _complete_input(self, input_text: str):
        """Internal method to complete the input process."""
        self._input_result = input_text
        self.pending_prompt = None
        self.display = False
        self._single_input.value = ""
        self._multi_input.text = ""
        self._multiline_mode = False
        self._update_mode_display()
        self._app.agent_state = "RUNNING"
        self._app.update_content()
        # Reset scroll position to bottom since input container disappearing changes layout
        # somehow scroll_to doesn't work.
        self._app._vscroll.scroll_y = 0
        self._input_event.set()

    def action_toggle_mode(self) -> None:
        """Switch from single-line to multi-line mode (one-way only)."""
        if self.pending_prompt is None or self._multiline_mode:
            return

        self._multiline_mode = True
        self._update_mode_display()
        self.on_focus()

    def _update_mode_display(self) -> None:
        """Update the display based on current mode."""
        if self._multiline_mode:
            self._multi_input.text = self._single_input.value
            self._single_input.display = False
            self._multi_input.display = True
            self._hint_text.update(
                "[reverse][bold][$accent] Ctrl+D [/][/][/] to submit, [reverse][bold][$accent] Tab [/][/][/] to switch focus with other controls"
            )
        else:
            self._hint_text.update(
                "[reverse][bold][$accent] Enter [/][/][/] to submit, [reverse][bold][$accent] Ctrl+T [/][/][/] to switch to multi-line input, [reverse][bold][$accent] Tab [/][/][/] to switch focus with other controls",
            )
            self._multi_input.display = False
            self._single_input.display = True

    def on_input_submitted(self, event: Input.Submitted) -> None:
        """Handle single-line input submission."""
        if not self._multiline_mode:
            text = event.input.value.strip()
            self._complete_input(text)

    def on_key(self, event: Key) -> None:
        """Handle key events."""
        if event.key == "ctrl+t" and not self._multiline_mode:
            event.prevent_default()
            self.action_toggle_mode()
            return

        if self._multiline_mode and event.key == "ctrl+d":
            event.prevent_default()
            self._complete_input(self._multi_input.text.strip())
            return

        if event.key == "escape":
            event.prevent_default()
            self.can_focus = False
            self._app.set_focus(None)
            return


class TextualAgent(App):
    BINDINGS = [
        Binding("right,l", "next_step", "Step++", tooltip="Show next step of the agent"),
        Binding("left,h", "previous_step", "Step--", tooltip="Show previous step of the agent"),
        Binding("0", "first_step", "Step=0", tooltip="Show first step of the agent", show=False),
        Binding("$", "last_step", "Step=-1", tooltip="Show last step of the agent", show=False),
        Binding("j,down", "scroll_down", "Scroll down", show=False),
        Binding("k,up", "scroll_up", "Scroll up", show=False),
        Binding("q,ctrl+q", "quit", "Quit", tooltip="Quit the agent"),
        Binding("y,ctrl+y", "yolo", "YOLO mode", tooltip="Switch to YOLO Mode (LM actions will execute immediately)"),
        Binding(
            "c",
            "confirm",
            "CONFIRM mode",
            tooltip="Switch to Confirm Mode (LM proposes commands and you confirm/reject them)",
        ),
        Binding("u,ctrl+u", "human", "HUMAN mode", tooltip="Switch to Human Mode (you can now type commands directly)"),
        Binding("f1,question_mark", "toggle_help_panel", "Help", tooltip="Show help"),
    ]

    def __init__(self, model, env, **kwargs):
        css_path = os.environ.get("MSWEA_MINI_STYLE_PATH", str(Path(__file__).parent.parent / "config" / "mini.tcss"))
        self.__class__.CSS = Path(css_path).read_text()
        super().__init__()
        self.agent_state = "UNINITIALIZED"
        self.agent = _TextualAgent(self, model=model, env=env, **kwargs)
        self._i_step = 0
        self.n_steps = 1
        self.input_container = SmartInputContainer(self)
        self.log_handler = AddLogEmitCallback(lambda record: self.call_from_thread(self.on_log_message_emitted, record))
        logging.getLogger().addHandler(self.log_handler)
        self._spinner = Spinner("dots")
        self.exit_status: str = "ExitStatusUnset"
        self.result: str = ""

        self._vscroll = VerticalScroll()

    def run(self, task: str, **kwargs) -> tuple[str, str]:
        threading.Thread(target=lambda: self.agent.run(task, **kwargs), daemon=True).start()
        super().run()
        return self.exit_status, self.result

    # --- Basics ---

    @property
    def i_step(self) -> int:
        """Current step index."""
        return self._i_step

    @i_step.setter
    def i_step(self, value: int) -> None:
        """Set current step index, automatically clamping to valid bounds."""
        if value != self._i_step:
            self._i_step = max(0, min(value, self.n_steps - 1))
            self._vscroll.scroll_to(y=0, animate=False)
            self.update_content()

    def compose(self) -> ComposeResult:
        yield Header()
        with Container(id="main"):
            with self._vscroll:
                with Vertical(id="content"):
                    pass
                yield self.input_container
        yield Footer()

    def on_mount(self) -> None:
        self.agent_state = "RUNNING"
        self.update_content()
        self.set_interval(1 / 8, self._update_headers)

    @property
    def messages(self) -> list[dict]:
        return self.agent.messages

    @property
    def model(self):
        return self.agent.model

    @property
    def env(self):
        return self.agent.env

    # --- Reacting to events ---

    def on_message_added(self) -> None:
        auto_follow = self.i_step == self.n_steps - 1 and self._vscroll.scroll_y <= 1
        self.n_steps = len(_messages_to_steps(self.agent.messages))
        self.update_content()
        if auto_follow:
            self.action_last_step()

    def on_log_message_emitted(self, record: logging.LogRecord) -> None:
        """Handle log messages of warning level or higher by showing them as notifications."""
        if record.levelno >= logging.WARNING:
            self.notify(f"[{record.levelname}] {record.getMessage()}", severity="warning")

    def on_unmount(self) -> None:
        """Clean up the log handler when the app shuts down."""
        if hasattr(self, "log_handler"):
            logging.getLogger().removeHandler(self.log_handler)

    def on_agent_finished(self, exit_status: str, result: str):
        self.agent_state = "STOPPED"
        self.notify(f"Agent finished with status: {exit_status}")
        self.exit_status = exit_status
        self.result = result
        self.update_content()

    # --- UI update logic ---

    def update_content(self) -> None:
        container = self.query_one("#content", Vertical)
        container.remove_children()
        items = _messages_to_steps(self.agent.messages)

        if not items:
            container.mount(Static("Waiting for agent to start..."))
            return

        for message in items[self.i_step]:
            if isinstance(message["content"], list):
                content_str = "\n".join([item["text"] for item in message["content"]])
            else:
                content_str = str(message["content"])
            message_container = Vertical(classes="message-container")
            container.mount(message_container)
            role = message["role"].replace("assistant", "mini-swe-agent")
            message_container.mount(Static(role.upper(), classes="message-header"))
            message_container.mount(Static(Text(content_str, no_wrap=False), classes="message-content"))

        if self.input_container.pending_prompt is not None:
            self.agent_state = "AWAITING_INPUT"
        self.input_container.display = self.input_container.pending_prompt is not None and self.i_step == len(items) - 1
        if self.input_container.display:
            self.input_container.on_focus()

        self._update_headers()
        self.refresh()

    def _update_headers(self) -> None:
        """Update just the title with current state and spinner if needed."""
        status_text = self.agent_state
        if self.agent_state == "RUNNING":
            spinner_frame = str(self._spinner.render(time.time())).strip()
            status_text = f"{self.agent_state} {spinner_frame}"
        self.title = f"Step {self.i_step + 1}/{self.n_steps} - {status_text} - Cost: ${self.agent.model.cost:.2f}"
        try:
            self.query_one("Header").set_class(self.agent_state == "RUNNING", "running")
        except NoMatches:  # might be called when shutting down
            pass

    # --- Other textual overrides ---

    def get_system_commands(self, screen: Screen) -> Iterable[SystemCommand]:
        # Add to palette
        yield from super().get_system_commands(screen)
        for binding in self.BINDINGS:
            description = f"{binding.description} (shortcut {' OR '.join(binding.key.split(','))})"  # type: ignore[attr-defined]
            yield SystemCommand(description, binding.tooltip, binding.action)  # type: ignore[attr-defined]

    # --- Textual bindings ---

    def action_yolo(self):
        self.agent.config.mode = "yolo"
        if self.input_container.pending_prompt is not None:
            self.input_container._complete_input("")  # accept
        self.notify("YOLO mode enabled - LM actions will execute immediately")

    def action_human(self):
        if self.agent.config.mode == "confirm" and self.input_container.pending_prompt is not None:
            self.input_container._complete_input("User switched to manual mode, this command will be ignored")
        self.agent.config.mode = "human"
        self.notify("Human mode enabled - you can now type commands directly")

    def action_confirm(self):
        if self.agent.config.mode == "human" and self.input_container.pending_prompt is not None:
            self.input_container._complete_input("")  # just submit blank action
        self.agent.config.mode = "confirm"
        self.notify("Confirm mode enabled - LM proposes commands and you confirm/reject them")

    def action_next_step(self) -> None:
        self.i_step += 1

    def action_previous_step(self) -> None:
        self.i_step -= 1

    def action_first_step(self) -> None:
        self.i_step = 0

    def action_last_step(self) -> None:
        self.i_step = self.n_steps - 1

    def action_scroll_down(self) -> None:
        self._vscroll.scroll_to(y=self._vscroll.scroll_target_y + 15)

    def action_scroll_up(self) -> None:
        self._vscroll.scroll_to(y=self._vscroll.scroll_target_y - 15)

    def action_toggle_help_panel(self) -> None:
        if self.query("HelpPanel"):
            self.action_hide_help_panel()
        else:
            self.action_show_help_panel()



================================================
FILE: src/minisweagent/config/README.md
================================================
# Configs

* `mini.yaml` - Default config for `mini`/`agents/interactive.py` or `mini -v`/`agents/interactive_textual.py` agent.
* `mini_no_temp.yaml` - Same as `mini.yaml` but without the temperature setting
* `default.yaml` - Default config for the `default.py` agent.
* `github_issue.yaml` - Config for the `run/github_issue.py` entry point.

## Extras

* `extra/swebench.yaml` - Config for the `run/extra/swebench.py` entry point.



================================================
FILE: src/minisweagent/config/__init__.py
================================================
"""Configuration files and utilities for mini-SWE-agent."""

import os
from pathlib import Path

builtin_config_dir = Path(__file__).parent


def get_config_path(config_spec: str | Path) -> Path:
    """Get the path to a config file."""
    config_spec = Path(config_spec)
    if config_spec.suffix != ".yaml":
        config_spec = config_spec.with_suffix(".yaml")
    candidates = [
        Path(config_spec),
        Path(os.getenv("MSWEA_CONFIG_DIR", ".")) / config_spec,
        builtin_config_dir / config_spec,
        builtin_config_dir / "extra" / config_spec,
    ]
    for candidate in candidates:
        if candidate.exists():
            return candidate

    raise FileNotFoundError(f"Could not find config file for {config_spec} (tried: {candidates})")


__all__ = ["builtin_config_dir", "get_config_path"]



================================================
FILE: src/minisweagent/config/default.yaml
================================================
agent:
  system_template: |
    You are a helpful assistant that can interact with a computer.

    Your response must contain exactly ONE bash code block with ONE command (or commands connected with && or ||).
    Include a THOUGHT section before your command where you explain your reasoning process.
    Format your response as shown in <format_example>.

    <format_example>
    Your reasoning and analysis here. Explain why you want to perform the action.

    ```bash
    your_command_here
    ```
    </format_example>

    Failure to follow these rules will cause your response to be rejected.
  instance_template: |
    Please solve this issue: {{task}}

    You can execute bash commands and edit files to implement the necessary changes.

    ## Recommended Workflow

    This workflows should be done step-by-step so that you can iterate on your changes and any possible problems.

    1. Analyze the codebase by finding and reading relevant files
    2. Create a script to reproduce the issue
    3. Edit the source code to resolve the issue
    4. Verify your fix works by running your script again
    5. Test edge cases to ensure your fix is robust
    6. Submit your changes and finish your work by issuing the following command: `echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT`.
       Do not combine it with any other command. <important>After this command, you cannot continue working on this task.</important>

    ## Important Rules

    1. Every response must contain exactly one action
    2. The action must be enclosed in triple backticks
    3. Directory or environment variable changes are not persistent. Every action is executed in a new subshell.
       However, you can prefix any action with `MY_ENV_VAR=MY_VALUE cd /path/to/working/dir && ...` or write/load environment variables from files

    <system_information>
    {{system}} {{release}} {{version}} {{machine}} {{processor}}
    </system_information>

    ## Formatting your response

    Here is an example of a correct response:

    <example_response>
    THOUGHT: I need to understand the structure of the repository first. Let me check what files are in the current directory to get a better understanding of the codebase.

    ```bash
    ls -la
    ```
    </example_response>

    ## Useful command examples

    ### Create a new file:

    ```bash
    cat <<'EOF' > newfile.py
    import numpy as np
    hello = "world"
    print(hello)
    EOF
    ```

    ### Edit files with sed:

    {%- if system == "Darwin" -%}
    <important>
    You are on MacOS. For all the below examples, you need to use `sed -i ''` instead of `sed -i`.
    </important>
    {%- endif -%}

    ```bash
    # Replace all occurrences
    sed -i 's/old_string/new_string/g' filename.py

    # Replace only first occurrence
    sed -i 's/old_string/new_string/' filename.py

    # Replace first occurrence on line 1
    sed -i '1s/old_string/new_string/' filename.py

    # Replace all occurrences in lines 1-10
    sed -i '1,10s/old_string/new_string/g' filename.py
    ```

    ### View file content:

    ```bash
    # View specific lines with numbers
    nl -ba filename.py | sed -n '10,20p'
    ```

    ### Any other command you want to run

    ```bash
    anything
    ```
  action_observation_template: |
    <returncode>{{output.returncode}}</returncode>
    {% if output.output | length < 10000 -%}
    <output>
    {{ output.output -}}
    </output>
    {%- else -%}
    <warning>
    The output of your last command was too long.
    Please try a different command that produces less output.
    If you're looking at a file you can try use head, tail or sed to view a smaller number of lines selectively.
    If you're using grep or find and it produced too much output, you can use a more selective search pattern.
    If you really need to see something from the full command's output, you can redirect output to a file and then search in that file.
    </warning>
    {%- set elided_chars = output.output | length - 10000 -%}
    <output_head>
    {{ output.output[:5000] }}
    </output_head>
    <elided_chars>
    {{ elided_chars }} characters elided
    </elided_chars>
    <output_tail>
    {{ output.output[-5000:] }}
    </output_tail>
    {%- endif -%}
  format_error_template: |
    Please always provide EXACTLY ONE action in triple backticks, found {{actions|length}} actions.
    If you want to end the task, please issue the following command: `echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT`
    without any other command.
    Else, please format your response exactly as follows:

    <response_example>
    Here are some thoughts about why you want to perform the action.

    ```bash
    <action>
    ```
    </response_example>

    Note: In rare cases, if you need to reference a similar format in your command, you might have
    to proceed in two steps, first writing TRIPLEBACKTICKSBASH, then replacing them with ```bash.
  step_limit: 0.
  cost_limit: 0.
environment:
  env:
    PAGER: cat
    MANPAGER: cat
    LESS: -R
    PIP_PROGRESS_BAR: 'off'
    TQDM_DISABLE: '1'
model:
  model_kwargs:
    temperature: 0.0
    drop_params: true


================================================
FILE: src/minisweagent/config/github_issue.yaml
================================================
agent:
  system_template: |
    You are a helpful assistant that can interact with a computer.

    Your response must contain exactly ONE bash code block with ONE command (or commands connected with && or ||).
    Include a THOUGHT section before your command where you explain your reasoning process.
    Format your response as shown in <format_example>.

    <format_example>
    Your reasoning and analysis here. Explain why you want to perform the action.

    ```bash
    your_command_here
    ```
    </format_example>

    Failure to follow these rules will cause your response to be rejected.
    To finish, issue the following command: `echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT`
    without any other command.
  instance_template: |
    Please solve this issue: {{task}}

    You can execute bash commands and edit files to implement the necessary changes.

    ## Recommended Workflow
    1. Analyze the codebase by finding and reading relevant files
    2. Create a script to reproduce the issue
    3. Edit the source code to resolve the issue
    4. Verify your fix works by running your script again
    5. Test edge cases to ensure your fix is robust

    ## Important Rules

    1. Every response must contain exactly one action
    2. The action must be enclosed in triple backticks
    3. Directory or environment variable changes are not persistent. Every action is executed in a new subshell.
       However, you can prefix any action with `MY_ENV_VAR=MY_VALUE cd /path/to/working/dir && ...` or write/load environment variables from files
    4. To finish, issue the following command: `echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT`.
       Do not combine it with any other command.

    ## Formatting your response

    Here is an example of a correct response:

    <example_response>
    THOUGHT: I need to understand the structure of the repository first. Let me check what files are in the current directory to get a better understanding of the codebase.

    ```bash
    ls -la
    ```
    </example_response>

    ## Useful command examples

    ### Create a new file:

    ```bash
    cat <<'EOF' > newfile.py
    import numpy as np
    hello = "world"
    print(hello)
    EOF
    ```

    ### Edit files with sed:

    ```bash
    # Replace all occurrences
    sed -i 's/old_string/new_string/g' filename.py

    # Replace only first occurrence
    sed -i 's/old_string/new_string/' filename.py

    # Replace first occurrence on line 1
    sed -i '1s/old_string/new_string/' filename.py

    # Replace all occurrences in lines 1-10
    sed -i '1,10s/old_string/new_string/g' filename.py
    ```

    ### View file content:

    ```bash
    # View specific lines with numbers
    nl -ba filename.py | sed -n '10,20p'
    ```

    ### Any other command you want to run

    ```bash
    anything
    ```
  action_observation_template: |
    <returncode>{{output.returncode}}</returncode>
    {% if output.output | length < 10000 -%}
    <output>
    {{ output.output -}}
    </output>
    {%- else -%}
    <warning>
    The output of your last command was too long.
    Please try a different command that produces less output.
    If you're looking at a file you can try use head, tail or sed to view a smaller number of lines selectively.
    If you're using grep or find and it produced too much output, you can use a more selective search pattern.
    If you really need to see something from the full command's output, you can redirect output to a file and then search in that file.
    </warning>
    {%- set elided_chars = output.output | length - 10000 -%}
    <output_head>
    {{ output.output[:5000] }}
    </output_head>
    <elided_chars>
    {{ elided_chars }} characters elided
    </elided_chars>
    <output_tail>
    {{ output.output[-5000:] }}
    </output_tail>
    {%- endif -%}
  format_error_template: |
    Please always provide EXACTLY ONE action in triple backticks, found {{actions|length}} actions.
    If you want to end the task, please issue the following command: `echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT`
    without any other command.
    Else, please format your response exactly as follows:

    <response_example>
    Here are some thoughts about why you want to perform the action.

    ```bash
    <action>
    ```
    </response_example>
  step_limit: 0.
  cost_limit: 0.

environment:
  image: "python:3.11"
  cwd: "/testbed"
  env:
    PAGER: cat
    MANPAGER: cat
    LESS: -R
    PIP_PROGRESS_BAR: 'off'
    TQDM_DISABLE: '1'
model:
  model_kwargs:
    temperature: 0.0
    drop_params: true


================================================
FILE: src/minisweagent/config/mini.tcss
================================================
Screen {
    layout: grid;
    grid-size: 1;
    grid-rows: auto 1fr auto;
}

#main {
    height: 100%;
    padding: 1;
    layout: vertical;
}

Footer {
    dock: bottom;
    content-align: center middle;
}

#content {
    height: auto;
    min-height: 0;
}

.smart-input-container {
    height: auto;
    margin-top: 0;
    padding: 0;
    min-height: 0;
}

.multi-input {
    height: auto;
    max-height: 20;
    min-height: 3;
}

.prompt-display {
    margin-bottom: 1;
    padding: 0 1;
    text-style: bold;
}

.hint-text{
    margin-top: 1;
    margin-bottom: 1;
    padding: 0 1;
    color: white;
}

.message-container {
    margin: 1;
    padding: 1;
    background: $surface;
    height: auto;
    width: 100%;
}

.message-header {
    text-align: left;
    color: $primary;
    padding: 0 1;
    text-style: bold;
}

.input-request-header {
    color: $warning;
}

.message-content {
    margin-top: 1;
    padding: 0 1;
}

Header.running {
    background: $error;
}

.button-container {
    layout: horizontal;
    align-horizontal: center;
    margin-top: 1;
}

.button-container Button {
    margin: 0 1;
    min-width: 10;
}


================================================
FILE: src/minisweagent/config/mini.yaml
================================================
agent:
  system_template: |
    You are a helpful assistant that can interact with a computer.

    Your response must contain exactly ONE bash code block with ONE command (or commands connected with && or ||).
    Include a THOUGHT section before your command where you explain your reasoning process.
    Format your response as shown in <format_example>.

    <format_example>
    Your reasoning and analysis here. Explain why you want to perform the action.

    ```bash
    your_command_here
    ```
    </format_example>

    Failure to follow these rules will cause your response to be rejected.
  instance_template: |
    Please solve this issue: {{task}}

    You can execute bash commands and edit files to implement the necessary changes.

    ## Recommended Workflow

    This workflows should be done step-by-step so that you can iterate on your changes and any possible problems.

    1. Analyze the codebase by finding and reading relevant files
    2. Create a script to reproduce the issue
    3. Edit the source code to resolve the issue
    4. Verify your fix works by running your script again
    5. Test edge cases to ensure your fix is robust
    6. Submit your changes and finish your work by issuing the following command: `echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT`.
       Do not combine it with any other command. <important>After this command, you cannot continue working on this task.</important>

    ## Important Rules

    1. Every response must contain exactly one action
    2. The action must be enclosed in triple backticks
    3. Directory or environment variable changes are not persistent. Every action is executed in a new subshell.
       However, you can prefix any action with `MY_ENV_VAR=MY_VALUE cd /path/to/working/dir && ...` or write/load environment variables from files

    <system_information>
    {{system}} {{release}} {{version}} {{machine}} {{processor}}
    </system_information>

    ## Formatting your response

    Here is an example of a correct response:

    <example_response>
    THOUGHT: I need to understand the structure of the repository first. Let me check what files are in the current directory to get a better understanding of the codebase.

    ```bash
    ls -la
    ```
    </example_response>

    ## Useful command examples

    ### Create a new file:

    ```bash
    cat <<'EOF' > newfile.py
    import numpy as np
    hello = "world"
    print(hello)
    EOF
    ```

    ### Edit files with sed:

    {%- if system == "Darwin" -%}
    <important>
    You are on MacOS. For all the below examples, you need to use `sed -i ''` instead of `sed -i`.
    </important>
    {%- endif -%}

    ```bash
    # Replace all occurrences
    sed -i 's/old_string/new_string/g' filename.py

    # Replace only first occurrence
    sed -i 's/old_string/new_string/' filename.py

    # Replace first occurrence on line 1
    sed -i '1s/old_string/new_string/' filename.py

    # Replace all occurrences in lines 1-10
    sed -i '1,10s/old_string/new_string/g' filename.py
    ```

    ### View file content:

    ```bash
    # View specific lines with numbers
    nl -ba filename.py | sed -n '10,20p'
    ```

    ### Any other command you want to run

    ```bash
    anything
    ```
  action_observation_template: |
    <returncode>{{output.returncode}}</returncode>
    {% if output.output | length < 10000 -%}
    <output>
    {{ output.output -}}
    </output>
    {%- else -%}
    <warning>
    The output of your last command was too long.
    Please try a different command that produces less output.
    If you're looking at a file you can try use head, tail or sed to view a smaller number of lines selectively.
    If you're using grep or find and it produced too much output, you can use a more selective search pattern.
    If you really need to see something from the full command's output, you can redirect output to a file and then search in that file.
    </warning>
    {%- set elided_chars = output.output | length - 10000 -%}
    <output_head>
    {{ output.output[:5000] }}
    </output_head>
    <elided_chars>
    {{ elided_chars }} characters elided
    </elided_chars>
    <output_tail>
    {{ output.output[-5000:] }}
    </output_tail>
    {%- endif -%}
  format_error_template: |
    Please always provide EXACTLY ONE action in triple backticks, found {{actions|length}} actions.
    If you want to end the task, please issue the following command: `echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT`
    without any other command.
    Else, please format your response exactly as follows:

    <response_example>
    Here are some thoughts about why you want to perform the action.

    ```bash
    <action>
    ```
    </response_example>

    Note: In rare cases, if you need to reference a similar format in your command, you might have
    to proceed in two steps, first writing TRIPLEBACKTICKSBASH, then replacing them with ```bash.
  step_limit: 0.
  cost_limit: 3.
  mode: confirm
environment:
  env:
    PAGER: cat
    MANPAGER: cat
    LESS: -R
    PIP_PROGRESS_BAR: 'off'
    TQDM_DISABLE: '1'
model:
  model_kwargs:
    temperature: 0.0
    drop_params: true


================================================
FILE: src/minisweagent/config/mini_no_temp.yaml
================================================
# Identical config file to mini.yaml, but without temperature=0.0
agent:
  system_template: |
    You are a helpful assistant that can interact with a computer.

    Your response must contain exactly ONE bash code block with ONE command (or commands connected with && or ||).
    Include a THOUGHT section before your command where you explain your reasoning process.
    Format your response as shown in <format_example>.

    <format_example>
    Your reasoning and analysis here. Explain why you want to perform the action.

    ```bash
    your_command_here
    ```
    </format_example>

    Failure to follow these rules will cause your response to be rejected.
  instance_template: |
    Please solve this issue: {{task}}

    You can execute bash commands and edit files to implement the necessary changes.

    ## Recommended Workflow

    This workflows should be done step-by-step so that you can iterate on your changes and any possible problems.

    1. Analyze the codebase by finding and reading relevant files
    2. Create a script to reproduce the issue
    3. Edit the source code to resolve the issue
    4. Verify your fix works by running your script again
    5. Test edge cases to ensure your fix is robust
    6. Submit your changes and finish your work by issuing the following command: `echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT`.
       Do not combine it with any other command. <important>After this command, you cannot continue working on this task.</important>

    ## Important Rules

    1. Every response must contain exactly one action
    2. The action must be enclosed in triple backticks
    3. Directory or environment variable changes are not persistent. Every action is executed in a new subshell.
       However, you can prefix any action with `MY_ENV_VAR=MY_VALUE cd /path/to/working/dir && ...` or write/load environment variables from files

    <system_information>
    {{system}} {{release}} {{version}} {{machine}} {{processor}}
    </system_information>

    ## Formatting your response

    Here is an example of a correct response:

    <example_response>
    THOUGHT: I need to understand the structure of the repository first. Let me check what files are in the current directory to get a better understanding of the codebase.

    ```bash
    ls -la
    ```
    </example_response>

    ## Useful command examples

    ### Create a new file:

    ```bash
    cat <<'EOF' > newfile.py
    import numpy as np
    hello = "world"
    print(hello)
    EOF
    ```

    ### Edit files with sed:

    {%- if system == "Darwin" -%}
    <important>
    You are on MacOS. For all the below examples, you need to use `sed -i ''` instead of `sed -i`.
    </important>
    {%- endif -%}

    ```bash
    # Replace all occurrences
    sed -i 's/old_string/new_string/g' filename.py

    # Replace only first occurrence
    sed -i 's/old_string/new_string/' filename.py

    # Replace first occurrence on line 1
    sed -i '1s/old_string/new_string/' filename.py

    # Replace all occurrences in lines 1-10
    sed -i '1,10s/old_string/new_string/g' filename.py
    ```

    ### View file content:

    ```bash
    # View specific lines with numbers
    nl -ba filename.py | sed -n '10,20p'
    ```

    ### Any other command you want to run

    ```bash
    anything
    ```
  action_observation_template: |
    <returncode>{{output.returncode}}</returncode>
    {% if output.output | length < 10000 -%}
    <output>
    {{ output.output -}}
    </output>
    {%- else -%}
    <warning>
    The output of your last command was too long.
    Please try a different command that produces less output.
    If you're looking at a file you can try use head, tail or sed to view a smaller number of lines selectively.
    If you're using grep or find and it produced too much output, you can use a more selective search pattern.
    If you really need to see something from the full command's output, you can redirect output to a file and then search in that file.
    </warning>
    {%- set elided_chars = output.output | length - 10000 -%}
    <output_head>
    {{ output.output[:5000] }}
    </output_head>
    <elided_chars>
    {{ elided_chars }} characters elided
    </elided_chars>
    <output_tail>
    {{ output.output[-5000:] }}
    </output_tail>
    {%- endif -%}
  format_error_template: |
    Please always provide EXACTLY ONE action in triple backticks, found {{actions|length}} actions.
    If you want to end the task, please issue the following command: `echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT`
    without any other command.
    Else, please format your response exactly as follows:

    <response_example>
    Here are some thoughts about why you want to perform the action.

    ```bash
    <action>
    ```
    </response_example>

    Note: In rare cases, if you need to reference a similar format in your command, you might have
    to proceed in two steps, first writing TRIPLEBACKTICKSBASH, then replacing them with ```bash.
  step_limit: 0.
  cost_limit: 3.
  mode: confirm
environment:
  env:
    PAGER: cat
    MANPAGER: cat
    LESS: -R
    PIP_PROGRESS_BAR: 'off'
    TQDM_DISABLE: '1'
model:
  model_kwargs:
    drop_params: true


================================================
FILE: src/minisweagent/config/extra/__init__.py
================================================
"""Extra configuration files for mini-SWE-agent."""



================================================
FILE: src/minisweagent/config/extra/swebench.yaml
================================================
agent:
  system_template: |
    You are a helpful assistant that can interact multiple times with a computer shell to solve programming tasks.
    Your response must contain exactly ONE bash code block with ONE command (or commands connected with && or ||).

    Include a THOUGHT section before your command where you explain your reasoning process.
    Format your response as shown in <format_example>.

    <format_example>
    THOUGHT: Your reasoning and analysis here

    ```bash
    your_command_here
    ```
    </format_example>

    Failure to follow these rules will cause your response to be rejected.
  instance_template: |
    <pr_description>
    Consider the following PR description:
    {{task}}
    </pr_description>

    <instructions>
    # Task Instructions

    ## Overview
    You're a software engineer interacting continuously with a computer by submitting commands.
    You'll be helping implement necessary changes to meet requirements in the PR description.
    Your task is specifically to make changes to non-test files in the current directory in order to fix the issue described in the PR description in a way that is general and consistent with the codebase.

    IMPORTANT: This is an interactive process where you will think and issue ONE command, see its result, then think and issue your next command.

    For each response:
    1. Include a THOUGHT section explaining your reasoning and what you're trying to accomplish
    2. Provide exactly ONE bash command to execute

    ## Important Boundaries
    - MODIFY: Regular source code files in {{working_dir}}
    - DO NOT MODIFY: Tests, configuration files (pyproject.toml, setup.cfg, etc.)

    ## Recommended Workflow
    1. Analyze the codebase by finding and reading relevant files
    2. Create a script to reproduce the issue
    3. Edit the source code to resolve the issue
    4. Verify your fix works by running your script again
    5. Test edge cases to ensure your fix is robust

    ## Command Execution Rules
    You are operating in an environment where
    1. You write a single command
    2. The system executes that command in a subshell
    3. You see the result
    4. You write your next command

    Each response should include:
    1. A **THOUGHT** section where you explain your reasoning and plan
    2. A single bash code block with your command

    Format your responses like this:

    <format_example>
    THOUGHT: Here I explain my reasoning process, analysis of the current situation,
    and what I'm trying to accomplish with the command below.

    ```bash
    your_command_here
    ```
    </format_example>

    Commands must be specified in a single bash code block:

    ```bash
    your_command_here
    ```

    **CRITICAL REQUIREMENTS:**
    - Your response SHOULD include a THOUGHT section explaining your reasoning
    - Your response MUST include EXACTLY ONE bash code block
    - This bash block MUST contain EXACTLY ONE command (or a set of commands connected with && or ||)
    - If you include zero or multiple bash blocks, or no command at all, YOUR RESPONSE WILL FAIL
    - Do NOT try to run multiple independent commands in separate blocks in one response
    - Directory or environment variable changes are not persistent. Every action is executed in a new subshell.
    - However, you can prefix any action with `MY_ENV_VAR=MY_VALUE cd /path/to/working/dir && ...` or write/load environment variables from files

    Example of a CORRECT response:
    <example_response>
    THOUGHT: I need to understand the structure of the repository first. Let me check what files are in the current directory to get a better understanding of the codebase.

    ```bash
    ls -la
    ```
    </example_response>

    Example of an INCORRECT response:
    <example_response>
    THOUGHT: I need to examine the codebase and then look at a specific file. I'll run multiple commands to do this.

    ```bash
    ls -la
    ```

    Now I'll read the file:

    ```bash
    cat file.txt
    ```
    </example_response>

    If you need to run multiple commands, either:
    1. Combine them in one block using && or ||
    ```bash
    command1 && command2 || echo "Error occurred"
    ```

    2. Wait for the first command to complete, see its output, then issue the next command in your following response.

    ## Environment Details
    - You have a full Linux shell environment
    - Always use non-interactive flags (-y, -f) for commands
    - Avoid interactive tools like vi, nano, or any that require user input
    - If a command isn't available, you can install it

    ## Useful Command Examples

    ### Create a new file:
    ```bash
    cat <<'EOF' > newfile.py
    import numpy as np
    hello = "world"
    print(hello)
    EOF
    ```

    ### Edit files with sed:
    ```bash
    # Replace all occurrences
    sed -i 's/old_string/new_string/g' filename.py

    # Replace only first occurrence
    sed -i 's/old_string/new_string/' filename.py

    # Replace first occurrence on line 1
    sed -i '1s/old_string/new_string/' filename.py

    # Replace all occurrences in lines 1-10
    sed -i '1,10s/old_string/new_string/g' filename.py
    ```

    ### View file content:
    ```bash
    # View specific lines with numbers
    nl -ba filename.py | sed -n '10,20p'
    ```

    ### Any other command you want to run
    ```bash
    anything
    ```

    ## Submission
    When you've completed your work (reading, editing, testing), and cannot make further progress
    issue exactly the following command:

    ```bash
    echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT && git add -A && git diff --cached
    ```

    This command will submit your work.
    You cannot continue working (reading, editing, testing) in any way on this task after submitting.
    </instructions>
  action_observation_template: |
    <returncode>{{output.returncode}}</returncode>
    {% if output.output | length < 10000 -%}
    <output>
    {{ output.output -}}
    </output>
    {%- else -%}
    <warning>
    The output of your last command was too long.
    Please try a different command that produces less output.
    If you're looking at a file you can try use head, tail or sed to view a smaller number of lines selectively.
    If you're using grep or find and it produced too much output, you can use a more selective search pattern.
    If you really need to see something from the full command's output, you can redirect output to a file and then search in that file.
    </warning>
    {%- set elided_chars = output.output | length - 10000 -%}
    <output_head>
    {{ output.output[:5000] }}
    </output_head>
    <elided_chars>
    {{ elided_chars }} characters elided
    </elided_chars>
    <output_tail>
    {{ output.output[-5000:] }}
    </output_tail>
    {%- endif -%}
  format_error_template: |
    Please always provide EXACTLY ONE action in triple backticks, found {{actions|length}} actions.

    Please format your action in triple backticks as shown in <response_example>.

    <response_example>
    Here are some thoughts about why you want to perform the action.

    ```bash
    <action>
    ```
    </response_example>

    If you have completed your assignment, please consult the first message about how to
    submit your solution (you will not be able to continue working on this task after that).
  step_limit: 250
  cost_limit: 3.

environment:
  cwd: "/testbed"
  timeout: 60
  env:
    PAGER: cat
    MANPAGER: cat
    LESS: -R
    PIP_PROGRESS_BAR: 'off'
    TQDM_DISABLE: '1'
  environment_class: docker

model:
  model_name: "claude-sonnet-4-20250514"
  model_kwargs:
    drop_params: true
    temperature: 0.0



================================================
FILE: src/minisweagent/config/extra/swebench_roulette.yaml
================================================
agent:
  system_template: |
    You are a helpful assistant that can interact multiple times with a computer shell to solve programming tasks.
    Your response must contain exactly ONE bash code block with ONE command (or commands connected with && or ||).

    Include a THOUGHT section before your command where you explain your reasoning process.
    Format your response as shown in <format_example>.

    <format_example>
    THOUGHT: Your reasoning and analysis here

    ```bash
    your_command_here
    ```
    </format_example>

    Failure to follow these rules will cause your response to be rejected.
  instance_template: |
    <pr_description>
    Consider the following PR description:
    {{task}}
    </pr_description>

    <instructions>
    # Task Instructions

    ## Overview
    You're a software engineer interacting continuously with a computer by submitting commands.
    You'll be helping implement necessary changes to meet requirements in the PR description.
    Your task is specifically to make changes to non-test files in the current directory in order to fix the issue described in the PR description in a way that is general and consistent with the codebase.

    IMPORTANT: This is an interactive process where you will think and issue ONE command, see its result, then think and issue your next command.

    For each response:
    1. Include a THOUGHT section explaining your reasoning and what you're trying to accomplish
    2. Provide exactly ONE bash command to execute

    ## Important Boundaries
    - MODIFY: Regular source code files in {{working_dir}}
    - DO NOT MODIFY: Tests, configuration files (pyproject.toml, setup.cfg, etc.)

    ## Recommended Workflow
    1. Analyze the codebase by finding and reading relevant files
    2. Create a script to reproduce the issue
    3. Edit the source code to resolve the issue
    4. Verify your fix works by running your script again
    5. Test edge cases to ensure your fix is robust

    ## Command Execution Rules
    You are operating in an environment where
    1. You write a single command
    2. The system executes that command in a subshell
    3. You see the result
    4. You write your next command

    Each response should include:
    1. A **THOUGHT** section where you explain your reasoning and plan
    2. A single bash code block with your command

    Format your responses like this:

    <format_example>
    THOUGHT: Here I explain my reasoning process, analysis of the current situation,
    and what I'm trying to accomplish with the command below.

    ```bash
    your_command_here
    ```
    </format_example>

    Commands must be specified in a single bash code block:

    ```bash
    your_command_here
    ```

    **CRITICAL REQUIREMENTS:**
    - Your response SHOULD include a THOUGHT section explaining your reasoning
    - Your response MUST include EXACTLY ONE bash code block
    - This bash block MUST contain EXACTLY ONE command (or a set of commands connected with && or ||)
    - If you include zero or multiple bash blocks, or no command at all, YOUR RESPONSE WILL FAIL
    - Do NOT try to run multiple independent commands in separate blocks in one response
    - Directory or environment variable changes are not persistent. Every action is executed in a new subshell.
    - However, you can prefix any action with `MY_ENV_VAR=MY_VALUE cd /path/to/working/dir && ...` or write/load environment variables from files

    Example of a CORRECT response:
    <example_response>
    THOUGHT: I need to understand the structure of the repository first. Let me check what files are in the current directory to get a better understanding of the codebase.

    ```bash
    ls -la
    ```
    </example_response>

    Example of an INCORRECT response:
    <example_response>
    THOUGHT: I need to examine the codebase and then look at a specific file. I'll run multiple commands to do this.

    ```bash
    ls -la
    ```

    Now I'll read the file:

    ```bash
    cat file.txt
    ```
    </example_response>

    If you need to run multiple commands, either:
    1. Combine them in one block using && or ||
    ```bash
    command1 && command2 || echo "Error occurred"
    ```

    2. Wait for the first command to complete, see its output, then issue the next command in your following response.

    ## Environment Details
    - You have a full Linux shell environment
    - Always use non-interactive flags (-y, -f) for commands
    - Avoid interactive tools like vi, nano, or any that require user input
    - If a command isn't available, you can install it

    ## Useful Command Examples

    ### Create a new file:
    ```bash
    cat <<'EOF' > newfile.py
    import numpy as np
    hello = "world"
    print(hello)
    EOF
    ```

    ### Edit files with sed:
    ```bash
    # Replace all occurrences
    sed -i 's/old_string/new_string/g' filename.py

    # Replace only first occurrence
    sed -i 's/old_string/new_string/' filename.py

    # Replace first occurrence on line 1
    sed -i '1s/old_string/new_string/' filename.py

    # Replace all occurrences in lines 1-10
    sed -i '1,10s/old_string/new_string/g' filename.py
    ```

    ### View file content:
    ```bash
    # View specific lines with numbers
    nl -ba filename.py | sed -n '10,20p'
    ```

    ### Any other command you want to run
    ```bash
    anything
    ```

    ## Submission
    When you've completed your work (reading, editing, testing), and cannot make further progress
    issue exactly the following command:

    ```bash
    echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT && git add -A && git diff --cached
    ```

    This command will submit your work.
    You cannot continue working (reading, editing, testing) in any way on this task after submitting.
    </instructions>
  action_observation_template: |
    <returncode>{{output.returncode}}</returncode>
    {% if output.output | length < 10000 -%}
    <output>
    {{ output.output -}}
    </output>
    {%- else -%}
    <warning>
    The output of your last command was too long.
    Please try a different command that produces less output.
    If you're looking at a file you can try use head, tail or sed to view a smaller number of lines selectively.
    If you're using grep or find and it produced too much output, you can use a more selective search pattern.
    If you really need to see something from the full command's output, you can redirect output to a file and then search in that file.
    </warning>
    {%- set elided_chars = output.output | length - 10000 -%}
    <output_head>
    {{ output.output[:5000] }}
    </output_head>
    <elided_chars>
    {{ elided_chars }} characters elided
    </elided_chars>
    <output_tail>
    {{ output.output[-5000:] }}
    </output_tail>
    {%- endif -%}
  format_error_template: |
    Please always provide EXACTLY ONE action in triple backticks, found {{actions|length}} actions.

    Please format your action in triple backticks as shown in <response_example>.

    <response_example>
    Here are some thoughts about why you want to perform the action.

    ```bash
    <action>
    ```
    </response_example>

    If you have completed your assignment, please consult the first message about how to
    submit your solution (you will not be able to continue working on this task after that).
  step_limit: 250
  cost_limit: 3.

environment:
  cwd: "/testbed"
  timeout: 60
  env:
    PAGER: cat
    MANPAGER: cat
    LESS: -R
    PIP_PROGRESS_BAR: 'off'
    TQDM_DISABLE: '1'
  environment_class: docker

model:
  model_name: "roulette"
  model_class: "minisweagent.models.extra.roulette.RouletteModel"
  model_kwargs:
    - model_name: "claude-sonnet-4-20250514"
      model_kwargs:
        temperature: 0.
    - model_name: "gpt-5"


================================================
FILE: src/minisweagent/environments/README.md
================================================
# Environments

* `local.py` - Execute code with `subprocess.run`
* `docker.py` - Execute code in a docker or podman container
* `singularity.py` - Execute code in a singularity or apptainer container

## Extras

* `extra/swerex_docker.py` - Execute environments with docker via [swerex](https://github.com/swe-agent/swe-rex)



================================================
FILE: src/minisweagent/environments/__init__.py
================================================
"""Environment implementations for mini-SWE-agent."""

import copy
import importlib

from minisweagent import Environment

_ENVIRONMENT_MAPPING = {
    "docker": "minisweagent.environments.docker.DockerEnvironment",
    "singularity": "minisweagent.environments.singularity.SingularityEnvironment",
    "local": "minisweagent.environments.local.LocalEnvironment",
    "swerex_docker": "minisweagent.environments.extra.swerex_docker.SwerexDockerEnvironment",
}


def get_environment_class(spec: str) -> type[Environment]:
    full_path = _ENVIRONMENT_MAPPING.get(spec, spec)
    try:
        module_name, class_name = full_path.rsplit(".", 1)
        module = importlib.import_module(module_name)
        return getattr(module, class_name)
    except (ValueError, ImportError, AttributeError):
        msg = f"Unknown environment type: {spec} (resolved to {full_path}, available: {_ENVIRONMENT_MAPPING})"
        raise ValueError(msg)


def get_environment(config: dict, *, default_type: str = "") -> Environment:
    config = copy.deepcopy(config)
    environment_class = config.pop("environment_class", default_type)
    return get_environment_class(environment_class)(**config)



================================================
FILE: src/minisweagent/environments/docker.py
================================================
import logging
import os
import shlex
import subprocess
import uuid
from dataclasses import asdict, dataclass, field
from typing import Any


@dataclass
class DockerEnvironmentConfig:
    image: str
    cwd: str = "/"
    """Working directory in which to execute commands."""
    env: dict[str, str] = field(default_factory=dict)
    """Environment variables to set in the container."""
    forward_env: list[str] = field(default_factory=list)
    """Environment variables to forward to the container.
    Variables are only forwarded if they are set in the host environment.
    In case of conflict with `env`, the `env` variables take precedence.
    """
    timeout: int = 30
    """Timeout for executing commands in the container."""
    executable: str = os.getenv("MSWEA_DOCKER_EXECUTABLE", "docker")
    """Path to the docker/container executable."""
    run_args: list[str] = field(default_factory=list)
    """Additional arguments to pass to the docker/container executable."""
    container_timeout: str = "2h"
    """Max duration to keep container running. Uses the same format as the sleep command."""


class DockerEnvironment:
    def __init__(self, *, config_class: type = DockerEnvironmentConfig, **kwargs):
        """This class executes bash commands in a Docker container using direct docker commands.
        See `DockerEnvironmentConfig` for keyword arguments.
        """
        self.logger = logging.getLogger("minisweagent.environment")
        self.container_id: str | None = None
        self.config = config_class(**kwargs)
        self._start_container()

    def get_template_vars(self) -> dict[str, Any]:
        return asdict(self.config)

    def _start_container(self):
        """Start the Docker container and return the container ID."""
        container_name = f"minisweagent-{uuid.uuid4().hex[:8]}"
        cmd = [
            self.config.executable,
            "run",
            "-d",
            "--name",
            container_name,
            "-w",
            self.config.cwd,
            *self.config.run_args,
            self.config.image,
            "sleep",
            self.config.container_timeout,
        ]
        self.logger.debug(f"Starting container with command: {shlex.join(cmd)}")
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=120,  # docker pull might take a while
            check=True,
        )
        self.logger.info(f"Started container {container_name} with ID {result.stdout.strip()}")
        self.container_id = result.stdout.strip()

    def execute(self, command: str, cwd: str = "") -> dict[str, Any]:
        """Execute a command in the Docker container and return the result as a dict."""
        cwd = cwd or self.config.cwd
        assert self.container_id, "Container not started"

        cmd = [self.config.executable, "exec", "-w", cwd]
        for key in self.config.forward_env:
            if (value := os.getenv(key)) is not None:
                cmd.extend(["-e", f"{key}={value}"])
        for key, value in self.config.env.items():
            cmd.extend(["-e", f"{key}={value}"])
        cmd.extend([self.container_id, "bash", "-lc", command])

        result = subprocess.run(
            cmd,
            text=True,
            timeout=self.config.timeout,
            encoding="utf-8",
            errors="replace",
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
        )
        return {"output": result.stdout, "returncode": result.returncode}

    def cleanup(self):
        """Stop and remove the Docker container."""
        if getattr(self, "container_id", None) is not None:  # if init fails early, container_id might not be set
            cmd = f"(timeout 60 {self.config.executable} stop {self.container_id} || {self.config.executable} rm -f {self.container_id}) >/dev/null 2>&1 &"
            subprocess.Popen(cmd, shell=True)

    def __del__(self):
        """Cleanup container when object is destroyed."""
        self.cleanup()



================================================
FILE: src/minisweagent/environments/local.py
================================================
import os
import platform
import subprocess
from dataclasses import asdict, dataclass, field
from typing import Any


@dataclass
class LocalEnvironmentConfig:
    cwd: str = ""
    env: dict[str, str] = field(default_factory=dict)
    timeout: int = 30


class LocalEnvironment:
    def __init__(self, *, config_class: type = LocalEnvironmentConfig, **kwargs):
        """This class executes bash commands directly on the local machine."""
        self.config = config_class(**kwargs)

    def execute(self, command: str, cwd: str = ""):
        """Execute a command in the local environment and return the result as a dict."""
        cwd = cwd or self.config.cwd or os.getcwd()
        result = subprocess.run(
            command,
            shell=True,
            text=True,
            cwd=cwd,
            env=os.environ | self.config.env,
            timeout=self.config.timeout,
            encoding="utf-8",
            errors="replace",
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
        )
        return {"output": result.stdout, "returncode": result.returncode}

    def get_template_vars(self) -> dict[str, Any]:
        return asdict(self.config) | platform.uname()._asdict() | os.environ



================================================
FILE: src/minisweagent/environments/singularity.py
================================================
#!/usr/bin/env python3

import logging
import os
import shutil
import subprocess
import tempfile
import uuid
from dataclasses import asdict, dataclass, field
from pathlib import Path
from typing import Any


@dataclass
class SingularityEnvironmentConfig:
    image: str
    cwd: str = "/"
    env: dict[str, str] = field(default_factory=dict)
    """Environment variables to set in the container."""
    forward_env: list[str] = field(default_factory=list)
    """Environment variables to forward to the container."""
    timeout: int = 30
    """Timeout for executing commands in the container."""
    executable: str = os.getenv("MSWEA_SINGULARITY_EXECUTABLE", "singularity")
    """Path to the singularity executable."""


class SingularityEnvironment:
    def __init__(self, **kwargs):
        """Singularity environment. See `SingularityEnvironmentConfig` for kwargs."""
        self.logger = logging.getLogger("minisweagent.environment")
        self.config = SingularityEnvironmentConfig(**kwargs)
        self.sandbox_dir = Path(tempfile.gettempdir()) / f"minisweagent-{uuid.uuid4().hex[:8]}"
        subprocess.run(
            [self.config.executable, "build", "--sandbox", self.sandbox_dir, self.config.image],
            check=True,
        )

    def get_template_vars(self) -> dict[str, Any]:
        return asdict(self.config)

    def execute(self, command: str, cwd: str = "") -> dict[str, Any]:
        """Execute a command in a Singularity container and return the result as a dict."""
        cmd = [self.config.executable, "exec"]

        # Do not inherit directories and env vars from host
        cmd.extend(["--contain", "--cleanenv"])

        work_dir = cwd or self.config.cwd
        if work_dir and work_dir != "/":
            cmd.extend(["--pwd", work_dir])

        for key in self.config.forward_env:
            if (value := os.getenv(key)) is not None:
                cmd.extend(["--env", f"{key}={value}"])
        for key, value in self.config.env.items():
            cmd.extend(["--env", f"{key}={value}"])

        cmd.extend(["--writable", str(self.sandbox_dir), "bash", "-c", command])
        result = subprocess.run(
            cmd,
            text=True,
            timeout=self.config.timeout,
            encoding="utf-8",
            errors="replace",
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
        )
        return {"output": result.stdout, "returncode": result.returncode}

    def cleanup(self):
        if self.sandbox_dir.exists():
            shutil.rmtree(self.sandbox_dir)

    def __del__(self):
        """Cleanup sandbox when object is destroyed."""
        self.cleanup()



================================================
FILE: src/minisweagent/environments/extra/__init__.py
================================================
[Empty file]


================================================
FILE: src/minisweagent/environments/extra/swerex_docker.py
================================================
import asyncio
from dataclasses import asdict, dataclass, field
from typing import Any

from swerex.deployment.docker import DockerDeployment
from swerex.runtime.abstract import Command as RexCommand


@dataclass
class SwerexDockerEnvironmentConfig:
    image: str
    cwd: str = "/"
    """Working directory in which to execute commands."""
    timeout: int = 30
    """Timeout for executing commands in the container."""
    deployment_extra_kwargs: dict[str, Any] = field(default_factory=dict)
    """Extra kwargs to pass to DockerDeployment."""


class SwerexDockerEnvironment:
    def __init__(self, **kwargs):
        """This class executes bash commands in a Docker container using SWE-ReX for sandboxing."""
        self.config = SwerexDockerEnvironmentConfig(**kwargs)
        self.deployment = DockerDeployment(image=self.config.image, **self.config.deployment_extra_kwargs)
        asyncio.run(self.deployment.start())

    def execute(self, command: str, cwd: str = "") -> dict[str, Any]:
        """Execute a command in the environment and return the raw output."""
        output = asyncio.run(
            self.deployment.runtime.execute(
                RexCommand(
                    command=command,
                    shell=True,
                    check=False,
                    cwd=cwd or self.config.cwd,
                    timeout=self.config.timeout,
                    merge_output_streams=True,
                )
            )
        )
        return {
            "output": output.stdout,
            "returncode": output.exit_code,
        }

    def get_template_vars(self) -> dict[str, Any]:
        return asdict(self.config)



================================================
FILE: src/minisweagent/models/README.md
================================================
# LM interfaces

* `litellm_model.py` - Wrapper for [Litellm](https://github.com/BerriAI/litellm) models
   (should support most of all models).
* `anthropic.py` - Anthropic models have some special needs, so we have a separate interface for them.
* `test_models.py` - Deterministic models that can be used for internal testing


================================================
FILE: src/minisweagent/models/__init__.py
================================================
"""This file provides convenience functions for selecting models.
You can ignore this file completely if you explicitly set your model in your run script.
"""

import copy
import importlib
import os
import threading

from minisweagent import Model


class GlobalModelStats:
    """Global model statistics tracker with optional limits."""

    def __init__(self):
        self._cost = 0.0
        self._n_calls = 0
        self._lock = threading.Lock()
        self.cost_limit = float(os.getenv("MSWEA_GLOBAL_COST_LIMIT", "0"))
        self.call_limit = int(os.getenv("MSWEA_GLOBAL_CALL_LIMIT", "0"))
        if (self.cost_limit > 0 or self.call_limit > 0) and not os.getenv("MSWEA_SILENT_STARTUP"):
            print(f"Global cost/call limit: ${self.cost_limit:.4f} / {self.call_limit}")

    def add(self, cost: float) -> None:
        """Add a model call with its cost, checking limits."""
        with self._lock:
            self._cost += cost
            self._n_calls += 1
        if 0 < self.cost_limit < self._cost or 0 < self.call_limit < self._n_calls + 1:
            raise RuntimeError(f"Global cost/call limit exceeded: ${self._cost:.4f} / {self._n_calls + 1}")

    @property
    def cost(self) -> float:
        return self._cost

    @property
    def n_calls(self) -> int:
        return self._n_calls


GLOBAL_MODEL_STATS = GlobalModelStats()


def get_model(input_model_name: str | None = None, config: dict | None = None) -> Model:
    """Get an initialized model object from any kind of user input or settings."""
    resolved_model_name = get_model_name(input_model_name, config)
    if config is None:
        config = {}
    config = copy.deepcopy(config)
    config["model_name"] = resolved_model_name

    # API key resolution (from env -> config -> None)
    if "model_kwargs" not in config:
        config["model_kwargs"] = {}
    if from_env := os.getenv("MSWEA_MODEL_API_KEY"):
        config["model_kwargs"]["api_key"] = from_env

    model_class = config.pop("model_class", "")
    return get_model_class(resolved_model_name, model_class)(**config)


def get_model_name(input_model_name: str | None = None, config: dict | None = None) -> str:
    """Get a model name from any kind of user input or settings."""
    if config is None:
        config = {}
    if input_model_name:
        return input_model_name
    if from_config := config.get("model_name"):
        return from_config
    if from_env := os.getenv("MSWEA_MODEL_NAME"):
        return from_env
    raise ValueError("No default model set. Please run `mini-extra config setup` to set one.")


_MODEL_CLASS_MAPPING = {
    "anthropic": "minisweagent.models.anthropic.AnthropicModel",
    "litellm": "minisweagent.models.litellm_model.LitellmModel",
}


def get_model_class(model_name: str, model_class: str = "") -> type:
    """Select the best model class.

    If a model_class is provided (as shortcut name, or as full import path,
    e.g., "anthropic" or "minisweagent.models.anthropic.AnthropicModel"),
    it takes precedence over the `model_name`.
    Otherwise, the model_name is used to select the best model class.
    """
    if model_class:
        full_path = _MODEL_CLASS_MAPPING.get(model_class, model_class)
        try:
            module_name, class_name = full_path.rsplit(".", 1)
            module = importlib.import_module(module_name)
            return getattr(module, class_name)
        except (ValueError, ImportError, AttributeError):
            msg = f"Unknown model class: {model_class} (resolved to {full_path}, available: {_MODEL_CLASS_MAPPING})"
            raise ValueError(msg)

    if any(s in model_name.lower() for s in ["anthropic", "sonnet", "opus", "claude"]):
        from minisweagent.models.anthropic import AnthropicModel

        return AnthropicModel

    # Default to LitellmModel
    from minisweagent.models.litellm_model import LitellmModel

    return LitellmModel



================================================
FILE: src/minisweagent/models/anthropic.py
================================================
import os

from minisweagent.models.litellm_model import LitellmModel
from minisweagent.models.utils.cache_control import set_cache_control
from minisweagent.models.utils.key_per_thread import get_key_per_thread


class AnthropicModel(LitellmModel):
    """For the use of anthropic models, we need to add explicit cache control marks
    to the messages or we lose out on the benefits of the cache.
    Because break points are limited per key, we also need to rotate between different keys
    if running with multiple agents in parallel threads.
    """

    def query(self, messages: list[dict], **kwargs) -> dict:
        api_key = None
        if rotating_keys := os.getenv("ANTHROPIC_API_KEYS"):
            api_key = get_key_per_thread(rotating_keys.split("::"))
        return super().query(set_cache_control(messages), api_key=api_key, **kwargs)



================================================
FILE: src/minisweagent/models/litellm_model.py
================================================
import json
import logging
import os
from dataclasses import asdict, dataclass, field
from pathlib import Path
from typing import Any

import litellm
from tenacity import (
    before_sleep_log,
    retry,
    retry_if_not_exception_type,
    stop_after_attempt,
    wait_exponential,
)

from minisweagent.models import GLOBAL_MODEL_STATS

logger = logging.getLogger("litellm_model")


@dataclass
class LitellmModelConfig:
    model_name: str
    model_kwargs: dict[str, Any] = field(default_factory=dict)
    litellm_model_registry: Path | str | None = os.getenv("LITELLM_MODEL_REGISTRY_PATH")


class LitellmModel:
    def __init__(self, **kwargs):
        self.config = LitellmModelConfig(**kwargs)
        self.cost = 0.0
        self.n_calls = 0
        if self.config.litellm_model_registry and Path(self.config.litellm_model_registry).is_file():
            litellm.utils.register_model(json.loads(Path(self.config.litellm_model_registry).read_text()))

    @retry(
        stop=stop_after_attempt(10),
        wait=wait_exponential(multiplier=1, min=4, max=60),
        before_sleep=before_sleep_log(logger, logging.WARNING),
        retry=retry_if_not_exception_type(
            (
                litellm.exceptions.UnsupportedParamsError,
                litellm.exceptions.NotFoundError,
                litellm.exceptions.PermissionDeniedError,
                litellm.exceptions.ContextWindowExceededError,
                litellm.exceptions.APIError,
                litellm.exceptions.AuthenticationError,
                KeyboardInterrupt,
            )
        ),
    )
    def _query(self, messages: list[dict[str, str]], **kwargs):
        try:
            return litellm.completion(
                model=self.config.model_name, messages=messages, **(self.config.model_kwargs | kwargs)
            )
        except litellm.exceptions.AuthenticationError as e:
            e.message += " You can permanently set your API key with `mini-extra config set KEY VALUE`."
            raise e

    def query(self, messages: list[dict[str, str]], **kwargs) -> dict:
        response = self._query(messages, **kwargs)
        cost = litellm.cost_calculator.completion_cost(response)
        self.n_calls += 1
        self.cost += cost
        GLOBAL_MODEL_STATS.add(cost)
        return {
            "content": response.choices[0].message.content or "",  # type: ignore
        }

    def get_template_vars(self) -> dict[str, Any]:
        return asdict(self.config) | {"n_model_calls": self.n_calls, "model_cost": self.cost}



================================================
FILE: src/minisweagent/models/test_models.py
================================================
import logging
import time
from dataclasses import asdict, dataclass
from typing import Any

from minisweagent.models import GLOBAL_MODEL_STATS


@dataclass
class DeterministicModelConfig:
    outputs: list[str]
    model_name: str = "deterministic"
    cost_per_call: float = 1.0


class DeterministicModel:
    def __init__(self, **kwargs):
        """
        Initialize with a list of outputs to return in sequence.
        """
        self.config = DeterministicModelConfig(**kwargs)
        self.current_index = -1
        self.cost = 0.0
        self.n_calls = 0

    def query(self, messages: list[dict[str, str]], **kwargs) -> dict:
        self.current_index += 1
        output = self.config.outputs[self.current_index]
        if "/sleep" in output:
            print("SLEEPING")
            time.sleep(float(output.split("/sleep")[1]))
            return self.query(messages, **kwargs)
        if "/warning" in output:
            logging.warning(output.split("/warning")[1])
            return self.query(messages, **kwargs)
        self.n_calls += 1
        self.cost += self.config.cost_per_call
        GLOBAL_MODEL_STATS.add(self.config.cost_per_call)
        return {"content": output}

    def get_template_vars(self) -> dict[str, Any]:
        return asdict(self.config) | {"n_model_calls": self.n_calls, "model_cost": self.cost}



================================================
FILE: src/minisweagent/models/extra/__init__.py
================================================
[Empty file]


================================================
FILE: src/minisweagent/models/extra/roulette.py
================================================
import random
from collections.abc import Callable
from dataclasses import asdict, dataclass

from minisweagent import Model
from minisweagent.models import get_model


@dataclass
class RouletteModelConfig:
    model_kwargs: list[dict]
    """The models to choose from"""
    model_name: str = "roulette"


class RouletteModel:
    def __init__(self, *, config_class: Callable = RouletteModelConfig, **kwargs):
        """This "meta"-model randomly selects one of the models at every call"""
        self.config = config_class(**kwargs)
        self.models = [get_model(config=config) for config in self.config.model_kwargs]

    @property
    def cost(self) -> float:
        return sum(model.cost for model in self.models)

    @property
    def n_calls(self) -> int:
        return sum(model.n_calls for model in self.models)

    def get_template_vars(self) -> dict:
        return asdict(self.config) | {"n_model_calls": self.n_calls, "model_cost": self.cost}

    def select_model(self) -> Model:
        return random.choice(self.models)

    def query(self, *args, **kwargs) -> dict:
        model = self.select_model()
        response = model.query(*args, **kwargs)
        response["model_name"] = model.config.model_name
        return response


@dataclass
class InterleavingModelConfig:
    model_kwargs: list[dict]
    sequence: list[int] | None = None
    """If set to 0, 0, 1, we will return the first model 2 times, then the second model 1 time,
    then the first model again, etc."""
    model_name: str = "interleaving"


class InterleavingModel(RouletteModel):
    def __init__(self, *, config_class: Callable = InterleavingModelConfig, **kwargs):
        """This "meta"-model alternates between the models in the sequence for every call"""
        super().__init__(config_class=config_class, **kwargs)

    def select_model(self) -> Model:
        if self.config.sequence is None:
            i_model = self.n_calls % len(self.models)
        else:
            i_model = self.config.sequence[self.n_calls % len(self.config.sequence)]
        return self.models[i_model]



================================================
FILE: src/minisweagent/models/utils/__init__.py
================================================
[Empty file]


================================================
FILE: src/minisweagent/models/utils/cache_control.py
================================================
def _get_content_text(entry: dict) -> str:
    if isinstance(entry["content"], str):
        return entry["content"]
    assert len(entry["content"]) == 1, "Expected single message in content"
    return entry["content"][0]["text"]


def _clear_cache_control(entry: dict) -> None:
    if isinstance(entry["content"], list):
        assert len(entry["content"]) == 1, "Expected single message in content"
        entry["content"][0].pop("cache_control", None)
    entry.pop("cache_control", None)


def _set_cache_control(entry: dict) -> None:
    if not isinstance(entry["content"], list):
        entry["content"] = [  # type: ignore
            {
                "type": "text",
                "text": _get_content_text(entry),
                "cache_control": {"type": "ephemeral"},
            }
        ]
    else:
        entry["content"][0]["cache_control"] = {"type": "ephemeral"}
    if entry["role"] == "tool":
        # Workaround for weird bug
        entry["content"][0].pop("cache_control", None)
        entry["cache_control"] = {"type": "ephemeral"}


def set_cache_control(messages: list[dict], last_n_messages_offset: int = 0) -> list[dict]:
    """This messages processor adds manual cache control marks to the messages."""
    new_messages = []
    n_tagged = 0
    for i_entry, entry in enumerate(reversed(messages)):
        _clear_cache_control(entry)
        if n_tagged < 2 and entry["role"] in ["user"] and i_entry >= last_n_messages_offset:
            _set_cache_control(entry)
            n_tagged += 1
        new_messages.append(entry)
    return list(reversed(new_messages))



================================================
FILE: src/minisweagent/models/utils/key_per_thread.py
================================================
"""Utility for anthropic where we need different keys for different parallel
agents to not mess up prompt caching.
"""

import threading
from typing import Any

_THREADS_THAT_USED_API_KEYS: list[Any] = []


def get_key_per_thread(api_keys: list[Any]) -> Any:
    """Choose key based on thread name. Returns None if no keys are available."""
    thread_name = threading.current_thread().name
    if thread_name not in _THREADS_THAT_USED_API_KEYS:
        _THREADS_THAT_USED_API_KEYS.append(thread_name)
    thread_idx = _THREADS_THAT_USED_API_KEYS.index(thread_name)
    key_idx = thread_idx % len(api_keys)
    return api_keys[key_idx] or None



================================================
FILE: src/minisweagent/run/README.md
================================================
# Entry points / run scripts

## Work locally (i.e., without a sandbox)

* `hello_world.py` - Extremely simple example of how to use the `default.py` agent.
* `mini.py` - Uses the `interactive.py` or `interactive_textual.py` agent/simple UI

## Work with a sandbox

* `github_issue.py` - Solve a GitHub issue with a docker environment.

## Extras

* `extra/swebench.py` - Benchmark the performance of the `default.py` agent.


================================================
FILE: src/minisweagent/run/__init__.py
================================================
"""Run scripts for mini-SWE-agent."""



================================================
FILE: src/minisweagent/run/github_issue.py
================================================
#!/usr/bin/env python3
import os
from pathlib import Path

import requests
import typer
import yaml
from rich.console import Console

from minisweagent.agents.interactive import InteractiveAgent
from minisweagent.config import builtin_config_dir, get_config_path
from minisweagent.environments.docker import DockerEnvironment
from minisweagent.models import get_model
from minisweagent.run.extra.config import configure_if_first_time
from minisweagent.run.utils.save import save_traj

DEFAULT_CONFIG = Path(os.getenv("MSWEA_GITHUB_CONFIG_PATH", builtin_config_dir / "github_issue.yaml"))
console = Console(highlight=False)
app = typer.Typer(rich_markup_mode="rich", add_completion=False)


def fetch_github_issue(issue_url: str) -> str:
    """Fetch GitHub issue text from the URL."""
    # Convert GitHub issue URL to API URL
    api_url = issue_url.replace("github.com", "api.github.com/repos").replace("/issues/", "/issues/")

    headers = {}
    if github_token := os.getenv("GITHUB_TOKEN"):
        headers["Authorization"] = f"token {github_token}"

    response = requests.get(api_url, headers=headers)
    issue_data = response.json()

    title = issue_data["title"]
    body = issue_data["body"] or ""

    return f"GitHub Issue: {title}\n\n{body}"


@app.command()
def main(
    issue_url: str = typer.Option(prompt="Enter GitHub issue URL", help="GitHub issue URL"),
    config: Path = typer.Option(DEFAULT_CONFIG, "-c", "--config", help="Path to config file"),
    model: str | None = typer.Option(None, "-m", "--model", help="Model to use"),
    yolo: bool = typer.Option(False, "-y", "--yolo", help="Run without confirmation"),
) -> InteractiveAgent:
    """Run mini-SWE-agent on a GitHub issue"""
    configure_if_first_time()

    _config = yaml.safe_load(get_config_path(config).read_text())
    _agent_config = _config.get("agent", {})
    if yolo:
        _agent_config["mode"] = "yolo"

    task = fetch_github_issue(issue_url)

    agent = InteractiveAgent(
        get_model(model, _config.get("model", {})),
        DockerEnvironment(**_config.get("environment", {})),
        **_agent_config,
    )

    repo_url = issue_url.split("/issues/")[0]
    if github_token := os.getenv("GITHUB_TOKEN"):
        repo_url = repo_url.replace("https://github.com/", f"https://{github_token}@github.com/") + ".git"

    agent.env.execute(f"git clone {repo_url} /testbed", cwd="/")

    exit_status, result = None, None
    try:
        exit_status, result = agent.run(task)
    except KeyboardInterrupt:
        console.print("\n[bold red]KeyboardInterrupt -- goodbye[/bold red]")
    finally:
        save_traj(agent, Path("traj.json"), exit_status=exit_status, result=result)
    return agent


if __name__ == "__main__":
    app()



================================================
FILE: src/minisweagent/run/hello_world.py
================================================
import os
from pathlib import Path

import typer
import yaml

from minisweagent import package_dir
from minisweagent.agents.default import DefaultAgent
from minisweagent.environments.local import LocalEnvironment
from minisweagent.models.litellm_model import LitellmModel

app = typer.Typer()


@app.command()
def main(
    task: str = typer.Option(..., "-t", "--task", help="Task/problem statement", show_default=False, prompt=True),
    model_name: str = typer.Option(
        os.getenv("MSWEA_MODEL_NAME"),
        "-m",
        "--model",
        help="Model name (defaults to MSWEA_MODEL_NAME env var)",
        prompt="What model do you want to use?",
    ),
) -> DefaultAgent:
    agent = DefaultAgent(
        LitellmModel(model_name=model_name),
        LocalEnvironment(),
        **yaml.safe_load(Path(package_dir / "config" / "default.yaml").read_text())["agent"],
    )
    agent.run(task)
    return agent


if __name__ == "__main__":
    app()



================================================
FILE: src/minisweagent/run/inspector.py
================================================
#!/usr/bin/env python3
"""
Simple trajectory inspector for browsing agent conversation trajectories.

[not dim]
More information about the usage: [bold green]https://mini-swe-agent.com/latest/usage/inspector/[/bold green]
[/not dim]
"""

import json
import os
from pathlib import Path

import typer
from rich.text import Text
from textual.app import App, ComposeResult
from textual.binding import Binding
from textual.containers import Container, Vertical, VerticalScroll
from textual.widgets import Footer, Header, Static

from minisweagent.agents.interactive_textual import _messages_to_steps

app = typer.Typer(rich_markup_mode="rich", add_completion=False)


class TrajectoryInspector(App):
    BINDINGS = [
        Binding("right,l", "next_step", "Step++"),
        Binding("left,h", "previous_step", "Step--"),
        Binding("0", "first_step", "Step=0"),
        Binding("$", "last_step", "Step=-1"),
        Binding("j,down", "scroll_down", "Scroll down"),
        Binding("k,up", "scroll_up", "Scroll up"),
        Binding("L", "next_trajectory", "Next trajectory"),
        Binding("H", "previous_trajectory", "Previous trajectory"),
        Binding("q", "quit", "Quit"),
    ]

    def __init__(self, trajectory_files: list[Path]):
        css_path = os.environ.get(
            "MSWEA_INSPECTOR_STYLE_PATH", str(Path(__file__).parent.parent / "config" / "mini.tcss")
        )
        self.__class__.CSS = Path(css_path).read_text()

        super().__init__()
        self.trajectory_files = trajectory_files
        self._i_trajectory = 0
        self._i_step = 0
        self.messages = []
        self.steps = []

        if trajectory_files:
            self._load_current_trajectory()

    # --- Basics ---

    @property
    def i_step(self) -> int:
        """Current step index."""
        return self._i_step

    @i_step.setter
    def i_step(self, value: int) -> None:
        """Set current step index, automatically clamping to valid bounds."""
        if value != self._i_step and self.n_steps > 0:
            self._i_step = max(0, min(value, self.n_steps - 1))
            self.query_one(VerticalScroll).scroll_to(y=0, animate=False)
            self.update_content()

    @property
    def n_steps(self) -> int:
        """Number of steps in current trajectory."""
        return len(self.steps)

    @property
    def i_trajectory(self) -> int:
        """Current trajectory index."""
        return self._i_trajectory

    @i_trajectory.setter
    def i_trajectory(self, value: int) -> None:
        """Set current trajectory index, automatically clamping to valid bounds."""
        if value != self._i_trajectory and self.n_trajectories > 0:
            self._i_trajectory = max(0, min(value, self.n_trajectories - 1))
            self._load_current_trajectory()
            self.query_one(VerticalScroll).scroll_to(y=0, animate=False)
            self.update_content()

    @property
    def n_trajectories(self) -> int:
        """Number of trajectory files."""
        return len(self.trajectory_files)

    def _load_current_trajectory(self) -> None:
        """Load the currently selected trajectory file."""
        if not self.trajectory_files:
            self.messages = []
            self.steps = []
            return

        trajectory_file = self.trajectory_files[self.i_trajectory]
        try:
            data = json.loads(trajectory_file.read_text())

            if isinstance(data, list):
                self.messages = data
            elif isinstance(data, dict) and "messages" in data:
                self.messages = data["messages"]
            else:
                raise ValueError("Unrecognized trajectory format")

            self.steps = _messages_to_steps(self.messages)
            self._i_step = 0
        except (json.JSONDecodeError, FileNotFoundError, ValueError) as e:
            self.messages = []
            self.steps = []
            self.notify(f"Error loading {trajectory_file.name}: {e}", severity="error")

    @property
    def current_trajectory_name(self) -> str:
        """Get the name of the current trajectory file."""
        if not self.trajectory_files:
            return "No trajectories"
        return self.trajectory_files[self.i_trajectory].name

    def compose(self) -> ComposeResult:
        yield Header()
        with Container(id="main"):
            with VerticalScroll():
                yield Vertical(id="content")
        yield Footer()

    def on_mount(self) -> None:
        self.update_content()

    def update_content(self) -> None:
        """Update the displayed content."""
        container = self.query_one("#content", Vertical)
        container.remove_children()

        if not self.steps:
            container.mount(Static("No trajectory loaded or empty trajectory"))
            self.title = "Trajectory Inspector - No Data"
            return

        for message in self.steps[self.i_step]:
            if isinstance(message["content"], list):
                content_str = "\n".join([item["text"] for item in message["content"]])
            else:
                content_str = str(message["content"])
            message_container = Vertical(classes="message-container")
            container.mount(message_container)
            role = message["role"].replace("assistant", "mini-swe-agent")
            message_container.mount(Static(role.upper(), classes="message-header"))
            message_container.mount(Static(Text(content_str, no_wrap=False), classes="message-content"))

        self.title = (
            f"Trajectory {self.i_trajectory + 1}/{self.n_trajectories} - "
            f"{self.current_trajectory_name} - "
            f"Step {self.i_step + 1}/{self.n_steps}"
        )

    # --- Navigation actions ---

    def action_next_step(self) -> None:
        self.i_step += 1

    def action_previous_step(self) -> None:
        self.i_step -= 1

    def action_first_step(self) -> None:
        self.i_step = 0

    def action_last_step(self) -> None:
        self.i_step = self.n_steps - 1

    def action_next_trajectory(self) -> None:
        self.i_trajectory += 1

    def action_previous_trajectory(self) -> None:
        self.i_trajectory -= 1

    def action_scroll_down(self) -> None:
        vs = self.query_one(VerticalScroll)
        vs.scroll_to(y=vs.scroll_target_y + 15)

    def action_scroll_up(self) -> None:
        vs = self.query_one(VerticalScroll)
        vs.scroll_to(y=vs.scroll_target_y - 15)


@app.command(help=__doc__)
def main(
    path: str = typer.Argument(".", help="Directory to search for trajectory files or specific trajectory file"),
) -> None:
    path_obj = Path(path)

    if path_obj.is_file():
        trajectory_files = [path_obj]
    elif path_obj.is_dir():
        trajectory_files = sorted(path_obj.rglob("*.traj.json"))
        if not trajectory_files:
            raise typer.BadParameter(f"No trajectory files found in '{path}'")
    else:
        raise typer.BadParameter(f"Error: Path '{path}' does not exist")

    inspector = TrajectoryInspector(trajectory_files)
    inspector.run()


if __name__ == "__main__":
    app()



================================================
FILE: src/minisweagent/run/mini.py
================================================
#!/usr/bin/env python3

"""Run mini-SWE-agent in your local environment. This is the default executable `mini`."""
# Read this first: https://mini-swe-agent.com/latest/usage/mini/  (usage)

import os
import traceback
from pathlib import Path
from typing import Any

import typer
import yaml
from prompt_toolkit.formatted_text import HTML
from prompt_toolkit.history import FileHistory
from prompt_toolkit.shortcuts import PromptSession
from rich.console import Console

from minisweagent import global_config_dir
from minisweagent.agents.interactive import InteractiveAgent
from minisweagent.agents.interactive_textual import TextualAgent
from minisweagent.config import builtin_config_dir, get_config_path
from minisweagent.environments.local import LocalEnvironment
from minisweagent.models import get_model
from minisweagent.run.extra.config import configure_if_first_time
from minisweagent.run.utils.save import save_traj
from minisweagent.utils.log import logger

DEFAULT_CONFIG = Path(os.getenv("MSWEA_MINI_CONFIG_PATH", builtin_config_dir / "mini.yaml"))
DEFAULT_OUTPUT = global_config_dir / "last_mini_run.traj.json"
console = Console(highlight=False)
app = typer.Typer(rich_markup_mode="rich")
prompt_session = PromptSession(history=FileHistory(global_config_dir / "mini_task_history.txt"))
_HELP_TEXT = """Run mini-SWE-agent in your local environment.

[not dim]
There are two different user interfaces:

[bold green]mini[/bold green] Simple REPL-style interface
[bold green]mini -v[/bold green] Pager-style interface (Textual)

More information about the usage: [bold green]https://mini-swe-agent.com/latest/usage/mini/[/bold green]
[/not dim]
"""


# fmt: off
@app.command(help=_HELP_TEXT)
def main(
    visual: bool = typer.Option(False, "-v", "--visual", help="Toggle (pager-style) UI (Textual) depending on the MSWEA_VISUAL_MODE_DEFAULT environment setting",),
    model_name: str | None = typer.Option( None, "-m", "--model", help="Model to use",),
    task: str | None = typer.Option(None, "-t", "--task", help="Task/problem statement", show_default=False),
    yolo: bool = typer.Option(False, "-y", "--yolo", help="Run without confirmation"),
    cost_limit: float | None = typer.Option(None, "-l", "--cost-limit", help="Cost limit. Set to 0 to disable."),
    config_spec: Path = typer.Option(DEFAULT_CONFIG, "-c", "--config", help="Path to config file"),
    output: Path | None = typer.Option(DEFAULT_OUTPUT, "-o", "--output", help="Output trajectory file"),
    exit_immediately: bool = typer.Option( False, "--exit-immediately", help="Exit immediately when the agent wants to finish instead of prompting."),
) -> Any:
    # fmt: on
    configure_if_first_time()
    config = yaml.safe_load(get_config_path(config_spec).read_text())

    if not task:
        console.print("[bold yellow]What do you want to do?")
        task = prompt_session.prompt(
            "",
            multiline=True,
            bottom_toolbar=HTML(
                "Submit task: <b fg='yellow' bg='black'>Esc+Enter</b> | "
                "Navigate history: <b fg='yellow' bg='black'>Arrow Up/Down</b> | "
                "Search history: <b fg='yellow' bg='black'>Ctrl+R</b>"
            ),
        )
        console.print("[bold green]Got that, thanks![/bold green]")

    config["agent"]["mode"] = "confirm" if not yolo else "yolo"
    if cost_limit:
        config["agent"]["cost_limit"] = cost_limit
    if exit_immediately:
        config["agent"]["confirm_exit"] = False
    model = get_model(model_name, config.get("model", {}))
    env = LocalEnvironment(**config.get("env", {}))

    # Both visual flag and the MSWEA_VISUAL_MODE_DEFAULT flip the mode, so it's essentially a XOR
    agent_class = InteractiveAgent
    if visual == (os.getenv("MSWEA_VISUAL_MODE_DEFAULT", "false") == "false"):
        agent_class = TextualAgent

    agent = agent_class(model, env, **config.get("agent", {}))
    exit_status, result, extra_info = None, None, None
    try:
        exit_status, result = agent.run(task)  # type: ignore[arg-type]
    except Exception as e:
        logger.error(f"Error running agent: {e}", exc_info=True)
        exit_status, result = type(e).__name__, str(e)
        extra_info = {"traceback": traceback.format_exc()}
    finally:
        if output:
            save_traj(agent, output, exit_status=exit_status, result=result, extra_info=extra_info)  # type: ignore[arg-type]
    return agent


if __name__ == "__main__":
    app()



================================================
FILE: src/minisweagent/run/mini_extra.py
================================================
#!/usr/bin/env python3

import sys
from importlib import import_module

from rich.console import Console

subcommands = [
    ("minisweagent.run.extra.config", ["config"], "Manage the global config file"),
    ("minisweagent.run.inspector", ["inspect", "i", "inspector"], "Run inspector (browse trajectories)"),
    ("minisweagent.run.github_issue", ["github-issue", "gh"], "Run on a GitHub issue"),
    ("minisweagent.run.extra.swebench", ["swebench"], "Evaluate on SWE-bench (batch mode)"),
    ("minisweagent.run.extra.swebench_single", ["swebench-single"], "Evaluate on SWE-bench (single instance)"),
]


def get_docstring() -> str:
    lines = [
        "This is the [yellow]central entry point for all extra commands[/yellow] from mini-swe-agent.",
        "",
        "Available sub-commands:",
        "",
    ]
    for _, aliases, description in subcommands:
        alias_text = " or ".join(f"[bold green]{alias}[/bold green]" for alias in aliases)
        lines.append(f"  {alias_text}: {description}")
    return "\n".join(lines)


def main():
    args = sys.argv[1:]

    if len(args) == 0 or len(args) == 1 and args[0] in ["-h", "--help"]:
        return Console().print(get_docstring())

    for module_path, aliases, _ in subcommands:
        if args[0] in aliases:
            return import_module(module_path).app(args[1:], prog_name=f"mini-extra {aliases[0]}")

    return Console().print(get_docstring())


if __name__ == "__main__":
    main()



================================================
FILE: src/minisweagent/run/extra/__init__.py
================================================
[Empty file]


================================================
FILE: src/minisweagent/run/extra/config.py
================================================
"""Utility to manage the global config file.

You can also directly edit the `.env` file in the config directory.

It is located at [bold green]{global_config_file}[/bold green].
"""

import os
import subprocess

from dotenv import set_key, unset_key
from prompt_toolkit import prompt
from rich.console import Console
from rich.rule import Rule
from typer import Argument, Typer

from minisweagent import global_config_file

app = Typer(
    help=__doc__.format(global_config_file=global_config_file),  # type: ignore
    no_args_is_help=True,
    rich_markup_mode="rich",
    add_completion=False,
)
console = Console(highlight=False)


_SETUP_HELP = """To get started, we need to set up your global config file.

You can edit it manually or use the [bold green]mini-extra config set[/bold green] or [bold green]mini-extra config edit[/bold green] commands.

This setup will ask you for your model and an API key.

Here's a few popular models and the required API keys:

[bold green]claude-sonnet-4-20250514[/bold green] ([bold green]ANTHROPIC_API_KEY[/bold green])
[bold green]openai/gpt-5[/bold green] or [bold green]openai/gpt-5-mini[/bold green] ([bold green]OPENAI_API_KEY[/bold green])

[bold yellow]You can leave any setting blank to skip it.[/bold yellow]

More information at https://mini-swe-agent.com/latest/quickstart/
To find the best model, check the leaderboard at https://swebench.com/
"""


def configure_if_first_time():
    if not os.getenv("MSWEA_CONFIGURED"):
        console.print(Rule())
        setup()
        console.print(Rule())


@app.command()
def setup():
    """Setup the global config file."""
    console.print(_SETUP_HELP.format(global_config_file=global_config_file))
    default_model = prompt(
        "Enter your default model (e.g., claude-sonnet-4-20250514): ", default=os.getenv("MSWEA_MODEL_NAME", "")
    ).strip()
    if default_model:
        set_key(global_config_file, "MSWEA_MODEL_NAME", default_model)
    console.print(
        "[bold yellow]If you already have your API keys set as environment variables, you can ignore the next question.[/bold yellow]"
    )
    key_name = prompt("Enter your API key name (e.g., ANTHROPIC_API_KEY): ").strip()
    key_value = None
    if key_name:
        key_value = prompt("Enter your API key value (e.g., sk-1234567890): ", default=os.getenv(key_name, "")).strip()
        if key_value:
            set_key(global_config_file, key_name, key_value)
    if not key_value:
        console.print(
            "[bold red]API key setup not completed.[/bold red] Totally fine if you have your keys as environment variables."
        )
    set_key(global_config_file, "MSWEA_CONFIGURED", "true")
    console.print(
        "\n[bold yellow]Config finished.[/bold yellow] If you want to revisit it, run [bold green]mini-extra config setup[/bold green]."
    )


@app.command()
def set(
    key: str | None = Argument(None, help="The key to set"),
    value: str | None = Argument(None, help="The value to set"),
):
    """Set a key in the global config file."""
    if key is None:
        key = prompt("Enter the key to set: ")
    if value is None:
        value = prompt(f"Enter the value for {key}: ")
    set_key(global_config_file, key, value)


@app.command()
def unset(key: str | None = Argument(None, help="The key to unset")):
    """Unset a key in the global config file."""
    if key is None:
        key = prompt("Enter the key to unset: ")
    unset_key(global_config_file, key)


@app.command()
def edit():
    """Edit the global config file."""
    editor = os.getenv("EDITOR", "nano")
    subprocess.run([editor, global_config_file])


if __name__ == "__main__":
    app()



================================================
FILE: src/minisweagent/run/extra/swebench.py
================================================
#!/usr/bin/env python3

"""Run mini-SWE-agent on SWE-bench instances in batch mode."""
# Read this first: https://mini-swe-agent.com/latest/usage/swebench/  (usage docs)

import concurrent.futures
import json
import random
import re
import threading
import time
import traceback
from pathlib import Path

import typer
import yaml
from datasets import load_dataset
from rich.live import Live

from minisweagent import Environment
from minisweagent.agents.default import DefaultAgent
from minisweagent.config import builtin_config_dir, get_config_path
from minisweagent.environments import get_environment
from minisweagent.models import get_model
from minisweagent.run.extra.utils.batch_progress import RunBatchProgressManager
from minisweagent.run.utils.save import save_traj
from minisweagent.utils.log import add_file_handler, logger

_HELP_TEXT = """Run mini-SWE-agent on SWEBench instances.

[not dim]
More information about the usage: [bold green]https://mini-swe-agent.com/latest/usage/swebench/[/bold green]
[/not dim]
"""

app = typer.Typer(rich_markup_mode="rich", add_completion=False)

DATASET_MAPPING = {
    "full": "princeton-nlp/SWE-Bench",
    "verified": "princeton-nlp/SWE-Bench_Verified",
    "lite": "princeton-nlp/SWE-Bench_Lite",
    "multimodal": "princeton-nlp/SWE-Bench_Multimodal",
    "multilingual": "swe-bench/SWE-Bench_Multilingual",
    "smith": "SWE-bench/SWE-smith",
    "_test": "klieret/swe-bench-dummy-test-dataset",
}


_OUTPUT_FILE_LOCK = threading.Lock()


class ProgressTrackingAgent(DefaultAgent):
    """Simple wrapper around DefaultAgent that provides progress updates."""

    def __init__(self, *args, progress_manager: RunBatchProgressManager, instance_id: str = "", **kwargs):
        super().__init__(*args, **kwargs)
        self.progress_manager: RunBatchProgressManager = progress_manager
        self.instance_id = instance_id

    def step(self) -> dict:
        """Override step to provide progress updates."""
        self.progress_manager.update_instance_status(
            self.instance_id, f"Step {self.model.n_calls + 1:3d} (${self.model.cost:.2f})"
        )
        return super().step()


def get_swebench_docker_image_name(instance: dict) -> str:
    """Get the image name for a SWEBench instance."""
    image_name = instance.get("image_name", None)
    if image_name is None:
        # Docker doesn't allow double underscore, so we replace them with a magic token
        iid = instance["instance_id"]
        id_docker_compatible = iid.replace("__", "_1776_")
        image_name = f"swebench/sweb.eval.x86_64.{id_docker_compatible}:latest".lower()
    return image_name


def get_sb_environment(config: dict, instance: dict) -> Environment:
    image_name = get_swebench_docker_image_name(instance)
    env_config = config.get("environment", {})
    if env_config.get("environment_class") == "singularity":
        image_name = "docker://" + image_name
    env_config["image"] = image_name
    return get_environment(env_config, default_type="docker")


def update_preds_file(output_path: Path, instance_id: str, model_name: str, result: str):
    """Update the output JSON file with results from a single instance."""
    with _OUTPUT_FILE_LOCK:
        output_data = {}
        if output_path.exists():
            output_data = json.loads(output_path.read_text())
        output_data[instance_id] = {
            "model_name_or_path": model_name,
            "instance_id": instance_id,
            "model_patch": result,
        }
        output_path.write_text(json.dumps(output_data, indent=2))


def remove_from_preds_file(output_path: Path, instance_id: str):
    """Remove an instance from the predictions file."""
    if not output_path.exists():
        return
    with _OUTPUT_FILE_LOCK:
        output_data = json.loads(output_path.read_text())
        if instance_id in output_data:
            del output_data[instance_id]
            output_path.write_text(json.dumps(output_data, indent=2))


def process_instance(
    instance: dict,
    output_dir: Path,
    config: dict,
    progress_manager: RunBatchProgressManager,
) -> None:
    """Process a single SWEBench instance."""
    instance_id = instance["instance_id"]
    instance_dir = output_dir / instance_id
    # avoid inconsistent state if something here fails and there's leftover previous files
    remove_from_preds_file(output_dir / "preds.json", instance_id)
    (instance_dir / f"{instance_id}.traj.json").unlink(missing_ok=True)
    model = get_model(config=config.get("model", {}))
    task = instance["problem_statement"]

    progress_manager.on_instance_start(instance_id)
    progress_manager.update_instance_status(instance_id, "Pulling/starting docker")

    agent = None
    extra_info = None

    try:
        env = get_sb_environment(config, instance)
        agent = ProgressTrackingAgent(
            model,
            env,
            progress_manager=progress_manager,
            instance_id=instance_id,
            **config.get("agent", {}),
        )
        exit_status, result = agent.run(task)
    except Exception as e:
        logger.error(f"Error processing instance {instance_id}: {e}", exc_info=True)
        exit_status, result = type(e).__name__, str(e)
        extra_info = {"traceback": traceback.format_exc()}
    finally:
        save_traj(
            agent,
            instance_dir / f"{instance_id}.traj.json",
            exit_status=exit_status,
            result=result,
            extra_info=extra_info,
            instance_id=instance_id,
            print_fct=logger.info,
        )
        update_preds_file(output_dir / "preds.json", instance_id, model.config.model_name, result)
        progress_manager.on_instance_end(instance_id, exit_status)


def filter_instances(
    instances: list[dict], *, filter_spec: str, slice_spec: str = "", shuffle: bool = False
) -> list[dict]:
    """Filter and slice a list of SWEBench instances."""
    if shuffle:
        instances = sorted(instances.copy(), key=lambda x: x["instance_id"])
        random.seed(42)
        random.shuffle(instances)
    before_filter = len(instances)
    instances = [instance for instance in instances if re.match(filter_spec, instance["instance_id"])]
    if (after_filter := len(instances)) != before_filter:
        logger.info(f"Instance filter: {before_filter} -> {after_filter} instances")
    if slice_spec:
        values = [int(x) if x else None for x in slice_spec.split(":")]
        instances = instances[slice(*values)]
        if (after_slice := len(instances)) != before_filter:
            logger.info(f"Instance slice: {before_filter} -> {after_slice} instances")
    return instances


# fmt: off
@app.command(help=_HELP_TEXT)
def main(
    subset: str = typer.Option("lite", "--subset", help="SWEBench subset to use or path to a dataset", rich_help_panel="Data selection"),
    split: str = typer.Option("dev", "--split", help="Dataset split", rich_help_panel="Data selection"),
    slice_spec: str = typer.Option("", "--slice", help="Slice specification (e.g., '0:5' for first 5 instances)", rich_help_panel="Data selection"),
    filter_spec: str = typer.Option("", "--filter", help="Filter instance IDs by regex", rich_help_panel="Data selection"),
    shuffle: bool = typer.Option(False, "--shuffle", help="Shuffle instances", rich_help_panel="Data selection"),
    output: str = typer.Option("", "-o", "--output", help="Output directory", rich_help_panel="Basic"),
    workers: int = typer.Option(1, "-w", "--workers", help="Number of worker threads for parallel processing", rich_help_panel="Basic"),
    model: str | None = typer.Option(None, "-m", "--model", help="Model to use", rich_help_panel="Basic"),
    model_class: str | None = typer.Option(None, "-c", "--model-class", help="Model class to use (e.g., 'anthropic' or 'minisweagent.models.anthropic.AnthropicModel')", rich_help_panel="Advanced"),
    redo_existing: bool = typer.Option(False, "--redo-existing", help="Redo existing instances", rich_help_panel="Data selection"),
    config_spec: Path = typer.Option( builtin_config_dir / "extra" / "swebench.yaml", "-c", "--config", help="Path to a config file", rich_help_panel="Basic"),
    environment_class: str | None = typer.Option( None, "--environment-class", help="Environment type to use. Recommended are docker or singularity", rich_help_panel="Advanced"),
) -> None:
    # fmt: on
    output_path = Path(output)
    output_path.mkdir(parents=True, exist_ok=True)
    logger.info(f"Results will be saved to {output_path}")
    add_file_handler(output_path / "minisweagent.log")

    dataset_path = DATASET_MAPPING.get(subset, subset)
    logger.info(f"Loading dataset {dataset_path}, split {split}...")
    instances = list(load_dataset(dataset_path, split=split))

    instances = filter_instances(instances, filter_spec=filter_spec, slice_spec=slice_spec, shuffle=shuffle)
    if not redo_existing and (output_path / "preds.json").exists():
        existing_instances = list(json.loads((output_path / "preds.json").read_text()).keys())
        logger.info(f"Skipping {len(existing_instances)} existing instances")
        instances = [instance for instance in instances if instance["instance_id"] not in existing_instances]
    logger.info(f"Running on {len(instances)} instances...")


    config = yaml.safe_load(get_config_path(config_spec).read_text())
    if environment_class is not None:
        config.setdefault("environment", {})["environment_class"] = environment_class
    if model is not None:
        config.setdefault("model", {})["model_name"] = model
    if model_class is not None:
        config.setdefault("model", {})["model_class"] = model_class

    progress_manager = RunBatchProgressManager(len(instances), output_path / f"exit_statuses_{time.time()}.yaml")

    def process_futures(futures: dict[concurrent.futures.Future, str]):
        for future in concurrent.futures.as_completed(futures):
            try:
                future.result()
            except concurrent.futures.CancelledError:
                pass
            except Exception as e:
                instance_id = futures[future]
                logger.error(f"Error in future for instance {instance_id}: {e}", exc_info=True)
                progress_manager.on_uncaught_exception(instance_id, e)

    with Live(progress_manager.render_group, refresh_per_second=4):
        with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:
            futures = {
                executor.submit(process_instance, instance, output_path, config, progress_manager): instance[
                    "instance_id"
                ]
                for instance in instances
            }
            try:
                process_futures(futures)
            except KeyboardInterrupt:
                logger.info("Cancelling all pending jobs. Press ^C again to exit immediately.")
                for future in futures:
                    if not future.running() and not future.done():
                        future.cancel()
                process_futures(futures)


if __name__ == "__main__":
    app()



================================================
FILE: src/minisweagent/run/extra/swebench_single.py
================================================
"""Run on a single SWE-Bench instance."""

import traceback
from pathlib import Path

import typer
import yaml
from datasets import load_dataset

from minisweagent import global_config_dir
from minisweagent.agents.interactive import InteractiveAgent
from minisweagent.config import builtin_config_dir, get_config_path
from minisweagent.models import get_model
from minisweagent.run.extra.swebench import (
    DATASET_MAPPING,
    get_sb_environment,
)
from minisweagent.run.utils.save import save_traj
from minisweagent.utils.log import logger

app = typer.Typer(add_completion=False)

DEFAULT_OUTPUT = global_config_dir / "last_swebench_single_run.traj.json"


# fmt: off
@app.command()
def main(
    subset: str = typer.Option("lite", "--subset", help="SWEBench subset to use or path to a dataset", rich_help_panel="Data selection"),
    split: str = typer.Option("dev", "--split", help="Dataset split", rich_help_panel="Data selection"),
    instance_spec: str = typer.Option(0, "-i", "--instance", help="SWE-Bench instance ID or index", rich_help_panel="Data selection"),
    model_name: str | None = typer.Option(None, "-m", "--model", help="Model to use", rich_help_panel="Basic"),
    model_class: str | None = typer.Option(None, "-c", "--model-class", help="Model class to use (e.g., 'anthropic' or 'minisweagent.models.anthropic.AnthropicModel')", rich_help_panel="Advanced"),
    config_path: Path = typer.Option( builtin_config_dir / "extra" / "swebench.yaml", "-c", "--config", help="Path to a config file", rich_help_panel="Basic"),
    environment_class: str | None = typer.Option(None, "--environment-class", rich_help_panel="Advanced"),
    exit_immediately: bool = typer.Option( False, "--exit-immediately", help="Exit immediately when the agent wants to finish instead of prompting.", rich_help_panel="Basic"),
    output: Path = typer.Option(DEFAULT_OUTPUT, "-o", "--output", help="Output trajectory file", rich_help_panel="Basic"),
) -> None:
    # fmt: on
    """Run on a single SWE-Bench instance."""
    dataset_path = DATASET_MAPPING.get(subset, subset)
    logger.info(f"Loading dataset from {dataset_path}, split {split}...")
    instances = {
        inst["instance_id"]: inst  # type: ignore
        for inst in load_dataset(dataset_path, split=split)
    }
    if instance_spec.isnumeric():
        instance_spec = sorted(instances.keys())[int(instance_spec)]
    instance: dict = instances[instance_spec]  # type: ignore

    config = yaml.safe_load(get_config_path(config_path).read_text())
    if environment_class is not None:
        config.setdefault("environment", {})["environment_class"] = environment_class
    if model_class is not None:
        config.setdefault("model", {})["model_class"] = model_class
    if exit_immediately:
        config.setdefault("agent", {})["confirm_exit"] = False
    env = get_sb_environment(config, instance)
    agent = InteractiveAgent(
        get_model(model_name, config.get("model", {})),
        env,
        **({"mode": "yolo"} | config.get("agent", {})),
    )

    exit_status, result, extra_info = None, None, None
    try:
        exit_status, result = agent.run(instance["problem_statement"])  # type: ignore[arg-type]
    except Exception as e:
        logger.error(f"Error processing instance {instance_spec}: {e}", exc_info=True)
        exit_status, result = type(e).__name__, str(e)
        extra_info = {"traceback": traceback.format_exc()}
    finally:
        save_traj(agent, output, exit_status=exit_status, result=result, extra_info=extra_info)  # type: ignore[arg-type]


if __name__ == "__main__":
    app()



================================================
FILE: src/minisweagent/run/extra/utils/__init__.py
================================================
[Empty file]


================================================
FILE: src/minisweagent/run/extra/utils/batch_progress.py
================================================
"""This module contains an auxiliary class for rendering progress of a batch run.
It's identical to the one used in swe-agent.
"""

import collections
import time
from datetime import timedelta
from pathlib import Path
from threading import Lock

import yaml
from rich.console import Group
from rich.progress import (
    BarColumn,
    MofNCompleteColumn,
    Progress,
    SpinnerColumn,
    TaskID,
    TaskProgressColumn,
    TextColumn,
    TimeElapsedColumn,
)
from rich.table import Table

import minisweagent.models


def _shorten_str(s: str, max_len: int, shorten_left=False) -> str:
    if not shorten_left:
        s = s[: max_len - 3] + "..." if len(s) > max_len else s
    else:
        s = "..." + s[-max_len + 3 :] if len(s) > max_len else s
    return f"{s:<{max_len}}"


class RunBatchProgressManager:
    def __init__(
        self,
        num_instances: int,
        yaml_report_path: Path | None = None,
    ):
        """This class manages a progress bar/UI for run-batch

        Args:
            num_instances: Number of task instances
            yaml_report_path: Path to save a yaml report of the instances and their exit statuses
        """

        self._spinner_tasks: dict[str, TaskID] = {}
        """We need to map instance ID to the task ID that is used by the rich progress bar."""

        self._lock = Lock()
        self._start_time = time.time()
        self._total_instances = num_instances

        self._instances_by_exit_status = collections.defaultdict(list)
        self._main_progress_bar = Progress(
            SpinnerColumn(spinner_name="dots2"),
            TextColumn("[progress.description]{task.description} (${task.fields[total_cost]})"),
            BarColumn(),
            MofNCompleteColumn(),
            TaskProgressColumn(),
            TimeElapsedColumn(),
            TextColumn("[cyan]{task.fields[eta]}[/cyan]"),
            # Wait 5 min before estimating speed
            speed_estimate_period=60 * 5,
        )
        self._task_progress_bar = Progress(
            SpinnerColumn(spinner_name="dots2"),
            TextColumn("{task.fields[instance_id]}"),
            TextColumn("{task.fields[status]}"),
            TimeElapsedColumn(),
        )
        """Task progress bar for individual instances. There's only one progress bar
        with one task for each instance.
        """

        self._main_task_id = self._main_progress_bar.add_task(
            "[cyan]Overall Progress", total=num_instances, total_cost="0.00", eta=""
        )

        self.render_group = Group(Table(), self._task_progress_bar, self._main_progress_bar)
        self._yaml_report_path = yaml_report_path

    @property
    def n_completed(self) -> int:
        return sum(len(instances) for instances in self._instances_by_exit_status.values())

    def _get_eta_text(self) -> str:
        """Calculate estimated time remaining based on current progress."""
        try:
            estimated_remaining = (
                (time.time() - self._start_time) / self.n_completed * (self._total_instances - self.n_completed)
            )
            return f"eta: {timedelta(seconds=int(estimated_remaining))}"
        except ZeroDivisionError:
            return ""

    def update_exit_status_table(self):
        # We cannot update the existing table, so we need to create a new one and
        # assign it back to the render group.
        t = Table()
        t.add_column("Exit Status")
        t.add_column("Count", justify="right", style="bold cyan")
        t.add_column("Most recent instances")
        t.show_header = False
        with self._lock:
            t.show_header = True
            # Sort by number of instances in descending order
            sorted_items = sorted(self._instances_by_exit_status.items(), key=lambda x: len(x[1]), reverse=True)
            for status, instances in sorted_items:
                instances_str = _shorten_str(", ".join(reversed(instances)), 55)
                t.add_row(status, str(len(instances)), instances_str)
        assert self.render_group is not None
        self.render_group.renderables[0] = t

    def _update_total_costs(self) -> None:
        with self._lock:
            self._main_progress_bar.update(
                self._main_task_id,
                total_cost=f"{minisweagent.models.GLOBAL_MODEL_STATS.cost:.2f}",
                eta=self._get_eta_text(),
            )

    def update_instance_status(self, instance_id: str, message: str):
        assert self._task_progress_bar is not None
        assert self._main_progress_bar is not None
        with self._lock:
            self._task_progress_bar.update(
                self._spinner_tasks[instance_id],
                status=_shorten_str(message, 30),
                instance_id=_shorten_str(instance_id, 25, shorten_left=True),
            )
        self._update_total_costs()

    def on_instance_start(self, instance_id: str):
        with self._lock:
            self._spinner_tasks[instance_id] = self._task_progress_bar.add_task(
                description=f"Task {instance_id}",
                status="Task initialized",
                total=None,
                instance_id=instance_id,
            )

    def on_instance_end(self, instance_id: str, exit_status: str | None) -> None:
        self._instances_by_exit_status[exit_status].append(instance_id)
        with self._lock:
            try:
                self._task_progress_bar.remove_task(self._spinner_tasks[instance_id])
            except KeyError:
                pass
            self._main_progress_bar.update(TaskID(0), advance=1, eta=self._get_eta_text())
        self.update_exit_status_table()
        self._update_total_costs()
        if self._yaml_report_path is not None:
            self._save_overview_data_yaml(self._yaml_report_path)

    def on_uncaught_exception(self, instance_id: str, exception: Exception) -> None:
        self.on_instance_end(instance_id, f"Uncaught {type(exception).__name__}")

    def print_report(self) -> None:
        """Print complete list of instances and their exit statuses."""
        for status, instances in self._instances_by_exit_status.items():
            print(f"{status}: {len(instances)}")
            for instance in instances:
                print(f"  {instance}")

    def _get_overview_data(self) -> dict:
        """Get data like exit statuses, total costs, etc."""
        return {
            # convert defaultdict to dict because of serialization
            "instances_by_exit_status": dict(self._instances_by_exit_status),
        }

    def _save_overview_data_yaml(self, path: Path) -> None:
        """Save a yaml report of the instances and their exit statuses."""
        with self._lock:
            path.write_text(yaml.dump(self._get_overview_data(), indent=4))



================================================
FILE: src/minisweagent/run/utils/__init__.py
================================================
[Empty file]


================================================
FILE: src/minisweagent/run/utils/save.py
================================================
import dataclasses
import json
from collections.abc import Callable
from pathlib import Path
from typing import Any

from minisweagent import Agent, __version__


def _get_class_name_with_module(obj: Any) -> str:
    """Get the full class name with module path."""
    return f"{obj.__class__.__module__}.{obj.__class__.__name__}"


def _asdict(obj: Any) -> dict:
    """Convert config objects to dicts."""
    if dataclasses.is_dataclass(obj):
        return dataclasses.asdict(obj)  # type: ignore[arg-type]
    return obj  # let's try our luck


def save_traj(
    agent: Agent | None,
    path: Path,
    *,
    print_path: bool = True,
    exit_status: str | None = None,
    result: str | None = None,
    extra_info: dict | None = None,
    print_fct: Callable = print,
    **kwargs,
):
    """Save the trajectory of the agent to a file.

    Args:
        agent: The agent to save the trajectory of.
        path: The path to save the trajectory to.
        print_path: Whether to print confirmation of path to the terminal.
        exit_status: The exit status of the agent.
        result: The result/submission of the agent.
        extra_info: Extra information to save (will be merged into the info dict).
        **kwargs: Additional information to save (will be merged into top level)

    """
    data = {
        "info": {
            "exit_status": exit_status,
            "submission": result,
            "model_stats": {
                "instance_cost": 0.0,
                "api_calls": 0,
            },
            "mini_version": __version__,
        },
        "messages": [],
        "trajectory_format": "mini-swe-agent-1",
    } | kwargs
    if agent is not None:
        data["info"]["model_stats"]["instance_cost"] = agent.model.cost
        data["info"]["model_stats"]["api_calls"] = agent.model.n_calls
        data["messages"] = agent.messages
        data["info"]["config"] = {
            "agent": _asdict(agent.config),
            "model": _asdict(agent.model.config),
            "environment": _asdict(agent.env.config),
            "agent_type": _get_class_name_with_module(agent),
            "model_type": _get_class_name_with_module(agent.model),
            "environment_type": _get_class_name_with_module(agent.env),
        }
    if extra_info:
        data["info"].update(extra_info)

    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(data, indent=2))
    if print_path:
        print_fct(f"Saved trajectory to '{path}'")



================================================
FILE: src/minisweagent/utils/__init__.py
================================================
[Empty file]


================================================
FILE: src/minisweagent/utils/log.py
================================================
import logging
from pathlib import Path

from rich.logging import RichHandler


def _setup_root_logger() -> None:
    logger = logging.getLogger("minisweagent")
    logger.setLevel(logging.DEBUG)
    _handler = RichHandler(
        show_path=False,
        show_time=False,
        show_level=False,
        markup=True,
    )
    _formatter = logging.Formatter("%(name)s: %(levelname)s: %(message)s")
    _handler.setFormatter(_formatter)
    logger.addHandler(_handler)


def add_file_handler(path: Path | str, level: int = logging.DEBUG, *, print_path: bool = True) -> None:
    logger = logging.getLogger("minisweagent")
    handler = logging.FileHandler(path)
    handler.setLevel(level)
    formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    if print_path:
        print(f"Logging to '{path}'")


_setup_root_logger()
logger = logging.getLogger("minisweagent")


__all__ = ["logger"]



================================================
FILE: tests/__init__.py
================================================
[Empty file]


================================================
FILE: tests/conftest.py
================================================
import json
import threading
from pathlib import Path

import pytest

from minisweagent.models import GLOBAL_MODEL_STATS

# Global lock for tests that modify global state - this works across threads
_global_stats_lock = threading.Lock()


@pytest.fixture
def reset_global_stats():
    """Reset global model stats and ensure exclusive access for tests that need it.

    This fixture should be used by any test that depends on global model stats
    to ensure thread safety and test isolation.
    """
    with _global_stats_lock:
        # Reset at start
        GLOBAL_MODEL_STATS._cost = 0.0  # noqa: protected-access
        GLOBAL_MODEL_STATS._n_calls = 0  # noqa: protected-access
        yield
        # Reset at end to clean up
        GLOBAL_MODEL_STATS._cost = 0.0  # noqa: protected-access
        GLOBAL_MODEL_STATS._n_calls = 0  # noqa: protected-access


def get_test_data(trajectory_name: str) -> dict[str, list[str]]:
    """Load test fixtures from a trajectory JSON file"""
    json_path = Path(__file__).parent / "test_data" / f"{trajectory_name}.traj.json"
    with json_path.open() as f:
        trajectory = json.load(f)

    # Extract model responses (assistant messages, starting from index 2)
    model_responses = []
    # Extract expected observations (user messages, starting from index 3)
    expected_observations = []

    for i, message in enumerate(trajectory):
        if i < 2:  # Skip system message (0) and initial user message (1)
            continue

        if message["role"] == "assistant":
            model_responses.append(message["content"])
        elif message["role"] == "user":
            expected_observations.append(message["content"])

    return {"model_responses": model_responses, "expected_observations": expected_observations}


@pytest.fixture
def github_test_data():
    """Load GitHub issue test fixtures"""
    return get_test_data("github_issue")


@pytest.fixture
def local_test_data():
    """Load local test fixtures"""
    return get_test_data("local")



================================================
FILE: tests/agents/__init__.py
================================================
[Empty file]


================================================
FILE: tests/agents/test_default.py
================================================
import pytest

from minisweagent.agents.default import DefaultAgent, NonTerminatingException
from minisweagent.environments.local import LocalEnvironment
from minisweagent.models.test_models import DeterministicModel


def test_successful_completion():
    """Test agent completes successfully when COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT is encountered."""
    agent = DefaultAgent(
        model=DeterministicModel(
            outputs=[
                "I'll echo a message\n```bash\necho 'hello world'\n```",
                "Now finishing\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'Task completed successfully'\n```",
            ]
        ),
        env=LocalEnvironment(),
    )

    exit_status, result = agent.run("Echo hello world then finish")
    assert exit_status == "Submitted"
    assert result == "Task completed successfully\n"
    assert agent.model.n_calls == 2
    assert len(agent.messages) == 6  # system, user, assistant, user, assistant, user


def test_step_limit_enforcement():
    """Test agent stops when step limit is reached."""
    agent = DefaultAgent(
        model=DeterministicModel(
            outputs=["First command\n```bash\necho 'step1'\n```", "Second command\n```bash\necho 'step2'\n```"]
        ),
        env=LocalEnvironment(),
        step_limit=1,
    )

    exit_status, _ = agent.run("Run multiple commands")
    assert exit_status == "LimitsExceeded"
    assert agent.model.n_calls == 1


def test_cost_limit_enforcement():
    """Test agent stops when cost limit is reached."""
    model = DeterministicModel(outputs=["```bash\necho 'test'\n```"])

    agent = DefaultAgent(
        model=model,
        env=LocalEnvironment(),
        cost_limit=0.5,
    )

    exit_status, _ = agent.run("Test cost limit")
    assert exit_status == "LimitsExceeded"


def test_format_error_handling():
    """Test agent handles malformed action formats properly."""
    agent = DefaultAgent(
        model=DeterministicModel(
            outputs=[
                "No code blocks here",
                "Multiple blocks\n```bash\necho 'first'\n```\n```bash\necho 'second'\n```",
                "Now correct\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'done'\n```",
            ]
        ),
        env=LocalEnvironment(),
    )

    exit_status, result = agent.run("Test format errors")
    assert exit_status == "Submitted"
    assert result == "done\n"
    assert agent.model.n_calls == 3
    # Should have error messages in conversation
    assert (
        len([msg for msg in agent.messages if "Please always provide EXACTLY ONE action" in msg.get("content", "")])
        == 2
    )


def test_timeout_handling():
    """Test agent handles command timeouts properly."""
    agent = DefaultAgent(
        model=DeterministicModel(
            outputs=[
                "Long sleep\n```bash\nsleep 5\n```",  # This will timeout
                "Quick finish\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'recovered'\n```",
            ]
        ),
        env=LocalEnvironment(timeout=1),  # Very short timeout
    )

    exit_status, result = agent.run("Test timeout handling")
    assert exit_status == "Submitted"
    assert result == "recovered\n"
    # Should have timeout error message
    assert len([msg for msg in agent.messages if "timed out" in msg.get("content", "")]) == 1


def test_timeout_captures_partial_output():
    """Test that timeout error captures partial output from commands that produce output before timing out."""
    num1, num2 = 111, 9
    calculation_command = f"echo $(({num1}*{num2})); sleep 10"
    expected_output = str(num1 * num2)
    agent = DefaultAgent(
        model=DeterministicModel(
            outputs=[
                f"Output then sleep\n```bash\n{calculation_command}\n```",
                "Quick finish\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'recovered'\n```",
            ]
        ),
        env=LocalEnvironment(timeout=1),
    )
    exit_status, result = agent.run("Test timeout with partial output")
    assert exit_status == "Submitted"
    assert result == "recovered\n"  # final output should be `recovered` from the last command
    timed_out_messages = [msg for msg in agent.messages if "timed out" in msg.get("content", "")]
    assert len(timed_out_messages) == 1
    assert expected_output in timed_out_messages[0]["content"]  # ensure timed out output is still captured


def test_parse_action_success():
    """Test action parsing works correctly for valid formats."""
    agent = DefaultAgent(
        model=DeterministicModel(outputs=[]),
        env=LocalEnvironment(),
    )

    # Test different valid formats
    result = agent.parse_action({"content": "```bash\necho 'test'\n```"})
    assert result["action"] == "echo 'test'"
    assert result["content"] == "```bash\necho 'test'\n```"

    result = agent.parse_action({"content": "```bash\nls -la\n```"})
    assert result["action"] == "ls -la"
    assert result["content"] == "```bash\nls -la\n```"

    result = agent.parse_action({"content": "Some text\n```bash\necho 'hello'\n```\nMore text"})
    assert result["action"] == "echo 'hello'"
    assert result["content"] == "Some text\n```bash\necho 'hello'\n```\nMore text"


def test_parse_action_failures():
    """Test action parsing raises appropriate exceptions for invalid formats."""
    agent = DefaultAgent(
        model=DeterministicModel(outputs=[]),
        env=LocalEnvironment(),
    )

    # No code blocks
    with pytest.raises(NonTerminatingException):
        agent.parse_action({"content": "No code blocks here"})

    # Multiple code blocks
    with pytest.raises(NonTerminatingException):
        agent.parse_action({"content": "```bash\necho 'first'\n```\n```bash\necho 'second'\n```"})

    # Code block without bash language specifier
    with pytest.raises(NonTerminatingException):
        agent.parse_action({"content": "```\nls -la\n```"})


def test_message_history_tracking():
    """Test that messages are properly added and tracked."""
    agent = DefaultAgent(
        model=DeterministicModel(
            outputs=[
                "Response 1\n```bash\necho 'test1'\n```",
                "Response 2\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'done'\n```",
            ]
        ),
        env=LocalEnvironment(),
    )

    exit_status, result = agent.run("Track messages")
    assert exit_status == "Submitted"
    assert result == "done\n"

    # After completion should have full conversation
    assert len(agent.messages) == 6
    assert [msg["role"] for msg in agent.messages] == ["system", "user", "assistant", "user", "assistant", "user"]


def test_multiple_steps_before_completion():
    """Test agent can handle multiple steps before finding completion signal."""
    agent = DefaultAgent(
        model=DeterministicModel(
            outputs=[
                "Step 1\n```bash\necho 'first'\n```",
                "Step 2\n```bash\necho 'second'\n```",
                "Step 3\n```bash\necho 'third'\n```",
                "Final step\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'completed all steps'\n```",
            ]
        ),
        env=LocalEnvironment(),
        cost_limit=5.0,  # Increase cost limit to allow all 4 calls (4.0 total cost)
    )

    exit_status, result = agent.run("Multi-step task")
    assert exit_status == "Submitted"
    assert result == "completed all steps\n"
    assert agent.model.n_calls == 4

    # Check that all intermediate outputs are captured (final step doesn't get observation due to termination)
    observations = [
        msg["content"] for msg in agent.messages if msg["role"] == "user" and "Observation:" in msg["content"]
    ]
    assert len(observations) == 3
    assert "first" in observations[0]
    assert "second" in observations[1]
    assert "third" in observations[2]


def test_custom_config():
    """Test agent works with custom configuration."""
    agent = DefaultAgent(
        model=DeterministicModel(
            outputs=[
                "Test response\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'custom config works'\n```"
            ]
        ),
        env=LocalEnvironment(),
        system_template="You are a test assistant.",
        instance_template="Task: {{task}}. Return bash command.",
        step_limit=2,
        cost_limit=1.0,
    )

    exit_status, result = agent.run("Test custom config")
    assert exit_status == "Submitted"
    assert result == "custom config works\n"
    assert agent.messages[0]["content"] == "You are a test assistant."
    assert "Test custom config" in agent.messages[1]["content"]


def test_render_template_model_stats():
    """Test that render_template has access to n_model_calls and model_cost from model."""
    agent = DefaultAgent(
        model=DeterministicModel(outputs=["output1", "output2"]),
        env=LocalEnvironment(),
    )

    # Make some model calls to generate stats
    agent.model.query([])
    agent.model.query([])

    # Test template rendering with model stats
    template = "Calls: {{n_model_calls}}, Cost: {{model_cost}}"
    result = agent.render_template(template)

    assert result == "Calls: 2, Cost: 2.0"



================================================
FILE: tests/agents/test_interactive.py
================================================
from unittest.mock import patch

from minisweagent.agents.interactive import InteractiveAgent
from minisweagent.environments.local import LocalEnvironment
from minisweagent.models.test_models import DeterministicModel


def test_successful_completion_with_confirmation():
    """Test agent completes successfully when user confirms all actions."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt", side_effect=["", ""]
    ):  # Confirm action with Enter, then no new task
        agent = InteractiveAgent(
            model=DeterministicModel(
                outputs=["Finishing\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'completed'\n```"]
            ),
            env=LocalEnvironment(),
        )

        exit_status, result = agent.run("Test completion with confirmation")
        assert exit_status == "Submitted"
        assert result == "completed\n"
        assert agent.model.n_calls == 1


def test_action_rejection_and_recovery():
    """Test agent handles action rejection and can recover."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[
            "User rejected this action",  # Reject first action
            "",  # Confirm second action
            "",  # No new task when agent wants to finish
        ],
    ):
        agent = InteractiveAgent(
            model=DeterministicModel(
                outputs=[
                    "First try\n```bash\necho 'first attempt'\n```",
                    "Second try\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'recovered'\n```",
                ]
            ),
            env=LocalEnvironment(),
        )

        exit_status, result = agent.run("Test action rejection")
        assert exit_status == "Submitted"
        assert result == "recovered\n"
        assert agent.model.n_calls == 2
        # Should have rejection message in conversation
        rejection_messages = [msg for msg in agent.messages if "User rejected this action" in msg.get("content", "")]
        assert len(rejection_messages) == 1


def test_yolo_mode_activation():
    """Test entering yolo mode disables confirmations."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[
            "/y",  # Enter yolo mode
            "",  # This should be ignored since yolo mode is on
            "",  # No new task when agent wants to finish
        ],
    ):
        agent = InteractiveAgent(
            model=DeterministicModel(
                outputs=["Test command\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'yolo works'\n```"]
            ),
            env=LocalEnvironment(),
        )

        exit_status, result = agent.run("Test yolo mode")
        assert exit_status == "Submitted"
        assert result == "yolo works\n"
        assert agent.config.mode == "yolo"


def test_help_command():
    """Test help command shows help and continues normally."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[
            "/h",  # Show help
            "",  # Confirm action after help
            "",  # No new task when agent wants to finish
        ],
    ):
        with patch("minisweagent.agents.interactive.console.print") as mock_print:
            agent = InteractiveAgent(
                model=DeterministicModel(
                    outputs=["Test help\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'help shown'\n```"]
                ),
                env=LocalEnvironment(),
            )

            exit_status, result = agent.run("Test help command")
            assert exit_status == "Submitted"
            assert result == "help shown\n"
            # Check that help was printed
            help_calls = [call for call in mock_print.call_args_list if "/y" in str(call)]
            assert len(help_calls) > 0


def test_whitelisted_actions_skip_confirmation():
    """Test that whitelisted actions don't require confirmation."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[""],  # No new task when agent wants to finish
    ):
        agent = InteractiveAgent(
            model=DeterministicModel(
                outputs=[
                    "Whitelisted\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'no confirmation needed'\n```"
                ]
            ),
            env=LocalEnvironment(),
            whitelist_actions=[r"echo.*"],
        )

        exit_status, result = agent.run("Test whitelisted actions")
        assert exit_status == "Submitted"
        assert result == "no confirmation needed\n"


def _test_interruption_helper(interruption_input, expected_message_fragment, problem_statement="Test interruption"):
    """Helper function for testing interruption scenarios."""
    agent = InteractiveAgent(
        model=DeterministicModel(
            outputs=[
                "Initial step\n```bash\necho 'will be interrupted'\n```",
                "Recovery\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'recovered from interrupt'\n```",
            ]
        ),
        env=LocalEnvironment(),
    )

    # Mock the query to raise KeyboardInterrupt on first call, then work normally
    original_query = agent.query
    call_count = 0

    def mock_query(*args, **kwargs):
        nonlocal call_count
        call_count += 1
        if call_count == 1:
            raise KeyboardInterrupt()
        return original_query(*args, **kwargs)

    # Mock console.input based on the interruption_input parameter
    input_call_count = 0

    def mock_input(prompt):
        nonlocal input_call_count
        input_call_count += 1
        if input_call_count == 1:
            return interruption_input  # For the interruption handling
        return ""  # Confirm all subsequent actions

    with patch("minisweagent.agents.interactive.prompt_session.prompt", side_effect=mock_input):
        with patch.object(agent, "query", side_effect=mock_query):
            exit_status, result = agent.run(problem_statement)

    assert exit_status == "Submitted"
    assert result == "recovered from interrupt\n"
    # Check that the expected interruption message was added
    interrupt_messages = [msg for msg in agent.messages if expected_message_fragment in msg.get("content", "")]
    assert len(interrupt_messages) == 1

    return agent, interrupt_messages[0]


def test_interruption_handling_with_message():
    """Test that interruption with user message is handled properly."""
    agent, interrupt_message = _test_interruption_helper("User interrupted", "Interrupted by user")

    # Additional verification specific to this test
    assert "User interrupted" in interrupt_message["content"]


def test_interruption_handling_empty_message():
    """Test that interruption with empty input is handled properly."""
    _test_interruption_helper("", "Temporary interruption caught")


def test_multiple_confirmations_and_commands():
    """Test complex interaction with multiple confirmations and commands."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[
            "reject first",  # Reject first action
            "/h",  # Show help for second action
            "/y",  # After help, enter yolo mode
            "",  # After yolo mode enabled, confirm (but yolo mode will skip future confirmations)
            "",  # No new task when agent wants to finish
        ],
    ):
        agent = InteractiveAgent(
            model=DeterministicModel(
                outputs=[
                    "First action\n```bash\necho 'first'\n```",
                    "Second action\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'complex flow completed'\n```",
                ]
            ),
            env=LocalEnvironment(),
        )

        exit_status, result = agent.run("Test complex interaction flow")
        assert exit_status == "Submitted"
        assert result == "complex flow completed\n"
        assert agent.config.mode == "yolo"  # Should be in yolo mode
        assert agent.model.n_calls == 2


def test_non_whitelisted_action_requires_confirmation():
    """Test that non-whitelisted actions still require confirmation."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=["", ""],  # Confirm action, then no new task
    ):
        agent = InteractiveAgent(
            model=DeterministicModel(
                outputs=[
                    "Non-whitelisted\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'confirmed'\n```"
                ]
            ),
            env=LocalEnvironment(),
            whitelist_actions=[r"ls.*"],  # Only ls commands whitelisted
        )

        exit_status, result = agent.run("Test non-whitelisted action")
        assert exit_status == "Submitted"
        assert result == "confirmed\n"


# New comprehensive mode switching tests


def test_human_mode_basic_functionality():
    """Test human mode where user enters shell commands directly."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[
            "echo 'user command'",  # User enters shell command
            "echo 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'human mode works'",  # User enters final command
            "",  # No new task when agent wants to finish
        ],
    ):
        agent = InteractiveAgent(
            model=DeterministicModel(outputs=[]),  # LM shouldn't be called in human mode
            env=LocalEnvironment(),
            mode="human",
        )

        exit_status, result = agent.run("Test human mode")
        assert exit_status == "Submitted"
        assert result == "human mode works\n"
        assert agent.config.mode == "human"
        assert agent.model.n_calls == 0  # LM should not be called


def test_human_mode_switch_to_yolo():
    """Test switching from human mode to yolo mode."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[
            "/y",  # Switch to yolo mode from human mode
            "",  # Confirm action in yolo mode (though no confirmation needed)
            "",  # No new task when agent wants to finish
        ],
    ):
        agent = InteractiveAgent(
            model=DeterministicModel(
                outputs=[
                    "LM action\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'switched to yolo'\n```"
                ]
            ),
            env=LocalEnvironment(),
            mode="human",
        )

        exit_status, result = agent.run("Test human to yolo switch")
        assert exit_status == "Submitted"
        assert result == "switched to yolo\n"
        assert agent.config.mode == "yolo"
        assert agent.model.n_calls == 1


def test_human_mode_switch_to_confirm():
    """Test switching from human mode to confirm mode."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[
            "/c",  # Switch to confirm mode from human mode
            "",  # Confirm action in confirm mode
            "",  # No new task when agent wants to finish
        ],
    ):
        agent = InteractiveAgent(
            model=DeterministicModel(
                outputs=[
                    "LM action\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'switched to confirm'\n```"
                ]
            ),
            env=LocalEnvironment(),
            mode="human",
        )

        exit_status, result = agent.run("Test human to confirm switch")
        assert exit_status == "Submitted"
        assert result == "switched to confirm\n"
        assert agent.config.mode == "confirm"
        assert agent.model.n_calls == 1


def test_confirmation_mode_switch_to_human_with_rejection():
    """Test switching from confirm mode to human mode with /u command."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[
            "/u",  # Switch to human mode and reject action
            "echo 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'human command after rejection'",  # Human command
            "",  # No new task when agent wants to finish
        ],
    ):
        agent = InteractiveAgent(
            model=DeterministicModel(
                outputs=[
                    "LM action\n```bash\necho 'first action'\n```",
                    "Recovery action\n```bash\necho 'recovery'\n```",
                ]
            ),
            env=LocalEnvironment(),
            mode="confirm",
        )

        exit_status, result = agent.run("Test confirm to human switch")
        assert exit_status == "Submitted"
        assert result == "human command after rejection\n"
        assert agent.config.mode == "human"
        # Should have rejection message
        rejection_messages = [msg for msg in agent.messages if "Switching to human mode" in msg.get("content", "")]
        assert len(rejection_messages) == 1


def test_confirmation_mode_switch_to_yolo_and_continue():
    """Test switching from confirm mode to yolo mode with /y and continuing with action."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[
            "/y",  # Switch to yolo mode and confirm current action
            "",  # No new task when agent wants to finish
        ],
    ):
        agent = InteractiveAgent(
            model=DeterministicModel(
                outputs=[
                    "LM action\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'switched and continued'\n```"
                ]
            ),
            env=LocalEnvironment(),
            mode="confirm",
        )

        exit_status, result = agent.run("Test confirm to yolo switch")
        assert exit_status == "Submitted"
        assert result == "switched and continued\n"
        assert agent.config.mode == "yolo"


def test_mode_switch_during_keyboard_interrupt():
    """Test mode switching during keyboard interrupt handling."""
    agent = InteractiveAgent(
        model=DeterministicModel(
            outputs=[
                "Initial step\n```bash\necho 'will be interrupted'\n```",
                "Recovery\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'recovered after mode switch'\n```",
            ]
        ),
        env=LocalEnvironment(),
        mode="confirm",
    )

    # Mock the query to raise KeyboardInterrupt on first call
    original_query = agent.query
    call_count = 0

    def mock_query(*args, **kwargs):
        nonlocal call_count
        call_count += 1
        if call_count == 1:
            raise KeyboardInterrupt()
        return original_query(*args, **kwargs)

    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[
            "/y",  # Switch to yolo mode during interrupt
            "",  # Confirm subsequent actions (though yolo mode won't ask)
        ],
    ):
        with patch.object(agent, "query", side_effect=mock_query):
            exit_status, result = agent.run("Test interrupt mode switch")

    assert exit_status == "Submitted"
    assert result == "recovered after mode switch\n"
    assert agent.config.mode == "yolo"
    # Should have interruption message
    interrupt_messages = [msg for msg in agent.messages if "Temporary interruption caught" in msg.get("content", "")]
    assert len(interrupt_messages) == 1


def test_already_in_mode_behavior():
    """Test behavior when trying to switch to the same mode."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[
            "/c",  # Try to switch to confirm mode when already in confirm mode
            "",  # Confirm action after the "already in mode" recursive prompt
            "",  # No new task when agent wants to finish
        ],
    ):
        agent = InteractiveAgent(
            model=DeterministicModel(
                outputs=[
                    "Test action\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'already in mode'\n```"
                ]
            ),
            env=LocalEnvironment(),
            mode="confirm",
        )

        exit_status, result = agent.run("Test already in mode")
        assert exit_status == "Submitted"
        assert result == "already in mode\n"
        assert agent.config.mode == "confirm"


def test_all_mode_transitions_yolo_to_others():
    """Test transitions from yolo mode to other modes."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[
            "/c",  # Switch from yolo to confirm
            "",  # Confirm action in confirm mode
            "",  # No new task when agent wants to finish
        ],
    ):
        agent = InteractiveAgent(
            model=DeterministicModel(
                outputs=[
                    "First action\n```bash\necho 'yolo action'\n```",
                    "Second action\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'confirm action'\n```",
                ]
            ),
            env=LocalEnvironment(),
            mode="yolo",
        )

        # Trigger first action in yolo mode (should execute without confirmation)
        # Then interrupt to switch mode
        original_query = agent.query
        call_count = 0

        def mock_query(*args, **kwargs):
            nonlocal call_count
            call_count += 1
            if call_count == 2:  # Interrupt on second query
                raise KeyboardInterrupt()
            return original_query(*args, **kwargs)

        with patch.object(agent, "query", side_effect=mock_query):
            exit_status, result = agent.run("Test yolo to confirm transition")

        assert exit_status == "Submitted"
        assert result == "confirm action\n"
        assert agent.config.mode == "confirm"


def test_all_mode_transitions_confirm_to_human():
    """Test transition from confirm mode to human mode."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[
            "/u",  # Switch from confirm to human (rejecting action)
            "echo 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'human command'",  # User enters command in human mode
            "",  # No new task when agent wants to finish
        ],
    ):
        agent = InteractiveAgent(
            model=DeterministicModel(outputs=["LM action\n```bash\necho 'rejected action'\n```"]),
            env=LocalEnvironment(),
            mode="confirm",
        )

        exit_status, result = agent.run("Test confirm to human transition")
        assert exit_status == "Submitted"
        assert result == "human command\n"
        assert agent.config.mode == "human"


def test_help_command_from_different_contexts():
    """Test help command works from different contexts (confirmation, interrupt, human mode)."""
    # Test help during confirmation
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[
            "/h",  # Show help during confirmation
            "",  # Confirm after help
            "",  # No new task when agent wants to finish
        ],
    ):
        with patch("minisweagent.agents.interactive.console.print") as mock_print:
            agent = InteractiveAgent(
                model=DeterministicModel(
                    outputs=[
                        "Test action\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'help works'\n```"
                    ]
                ),
                env=LocalEnvironment(),
                mode="confirm",
            )

            exit_status, result = agent.run("Test help from confirmation")
            assert exit_status == "Submitted"
            assert result == "help works\n"
            # Verify help was shown
            help_calls = [call for call in mock_print.call_args_list if "Current mode: " in str(call)]
            assert len(help_calls) > 0


def test_help_command_from_human_mode():
    """Test help command works from human mode."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[
            "/h",  # Show help in human mode
            "echo 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'help in human mode'",  # User command after help
            "",  # No new task when agent wants to finish
        ],
    ):
        with patch("minisweagent.agents.interactive.console.print") as mock_print:
            agent = InteractiveAgent(
                model=DeterministicModel(outputs=[]),  # LM shouldn't be called
                env=LocalEnvironment(),
                mode="human",
            )

            exit_status, result = agent.run("Test help from human mode")
            assert exit_status == "Submitted"
            assert result == "help in human mode\n"
            # Verify help was shown
            help_calls = [call for call in mock_print.call_args_list if "Current mode: " in str(call)]
            assert len(help_calls) > 0


def test_complex_mode_switching_sequence():
    """Test complex sequence of mode switches across different contexts."""
    agent = InteractiveAgent(
        model=DeterministicModel(
            outputs=[
                "Action 1\n```bash\necho 'action1'\n```",
                "Action 2\n```bash\necho 'action2'\n```",
                "Action 3\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'final action'\n```",
            ]
        ),
        env=LocalEnvironment(),
        mode="confirm",
    )

    # Mock interruption on second query
    original_query = agent.query
    call_count = 0

    def mock_query(*args, **kwargs):
        nonlocal call_count
        call_count += 1
        if call_count == 2:
            raise KeyboardInterrupt()
        return original_query(*args, **kwargs)

    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[
            "/y",  # Confirm->Yolo during first action confirmation
            "/u",  # Yolo->Human during interrupt
            "/c",  # Human->Confirm in human mode
            "",  # Confirm final action
            "",  # No new task when agent wants to finish
            "",  # Extra empty input for any additional prompts
            "",  # Extra empty input for any additional prompts
        ],
    ):
        with patch.object(agent, "query", side_effect=mock_query):
            exit_status, result = agent.run("Test complex mode switching")

    assert exit_status == "Submitted"
    assert result == "final action\n"
    assert agent.config.mode == "confirm"  # Should end in confirm mode


def test_limits_exceeded_with_user_continuation():
    """Test that when limits are exceeded, user can provide new limits and execution continues."""
    # Create agent with very low limits that will be exceeded
    agent = InteractiveAgent(
        model=DeterministicModel(
            outputs=[
                "Step 1\n```bash\necho 'first step'\n```",
                "Step 2\n```bash\necho 'second step'\n```",
                "Final step\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'completed after limit increase'\n```",
            ],
            cost_per_call=0.6,  # Will exceed cost_limit=0.5 on first call
        ),
        env=LocalEnvironment(),
        step_limit=10,  # High enough to not interfere initially
        cost_limit=0.5,  # Will be exceeded with first model call (cost=0.6)
        mode="yolo",  # Use yolo mode to avoid confirmation prompts
    )

    # Mock input() to provide new limits when prompted
    with patch("builtins.input", side_effect=["10", "5.0"]):  # New step_limit=10, cost_limit=5.0
        with patch("minisweagent.agents.interactive.prompt_session.prompt", side_effect=[""]):  # No new task
            with patch("minisweagent.agents.interactive.console.print"):  # Suppress console output
                exit_status, result = agent.run("Test limits exceeded with continuation")

    assert exit_status == "Submitted"
    assert result == "completed after limit increase\n"
    assert agent.model.n_calls == 3  # Should complete all 3 steps
    assert agent.config.step_limit == 10  # Should have updated step limit
    assert agent.config.cost_limit == 5.0  # Should have updated cost limit


def test_limits_exceeded_multiple_times_with_continuation():
    """Test that limits can be exceeded and updated multiple times."""
    agent = InteractiveAgent(
        model=DeterministicModel(
            outputs=[
                "Step 1\n```bash\necho 'step1'\n```",
                "Step 2\n```bash\necho 'step2'\n```",
                "Step 3\n```bash\necho 'step3'\n```",
                "Step 4\n```bash\necho 'step4'\n```",
                "Final\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'completed after multiple increases'\n```",
            ],
            cost_per_call=1.0,  # Standard cost per call
        ),
        env=LocalEnvironment(),
        step_limit=1,  # Will be exceeded after first step
        cost_limit=100.0,  # High enough to not interfere
        mode="yolo",
    )

    # Mock input() to provide new limits multiple times
    # First limit increase: step_limit=2, then step_limit=10 when exceeded again
    with patch("builtins.input", side_effect=["2", "100.0", "10", "100.0"]):
        with patch("minisweagent.agents.interactive.prompt_session.prompt", side_effect=[""]):  # No new task
            with patch("minisweagent.agents.interactive.console.print"):
                exit_status, result = agent.run("Test multiple limit increases")

    assert exit_status == "Submitted"
    assert result == "completed after multiple increases\n"
    assert agent.model.n_calls == 5  # Should complete all 5 steps
    assert agent.config.step_limit == 10  # Should have final updated step limit


def test_continue_after_completion_with_new_task():
    """Test that user can provide a new task when agent wants to finish."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[
            "",  # Confirm first action
            "Create a new file",  # Provide new task when agent wants to finish
            "",  # Confirm second action for new task
            "",  # Don't provide another task after second completion (finish)
        ],
    ):
        agent = InteractiveAgent(
            model=DeterministicModel(
                outputs=[
                    "First task\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'first task completed'\n```",
                    "Second task\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'new task completed'\n```",
                ]
            ),
            env=LocalEnvironment(),
        )

        exit_status, result = agent.run("Complete the initial task")
        assert exit_status == "Submitted"
        assert result == "new task completed\n"
        assert agent.model.n_calls == 2
        # Should have the new task message in conversation
        new_task_messages = [
            msg for msg in agent.messages if "The user added a new task: Create a new file" in msg.get("content", "")
        ]
        assert len(new_task_messages) == 1


def test_continue_after_completion_without_new_task():
    """Test that agent finishes normally when user doesn't provide a new task."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[
            "",  # Confirm first action
            "",  # Don't provide new task when agent wants to finish (empty input)
        ],
    ):
        agent = InteractiveAgent(
            model=DeterministicModel(
                outputs=[
                    "Task completion\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'original task completed'\n```"
                ]
            ),
            env=LocalEnvironment(),
        )

        exit_status, result = agent.run("Complete the task")
        assert exit_status == "Submitted"
        assert result == "original task completed\n"
        assert agent.model.n_calls == 1
        # Should not have any new task messages
        new_task_messages = [msg for msg in agent.messages if "The user added a new task" in msg.get("content", "")]
        assert len(new_task_messages) == 0


def test_continue_after_completion_multiple_cycles():
    """Test multiple continuation cycles with new tasks."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[
            "",  # Confirm first action
            "Second task",  # Provide first new task
            "",  # Confirm second action
            "Third task",  # Provide second new task
            "",  # Confirm third action
            "",  # Don't provide another task (finish)
        ],
    ):
        agent = InteractiveAgent(
            model=DeterministicModel(
                outputs=[
                    "First\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'first completed'\n```",
                    "Second\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'second completed'\n```",
                    "Third\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'third completed'\n```",
                ]
            ),
            env=LocalEnvironment(),
        )

        exit_status, result = agent.run("Initial task")
        assert exit_status == "Submitted"
        assert result == "third completed\n"
        assert agent.model.n_calls == 3
        # Should have both new task messages
        new_task_messages = [msg for msg in agent.messages if "The user added a new task" in msg.get("content", "")]
        assert len(new_task_messages) == 2
        assert "Second task" in new_task_messages[0]["content"]
        assert "Third task" in new_task_messages[1]["content"]


def test_continue_after_completion_in_yolo_mode():
    """Test continuation when starting in yolo mode (no confirmations needed)."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[
            "Create a second task",  # Provide new task when agent wants to finish
            "",  # Don't provide another task after second completion (finish)
        ],
    ):
        agent = InteractiveAgent(
            model=DeterministicModel(
                outputs=[
                    "First\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'first completed'\n```",
                    "Second\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'second task completed'\n```",
                ]
            ),
            env=LocalEnvironment(),
            mode="yolo",  # Start in yolo mode
        )

        exit_status, result = agent.run("Initial task")
        assert exit_status == "Submitted"
        assert result == "second task completed\n"
        assert agent.config.mode == "yolo"
        assert agent.model.n_calls == 2
        # Should have the new task message
        new_task_messages = [msg for msg in agent.messages if "Create a second task" in msg.get("content", "")]
        assert len(new_task_messages) == 1


def test_confirm_exit_enabled_asks_for_confirmation():
    """Test that when confirm_exit=True, agent asks for confirmation before finishing."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=["", ""],  # Confirm action, then no new task (empty string to exit)
    ):
        agent = InteractiveAgent(
            model=DeterministicModel(
                outputs=["Finishing\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'completed'\n```"]
            ),
            env=LocalEnvironment(),
            confirm_exit=True,  # Should ask for confirmation
        )

        exit_status, result = agent.run("Test confirm exit enabled")
        assert exit_status == "Submitted"
        assert result == "completed\n"
        assert agent.model.n_calls == 1


def test_confirm_exit_disabled_exits_immediately():
    """Test that when confirm_exit=False, agent exits immediately without asking."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[""],  # Only confirm action, no exit confirmation needed
    ):
        agent = InteractiveAgent(
            model=DeterministicModel(
                outputs=["Finishing\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'completed'\n```"]
            ),
            env=LocalEnvironment(),
            confirm_exit=False,  # Should NOT ask for confirmation
        )

        exit_status, result = agent.run("Test confirm exit disabled")
        assert exit_status == "Submitted"
        assert result == "completed\n"
        assert agent.model.n_calls == 1


def test_confirm_exit_with_new_task_continues_execution():
    """Test that when user provides new task at exit confirmation, agent continues."""
    with patch(
        "minisweagent.agents.interactive.prompt_session.prompt",
        side_effect=[
            "",  # Confirm first action
            "Please do one more thing",  # Provide new task instead of exiting
            "",  # Confirm second action
            "",  # No new task on second exit confirmation
        ],
    ):
        agent = InteractiveAgent(
            model=DeterministicModel(
                outputs=[
                    "First task\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'first done'\n```",
                    "Additional task\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\necho 'additional done'\n```",
                ]
            ),
            env=LocalEnvironment(),
            confirm_exit=True,
        )

        exit_status, result = agent.run("Test exit with new task")
        assert exit_status == "Submitted"
        assert result == "additional done\n"
        assert agent.model.n_calls == 2
        # Check that the new task was added to the conversation
        new_task_messages = [msg for msg in agent.messages if "Please do one more thing" in msg.get("content", "")]
        assert len(new_task_messages) == 1


def test_confirm_exit_config_field_defaults():
    """Test that confirm_exit field has correct default value."""
    agent = InteractiveAgent(
        model=DeterministicModel(outputs=[]),
        env=LocalEnvironment(),
    )
    # Default should be True
    assert agent.config.confirm_exit is True


def test_confirm_exit_config_field_can_be_set():
    """Test that confirm_exit field can be explicitly set."""
    agent_with_confirm = InteractiveAgent(
        model=DeterministicModel(outputs=[]),
        env=LocalEnvironment(),
        confirm_exit=True,
    )
    assert agent_with_confirm.config.confirm_exit is True

    agent_without_confirm = InteractiveAgent(
        model=DeterministicModel(outputs=[]),
        env=LocalEnvironment(),
        confirm_exit=False,
    )
    assert agent_without_confirm.config.confirm_exit is False



================================================
FILE: tests/agents/test_interactive_textual.py
================================================
import asyncio
import logging
import threading
from unittest.mock import Mock

import pytest

from minisweagent.agents.interactive_textual import AddLogEmitCallback, SmartInputContainer, TextualAgent
from minisweagent.environments.local import LocalEnvironment
from minisweagent.models.test_models import DeterministicModel


def get_screen_text(app: TextualAgent) -> str:
    """Extract all text content from the app's UI."""
    text_parts = [app.title]

    # Get all Static widgets in the main content container
    content_container = app.query_one("#content")
    for static_widget in content_container.query("Static"):
        if static_widget.display:
            if hasattr(static_widget, "renderable") and static_widget.renderable:  # type: ignore[attr-defined]
                text_parts.append(str(static_widget.renderable))  # type: ignore[attr-defined]

    # Also check the input container if it's visible
    if app.input_container.display:
        for static_widget in app.input_container.query("Static"):
            if static_widget.display:
                if hasattr(static_widget, "renderable") and static_widget.renderable:  # type: ignore[attr-defined]
                    text_parts.append(str(static_widget.renderable))  # type: ignore[attr-defined]

    return "\n".join(text_parts)


async def type_text(pilot, text: str):
    """Type text character by character using pilot.press() to simulate real user input.

    This properly tests focus behavior and input handling, unlike setting .value directly.
    """
    for char in text:
        # Handle special characters that need key names instead of character literals
        if char == " ":
            await pilot.press("space")
        elif char == "\n":
            await pilot.press("enter")
        elif char == "\t":
            await pilot.press("tab")
        else:
            # For regular characters, pilot.press() can handle them directly
            await pilot.press(char)


@pytest.mark.slow
async def test_everything_integration_test():
    app = TextualAgent(
        model=DeterministicModel(
            outputs=[
                "/sleep 0.5",
                "THOUGHTT 1\n ```bash\necho '1'\n```",  # step 2
                "THOUGHTT 2\n ```bash\necho '2'\n```",  # step 3
                "THOUGHTT 3\n ```bash\necho '3'\n```",  # step 4
                "THOUGHTT 4\n ```bash\necho '4'\n```",  # step 5
                "THOUGHTT 5\n ```bash\necho '5'\n```",  # step 6
                "THOUGHTT 6\n ```bash\necho '6'\n```",  # step 7
                "FINISHING\n ```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\n```",
                "FINISHING2\n ```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\n```",
            ],
        ),
        env=LocalEnvironment(),
        mode="confirm",
        cost_limit=10.0,
    )
    assert app.agent.config.confirm_exit
    async with app.run_test() as pilot:
        # Start the agent with the task
        threading.Thread(target=lambda: app.agent.run("What's up?"), daemon=True).start()
        await pilot.pause(0.2)
        assert app.agent_state == "RUNNING"
        assert "You are a helpful assistant that can do anything." in get_screen_text(app)
        assert "press enter" not in get_screen_text(app).lower()
        assert "Step 1/1" in app.title

        print(">>> Agent autoforwards -> step 2, then waiting for input")
        await pilot.pause(0.7)
        assert "Step 2/2" in app.title
        assert app.agent_state == "AWAITING_INPUT"
        assert "AWAITING_INPUT" in app.title
        assert "echo '1'" in get_screen_text(app)
        assert "press enter to confirm or provide rejection reason" in get_screen_text(app).lower()

        print(">>> Confirm directly with enter first and we move on to page 3")
        print(get_screen_text(app))
        await pilot.press("enter")
        await pilot.pause(0.5)
        print("---")
        print(get_screen_text(app))
        print("--- if we didn't follow, here's some cluses")
        print(f"{pilot.app.i_step=}, {pilot.app.n_steps=}, {pilot.app._vscroll.scroll_target_y=}")  # type: ignore
        assert "Step 3/3" in app.title

        print(">>> Now, let's navigate to page 1")
        await pilot.press("escape")  # unfocus from the confirmation input
        await pilot.press("h")  # --> 2/3
        await pilot.press("h")
        assert "Step 1/3" in app.title
        assert "You are a helpful assistant that can do anything." in get_screen_text(app)
        assert "press enter" not in get_screen_text(app).lower()
        await pilot.press("h")
        # should remain on same page
        assert "Step 1/3" in app.title
        assert "You are a helpful assistant that can do anything." in get_screen_text(app)

        print(">>> Back to current latest page, because we're stilling waiting for confirmation")
        await pilot.press("l")  # no need for escape, because confirmation is only on last page
        assert "Step 2/3" in app.title
        await pilot.press("l")  # no need for escape, because confirmation is only on last page
        assert "Step 3/3" in app.title
        assert "AWAITING_INPUT" in app.title
        assert "echo '2'" in get_screen_text(app)

        print(">>> Reject with message - type rejection reason and submit")
        await type_text(pilot, "Not safe to execute")
        await pilot.press("enter")
        print(get_screen_text(app))
        await pilot.pause(0.3)
        assert "Step 4/4" in app.title
        assert "echo '3'" in get_screen_text(app)

        print(">>> Reject with message multiline input")
        await pilot.press("ctrl+t")
        await type_text(pilot, "Not safe to execute\n")
        await pilot.press("ctrl+d")
        print(get_screen_text(app))
        await pilot.pause(0.3)
        assert "Step 5/5" in app.title
        assert "echo '4'" in get_screen_text(app)

        print(">>> Switch tohuman mode")
        await pilot.press("escape")
        await pilot.press("u")

        assert pilot.app.agent.config.mode == "human"  # type: ignore[attr-defined]
        await pilot.pause(0.2)
        print(get_screen_text(app))
        assert "User switched to manual mode, this command will be ignored" in get_screen_text(app)
        assert "Enter your command" in get_screen_text(app)
        assert "Step 5/5" in app.title  # we didn't move because waiting for human command

        print(">>> Human gives command")
        await type_text(pilot, "echo 'human'")
        await pilot.press("enter")
        await pilot.pause(0.2)
        print(get_screen_text(app))
        assert "Step 6/6" in app.title
        assert "human" in get_screen_text(app)  # show the observation

        print(">>> Enter yolo mode & confirm")
        await pilot.press("escape")
        assert pilot.app.agent.config.mode == "human"  # type: ignore[attr-defined]
        await pilot.press("y")
        # Note that this will add one step, because we're basically now executing an empty human action
        assert pilot.app.agent.config.mode == "yolo"  # type: ignore[attr-defined]
        # await pilot.press("enter")  # still need to confirm once for step 3
        # next action will be executed automatically, so we see step 6 next
        await pilot.pause(0.2)
        assert "Step 10/10" in app.title
        assert "echo 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'" in get_screen_text(app)
        # await pilot.pause(0.1)
        # assert "press enter" not in get_screen_text(app).lower()
        print(get_screen_text(app))
        assert "AWAITING_INPUT" in app.title  # still waiting for confirmation of exit

        print(">>> Directly navigate to step 1")
        await pilot.press("escape")
        await pilot.press("0")
        assert "Step 1/10" in app.title
        assert "You are a helpful assistant that can do anything." in get_screen_text(app)

        print(">>> Directly navigate to step 9")
        await pilot.press("$")
        assert "Step 10/10" in app.title
        assert "COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT" in get_screen_text(app)

        print(">>> Give it a new task")
        assert "to give it a new task" in get_screen_text(app).lower()
        await type_text(pilot, "New task")
        await pilot.press("enter")
        await pilot.pause(0.2)

        print(">>> Exit confirmation should appear again")
        assert "Step 11/11" in app.title
        # assert "New task" in get_screen_text(app)
        assert "to give it a new task" in get_screen_text(app).lower()
        await pilot.press("enter")


def test_messages_to_steps_edge_cases():
    """Test the _messages_to_steps function with various edge cases."""
    from minisweagent.agents.interactive_textual import _messages_to_steps

    # Empty messages
    assert _messages_to_steps([]) == []

    # Single system message
    messages = [{"role": "system", "content": "Hello"}]
    assert _messages_to_steps(messages) == [messages]

    # User message ends a step
    messages = [
        {"role": "system", "content": "System"},
        {"role": "assistant", "content": "Assistant"},
        {"role": "user", "content": "User1"},
        {"role": "assistant", "content": "Assistant2"},
        {"role": "user", "content": "User2"},
    ]
    expected = [
        [
            {"role": "system", "content": "System"},
            {"role": "assistant", "content": "Assistant"},
            {"role": "user", "content": "User1"},
        ],
        [{"role": "assistant", "content": "Assistant2"}, {"role": "user", "content": "User2"}],
    ]
    assert _messages_to_steps(messages) == expected

    # No user messages (incomplete step)
    messages = [
        {"role": "system", "content": "System"},
        {"role": "assistant", "content": "Assistant"},
    ]
    expected = [messages]
    assert _messages_to_steps(messages) == expected


async def test_empty_agent_content():
    """Test app behavior with no messages."""
    app = TextualAgent(
        model=DeterministicModel(outputs=[]),
        env=LocalEnvironment(),
        mode="yolo",
    )
    async with app.run_test() as pilot:
        # Start the agent with the task
        threading.Thread(target=lambda: app.agent.run("Empty test"), daemon=True).start()
        # Initially should show waiting message
        await pilot.pause(0.1)
        content = get_screen_text(app)
        assert "Waiting for agent to start" in content or "You are a helpful assistant" in content


async def test_log_message_filtering():
    """Test that warning and error log messages trigger notifications."""
    app = TextualAgent(
        model=DeterministicModel(
            outputs=[
                "/warning Test warning message",
                "Normal response",
                "end: \n```bash\necho COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT\n```",
            ]
        ),
        env=LocalEnvironment(),
        mode="yolo",
    )

    # Mock the notify method to capture calls
    app.notify = Mock()

    async with app.run_test() as pilot:
        # Start the agent with the task
        threading.Thread(target=lambda: app.agent.run("Log test"), daemon=True).start()
        await pilot.pause(0.2)

        # Verify warning was emitted and handled (note the extra space in the actual format)
        app.notify.assert_any_call("[WARNING]  Test warning message", severity="warning")


async def test_list_content_rendering():
    """Test rendering of messages with list content vs string content."""
    # Create a model that will add messages with list content
    app = TextualAgent(
        model=DeterministicModel(
            outputs=["Simple response\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\n```"]
        ),
        env=LocalEnvironment(),
        mode="yolo",
    )

    async with app.run_test() as pilot:
        # Start the agent with the task
        threading.Thread(target=lambda: app.agent.run("Content test"), daemon=True).start()
        # Wait for the agent to finish its normal operation
        await pilot.pause(0.2)

        # Now manually add a message with list content to test rendering
        app.agent.messages.append({"role": "assistant", "content": [{"text": "Line 1"}, {"text": "Line 2"}]})

        # Trigger the message update logic to refresh step count and navigate to last step
        app.on_message_added()

        # Navigate to the last step to see our new message
        app.action_last_step()

        assert "Line 1\nLine 2" in get_screen_text(app)


async def test_confirmation_rejection_with_message():
    """Test rejecting an action with a custom message."""
    app = TextualAgent(
        model=DeterministicModel(outputs=["Test thought\n```bash\necho 'test'\n```"]),
        env=LocalEnvironment(),
        mode="confirm",
    )

    async with app.run_test() as pilot:
        # Start the agent with the task
        threading.Thread(target=lambda: app.agent.run("Rejection test"), daemon=True).start()
        await pilot.pause(0.1)

        # Wait for input prompt
        while app.agent_state != "AWAITING_INPUT":
            await pilot.pause(0.1)

        # Type rejection message and submit
        await type_text(pilot, "Not safe to run")
        await pilot.press("enter")
        await pilot.pause(0.1)

        # Verify the command was rejected with the message
        assert "Command not executed: Not safe to run" in get_screen_text(app)


async def test_agent_with_cost_limit():
    """Test agent behavior when cost limit is exceeded."""
    app = TextualAgent(
        model=DeterministicModel(outputs=["Response 1", "Response 2"]),
        env=LocalEnvironment(),
        mode="yolo",
        cost_limit=0.01,  # Very low limit
    )

    app.notify = Mock()

    async with app.run_test() as pilot:
        threading.Thread(target=lambda: app.agent.run("Cost limit test"), daemon=True).start()
        await pilot.pause(0.5)

        # Should eventually stop due to cost limit and notify with the exit status
        assert app.agent_state == "STOPPED"
        app.notify.assert_called_with("Agent finished with status: LimitsExceeded")


async def test_agent_with_step_limit():
    """Test agent behavior when step limit is exceeded."""
    app = TextualAgent(
        model=DeterministicModel(outputs=["Response 1", "Response 2", "Response 3"]),
        env=LocalEnvironment(),
        mode="yolo",
        step_limit=2,
    )

    app.notify = Mock()
    async with app.run_test() as pilot:
        # Start the agent with the task
        threading.Thread(target=lambda: app.agent.run("Step limit test"), daemon=True).start()
        await pilot.pause(0.5)
        assert app.agent_state == "STOPPED"
        app.notify.assert_called_with("Agent finished with status: LimitsExceeded")


async def test_whitelist_actions_bypass_confirmation():
    """Test that whitelisted actions bypass confirmation."""
    app = TextualAgent(
        model=DeterministicModel(
            outputs=["Whitelisted action\n```bash\necho 'safe' && echo 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\n```"]
        ),
        env=LocalEnvironment(),
        mode="confirm",
        whitelist_actions=[r"echo.*"],
    )

    async with app.run_test() as pilot:
        # Start the agent with the task
        threading.Thread(target=lambda: app.agent.run("Whitelist test"), daemon=True).start()
        await pilot.pause(0.2)

        # Should execute without confirmation because echo is whitelisted
        assert app.agent_state != "AWAITING_INPUT"
        assert "echo 'safe'" in get_screen_text(app)


async def test_input_container_multiple_actions():
    """Test input container handling multiple actions in sequence."""
    app = TextualAgent(
        model=DeterministicModel(
            outputs=[
                "First action\n```bash\necho '1'\n```",
                "Second action\n```bash\necho '2' && echo 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\n```",
            ]
        ),
        env=LocalEnvironment(),
        mode="confirm",
    )

    async with app.run_test() as pilot:
        # Start the agent with the task
        threading.Thread(target=lambda: app.agent.run("Multiple actions test"), daemon=True).start()
        await pilot.pause(0.1)

        # Confirm first action
        while app.agent_state != "AWAITING_INPUT":
            await pilot.pause(0.1)
        assert "echo '1'" in get_screen_text(app)
        await pilot.press("enter")

        # Wait for and confirm second action
        await pilot.pause(0.1)
        while app.agent_state != "AWAITING_INPUT":
            await pilot.pause(0.1)
        assert "echo '2'" in get_screen_text(app)
        await pilot.press("enter")


def test_log_handler_cleanup():
    """Test that log handler is properly cleaned up."""
    initial_handlers = len(logging.getLogger().handlers)

    app = TextualAgent(
        model=DeterministicModel(
            outputs=["Simple response\n```bash\necho 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\n```"]
        ),
        env=LocalEnvironment(),
        mode="yolo",
    )

    # Handler should be added
    assert len(logging.getLogger().handlers) == initial_handlers + 1

    # Simulate unmount
    app.on_unmount()

    # Handler should be removed
    assert len(logging.getLogger().handlers) == initial_handlers


def test_add_log_emit_callback():
    """Test the AddLogEmitCallback handler directly."""

    callback_called = False
    test_record = None

    def test_callback(record):
        nonlocal callback_called, test_record
        callback_called = True
        test_record = record

    handler = AddLogEmitCallback(test_callback)

    # Create a log record
    record = logging.LogRecord(
        name="test", level=logging.WARNING, pathname="test.py", lineno=1, msg="Test message", args=(), exc_info=None
    )

    handler.emit(record)

    assert callback_called
    assert test_record == record


async def test_yolo_mode_confirms_pending_action():
    """Test that pressing 'y' to switch to YOLO mode also confirms any pending action."""
    app = TextualAgent(
        model=DeterministicModel(
            outputs=[
                "Action requiring confirmation\n```bash\necho 'test' && echo 'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'\n```",
            ]
        ),
        env=LocalEnvironment(),
        mode="confirm",
    )

    async with app.run_test() as pilot:
        # Start the agent with the task
        threading.Thread(target=lambda: app.agent.run("YOLO confirmation test"), daemon=True).start()
        await pilot.pause(0.1)

        # Wait for input prompt
        while app.agent_state != "AWAITING_INPUT":
            await pilot.pause(0.1)

        # Verify we're in confirm mode and awaiting input
        assert app.agent.config.mode == "confirm"
        assert app.agent_state == "AWAITING_INPUT"
        assert "echo 'test'" in get_screen_text(app)
        assert "press enter to confirm or provide rejection reason" in get_screen_text(app).lower()

        # Press 'y' to switch to YOLO mode - first escape from input focus
        await pilot.press("escape")
        await pilot.press("y")
        await pilot.pause(0.1)

        # Verify mode changed to yolo
        assert app.agent.config.mode == "yolo"

        # Since we escaped from input first, we still need to confirm the action
        # Navigate back to the input step and confirm
        await pilot.press("$")  # Go to last step
        if app.agent_state == "AWAITING_INPUT":
            await pilot.press("enter")  # Confirm the action
            await pilot.pause(0.1)


# ===== SmartInputContainer Unit Tests =====

from textual.app import App
from textual.containers import Container


class DummyTestApp(App):
    """Minimal test app for providing Textual context."""

    def __init__(self):
        super().__init__()
        self.agent_state = "RUNNING"
        self.call_from_thread = Mock()
        self.update_content = Mock()
        self.set_focus = Mock()
        self._vscroll = Mock()
        self._vscroll.scroll_y = 0

    def compose(self):
        yield Container()


def create_mock_smart_input_container(app):
    """Create SmartInputContainer with proper mocking to avoid type issues."""
    from typing import cast

    # Create actual SmartInputContainer instance but use typing.cast to bypass type check
    return SmartInputContainer(cast("TextualAgent", app))  # type: ignore


async def test_smart_input_container_initialization():
    """Test SmartInputContainer initialization and default state."""
    app = DummyTestApp()
    async with app.run_test():
        container = create_mock_smart_input_container(app)

        assert container._app == app
        assert container._multiline_mode is False
        assert container.can_focus is True
        assert container.display is False
        assert container.pending_prompt is None
        assert container._input_result is None
        assert isinstance(container._input_event, threading.Event)


async def test_smart_input_container_request_input():
    """Test request_input method behavior."""
    app = DummyTestApp()
    async with app.run_test():
        container = create_mock_smart_input_container(app)

        # Start request_input in a thread since it blocks
        test_prompt = "Test prompt"
        result_container = {}

        def request_thread():
            result_container["result"] = container.request_input(test_prompt)

        thread = threading.Thread(target=request_thread)
        thread.start()

        # Give thread time to start and set up
        await asyncio.sleep(0.1)

        # Check that prompt was set
        assert container.pending_prompt == test_prompt
        assert app.call_from_thread.called

        # Complete the input with empty string (confirmation)
        container._complete_input("")

        # Wait for thread to complete
        thread.join(timeout=1)

        # Check results
        assert result_container["result"] == ""
        assert container.pending_prompt is None
        assert container.display is False


async def test_smart_input_container_complete_input():
    """Test _complete_input method resets state correctly."""
    app = DummyTestApp()
    async with app.run_test():
        container = SmartInputContainer(app)

        # Set up initial state
        container.pending_prompt = "Test prompt"
        container._multiline_mode = True
        container.display = True
        container._single_input.value = "test"
        container._multi_input.text = "test\nmultiline"

        # Complete input
        test_result = "User input result"
        container._complete_input(test_result)

        # Check state reset
        assert container._input_result == test_result
        assert container.pending_prompt is None
        assert container.display is False
        assert container._single_input.value == ""
        assert container._multi_input.text == ""
        assert container._multiline_mode is False
        assert app.agent_state == "RUNNING"
        assert app.update_content.called
        assert app._vscroll.scroll_y == 0


async def test_smart_input_container_toggle_mode():
    """Test switching from single-line to multi-line mode."""
    app = DummyTestApp()
    async with app.run_test():
        container = SmartInputContainer(app)

        # Set up single-line mode with content
        container.pending_prompt = "Test prompt"
        container._multiline_mode = False
        container._single_input.value = "test input"

        # Mock focus method
        container.on_focus = Mock()

        # Toggle to multiline mode
        container.action_toggle_mode()

        # Check mode changed
        assert container._multiline_mode is True
        assert container.on_focus.called


async def test_smart_input_container_toggle_mode_blocked():
    """Test that toggle mode is blocked when no pending prompt or already in multiline."""
    app = DummyTestApp()
    async with app.run_test():
        container = SmartInputContainer(app)

        # Test with no pending prompt
        container.pending_prompt = None
        initial_mode = container._multiline_mode
        container.action_toggle_mode()
        assert container._multiline_mode == initial_mode

        # Test when already in multiline mode
        container.pending_prompt = "Test prompt"
        container._multiline_mode = True
        container.action_toggle_mode()
        assert container._multiline_mode is True


async def test_smart_input_container_single_input_submission():
    """Test single-line input submission."""
    app = DummyTestApp()
    async with app.run_test():
        container = SmartInputContainer(app)

        # Set up for single-line mode
        container._multiline_mode = False
        container.pending_prompt = "Test prompt"

        # Mock the complete_input method
        container._complete_input = Mock()

        # Create mock input event
        mock_input = Mock()
        mock_input.id = "single-input"
        mock_input.value = "  test input  "

        mock_event = Mock()
        mock_event.input = mock_input

        # Trigger submission
        container.on_input_submitted(mock_event)

        # Check that complete_input was called with stripped text
        container._complete_input.assert_called_once_with("test input")


async def test_smart_input_container_single_input_submission_multiline_mode():
    """Test that single-line submission is ignored in multiline mode."""
    app = DummyTestApp()
    async with app.run_test():
        container = SmartInputContainer(app)

        # Set up for multiline mode
        container._multiline_mode = True
        container._complete_input = Mock()

        # Create mock input event
        mock_input = Mock()
        mock_input.id = "single-input"
        mock_input.value = "test input"

        mock_event = Mock()
        mock_event.input = mock_input

        # Trigger submission
        container.on_input_submitted(mock_event)

        # Check that complete_input was NOT called
        container._complete_input.assert_not_called()


async def test_smart_input_container_key_events():
    """Test key event handling for various scenarios."""
    app = DummyTestApp()
    async with app.run_test():
        container = SmartInputContainer(app)

        # Mock methods
        container.action_toggle_mode = Mock()
        container._complete_input = Mock()

        # Test Ctrl+T in single-line mode
        container._multiline_mode = False
        mock_event = Mock()
        mock_event.key = "ctrl+t"
        mock_event.prevent_default = Mock()

        container.on_key(mock_event)

        assert mock_event.prevent_default.called
        assert container.action_toggle_mode.called

        # Reset mocks
        container.action_toggle_mode.reset_mock()
        mock_event.prevent_default.reset_mock()

        # Test Ctrl+D in multiline mode
        container._multiline_mode = True
        container._multi_input.text = "  multiline\ntext  "
        mock_event.key = "ctrl+d"

        container.on_key(mock_event)

        assert mock_event.prevent_default.called
        container._complete_input.assert_called_once_with("multiline\ntext")

        # Reset mocks
        container._complete_input.reset_mock()
        mock_event.prevent_default.reset_mock()

        # Test Escape key
        mock_event.key = "escape"

        container.on_key(mock_event)

        assert mock_event.prevent_default.called
        assert container.can_focus is False
        app.set_focus.assert_called_once_with(None)


async def test_smart_input_container_key_events_no_action():
    """Test key events that should not trigger any action."""
    app = DummyTestApp()
    async with app.run_test():
        container = SmartInputContainer(app)

        # Mock methods
        container.action_toggle_mode = Mock()
        container._complete_input = Mock()

        # Test Ctrl+T in multiline mode (should not toggle)
        container._multiline_mode = True
        mock_event = Mock()
        mock_event.key = "ctrl+t"
        mock_event.prevent_default = Mock()

        container.on_key(mock_event)

        # Should not prevent default or call toggle
        assert not mock_event.prevent_default.called
        assert not container.action_toggle_mode.called

        # Test Ctrl+D in single-line mode (should not complete)
        container._multiline_mode = False
        mock_event.key = "ctrl+d"
        mock_event.prevent_default.reset_mock()

        container.on_key(mock_event)

        # Should not prevent default or complete input
        assert not mock_event.prevent_default.called
        assert not container._complete_input.called


async def test_smart_input_container_on_focus():
    """Test focus behavior in different modes."""
    app = DummyTestApp()
    async with app.run_test():
        container = SmartInputContainer(app)

        # Mock focus methods
        container._single_input.focus = Mock()
        container._multi_input.focus = Mock()

        # Test focus in single-line mode
        container._multiline_mode = False
        container.on_focus()

        assert container._single_input.focus.called
        assert not container._multi_input.focus.called

        # Reset and test focus in multiline mode
        container._single_input.focus.reset_mock()
        container._multi_input.focus.reset_mock()
        container._multiline_mode = True
        container.on_focus()

        assert not container._single_input.focus.called
        assert container._multi_input.focus.called


async def test_smart_input_container_on_mount():
    """Test widget initialization on mount."""
    app = DummyTestApp()
    async with app.run_test():
        container = SmartInputContainer(app)

        # Mock update method
        container._update_mode_display = Mock()

        # Trigger mount
        container.on_mount()

        # Check initialization
        assert container._multi_input.display is False
        assert container._update_mode_display.called



================================================
FILE: tests/config/__init__.py
================================================
[Empty file]


================================================
FILE: tests/config/test_swebench_template.py
================================================
from dataclasses import dataclass
from pathlib import Path

import yaml
from jinja2 import Template


@dataclass
class MockOutput:
    """Mock output object for testing the template"""

    returncode: int
    output: str


def test_action_observation_template_short_output():
    """Test that short output (< 10000 chars) is displayed in full"""
    # Load the swebench config
    config_path = Path(__file__).parent.parent.parent / "src" / "minisweagent" / "config" / "extra" / "swebench.yaml"
    with open(config_path) as f:
        config = yaml.safe_load(f)

    # Extract the template
    template_str = config["agent"]["action_observation_template"]
    template = Template(template_str)

    # Create mock output with short content
    output = MockOutput(returncode=0, output="Success! Operation completed.\nWarning: minor issue")

    # Render the template
    result = template.render(output=output)

    # Verify the result contains all parts and no truncation
    assert "<returncode>" in result
    assert "0" in result
    assert "<output>" in result
    assert "Success! Operation completed." in result
    assert "Warning: minor issue" in result

    # Should not contain truncation elements for short output
    assert "<output_head>" not in result
    assert "<elided_chars>" not in result
    assert "<output_tail>" not in result
    assert "<warning>" not in result


def test_action_observation_template_long_output():
    """Test that long output (> 10000 chars) is truncated with head/tail format"""
    # Load the swebench config
    config_path = Path(__file__).parent.parent.parent / "src" / "minisweagent" / "config" / "extra" / "swebench.yaml"
    with open(config_path) as f:
        config = yaml.safe_load(f)

    # Extract the template
    template_str = config["agent"]["action_observation_template"]
    template = Template(template_str)

    # Create mock output with long content
    long_output = "A" * 8000 + "B" * 3000  # 11000 characters total
    # Total will be > 10000 chars

    output = MockOutput(returncode=1, output=long_output)

    # Render the template
    result = template.render(output=output)

    # Should contain truncation elements for long output
    assert "<warning>" in result
    assert "The output of your last command was too long" in result
    assert "<output_head>" in result
    assert "<elided_chars>" in result
    assert "characters elided" in result
    assert "<output_tail>" in result

    # Should still contain the basic structure
    assert "<returncode>" in result
    assert "1" in result

    # Verify the head contains first part of output
    head_start = result.find("<output_head>")
    head_end = result.find("</output_head>")
    head_content = result[head_start:head_end]
    assert "AAAA" in head_content  # Should contain start of output

    # Verify the tail contains last part of output
    tail_start = result.find("<output_tail>")
    tail_end = result.find("</output_tail>")
    tail_content = result[tail_start:tail_end]
    assert "BBBB" in tail_content  # Should contain end of output


def test_action_observation_template_edge_case_exactly_10000_chars():
    """Test the boundary case where output is around 10000 characters"""
    # Load the swebench config
    config_path = Path(__file__).parent.parent.parent / "src" / "minisweagent" / "config" / "extra" / "swebench.yaml"
    with open(config_path) as f:
        config = yaml.safe_load(f)

    # Extract the template
    template_str = config["agent"]["action_observation_template"]
    template = Template(template_str)

    # Use a large amount of data that will definitely exceed 10000 chars when rendered
    output = MockOutput(returncode=0, output="X" * 10000)

    # Render the template
    result = template.render(output=output)

    # Should use truncated format for large output
    assert "<output_head>" in result
    assert "<elided_chars>" in result
    assert "<output_tail>" in result
    assert "<warning>" in result
    # The X's should still be present in head or tail
    assert "XXXX" in result


def test_action_observation_template_just_under_10000_chars():
    """Test that smaller output shows full output without truncation"""
    # Load the swebench config
    config_path = Path(__file__).parent.parent.parent / "src" / "minisweagent" / "config" / "extra" / "swebench.yaml"
    with open(config_path) as f:
        config = yaml.safe_load(f)

    # Extract the template
    template_str = config["agent"]["action_observation_template"]
    template = Template(template_str)

    # Use a reasonably sized output that should be well under 10000 chars when rendered
    output = MockOutput(returncode=0, output="Y" * 8000)

    # Render the template
    result = template.render(output=output)

    # Should show full output without truncation
    assert "<output_head>" not in result
    assert "<elided_chars>" not in result
    assert "<output_tail>" not in result
    assert "<warning>" not in result
    assert "Y" * 8000 in result



================================================
FILE: tests/environments/__init__.py
================================================
[Empty file]


================================================
FILE: tests/environments/test_docker.py
================================================
import os
import subprocess
from unittest.mock import patch

import pytest

from minisweagent.environments.docker import DockerEnvironment, DockerEnvironmentConfig


def is_docker_available():
    """Check if Docker is available and running."""
    try:
        subprocess.run(["docker", "version"], capture_output=True, check=True, timeout=5)
        return True
    except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
        return False


def is_podman_available():
    """Check if Podman is available and running."""
    try:
        subprocess.run(["podman", "version"], capture_output=True, check=True, timeout=5)
        return True
    except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
        return False


# Test parameters for both Docker and Podman
environment_params = [
    pytest.param(
        "docker",
        marks=pytest.mark.skipif(not is_docker_available(), reason="Docker not available"),
        id="docker",
    ),
    pytest.param(
        "podman",
        marks=pytest.mark.skipif(not is_podman_available(), reason="Podman not available"),
        id="podman",
    ),
]


@pytest.mark.parametrize("executable", environment_params)
def test_docker_environment_config_defaults(executable):
    """Test that DockerEnvironmentConfig has correct default values."""
    config = DockerEnvironmentConfig(image="python:3.11", executable=executable)

    assert config.image == "python:3.11"
    assert config.cwd == "/"
    assert config.env == {}
    assert config.forward_env == []
    assert config.timeout == 30
    assert config.executable == executable


@pytest.mark.slow
@pytest.mark.parametrize("executable", environment_params)
def test_docker_environment_basic_execution(executable):
    """Test basic command execution in Docker container."""
    env = DockerEnvironment(image="python:3.11", executable=executable)

    try:
        result = env.execute("echo 'hello world'")
        assert result["returncode"] == 0
        assert "hello world" in result["output"]
    finally:
        env.cleanup()


@pytest.mark.slow
@pytest.mark.parametrize("executable", environment_params)
def test_docker_environment_set_env_variables(executable):
    """Test setting environment variables in the container."""
    env = DockerEnvironment(
        image="python:3.11", executable=executable, env={"TEST_VAR": "test_value", "ANOTHER_VAR": "another_value"}
    )

    try:
        # Test single environment variable
        result = env.execute("echo $TEST_VAR")
        assert result["returncode"] == 0
        assert "test_value" in result["output"]

        # Test multiple environment variables
        result = env.execute("echo $TEST_VAR $ANOTHER_VAR")
        assert result["returncode"] == 0
        assert "test_value another_value" in result["output"]
    finally:
        env.cleanup()


@pytest.mark.slow
@pytest.mark.parametrize("executable", environment_params)
def test_docker_environment_forward_env_variables(executable):
    """Test forwarding environment variables from host to container."""
    with patch.dict(os.environ, {"HOST_VAR": "host_value", "ANOTHER_HOST_VAR": "another_host_value"}):
        env = DockerEnvironment(
            image="python:3.11", executable=executable, forward_env=["HOST_VAR", "ANOTHER_HOST_VAR"]
        )

        try:
            # Test single forwarded environment variable
            result = env.execute("echo $HOST_VAR")
            assert result["returncode"] == 0
            assert "host_value" in result["output"]

            # Test multiple forwarded environment variables
            result = env.execute("echo $HOST_VAR $ANOTHER_HOST_VAR")
            assert result["returncode"] == 0
            assert "host_value another_host_value" in result["output"]
        finally:
            env.cleanup()


@pytest.mark.slow
@pytest.mark.parametrize("executable", environment_params)
def test_docker_environment_forward_nonexistent_env_variables(executable):
    """Test forwarding non-existent environment variables (should be empty)."""
    env = DockerEnvironment(image="python:3.11", executable=executable, forward_env=["NONEXISTENT_VAR"])

    try:
        result = env.execute('echo "[$NONEXISTENT_VAR]"')
        assert result["returncode"] == 0
        assert "[]" in result["output"]  # Empty variable should result in empty string
    finally:
        env.cleanup()


@pytest.mark.slow
@pytest.mark.parametrize("executable", environment_params)
def test_docker_environment_combined_env_and_forward(executable):
    """Test both setting and forwarding environment variables together."""
    with patch.dict(os.environ, {"HOST_VAR": "from_host"}):
        env = DockerEnvironment(
            image="python:3.11", executable=executable, env={"SET_VAR": "from_config"}, forward_env=["HOST_VAR"]
        )

        try:
            result = env.execute("echo $SET_VAR $HOST_VAR")
            assert result["returncode"] == 0
            assert "from_config from_host" in result["output"]
        finally:
            env.cleanup()


@pytest.mark.slow
@pytest.mark.parametrize("executable", environment_params)
def test_docker_environment_env_override_forward(executable):
    """Test that explicitly set env variables take precedence over forwarded ones."""
    with patch.dict(os.environ, {"CONFLICT_VAR": "from_host"}):
        env = DockerEnvironment(
            image="python:3.11",
            executable=executable,
            env={"CONFLICT_VAR": "from_config"},
            forward_env=["CONFLICT_VAR"],
        )

        try:
            result = env.execute("echo $CONFLICT_VAR")
            assert result["returncode"] == 0
            # The explicitly set env should take precedence (comes first in docker exec command)
            assert "from_config" in result["output"]
        finally:
            env.cleanup()


@pytest.mark.slow
@pytest.mark.parametrize("executable", environment_params)
def test_docker_environment_custom_cwd(executable):
    """Test executing commands in a custom working directory."""
    env = DockerEnvironment(image="python:3.11", executable=executable, cwd="/tmp")

    try:
        result = env.execute("pwd")
        assert result["returncode"] == 0
        assert "/tmp" in result["output"]
    finally:
        env.cleanup()


@pytest.mark.slow
@pytest.mark.parametrize("executable", environment_params)
def test_docker_environment_cwd_parameter_override(executable):
    """Test that the cwd parameter in execute() overrides the config cwd."""
    env = DockerEnvironment(image="python:3.11", executable=executable, cwd="/")

    try:
        result = env.execute("pwd", cwd="/tmp")
        assert result["returncode"] == 0
        assert "/tmp" in result["output"]
    finally:
        env.cleanup()


@pytest.mark.slow
@pytest.mark.parametrize("executable", environment_params)
def test_docker_environment_command_failure(executable):
    """Test that command failures are properly captured."""
    env = DockerEnvironment(image="python:3.11", executable=executable)

    try:
        result = env.execute("exit 42")
        assert result["returncode"] == 42
    finally:
        env.cleanup()


@pytest.mark.slow
@pytest.mark.parametrize("executable", environment_params)
def test_docker_environment_custom_container_timeout(executable):
    """Test that custom container_timeout is respected."""
    import time

    env = DockerEnvironment(image="python:3.11", executable=executable, container_timeout="3s")

    try:
        result = env.execute("echo 'container is running'")
        assert result["returncode"] == 0
        assert "container is running" in result["output"]
        time.sleep(5)
        with pytest.raises((subprocess.CalledProcessError, subprocess.TimeoutExpired)):
            # This command should fail because the container has stopped
            subprocess.run(
                [executable, "exec", env.container_id, "echo", "still running"],
                check=True,
                capture_output=True,
                timeout=2,
            )
    finally:
        env.cleanup()



================================================
FILE: tests/environments/test_init.py
================================================
import pytest

from minisweagent.environments import get_environment_class
from minisweagent.environments.local import LocalEnvironment


class TestGetEnvironmentClass:
    def test_get_environment_class_local_full_path(self):
        """Test that get_environment_class returns LocalEnvironment when given full module path."""
        env_class = get_environment_class("minisweagent.environments.local.LocalEnvironment")
        assert env_class is LocalEnvironment

    def test_get_environment_class_local_shorthand(self):
        """Test that get_environment_class returns LocalEnvironment when given shorthand."""
        env_class = get_environment_class("local")
        assert env_class is LocalEnvironment

    def test_get_environment_class_invalid_spec(self):
        """Test that get_environment_class raises ValueError for invalid spec."""
        with pytest.raises(ValueError, match="Unknown environment type"):
            get_environment_class("invalid_environment")

    def test_get_environment_class_invalid_module(self):
        """Test that get_environment_class raises ValueError for non-existent module."""
        with pytest.raises(ValueError, match="Unknown environment type"):
            get_environment_class("nonexistent.module.Class")



================================================
FILE: tests/environments/test_local.py
================================================
import os
import subprocess
import tempfile
from pathlib import Path
from unittest.mock import patch

import pytest

from minisweagent.environments.local import LocalEnvironment, LocalEnvironmentConfig


def test_local_environment_config_defaults():
    """Test that LocalEnvironmentConfig has correct default values."""
    config = LocalEnvironmentConfig()

    assert config.cwd == ""
    assert config.env == {}
    assert config.timeout == 30


def test_local_environment_basic_execution():
    """Test basic command execution in local environment."""
    env = LocalEnvironment()

    result = env.execute("echo 'hello world'")
    assert result["returncode"] == 0
    assert "hello world" in result["output"]


def test_local_environment_set_env_variables():
    """Test setting environment variables in the local environment."""
    env = LocalEnvironment(env={"TEST_VAR": "test_value", "ANOTHER_VAR": "another_value"})

    # Test single environment variable
    result = env.execute("echo $TEST_VAR")
    assert result["returncode"] == 0
    assert "test_value" in result["output"]

    # Test multiple environment variables
    result = env.execute("echo $TEST_VAR $ANOTHER_VAR")
    assert result["returncode"] == 0
    assert "test_value another_value" in result["output"]


def test_local_environment_existing_env_variables():
    """Test that existing environment variables are preserved and merged."""
    with patch.dict(os.environ, {"EXISTING_VAR": "existing_value"}):
        env = LocalEnvironment(env={"NEW_VAR": "new_value"})

        # Test that both existing and new variables are available
        result = env.execute("echo $EXISTING_VAR $NEW_VAR")
        assert result["returncode"] == 0
        assert "existing_value new_value" in result["output"]


def test_local_environment_env_variable_override():
    """Test that config env variables override existing ones."""
    with patch.dict(os.environ, {"CONFLICT_VAR": "original_value"}):
        env = LocalEnvironment(env={"CONFLICT_VAR": "override_value"})

        result = env.execute("echo $CONFLICT_VAR")
        assert result["returncode"] == 0
        assert "override_value" in result["output"]


def test_local_environment_custom_cwd():
    """Test executing commands in a custom working directory."""
    with tempfile.TemporaryDirectory() as temp_dir:
        env = LocalEnvironment(cwd=temp_dir)

        result = env.execute("pwd")
        assert result["returncode"] == 0
        assert temp_dir in result["output"]


def test_local_environment_cwd_parameter_override():
    """Test that the cwd parameter in execute() overrides the config cwd."""
    with tempfile.TemporaryDirectory() as temp_dir1, tempfile.TemporaryDirectory() as temp_dir2:
        env = LocalEnvironment(cwd=temp_dir1)

        # Execute with different cwd parameter
        result = env.execute("pwd", cwd=temp_dir2)
        assert result["returncode"] == 0
        assert temp_dir2 in result["output"]


def test_local_environment_default_cwd():
    """Test that commands use os.getcwd() when no cwd is specified."""
    env = LocalEnvironment()
    current_dir = os.getcwd()

    result = env.execute("pwd")
    assert result["returncode"] == 0
    assert current_dir in result["output"]


def test_local_environment_command_failure():
    """Test that command failures are properly captured."""
    env = LocalEnvironment()

    result = env.execute("exit 1")
    assert result["returncode"] == 1
    assert result["output"] == ""


def test_local_environment_nonexistent_command():
    """Test execution of non-existent command."""
    env = LocalEnvironment()

    result = env.execute("nonexistent_command_12345")
    assert result["returncode"] != 0
    assert "nonexistent_command_12345" in result["output"] or "command not found" in result["output"]


def test_local_environment_stderr_capture():
    """Test that stderr is properly captured."""
    env = LocalEnvironment()

    result = env.execute("echo 'error message' >&2")
    assert result["returncode"] == 0
    assert "error message" in result["output"]


def test_local_environment_timeout():
    """Test timeout functionality."""
    env = LocalEnvironment(timeout=1)

    with pytest.raises(subprocess.TimeoutExpired):
        env.execute("sleep 2")


def test_local_environment_custom_timeout():
    """Test custom timeout configuration."""
    config = LocalEnvironmentConfig(timeout=5)
    env = LocalEnvironment(**config.__dict__)

    assert env.config.timeout == 5


@pytest.mark.parametrize(
    ("command", "expected_returncode"),
    [
        ("echo 'test'", 0),
        ("exit 1", 1),
        ("exit 42", 42),
    ],
)
def test_local_environment_return_codes(command, expected_returncode):
    """Test that various return codes are properly captured."""
    env = LocalEnvironment()

    result = env.execute(command)
    assert result["returncode"] == expected_returncode


def test_local_environment_multiline_output():
    """Test handling of multiline command output."""
    env = LocalEnvironment()

    result = env.execute("echo -e 'line1\\nline2\\nline3'")
    assert result["returncode"] == 0
    output_lines = result["output"].strip().split("\n")
    assert len(output_lines) == 3
    assert "line1" in output_lines[0]
    assert "line2" in output_lines[1]
    assert "line3" in output_lines[2]


def test_local_environment_file_operations():
    """Test file operations in the local environment."""
    with tempfile.TemporaryDirectory() as temp_dir:
        env = LocalEnvironment(cwd=temp_dir)

        # Create a file
        result = env.execute("echo 'test content' > test.txt")
        assert result["returncode"] == 0

        # Read the file
        result = env.execute("cat test.txt")
        assert result["returncode"] == 0
        assert "test content" in result["output"]

        # Verify file exists
        test_file = Path(temp_dir) / "test.txt"
        assert test_file.exists()
        assert test_file.read_text().strip() == "test content"


def test_local_environment_shell_features():
    """Test that shell features like pipes and redirects work."""
    env = LocalEnvironment()

    # Test pipe
    result = env.execute("echo 'hello world' | grep 'world'")
    assert result["returncode"] == 0
    assert "hello world" in result["output"]

    # Test command substitution
    result = env.execute("echo $(echo 'nested')")
    assert result["returncode"] == 0
    assert "nested" in result["output"]



================================================
FILE: tests/environments/test_singularity.py
================================================
import os
import subprocess
from unittest.mock import patch

import pytest

from minisweagent.environments.singularity import SingularityEnvironment, SingularityEnvironmentConfig


def is_singularity_available():
    """Check if Singularity is available."""
    try:
        subprocess.run(["singularity", "version"], capture_output=True, check=True, timeout=5)
        return True
    except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
        return False


@pytest.mark.skipif(not is_singularity_available(), reason="Singularity not available")
def test_singularity_environment_config_defaults():
    """Test that SingularityEnvironmentConfig has correct default values."""
    config = SingularityEnvironmentConfig(image="python:3.11")

    assert config.image == "python:3.11"
    assert config.cwd == "/"
    assert config.env == {}
    assert config.forward_env == []
    assert config.timeout == 30
    assert config.executable == "singularity"


@pytest.mark.slow
@pytest.mark.skipif(not is_singularity_available(), reason="Singularity not available")
def test_singularity_environment_basic_execution():
    """Test basic command execution in Singularity container."""
    # Using a lightweight image that should be available or easily pulled
    env = SingularityEnvironment(image="docker://python:3.11-slim")

    result = env.execute("echo 'hello world'")
    assert result["returncode"] == 0
    assert "hello world" in result["output"]


@pytest.mark.slow
@pytest.mark.skipif(not is_singularity_available(), reason="Singularity not available")
def test_singularity_environment_set_env_variables():
    """Test setting environment variables in the container."""
    env = SingularityEnvironment(
        image="docker://python:3.11-slim", env={"TEST_VAR": "test_value", "ANOTHER_VAR": "another_value"}
    )

    # Test single environment variable
    result = env.execute("echo $TEST_VAR")
    assert result["returncode"] == 0
    assert "test_value" in result["output"]

    # Test multiple environment variables
    result = env.execute("echo $TEST_VAR $ANOTHER_VAR")
    assert result["returncode"] == 0
    assert "test_value another_value" in result["output"]


@pytest.mark.slow
@pytest.mark.skipif(not is_singularity_available(), reason="Singularity not available")
def test_singularity_environment_forward_env_variables():
    """Test forwarding environment variables from host to container."""
    with patch.dict(os.environ, {"HOST_VAR": "host_value", "ANOTHER_HOST_VAR": "another_host_value"}):
        env = SingularityEnvironment(image="docker://python:3.11-slim", forward_env=["HOST_VAR", "ANOTHER_HOST_VAR"])

        # Test single forwarded environment variable
        result = env.execute("echo $HOST_VAR")
        assert result["returncode"] == 0
        assert "host_value" in result["output"]

        # Test multiple forwarded environment variables
        result = env.execute("echo $HOST_VAR $ANOTHER_HOST_VAR")
        assert result["returncode"] == 0
        assert "host_value another_host_value" in result["output"]


@pytest.mark.slow
@pytest.mark.skipif(not is_singularity_available(), reason="Singularity not available")
def test_singularity_environment_forward_nonexistent_env_variables():
    """Test forwarding non-existent environment variables (should be empty)."""
    env = SingularityEnvironment(image="docker://python:3.11-slim", forward_env=["NONEXISTENT_VAR"])

    result = env.execute('echo "[$NONEXISTENT_VAR]"')
    assert result["returncode"] == 0
    assert "[]" in result["output"]  # Empty variable should result in empty string


@pytest.mark.slow
@pytest.mark.skipif(not is_singularity_available(), reason="Singularity not available")
def test_singularity_environment_combined_env_and_forward():
    """Test both setting and forwarding environment variables together."""
    with patch.dict(os.environ, {"HOST_VAR": "from_host"}):
        env = SingularityEnvironment(
            image="docker://python:3.11-slim", env={"SET_VAR": "from_config"}, forward_env=["HOST_VAR"]
        )

        result = env.execute("echo $SET_VAR $HOST_VAR")
        assert result["returncode"] == 0
        assert "from_config from_host" in result["output"]


@pytest.mark.slow
@pytest.mark.skipif(not is_singularity_available(), reason="Singularity not available")
def test_singularity_environment_env_override_forward():
    """Test that explicitly set env variables take precedence over forwarded ones."""
    with patch.dict(os.environ, {"CONFLICT_VAR": "from_host"}):
        env = SingularityEnvironment(
            image="docker://python:3.11-slim", env={"CONFLICT_VAR": "from_config"}, forward_env=["CONFLICT_VAR"]
        )

        result = env.execute("echo $CONFLICT_VAR")
        assert result["returncode"] == 0
        # The explicitly set env should take precedence (comes after forwarded in singularity exec command)
        assert "from_config" in result["output"]


@pytest.mark.slow
@pytest.mark.skipif(not is_singularity_available(), reason="Singularity not available")
def test_singularity_environment_custom_cwd():
    """Test executing commands in a custom working directory."""
    env = SingularityEnvironment(image="docker://python:3.11-slim", cwd="/tmp")

    result = env.execute("pwd")
    assert result["returncode"] == 0
    assert "/tmp" in result["output"]


@pytest.mark.slow
@pytest.mark.skipif(not is_singularity_available(), reason="Singularity not available")
def test_singularity_environment_cwd_parameter_override():
    """Test that the cwd parameter in execute() overrides the config cwd."""
    env = SingularityEnvironment(image="docker://python:3.11-slim", cwd="/")

    result = env.execute("pwd", cwd="/tmp")
    assert result["returncode"] == 0
    assert "/tmp" in result["output"]


@pytest.mark.slow
@pytest.mark.skipif(not is_singularity_available(), reason="Singularity not available")
def test_singularity_environment_command_failure():
    """Test that command failures are properly captured."""
    env = SingularityEnvironment(image="docker://python:3.11-slim")

    result = env.execute("exit 42")
    assert result["returncode"] == 42


@pytest.mark.slow
@pytest.mark.skipif(not is_singularity_available(), reason="Singularity not available")
def test_singularity_environment_timeout():
    """Test that the timeout configuration is respected."""
    env = SingularityEnvironment(image="docker://python:3.11-slim", timeout=1)

    # This should timeout and raise TimeoutExpired
    with pytest.raises(subprocess.TimeoutExpired):
        env.execute("sleep 5")



================================================
FILE: tests/environments/extra/__init__.py
================================================
# empty file



================================================
FILE: tests/environments/extra/test_swerex_docker.py
================================================
import pytest

from minisweagent.environments.extra.swerex_docker import SwerexDockerEnvironment


@pytest.mark.slow
def test_swerex_docker_basic_execution():
    """Test basic command execution in SwerexDockerEnvironment."""
    env = SwerexDockerEnvironment(image="python:3.11")

    result = env.execute("echo 'hello world'")

    assert isinstance(result, dict)
    assert "output" in result
    assert "returncode" in result
    assert result["returncode"] == 0
    assert "hello world" in result["output"]


@pytest.mark.slow
def test_swerex_docker_command_failure():
    """Test that command failures are properly captured in SwerexDockerEnvironment."""
    env = SwerexDockerEnvironment(image="python:3.11")

    result = env.execute("exit 1")

    assert isinstance(result, dict)
    assert "output" in result
    assert "returncode" in result
    assert result["returncode"] == 1



================================================
FILE: tests/models/__init__.py
================================================
[Empty file]


================================================
FILE: tests/models/test_anthropic_model.py
================================================
import os
from unittest.mock import patch

from minisweagent.models.anthropic import AnthropicModel
from minisweagent.models.utils.key_per_thread import get_key_per_thread


def test_anthropic_model_single_key():
    with patch.dict(os.environ, {"ANTHROPIC_API_KEYS": "test-key"}):
        with patch("minisweagent.models.litellm_model.LitellmModel.query") as mock_query:
            mock_query.return_value = "response"

            model = AnthropicModel(model_name="tardis")
            result = model.query(messages=[])

            assert result == "response"
            assert mock_query.call_count == 1
            assert mock_query.call_args.kwargs["api_key"] == "test-key"


def test_get_key_per_thread_returns_same_key():
    key = get_key_per_thread(["1", "2"])
    for _ in range(100):
        assert get_key_per_thread(["1", "2"]) == key


def test_anthropic_model_with_empty_api_keys():
    with patch.dict(os.environ, {"ANTHROPIC_API_KEYS": ""}):
        with patch("minisweagent.models.litellm_model.LitellmModel.query") as mock_query:
            mock_query.return_value = "response"

            AnthropicModel(model_name="tardis").query(messages=[])

            assert mock_query.call_args.kwargs["api_key"] is None



================================================
FILE: tests/models/test_cache_control.py
================================================
from minisweagent.models.utils.cache_control import set_cache_control


def test_set_cache_control_basic():
    """Test basic cache control functionality with simple input/output."""
    # Input: A messages with multiple messages including user messages
    input_messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello, how are you?"},
        {"role": "assistant", "content": "I'm doing well, thank you!"},
        {"role": "user", "content": "Can you help me with coding?"},
        {"role": "assistant", "content": "Of course! I'd be happy to help."},
    ]

    # Expected output: Cache control added to the last 2 user messages
    expected_output = [
        {"role": "system", "content": "You are a helpful assistant."},
        {
            "role": "user",
            "content": [{"type": "text", "text": "Hello, how are you?", "cache_control": {"type": "ephemeral"}}],
        },
        {"role": "assistant", "content": "I'm doing well, thank you!"},
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "Can you help me with coding?", "cache_control": {"type": "ephemeral"}}
            ],
        },
        {"role": "assistant", "content": "Of course! I'd be happy to help."},
    ]

    result = set_cache_control(input_messages)

    assert result == expected_output


def test_set_cache_control_with_offset():
    """Test cache control with last_n_messages_offset parameter."""
    input_messages = [
        {"role": "user", "content": "First message"},
        {"role": "user", "content": "Second message"},
        {"role": "user", "content": "Third message"},
    ]

    # With offset=1, should skip the last message and tag the previous ones
    result = set_cache_control(input_messages, last_n_messages_offset=1)

    # Only the first two messages should have cache control
    assert "cache_control" not in result[2].get("content", {})  # Third message should not have cache control
    assert isinstance(result[0]["content"], list)  # First message should have cache control
    assert isinstance(result[1]["content"], list)  # Second message should have cache control



================================================
FILE: tests/models/test_init.py
================================================
import os
from unittest.mock import patch

import pytest

from minisweagent.models import GlobalModelStats, get_model, get_model_class, get_model_name
from minisweagent.models.test_models import DeterministicModel


class TestGetModelName:
    # Common config used across tests - model_name should be direct, not nested under "model"
    CONFIG_WITH_MODEL_NAME = {"model_name": "config-model"}

    def test_input_model_name_takes_precedence(self):
        """Test that explicit input_model_name overrides all other sources."""
        with patch.dict(os.environ, {"MSWEA_MODEL_NAME": "env-model"}):
            assert get_model_name("input-model", self.CONFIG_WITH_MODEL_NAME) == "input-model"

    def test_config_takes_precedence_over_env(self):
        """Test that config takes precedence over environment variable."""
        with patch.dict(os.environ, {"MSWEA_MODEL_NAME": "env-model"}):
            assert get_model_name(None, self.CONFIG_WITH_MODEL_NAME) == "config-model"

    def test_env_var_fallback(self):
        """Test that environment variable is used when no config provided."""
        with patch.dict(os.environ, {"MSWEA_MODEL_NAME": "env-model"}):
            assert get_model_name(None, {}) == "env-model"

    def test_config_fallback(self):
        """Test that config model name is used when input and env are missing."""
        with patch.dict(os.environ, {}, clear=True):
            assert get_model_name(None, self.CONFIG_WITH_MODEL_NAME) == "config-model"

    def test_raises_error_when_no_model_configured(self):
        """Test that ValueError is raised when no model is configured anywhere."""
        with patch.dict(os.environ, {}, clear=True):
            with pytest.raises(
                ValueError, match="No default model set. Please run `mini-extra config setup` to set one."
            ):
                get_model_name(None, {})

            with pytest.raises(
                ValueError, match="No default model set. Please run `mini-extra config setup` to set one."
            ):
                get_model_name(None, None)


class TestGetModelClass:
    def test_anthropic_model_selection(self):
        """Test that anthropic-related model names return AnthropicModel."""
        from minisweagent.models.anthropic import AnthropicModel

        for name in ["anthropic", "sonnet", "opus", "claude-sonnet", "claude-opus"]:
            assert get_model_class(name) == AnthropicModel

    def test_litellm_model_fallback(self):
        """Test that non-anthropic model names return LitellmModel."""
        from minisweagent.models.litellm_model import LitellmModel

        for name in ["gpt-4", "gpt-3.5-turbo", "llama2", "random-model"]:
            assert get_model_class(name) == LitellmModel

    def test_partial_matches(self):
        """Test that partial string matches work correctly."""
        from minisweagent.models.anthropic import AnthropicModel
        from minisweagent.models.litellm_model import LitellmModel

        assert get_model_class("my-anthropic-model") == AnthropicModel
        assert get_model_class("sonnet-latest") == AnthropicModel
        assert get_model_class("opus-v2") == AnthropicModel
        assert get_model_class("gpt-anthropic-style") == AnthropicModel
        assert get_model_class("totally-different") == LitellmModel


class TestGetModel:
    def test_config_deep_copy(self):
        """Test that get_model preserves original config via deep copy."""
        original_config = {"model_kwargs": {"api_key": "original"}, "outputs": ["test"]}

        with patch("minisweagent.models.get_model_class") as mock_get_class:
            mock_get_class.return_value = lambda **kwargs: DeterministicModel(outputs=["test"], model_name="test")
            get_model("test-model", original_config)
            assert original_config["model_kwargs"]["api_key"] == "original"
            assert "model_name" not in original_config

    def test_integration_with_compatible_model(self):
        """Test get_model works end-to-end with a model that handles extra kwargs."""
        with patch("minisweagent.models.get_model_class") as mock_get_class:

            def compatible_model(**kwargs):
                # Filter to only what DeterministicModel accepts, provide defaults
                config_args = {k: v for k, v in kwargs.items() if k in ["outputs", "model_name"]}
                if "outputs" not in config_args:
                    config_args["outputs"] = ["default"]
                return DeterministicModel(**config_args)

            mock_get_class.return_value = compatible_model
            model = get_model("test-model", {"outputs": ["hello"]})
            assert isinstance(model, DeterministicModel)
            assert model.config.outputs == ["hello"]
            assert model.config.model_name == "test-model"

    def test_env_var_overrides_config_api_key(self):
        """Test that MSWEA_MODEL_API_KEY overrides config api_key."""
        with patch.dict(os.environ, {"MSWEA_MODEL_API_KEY": "env-key"}):
            # Capture the arguments passed to the model constructor
            captured_kwargs = {}

            def mock_model_constructor(**kwargs):
                captured_kwargs.update(kwargs)
                return DeterministicModel(
                    outputs=kwargs.get("outputs", ["test"]),
                    model_name=kwargs.get("model_name", "test"),
                )

            with patch("minisweagent.models.get_model_class") as mock_get_class:
                mock_get_class.return_value = mock_model_constructor

                config = {"model_kwargs": {"api_key": "config-key"}, "outputs": ["test"]}
                get_model("test-model", config)

                assert captured_kwargs["model_kwargs"]["api_key"] == "env-key"

    def test_config_api_key_used_when_no_env_var(self):
        """Test that config api_key is used when env var is not set."""
        with patch.dict(os.environ, {}, clear=True):
            # Capture the arguments passed to the model constructor
            captured_kwargs = {}

            def mock_model_constructor(**kwargs):
                captured_kwargs.update(kwargs)
                return DeterministicModel(
                    outputs=kwargs.get("outputs", ["test"]),
                    model_name=kwargs.get("model_name", "test"),
                )

            with patch("minisweagent.models.get_model_class") as mock_get_class:
                mock_get_class.return_value = mock_model_constructor

                config = {"model_kwargs": {"api_key": "config-key"}, "outputs": ["test"]}
                get_model("test-model", config)

                assert captured_kwargs["model_kwargs"]["api_key"] == "config-key"

    def test_env_var_sets_api_key_when_no_config_key(self):
        """Test that MSWEA_MODEL_API_KEY is used when config has no api_key."""
        with patch.dict(os.environ, {"MSWEA_MODEL_API_KEY": "env-key"}):
            # Capture the arguments passed to the model constructor
            captured_kwargs = {}

            def mock_model_constructor(**kwargs):
                captured_kwargs.update(kwargs)
                return DeterministicModel(
                    outputs=kwargs.get("outputs", ["test"]),
                    model_name=kwargs.get("model_name", "test"),
                )

            with patch("minisweagent.models.get_model_class") as mock_get_class:
                mock_get_class.return_value = mock_model_constructor

                config = {"outputs": ["test"]}
                get_model("test-model", config)
                assert captured_kwargs["model_kwargs"]["api_key"] == "env-key"

    def test_no_api_key_when_none_provided(self):
        """Test that no api_key is set when neither env var nor config provide one."""
        with patch.dict(os.environ, {}, clear=True):
            # Capture the arguments passed to the model constructor
            captured_kwargs = {}

            def mock_model_constructor(**kwargs):
                captured_kwargs.update(kwargs)
                return DeterministicModel(
                    outputs=kwargs.get("outputs", ["test"]),
                    model_name=kwargs.get("model_name", "test"),
                )

            with patch("minisweagent.models.get_model_class") as mock_get_class:
                mock_get_class.return_value = mock_model_constructor

                config = {"outputs": ["test"]}
                get_model("test-model", config)
                model_kwargs = captured_kwargs.get("model_kwargs", {})
                assert "api_key" not in model_kwargs


class TestGlobalModelStats:
    def test_prints_cost_limit_when_set(self, capsys):
        """Test that cost limit is printed when MSWEA_GLOBAL_COST_LIMIT is set."""
        with patch.dict(os.environ, {"MSWEA_GLOBAL_COST_LIMIT": "5.5"}, clear=True):
            GlobalModelStats()
            captured = capsys.readouterr()
            assert "Global cost/call limit: $5.5000 / 0" in captured.out

    def test_prints_call_limit_when_set(self, capsys):
        """Test that call limit is printed when MSWEA_GLOBAL_CALL_LIMIT is set."""
        with patch.dict(os.environ, {"MSWEA_GLOBAL_CALL_LIMIT": "10"}, clear=True):
            GlobalModelStats()
            captured = capsys.readouterr()
            assert "Global cost/call limit: $0.0000 / 10" in captured.out

    def test_prints_both_limits_when_both_set(self, capsys):
        """Test that both limits are printed when both environment variables are set."""
        with patch.dict(os.environ, {"MSWEA_GLOBAL_COST_LIMIT": "2.5", "MSWEA_GLOBAL_CALL_LIMIT": "5"}, clear=True):
            GlobalModelStats()
            captured = capsys.readouterr()
            assert "Global cost/call limit: $2.5000 / 5" in captured.out

    def test_no_print_when_silent_startup_set(self, capsys):
        """Test that limits are not printed when MSWEA_SILENT_STARTUP is set."""
        with patch.dict(
            os.environ,
            {"MSWEA_GLOBAL_COST_LIMIT": "5.0", "MSWEA_GLOBAL_CALL_LIMIT": "10", "MSWEA_SILENT_STARTUP": "1"},
            clear=True,
        ):
            GlobalModelStats()
            captured = capsys.readouterr()
            assert "Global cost/call limit" not in captured.out

    def test_no_print_when_no_limits_set(self, capsys):
        """Test that nothing is printed when no limits are set."""
        with patch.dict(os.environ, {}, clear=True):
            GlobalModelStats()
            captured = capsys.readouterr()
            assert "Global cost/call limit" not in captured.out

    def test_no_print_when_limits_are_zero(self, capsys):
        """Test that nothing is printed when limits are explicitly set to zero."""
        with patch.dict(os.environ, {"MSWEA_GLOBAL_COST_LIMIT": "0", "MSWEA_GLOBAL_CALL_LIMIT": "0"}, clear=True):
            GlobalModelStats()
            captured = capsys.readouterr()
            assert "Global cost/call limit" not in captured.out



================================================
FILE: tests/models/test_litellm_model.py
================================================
import json
import tempfile
from pathlib import Path
from unittest.mock import Mock, patch

import litellm
import pytest

from minisweagent.models.litellm_model import LitellmModel


def test_authentication_error_enhanced_message():
    """Test that AuthenticationError gets enhanced with config set instruction."""
    model = LitellmModel(model_name="gpt-4")

    # Create a mock exception that behaves like AuthenticationError
    original_error = Mock(spec=litellm.exceptions.AuthenticationError)
    original_error.message = "Invalid API key"

    with patch("litellm.completion") as mock_completion:
        # Make completion raise the mock error
        def side_effect(*args, **kwargs):
            raise litellm.exceptions.AuthenticationError("Invalid API key", llm_provider="openai", model="gpt-4")

        mock_completion.side_effect = side_effect

        with pytest.raises(litellm.exceptions.AuthenticationError) as exc_info:
            model._query([{"role": "user", "content": "test"}])

        # Check that the error message was enhanced
        assert "You can permanently set your API key with `mini-extra config set KEY VALUE`." in str(exc_info.value)


def test_model_registry_loading():
    """Test that custom model registry is loaded and registered when provided."""
    model_costs = {
        "my-custom-model": {
            "max_tokens": 4096,
            "input_cost_per_token": 0.0001,
            "output_cost_per_token": 0.0002,
            "litellm_provider": "openai",
            "mode": "chat",
        }
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
        json.dump(model_costs, f)
        registry_path = f.name

    try:
        with patch("litellm.utils.register_model") as mock_register:
            _model = LitellmModel(model_name="my-custom-model", litellm_model_registry=Path(registry_path))

            # Verify register_model was called with the correct data
            mock_register.assert_called_once_with(model_costs)
    except Exception as e:
        print(e)
        raise e
    finally:
        Path(registry_path).unlink()


def test_model_registry_none():
    """Test that no registry loading occurs when litellm_model_registry is None."""
    with patch("litellm.register_model") as mock_register:
        _model = LitellmModel(model_name="gpt-4", litellm_model_registry=None)

        # Verify register_model was not called
        mock_register.assert_not_called()


def test_model_registry_not_provided():
    """Test that no registry loading occurs when litellm_model_registry is not provided."""
    with patch("litellm.register_model") as mock_register:
        _model = LitellmModel(model_name="gpt-4o")

        # Verify register_model was not called
        mock_register.assert_not_called()



================================================
FILE: tests/models/test_test_models.py
================================================
import logging
import time

import minisweagent.models
from minisweagent.models.test_models import DeterministicModel, DeterministicModelConfig


def test_basic_functionality_and_cost_tracking(reset_global_stats):
    """Test basic model functionality, cost tracking, and default configuration."""
    model = DeterministicModel(outputs=["Hello", "World"])

    # Test first call with defaults
    assert model.query([{"role": "user", "content": "test"}]) == {"content": "Hello"}
    assert model.n_calls == 1
    assert model.cost == 1.0
    assert minisweagent.models.GLOBAL_MODEL_STATS.n_calls == 1
    assert minisweagent.models.GLOBAL_MODEL_STATS.cost == 1.0

    # Test second call and sequential outputs
    assert model.query([{"role": "user", "content": "test"}]) == {"content": "World"}
    assert model.n_calls == 2
    assert model.cost == 2.0
    assert minisweagent.models.GLOBAL_MODEL_STATS.n_calls == 2
    assert minisweagent.models.GLOBAL_MODEL_STATS.cost == 2.0


def test_custom_cost_and_multiple_models(reset_global_stats):
    """Test custom cost configuration and global tracking across multiple models."""
    model1 = DeterministicModel(outputs=["Response1"], cost_per_call=2.5)
    model2 = DeterministicModel(outputs=["Response2"], cost_per_call=3.0)

    assert model1.query([{"role": "user", "content": "test"}]) == {"content": "Response1"}
    assert model1.cost == 2.5
    assert minisweagent.models.GLOBAL_MODEL_STATS.cost == 2.5

    assert model2.query([{"role": "user", "content": "test"}]) == {"content": "Response2"}
    assert model2.cost == 3.0
    assert minisweagent.models.GLOBAL_MODEL_STATS.cost == 5.5
    assert minisweagent.models.GLOBAL_MODEL_STATS.n_calls == 2


def test_config_dataclass():
    """Test DeterministicModelConfig with custom values."""
    config = DeterministicModelConfig(outputs=["Test"], model_name="custom", cost_per_call=5.0)

    assert config.cost_per_call == 5.0
    assert config.model_name == "custom"

    model = DeterministicModel(**config.__dict__)
    assert model.config.cost_per_call == 5.0


def test_sleep_and_warning_commands(caplog):
    """Test special /sleep and /warning command handling."""
    # Test sleep command - processes sleep then returns actual output (counts as 1 call)
    model = DeterministicModel(outputs=["/sleep0.1", "After sleep"])
    start_time = time.time()
    assert model.query([{"role": "user", "content": "test"}]) == {"content": "After sleep"}
    assert time.time() - start_time >= 0.1
    assert model.n_calls == 1  # Sleep no longer counts as separate call

    # Test warning command - processes warning then returns actual output (counts as 1 call)
    model2 = DeterministicModel(outputs=["/warningTest message", "After warning"])
    with caplog.at_level(logging.WARNING):
        assert model2.query([{"role": "user", "content": "test"}]) == {"content": "After warning"}
    assert model2.n_calls == 1  # Warning no longer counts as separate call
    assert "Test message" in caplog.text



================================================
FILE: tests/run/__init__.py
================================================
[Empty file]


================================================
FILE: tests/run/test_batch_progress.py
================================================
import pytest
import yaml

from minisweagent.run.extra.utils.batch_progress import RunBatchProgressManager, _shorten_str


@pytest.fixture
def manager():
    """Create a basic RunBatchProgressManager for testing."""
    return RunBatchProgressManager(num_instances=5)


@pytest.fixture
def manager_with_yaml(tmp_path):
    """Create a RunBatchProgressManager with yaml reporting."""
    yaml_path = tmp_path / "report.yaml"
    return RunBatchProgressManager(num_instances=3, yaml_report_path=yaml_path), yaml_path


@pytest.mark.parametrize(
    ("text", "max_len", "shorten_left", "expected"),
    [
        ("hello", 10, False, "hello     "),
        ("hello world", 8, False, "hello..."),
        ("hello world", 8, True, "...world"),
        ("hello", 5, False, "hello"),
        ("hi", 5, False, "hi   "),
    ],
)
def test_shorten_str(text, max_len, shorten_left, expected):
    assert _shorten_str(text, max_len, shorten_left) == expected


def test_manager_initialization(manager):
    assert manager.n_completed == 0
    assert manager._instances_by_exit_status == {}


def test_manager_with_yaml_path(manager_with_yaml):
    manager, yaml_path = manager_with_yaml
    assert manager._yaml_report_path == yaml_path


def test_instance_lifecycle(manager):
    manager.on_instance_start("task_1")
    assert "task_1" in manager._spinner_tasks
    assert manager.n_completed == 0

    manager.on_instance_end("task_1", "success")
    assert manager.n_completed == 1
    assert manager._instances_by_exit_status["success"] == ["task_1"]


@pytest.mark.parametrize(
    "statuses",
    [
        ["success", "failed", "success", "timeout"],
        ["error", "error", "error"],
        ["success"] * 5,
    ],
)
def test_multiple_instances(manager, statuses):
    for i, status in enumerate(statuses, 1):
        instance_id = f"task_{i}"
        manager.on_instance_start(instance_id)
        manager.on_instance_end(instance_id, status)

    assert manager.n_completed == len(statuses)
    for status in set(statuses):
        expected_count = statuses.count(status)
        assert len(manager._instances_by_exit_status[status]) == expected_count


def test_uncaught_exception(manager):
    manager.on_instance_start("task_1")
    manager.on_uncaught_exception("task_1", ValueError("test error"))

    assert manager.n_completed == 1
    assert "Uncaught ValueError" in manager._instances_by_exit_status


def test_update_instance_status(manager):
    manager.on_instance_start("task_1")
    manager.update_instance_status("task_1", "Processing files...")


def test_yaml_report_generation(manager_with_yaml):
    manager, yaml_path = manager_with_yaml

    manager.on_instance_start("task_1")
    manager.on_instance_end("task_1", "success")
    manager.on_instance_start("task_2")
    manager.on_instance_end("task_2", "failed")

    assert yaml_path.exists()
    data = yaml.safe_load(yaml_path.read_text())
    assert data["instances_by_exit_status"]["success"] == ["task_1"]
    assert data["instances_by_exit_status"]["failed"] == ["task_2"]


def test_get_overview_data(manager):
    manager.on_instance_start("task_1")
    manager.on_instance_end("task_1", "success")

    overview_data = manager._get_overview_data()
    assert overview_data == {"instances_by_exit_status": {"success": ["task_1"]}}


def test_print_report(manager, capsys):
    """Test that print_report produces expected output."""
    manager.on_instance_start("task_1")
    manager.on_instance_end("task_1", "success")
    manager.on_instance_start("task_2")
    manager.on_instance_end("task_2", "failed")

    manager.print_report()

    captured = capsys.readouterr()
    assert "success: 1" in captured.out
    assert "failed: 1" in captured.out
    assert "task_1" in captured.out
    assert "task_2" in captured.out


def test_concurrent_operations(manager):
    """Test handling multiple operations without corruption."""
    instance_ids = [f"task_{i}" for i in range(10)]
    statuses = ["success", "failed", "timeout"] * 4

    for i, instance_id in enumerate(instance_ids):
        manager.on_instance_start(instance_id)
        manager.update_instance_status(instance_id, f"step {i}")
        manager.on_instance_end(instance_id, statuses[i % 3])

    assert manager.n_completed == 10
    assert sum(len(instances) for instances in manager._instances_by_exit_status.values()) == 10



================================================
FILE: tests/run/test_cli_integration.py
================================================
import re
import subprocess
import sys
from unittest.mock import Mock, patch

import pytest

from minisweagent.run.mini import DEFAULT_CONFIG, app, main


def strip_ansi_codes(text: str) -> str:
    """Remove ANSI escape sequences from text."""
    ansi_escape = re.compile(r"\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])")
    return ansi_escape.sub("", text)


def test_configure_if_first_time_called():
    """Test that configure_if_first_time is called when running mini main."""
    with (
        patch("minisweagent.run.mini.configure_if_first_time") as mock_configure,
        patch("minisweagent.run.mini.InteractiveAgent") as mock_interactive_agent_class,
        patch("minisweagent.run.mini.get_model") as mock_get_model,
        patch("minisweagent.run.mini.LocalEnvironment") as mock_env,
        patch("minisweagent.run.mini.get_config_path") as mock_get_config_path,
        patch("minisweagent.run.mini.yaml.safe_load") as mock_yaml_load,
    ):
        # Setup mocks
        mock_model = Mock()
        mock_get_model.return_value = mock_model
        mock_environment = Mock()
        mock_env.return_value = mock_environment
        mock_config_path = Mock()
        mock_config_path.read_text.return_value = ""
        mock_get_config_path.return_value = mock_config_path
        mock_yaml_load.return_value = {"agent": {"system_template": "test"}, "env": {}, "model": {}}

        # Setup mock agent instance
        mock_agent = Mock()
        mock_agent.run.return_value = ("Success", "Result")
        mock_interactive_agent_class.return_value = mock_agent

        # Call main function
        main(
            config_spec=DEFAULT_CONFIG,
            model_name="test-model",
            task="Test task",
            yolo=False,
            output=None,
            visual=False,
        )

        # Verify configure_if_first_time was called
        mock_configure.assert_called_once()


def test_mini_command_calls_run_interactive():
    """Test that mini command creates InteractiveAgent when visual=False."""
    with (
        patch("minisweagent.run.mini.configure_if_first_time"),
        patch("minisweagent.run.mini.InteractiveAgent") as mock_interactive_agent_class,
        patch("minisweagent.run.mini.get_model") as mock_get_model,
        patch("minisweagent.run.mini.LocalEnvironment") as mock_env,
        patch("minisweagent.run.mini.get_config_path") as mock_get_config_path,
        patch("minisweagent.run.mini.yaml.safe_load") as mock_yaml_load,
    ):
        # Setup mocks
        mock_model = Mock()
        mock_get_model.return_value = mock_model
        mock_environment = Mock()
        mock_env.return_value = mock_environment
        mock_config_path = Mock()
        mock_config_path.read_text.return_value = ""
        mock_get_config_path.return_value = mock_config_path
        mock_yaml_load.return_value = {"agent": {"system_template": "test", "mode": "confirm"}, "env": {}, "model": {}}

        # Setup mock agent instance
        mock_agent = Mock()
        mock_agent.run.return_value = ("Success", "Result")
        mock_interactive_agent_class.return_value = mock_agent

        # Call main function with task provided (so prompt is not called)
        main(
            config_spec=DEFAULT_CONFIG,
            model_name="test-model",
            task="Test task",
            yolo=False,
            output=None,
            visual=False,
        )

        # Verify InteractiveAgent was instantiated
        mock_interactive_agent_class.assert_called_once()
        args, kwargs = mock_interactive_agent_class.call_args
        assert args[0] == mock_model  # model
        assert args[1] == mock_environment  # env
        # Verify agent.run was called with the task
        mock_agent.run.assert_called_once_with("Test task")


def test_mini_v_command_calls_run_textual():
    """Test that mini -v command creates TextualAgent when visual=True."""
    with (
        patch("minisweagent.run.mini.configure_if_first_time"),
        patch("minisweagent.run.mini.TextualAgent") as mock_textual_agent_class,
        patch("minisweagent.run.mini.get_model") as mock_get_model,
        patch("minisweagent.run.mini.LocalEnvironment") as mock_env,
        patch("minisweagent.run.mini.get_config_path") as mock_get_config_path,
        patch("minisweagent.run.mini.yaml.safe_load") as mock_yaml_load,
    ):
        # Setup mocks
        mock_model = Mock()
        mock_get_model.return_value = mock_model
        mock_environment = Mock()
        mock_env.return_value = mock_environment
        mock_config_path = Mock()
        mock_config_path.read_text.return_value = ""
        mock_get_config_path.return_value = mock_config_path
        mock_yaml_load.return_value = {"agent": {"system_template": "test", "mode": "confirm"}, "env": {}, "model": {}}

        # Setup mock agent instance
        mock_agent = Mock()
        mock_agent.run.return_value = ("Success", "Result")
        mock_textual_agent_class.return_value = mock_agent

        # Call main function with visual=True and task provided
        main(
            config_spec=DEFAULT_CONFIG,
            model_name="test-model",
            task="Test task",
            yolo=False,
            output=None,
            visual=True,
        )

        # Verify TextualAgent was instantiated
        mock_textual_agent_class.assert_called_once()
        args, kwargs = mock_textual_agent_class.call_args
        assert args[0] == mock_model  # model
        assert args[1] == mock_environment  # env
        # Verify agent.run was called with the task
        mock_agent.run.assert_called_once_with("Test task")


def test_mini_calls_prompt_when_no_task_provided():
    """Test that mini calls prompt when no task is provided."""
    with (
        patch("minisweagent.run.mini.configure_if_first_time"),
        patch("minisweagent.run.mini.prompt_session.prompt") as mock_prompt,
        patch("minisweagent.run.mini.InteractiveAgent") as mock_interactive_agent_class,
        patch("minisweagent.run.mini.get_model") as mock_get_model,
        patch("minisweagent.run.mini.LocalEnvironment") as mock_env,
        patch("minisweagent.run.mini.get_config_path") as mock_get_config_path,
        patch("minisweagent.run.mini.yaml.safe_load") as mock_yaml_load,
    ):
        # Setup mocks
        mock_prompt.return_value = "User provided task"
        mock_model = Mock()
        mock_get_model.return_value = mock_model
        mock_environment = Mock()
        mock_env.return_value = mock_environment
        mock_config_path = Mock()
        mock_config_path.read_text.return_value = ""
        mock_get_config_path.return_value = mock_config_path
        mock_yaml_load.return_value = {"agent": {"system_template": "test", "mode": "confirm"}, "env": {}, "model": {}}

        # Setup mock agent instance
        mock_agent = Mock()
        mock_agent.run.return_value = ("Success", "Result")
        mock_interactive_agent_class.return_value = mock_agent

        # Call main function without task
        main(
            config_spec=DEFAULT_CONFIG,
            model_name="test-model",
            task=None,  # No task provided
            yolo=False,
            output=None,
            visual=False,
        )

        # Verify prompt was called
        mock_prompt.assert_called_once()

        # Verify InteractiveAgent was instantiated
        mock_interactive_agent_class.assert_called_once()
        # Verify agent.run was called with the task from prompt
        mock_agent.run.assert_called_once_with("User provided task")


def test_mini_v_calls_prompt_when_no_task_provided():
    """Test that mini -v calls prompt when no task is provided."""
    with (
        patch("minisweagent.run.mini.configure_if_first_time"),
        patch("minisweagent.run.mini.prompt_session.prompt") as mock_prompt,
        patch("minisweagent.run.mini.TextualAgent") as mock_textual_agent_class,
        patch("minisweagent.run.mini.get_model") as mock_get_model,
        patch("minisweagent.run.mini.LocalEnvironment") as mock_env,
        patch("minisweagent.run.mini.get_config_path") as mock_get_config_path,
        patch("minisweagent.run.mini.yaml.safe_load") as mock_yaml_load,
    ):
        # Setup mocks
        mock_prompt.return_value = "User provided visual task"
        mock_model = Mock()
        mock_get_model.return_value = mock_model
        mock_environment = Mock()
        mock_env.return_value = mock_environment
        mock_config_path = Mock()
        mock_config_path.read_text.return_value = ""
        mock_get_config_path.return_value = mock_config_path
        mock_yaml_load.return_value = {"agent": {"system_template": "test", "mode": "confirm"}, "env": {}, "model": {}}

        # Setup mock agent instance
        mock_agent = Mock()
        mock_agent.run.return_value = ("Success", "Result")
        mock_textual_agent_class.return_value = mock_agent

        # Call main function with visual=True but no task
        main(
            config_spec=DEFAULT_CONFIG,
            model_name="test-model",
            task=None,  # No task provided
            yolo=False,
            output=None,
            visual=True,
        )

        # Verify prompt was called
        mock_prompt.assert_called_once()

        # Verify TextualAgent was instantiated
        mock_textual_agent_class.assert_called_once()
        # Verify agent.run was called with the task from prompt
        mock_agent.run.assert_called_once_with("User provided visual task")


def test_mini_with_explicit_model():
    """Test that mini works with explicitly provided model."""
    with (
        patch("minisweagent.run.mini.configure_if_first_time"),
        patch("minisweagent.run.mini.InteractiveAgent") as mock_interactive_agent_class,
        patch("minisweagent.run.mini.get_model") as mock_get_model,
        patch("minisweagent.run.mini.LocalEnvironment") as mock_env,
        patch("minisweagent.run.mini.get_config_path") as mock_get_config_path,
        patch("minisweagent.run.mini.yaml.safe_load") as mock_yaml_load,
    ):
        # Setup mocks
        mock_model = Mock()
        mock_get_model.return_value = mock_model
        mock_environment = Mock()
        mock_env.return_value = mock_environment
        mock_config_path = Mock()
        mock_config_path.read_text.return_value = ""
        mock_get_config_path.return_value = mock_config_path
        mock_yaml_load.return_value = {
            "agent": {"system_template": "test", "mode": "yolo"},
            "env": {},
            "model": {"default_config": "test"},
        }

        # Setup mock agent instance
        mock_agent = Mock()
        mock_agent.run.return_value = ("Success", "Result")
        mock_interactive_agent_class.return_value = mock_agent

        # Call main function with explicit model
        main(
            config_spec=DEFAULT_CONFIG,
            model_name="gpt-4",
            task="Test task with explicit model",
            yolo=True,
            output=None,
            visual=False,
        )

        # Verify get_model was called with the explicit model
        mock_get_model.assert_called_once_with("gpt-4", {"default_config": "test"})

        # Verify InteractiveAgent was instantiated
        mock_interactive_agent_class.assert_called_once()
        # Verify agent.run was called
        mock_agent.run.assert_called_once_with("Test task with explicit model")


def test_yolo_mode_sets_correct_agent_config():
    """Test that yolo mode sets the correct agent configuration."""
    with (
        patch("minisweagent.run.mini.configure_if_first_time"),
        patch("minisweagent.run.mini.InteractiveAgent") as mock_interactive_agent_class,
        patch("minisweagent.run.mini.get_model") as mock_get_model,
        patch("minisweagent.run.mini.LocalEnvironment") as mock_env,
        patch("minisweagent.run.mini.get_config_path") as mock_get_config_path,
        patch("minisweagent.run.mini.yaml.safe_load") as mock_yaml_load,
    ):
        # Setup mocks
        mock_model = Mock()
        mock_get_model.return_value = mock_model
        mock_environment = Mock()
        mock_env.return_value = mock_environment
        mock_config_path = Mock()
        mock_config_path.read_text.return_value = ""
        mock_get_config_path.return_value = mock_config_path
        mock_yaml_load.return_value = {"agent": {"system_template": "test"}, "env": {}, "model": {}}

        # Setup mock agent instance
        mock_agent = Mock()
        mock_agent.run.return_value = ("Success", "Result")
        mock_interactive_agent_class.return_value = mock_agent

        # Call main function with yolo=True
        main(
            config_spec=DEFAULT_CONFIG,
            model_name="test-model",
            task="Test yolo task",
            yolo=True,
            output=None,
            visual=False,
        )

        # Verify InteractiveAgent was called with yolo mode
        mock_interactive_agent_class.assert_called_once()
        args, kwargs = mock_interactive_agent_class.call_args
        # The agent_config should contain the mode as a keyword argument
        assert kwargs.get("mode") == "yolo"
        # Verify agent.run was called
        mock_agent.run.assert_called_once_with("Test yolo task")


def test_confirm_mode_sets_correct_agent_config():
    """Test that confirm mode (default) sets the correct agent configuration."""
    with (
        patch("minisweagent.run.mini.configure_if_first_time"),
        patch("minisweagent.run.mini.InteractiveAgent") as mock_interactive_agent_class,
        patch("minisweagent.run.mini.get_model") as mock_get_model,
        patch("minisweagent.run.mini.LocalEnvironment") as mock_env,
        patch("minisweagent.run.mini.get_config_path") as mock_get_config_path,
        patch("minisweagent.run.mini.yaml.safe_load") as mock_yaml_load,
    ):
        # Setup mocks
        mock_model = Mock()
        mock_get_model.return_value = mock_model
        mock_environment = Mock()
        mock_env.return_value = mock_environment
        mock_config_path = Mock()
        mock_config_path.read_text.return_value = ""
        mock_get_config_path.return_value = mock_config_path
        mock_yaml_load.return_value = {"agent": {"system_template": "test"}, "env": {}, "model": {}}

        # Setup mock agent instance
        mock_agent = Mock()
        mock_agent.run.return_value = ("Success", "Result")
        mock_interactive_agent_class.return_value = mock_agent

        # Call main function with yolo=False (default)
        main(
            config_spec=DEFAULT_CONFIG,
            model_name="test-model",
            task="Test confirm task",
            yolo=False,
            output=None,
            visual=False,
        )

        # Verify InteractiveAgent was called with confirm mode
        mock_interactive_agent_class.assert_called_once()
        args, kwargs = mock_interactive_agent_class.call_args
        # The agent_config should contain the mode as a keyword argument
        assert kwargs.get("mode") == "confirm"
        # Verify agent.run was called
        mock_agent.run.assert_called_once_with("Test confirm task")


def test_mini_help():
    """Test that mini --help works correctly."""
    result = subprocess.run(
        [sys.executable, "-m", "minisweagent", "--help"],
        capture_output=True,
        text=True,
        timeout=10,
    )

    assert result.returncode == 0
    # Strip ANSI color codes for reliable text matching
    clean_output = strip_ansi_codes(result.stdout)
    assert "Run mini-SWE-agent in your local environment." in clean_output
    assert "--help" in clean_output
    assert "--config" in clean_output
    assert "--model" in clean_output
    assert "--task" in clean_output
    assert "--yolo" in clean_output
    assert "--output" in clean_output
    assert "--visual" in clean_output


def test_mini_help_with_typer_runner():
    """Test help functionality using typer's test runner."""
    from typer.testing import CliRunner

    runner = CliRunner()
    result = runner.invoke(app, ["--help"])

    assert result.exit_code == 0
    # Strip ANSI color codes for reliable text matching
    clean_output = strip_ansi_codes(result.stdout)
    assert "Run mini-SWE-agent in your local environment." in clean_output
    assert "--help" in clean_output
    assert "--config" in clean_output
    assert "--model" in clean_output
    assert "--task" in clean_output
    assert "--yolo" in clean_output
    assert "--output" in clean_output
    assert "--visual" in clean_output


def test_python_m_minisweagent_help():
    """Test that python -m minisweagent --help works correctly."""
    result = subprocess.run(
        [sys.executable, "-m", "minisweagent", "--help"],
        capture_output=True,
        text=True,
        timeout=10,
    )

    assert result.returncode == 0
    assert "mini-SWE-agent" in result.stdout


def test_mini_script_help():
    """Test that the mini script entry point help works."""
    result = subprocess.run(
        ["mini", "--help"],
        capture_output=True,
        text=True,
        timeout=10,
    )

    assert result.returncode == 0
    assert "mini-SWE-agent" in result.stdout


def test_mini_swe_agent_help():
    """Test that mini-swe-agent --help works correctly."""
    result = subprocess.run(
        ["mini-swe-agent", "--help"],
        capture_output=True,
        text=True,
        timeout=10,
    )

    assert result.returncode == 0
    clean_output = strip_ansi_codes(result.stdout)
    assert "mini-SWE-agent" in clean_output


def test_mini_extra_help():
    """Test that mini-extra --help works correctly."""
    result = subprocess.run(
        ["mini-extra", "--help"],
        capture_output=True,
        text=True,
        timeout=10,
    )

    assert result.returncode == 0
    clean_output = strip_ansi_codes(result.stdout)
    assert "central entry point for all extra commands" in clean_output
    assert "config" in clean_output
    assert "inspect" in clean_output
    assert "github-issue" in clean_output
    assert "swebench" in clean_output


def test_mini_e_help():
    """Test that mini-e --help works correctly."""
    result = subprocess.run(
        ["mini-e", "--help"],
        capture_output=True,
        text=True,
        timeout=10,
    )

    assert result.returncode == 0
    clean_output = strip_ansi_codes(result.stdout)
    assert "central entry point for all extra commands" in clean_output


@pytest.mark.parametrize(
    ("subcommand", "aliases"),
    [
        ("config", ["config"]),
        ("inspect", ["inspect", "i", "inspector"]),
        ("github-issue", ["github-issue", "gh"]),
        ("swebench", ["swebench"]),
        ("swebench-single", ["swebench-single"]),
    ],
)
def test_mini_extra_subcommand_help(subcommand: str, aliases: list[str]):
    """Test that mini-extra subcommands --help work correctly."""
    for alias in aliases:
        result = subprocess.run(
            ["mini-extra", alias, "--help"],
            capture_output=True,
            text=True,
            timeout=10,
        )

        assert result.returncode == 0
        # Just verify that help output is returned (content varies by subcommand)
        assert len(result.stdout) > 0


def test_mini_extra_config_help():
    """Test that mini-extra config --help works correctly."""
    result = subprocess.run(
        ["mini-extra", "config", "--help"],
        capture_output=True,
        text=True,
        timeout=10,
    )

    assert result.returncode == 0
    assert len(result.stdout) > 0
    # Config command should have help output
    clean_output = strip_ansi_codes(result.stdout)
    assert "--help" in clean_output


def test_exit_immediately_flag_sets_confirm_exit_false():
    """Test that --exit-immediately flag sets confirm_exit to False in agent config."""
    with (
        patch("minisweagent.run.mini.configure_if_first_time"),
        patch("minisweagent.run.mini.InteractiveAgent") as mock_interactive_agent_class,
        patch("minisweagent.run.mini.get_model") as mock_get_model,
        patch("minisweagent.run.mini.LocalEnvironment") as mock_env,
        patch("minisweagent.run.mini.get_config_path") as mock_get_config_path,
        patch("minisweagent.run.mini.yaml.safe_load") as mock_yaml_load,
    ):
        # Setup mocks
        mock_model = Mock()
        mock_get_model.return_value = mock_model
        mock_environment = Mock()
        mock_env.return_value = mock_environment
        mock_config_path = Mock()
        mock_config_path.read_text.return_value = ""
        mock_get_config_path.return_value = mock_config_path
        mock_yaml_load.return_value = {"agent": {"system_template": "test"}, "env": {}, "model": {}}

        # Create mock agent with config
        mock_agent = Mock()
        mock_agent.config.confirm_exit = False
        mock_agent.run.return_value = ("Success", "Result")
        mock_interactive_agent_class.return_value = mock_agent

        # Call main function with --exit-immediately flag
        agent = main(
            config_spec=DEFAULT_CONFIG,
            model_name="test-model",
            task="Test task",
            yolo=False,
            output=None,
            visual=False,
            exit_immediately=True,  # This should set confirm_exit=False
        )

        # Verify the agent's config has confirm_exit set to False
        assert agent.config.confirm_exit is False


def test_no_exit_immediately_flag_sets_confirm_exit_true():
    """Test that when --exit-immediately flag is not used, confirm_exit defaults to True."""
    with (
        patch("minisweagent.run.mini.configure_if_first_time"),
        patch("minisweagent.run.mini.InteractiveAgent") as mock_interactive_agent_class,
        patch("minisweagent.run.mini.get_model") as mock_get_model,
        patch("minisweagent.run.mini.LocalEnvironment") as mock_env,
        patch("minisweagent.run.mini.get_config_path") as mock_get_config_path,
        patch("minisweagent.run.mini.yaml.safe_load") as mock_yaml_load,
    ):
        # Setup mocks
        mock_model = Mock()
        mock_get_model.return_value = mock_model
        mock_environment = Mock()
        mock_env.return_value = mock_environment
        mock_config_path = Mock()
        mock_config_path.read_text.return_value = ""
        mock_get_config_path.return_value = mock_config_path
        mock_yaml_load.return_value = {"agent": {"system_template": "test"}, "env": {}, "model": {}}

        # Create mock agent with config
        mock_agent = Mock()
        mock_agent.config.confirm_exit = True
        mock_agent.run.return_value = ("Success", "Result")
        mock_interactive_agent_class.return_value = mock_agent

        # Call main function without --exit-immediately flag (defaults to False)
        agent = main(
            config_spec=DEFAULT_CONFIG,
            model_name="test-model",
            task="Test task",
            yolo=False,
            output=None,
            visual=False,
        )

        # Verify the agent's config has confirm_exit set to True
        assert agent.config.confirm_exit is True


def test_exit_immediately_flag_with_typer_runner():
    """Test --exit-immediately flag using typer's test runner."""
    from typer.testing import CliRunner

    with (
        patch("minisweagent.run.mini.configure_if_first_time"),
        patch("minisweagent.run.mini.InteractiveAgent") as mock_interactive_agent_class,
        patch("minisweagent.run.mini.get_model") as mock_get_model,
        patch("minisweagent.run.mini.LocalEnvironment") as mock_env,
        patch("minisweagent.run.mini.get_config_path") as mock_get_config_path,
        patch("minisweagent.run.mini.yaml.safe_load") as mock_yaml_load,
        patch("minisweagent.run.mini.save_traj"),
    ):
        # Setup mocks
        mock_model = Mock()
        mock_get_model.return_value = mock_model
        mock_environment = Mock()
        mock_env.return_value = mock_environment
        mock_config_path = Mock()
        mock_config_path.read_text.return_value = ""
        mock_get_config_path.return_value = mock_config_path
        mock_yaml_load.return_value = {"agent": {"system_template": "test"}, "env": {}, "model": {}}

        # Setup mock agent instance
        mock_agent = Mock()
        mock_agent.run.return_value = ("Success", "Result")
        # Mock the attributes needed for save_traj
        mock_agent.model.cost = 0.0
        mock_agent.model.n_calls = 0
        mock_agent.messages = []
        mock_interactive_agent_class.return_value = mock_agent

        runner = CliRunner()
        result = runner.invoke(app, ["--task", "Test task", "--exit-immediately", "--model", "test-model"])

        assert result.exit_code == 0
        mock_interactive_agent_class.assert_called_once()
        args, kwargs = mock_interactive_agent_class.call_args
        # The agent_config should contain confirm_exit as a keyword argument
        assert kwargs.get("confirm_exit") is False



================================================
FILE: tests/run/test_extra_config.py
================================================
import os
from unittest.mock import patch

from minisweagent.run.extra.config import app, configure_if_first_time, edit, set, setup, unset


class TestConfigSetup:
    """Test the setup function with various inputs."""

    def test_setup_with_all_inputs(self, tmp_path):
        """Test setup function when user provides all inputs."""
        config_file = tmp_path / ".env"

        with (
            patch("minisweagent.run.extra.config.global_config_file", config_file),
            patch("minisweagent.run.extra.config.prompt") as mock_prompt,
            patch("minisweagent.run.extra.config.console.print"),
        ):
            mock_prompt.side_effect = ["claude-sonnet-4-20250514", "ANTHROPIC_API_KEY", "sk-test123"]

            setup()

            # Verify the file was created and contains the expected content
            assert config_file.exists()
            content = config_file.read_text()
            assert "MSWEA_MODEL_NAME='claude-sonnet-4-20250514'" in content
            assert "ANTHROPIC_API_KEY='sk-test123'" in content
            assert "MSWEA_CONFIGURED='true'" in content

    def test_setup_with_model_only(self, tmp_path):
        """Test setup when user only provides model name."""
        config_file = tmp_path / ".env"

        with (
            patch("minisweagent.run.extra.config.global_config_file", config_file),
            patch("minisweagent.run.extra.config.prompt") as mock_prompt,
            patch("minisweagent.run.extra.config.console.print"),
        ):
            mock_prompt.side_effect = ["gpt-4", "", ""]

            setup()

            content = config_file.read_text()
            assert "MSWEA_MODEL_NAME='gpt-4'" in content
            assert "MSWEA_CONFIGURED='true'" in content
            # Should not contain any API key
            assert "ANTHROPIC_API_KEY" not in content
            assert "OPENAI_API_KEY" not in content

    def test_setup_with_empty_inputs(self, tmp_path):
        """Test setup when user provides empty inputs."""
        config_file = tmp_path / ".env"

        with (
            patch("minisweagent.run.extra.config.global_config_file", config_file),
            patch("minisweagent.run.extra.config.prompt") as mock_prompt,
            patch("minisweagent.run.extra.config.console.print"),
        ):
            mock_prompt.side_effect = ["", "", ""]

            setup()

            content = config_file.read_text()
            # Should only have configured flag
            assert "MSWEA_CONFIGURED='true'" in content
            assert "MSWEA_MODEL_NAME" not in content

    def test_setup_with_existing_env_vars(self, tmp_path):
        """Test setup when environment variables already exist."""
        config_file = tmp_path / ".env"

        with (
            patch("minisweagent.run.extra.config.global_config_file", config_file),
            patch("minisweagent.run.extra.config.prompt") as mock_prompt,
            patch("minisweagent.run.extra.config.console.print"),
            patch.dict(os.environ, {"MSWEA_MODEL_NAME": "existing-model", "ANTHROPIC_API_KEY": "existing-key"}),
        ):
            # When prompted, user accepts defaults (existing values)
            mock_prompt.side_effect = ["existing-model", "ANTHROPIC_API_KEY", "existing-key"]

            setup()

            content = config_file.read_text()
            assert "MSWEA_MODEL_NAME='existing-model'" in content
            assert "ANTHROPIC_API_KEY='existing-key'" in content

    def test_setup_key_name_but_no_value(self, tmp_path):
        """Test setup when user provides key name but no value."""
        config_file = tmp_path / ".env"

        with (
            patch("minisweagent.run.extra.config.global_config_file", config_file),
            patch("minisweagent.run.extra.config.prompt") as mock_prompt,
            patch("minisweagent.run.extra.config.console.print") as mock_print,
        ):
            mock_prompt.side_effect = ["gpt-4", "OPENAI_API_KEY", ""]

            setup()

            content = config_file.read_text()
            assert "MSWEA_MODEL_NAME='gpt-4'" in content
            assert "MSWEA_CONFIGURED='true'" in content
            # Should not contain the API key since no value was provided
            assert "OPENAI_API_KEY" not in content
            mock_print.assert_any_call(
                "[bold red]API key setup not completed.[/bold red] Totally fine if you have your keys as environment variables."
            )


class TestConfigSet:
    """Test the set function for setting individual key-value pairs."""

    def test_set_with_both_arguments_provided(self, tmp_path):
        """Test set command when both key and value are provided as arguments."""
        config_file = tmp_path / ".env"

        with patch("minisweagent.run.extra.config.global_config_file", config_file):
            set("MSWEA_MODEL_NAME", "claude-sonnet-4-20250514")

            assert config_file.exists()
            content = config_file.read_text()
            assert "MSWEA_MODEL_NAME='claude-sonnet-4-20250514'" in content

    def test_set_with_no_arguments_prompts_for_both(self, tmp_path):
        """Test set command when no arguments provided - should prompt for both key and value."""
        config_file = tmp_path / ".env"

        with (
            patch("minisweagent.run.extra.config.global_config_file", config_file),
            patch("minisweagent.run.extra.config.prompt") as mock_prompt,
        ):
            mock_prompt.side_effect = ["TEST_KEY", "test_value"]

            set(None, None)

            assert mock_prompt.call_count == 2
            mock_prompt.assert_any_call("Enter the key to set: ")
            mock_prompt.assert_any_call("Enter the value for TEST_KEY: ")

            content = config_file.read_text()
            assert "TEST_KEY='test_value'" in content

    def test_set_with_key_only_prompts_for_value(self, tmp_path):
        """Test set command when only key is provided - should prompt for value."""
        config_file = tmp_path / ".env"

        with (
            patch("minisweagent.run.extra.config.global_config_file", config_file),
            patch("minisweagent.run.extra.config.prompt") as mock_prompt,
        ):
            mock_prompt.return_value = "prompted_value"

            set("PROVIDED_KEY", None)

            mock_prompt.assert_called_once_with("Enter the value for PROVIDED_KEY: ")

            content = config_file.read_text()
            assert "PROVIDED_KEY='prompted_value'" in content

    def test_set_with_value_only_prompts_for_key(self, tmp_path):
        """Test set command when only value is provided - should prompt for key."""
        config_file = tmp_path / ".env"

        with (
            patch("minisweagent.run.extra.config.global_config_file", config_file),
            patch("minisweagent.run.extra.config.prompt") as mock_prompt,
        ):
            mock_prompt.return_value = "prompted_key"

            set(None, "PROVIDED_VALUE")

            mock_prompt.assert_called_once_with("Enter the key to set: ")

            content = config_file.read_text()
            assert "prompted_key='PROVIDED_VALUE'" in content

    def test_set_key_value(self, tmp_path):
        """Test setting a key-value pair (legacy test for compatibility)."""
        config_file = tmp_path / ".env"

        with patch("minisweagent.run.extra.config.global_config_file", config_file):
            set("MSWEA_MODEL_NAME", "claude-sonnet-4-20250514")

            assert config_file.exists()
            content = config_file.read_text()
            assert "MSWEA_MODEL_NAME='claude-sonnet-4-20250514'" in content

    def test_set_api_key(self, tmp_path):
        """Test setting an API key."""
        config_file = tmp_path / ".env"

        with patch("minisweagent.run.extra.config.global_config_file", config_file):
            set("ANTHROPIC_API_KEY", "sk-anthropic-test-key")

            content = config_file.read_text()
            assert "ANTHROPIC_API_KEY='sk-anthropic-test-key'" in content

    def test_set_multiple_keys(self, tmp_path):
        """Test setting multiple keys in sequence."""
        config_file = tmp_path / ".env"

        with patch("minisweagent.run.extra.config.global_config_file", config_file):
            set("MSWEA_MODEL_NAME", "gpt-4")
            set("OPENAI_API_KEY", "sk-openai-test")
            set("MSWEA_GLOBAL_COST_LIMIT", "10.00")

            content = config_file.read_text()
            assert "MSWEA_MODEL_NAME='gpt-4'" in content
            assert "OPENAI_API_KEY='sk-openai-test'" in content
            assert "MSWEA_GLOBAL_COST_LIMIT='10.00'" in content

    def test_set_overwrites_existing_key(self, tmp_path):
        """Test that setting a key overwrites existing value."""
        config_file = tmp_path / ".env"
        config_file.write_text("MSWEA_MODEL_NAME=old-model\nOTHER_KEY=other-value\n")

        with patch("minisweagent.run.extra.config.global_config_file", config_file):
            set("MSWEA_MODEL_NAME", "new-model")

            content = config_file.read_text()
            assert "MSWEA_MODEL_NAME='new-model'" in content
            assert "old-model" not in content
            # Other keys should remain unchanged
            assert "OTHER_KEY=other-value" in content

    def test_set_with_empty_strings_via_prompt(self, tmp_path):
        """Test set command when prompted values are empty strings."""
        config_file = tmp_path / ".env"

        with (
            patch("minisweagent.run.extra.config.global_config_file", config_file),
            patch("minisweagent.run.extra.config.prompt") as mock_prompt,
        ):
            mock_prompt.side_effect = ["EMPTY_KEY", ""]

            set(None, None)

            content = config_file.read_text()
            assert "EMPTY_KEY=''" in content


class TestConfigUnset:
    """Test the unset function for removing key-value pairs."""

    def test_unset_with_argument_provided(self, tmp_path):
        """Test unset command when key is provided as argument."""
        config_file = tmp_path / ".env"
        config_file.write_text("MSWEA_MODEL_NAME='gpt-4'\nOPENAI_API_KEY='sk-test123'\n")

        with patch("minisweagent.run.extra.config.global_config_file", config_file):
            unset("MSWEA_MODEL_NAME")

            content = config_file.read_text()
            assert "MSWEA_MODEL_NAME" not in content
            # Other keys should remain
            assert "OPENAI_API_KEY='sk-test123'" in content

    def test_unset_with_no_argument_prompts_for_key(self, tmp_path):
        """Test unset command when no argument provided - should prompt for key."""
        config_file = tmp_path / ".env"
        config_file.write_text("TEST_KEY='test_value'\nOTHER_KEY='other_value'\n")

        with (
            patch("minisweagent.run.extra.config.global_config_file", config_file),
            patch("minisweagent.run.extra.config.prompt") as mock_prompt,
        ):
            mock_prompt.return_value = "TEST_KEY"

            unset(None)

            mock_prompt.assert_called_once_with("Enter the key to unset: ")

            content = config_file.read_text()
            assert "TEST_KEY" not in content
            assert "OTHER_KEY='other_value'" in content

    def test_unset_existing_key(self, tmp_path):
        """Test unsetting an existing key (legacy test for compatibility)."""
        config_file = tmp_path / ".env"
        config_file.write_text("MSWEA_MODEL_NAME='gpt-4'\nOPENAI_API_KEY='sk-test123'\n")

        with patch("minisweagent.run.extra.config.global_config_file", config_file):
            unset("MSWEA_MODEL_NAME")

            content = config_file.read_text()
            assert "MSWEA_MODEL_NAME" not in content
            # Other keys should remain
            assert "OPENAI_API_KEY='sk-test123'" in content

    def test_unset_nonexistent_key(self, tmp_path):
        """Test unsetting a key that doesn't exist (should not error)."""
        config_file = tmp_path / ".env"
        config_file.write_text("MSWEA_MODEL_NAME='gpt-4'\n")

        with patch("minisweagent.run.extra.config.global_config_file", config_file):
            # Should not raise an exception
            unset("NONEXISTENT_KEY")

            content = config_file.read_text()
            # Original content should remain unchanged
            assert "MSWEA_MODEL_NAME='gpt-4'" in content

    def test_unset_from_empty_file(self, tmp_path):
        """Test unsetting from an empty file."""
        config_file = tmp_path / ".env"
        config_file.write_text("")

        with patch("minisweagent.run.extra.config.global_config_file", config_file):
            # Should not raise an exception
            unset("ANY_KEY")

            # File should remain empty
            content = config_file.read_text()
            assert content == ""

    def test_unset_from_file_with_multiple_keys(self, tmp_path):
        """Test unsetting one key from a file with multiple keys."""
        config_file = tmp_path / ".env"
        config_file.write_text(
            "MSWEA_MODEL_NAME='claude-sonnet-4-20250514'\n"
            "ANTHROPIC_API_KEY='sk-anthropic-key'\n"
            "OPENAI_API_KEY='sk-openai-key'\n"
            "MSWEA_CONFIGURED='true'\n"
        )

        with patch("minisweagent.run.extra.config.global_config_file", config_file):
            unset("ANTHROPIC_API_KEY")

            content = config_file.read_text()
            # Target key should be removed
            assert "ANTHROPIC_API_KEY" not in content
            # Other keys should remain
            assert "MSWEA_MODEL_NAME='claude-sonnet-4-20250514'" in content
            assert "OPENAI_API_KEY='sk-openai-key'" in content
            assert "MSWEA_CONFIGURED='true'" in content

    def test_unset_api_key_scenario(self, tmp_path):
        """Test unsetting an API key specifically."""
        config_file = tmp_path / ".env"
        config_file.write_text("MSWEA_MODEL_NAME='gpt-4'\nOPENAI_API_KEY='sk-old-key'\nMSWEA_CONFIGURED='true'\n")

        with patch("minisweagent.run.extra.config.global_config_file", config_file):
            unset("OPENAI_API_KEY")

            content = config_file.read_text()
            # API key should be completely removed
            assert "OPENAI_API_KEY" not in content
            assert "sk-old-key" not in content
            # Other config should remain
            assert "MSWEA_MODEL_NAME='gpt-4'" in content
            assert "MSWEA_CONFIGURED='true'" in content

    def test_unset_configured_flag(self, tmp_path):
        """Test unsetting the configured flag."""
        config_file = tmp_path / ".env"
        config_file.write_text("MSWEA_MODEL_NAME='gpt-4'\nMSWEA_CONFIGURED='true'\n")

        with patch("minisweagent.run.extra.config.global_config_file", config_file):
            unset("MSWEA_CONFIGURED")

            content = config_file.read_text()
            # Configured flag should be removed
            assert "MSWEA_CONFIGURED" not in content
            # Model should remain
            assert "MSWEA_MODEL_NAME='gpt-4'" in content


class TestConfigEdit:
    """Test the edit function."""

    def test_edit_with_default_editor(self, tmp_path):
        """Test edit function with default editor (nano)."""
        config_file = tmp_path / ".env"
        config_file.write_text("MSWEA_MODEL_NAME=test")

        with (
            patch("minisweagent.run.extra.config.global_config_file", config_file),
            patch("subprocess.run") as mock_run,
            patch.dict(os.environ, {}, clear=True),  # Clear EDITOR env var
        ):
            edit()

            mock_run.assert_called_once_with(["nano", config_file])

    def test_edit_with_custom_editor(self, tmp_path):
        """Test edit function with custom editor."""
        config_file = tmp_path / ".env"
        config_file.write_text("MSWEA_MODEL_NAME=test")

        with (
            patch("minisweagent.run.extra.config.global_config_file", config_file),
            patch("subprocess.run") as mock_run,
            patch.dict(os.environ, {"EDITOR": "vim"}),
        ):
            edit()

            mock_run.assert_called_once_with(["vim", config_file])


class TestConfigureIfFirstTime:
    """Test the configure_if_first_time function."""

    def test_configure_when_not_configured(self, tmp_path):
        """Test that setup is called when MSWEA_CONFIGURED is not set."""
        config_file = tmp_path / ".env"

        with (
            patch("minisweagent.run.extra.config.global_config_file", config_file),
            patch("minisweagent.run.extra.config.setup") as mock_setup,
            patch("minisweagent.run.extra.config.console.print") as mock_print,
            patch.dict(os.environ, {}, clear=True),  # Clear MSWEA_CONFIGURED
        ):
            configure_if_first_time()

            mock_setup.assert_called_once()
            mock_print.assert_called()

    def test_skip_configure_when_already_configured(self, tmp_path):
        """Test that setup is not called when MSWEA_CONFIGURED is set."""
        with (
            patch("minisweagent.run.extra.config.setup") as mock_setup,
            patch.dict(os.environ, {"MSWEA_CONFIGURED": "true"}),
        ):
            configure_if_first_time()

            mock_setup.assert_not_called()


class TestTyperAppIntegration:
    """Test the Typer app commands directly."""

    def test_set_command_via_typer(self, tmp_path):
        """Test the set command through the Typer app."""
        config_file = tmp_path / ".env"

        with (
            patch("minisweagent.run.extra.config.global_config_file", config_file),
            patch("typer.Option") as mock_option,
        ):
            # Mock the typer Option to return our test values
            mock_option.side_effect = (
                lambda default, **kwargs: "OPENAI_API_KEY" if "key" in str(kwargs) else "sk-test-key"
            )

            # Call the set function directly (as the app would)
            set("OPENAI_API_KEY", "sk-test-key")

            content = config_file.read_text()
            assert "OPENAI_API_KEY='sk-test-key'" in content

    def test_unset_command_via_typer(self, tmp_path):
        """Test the unset command through the Typer app."""
        config_file = tmp_path / ".env"
        config_file.write_text("OPENAI_API_KEY='sk-test-key'\nMSWEA_MODEL_NAME='gpt-4'\n")

        with patch("minisweagent.run.extra.config.global_config_file", config_file):
            # Call the unset function directly (as the app would)
            unset("OPENAI_API_KEY")

            content = config_file.read_text()
            assert "OPENAI_API_KEY" not in content
            assert "MSWEA_MODEL_NAME='gpt-4'" in content

    def test_app_help_contains_config_file_path(self):
        """Test that the app help string includes the config file path."""
        help_text = app.info.help
        assert help_text is not None
        assert "global_config_file" in help_text or ".env" in help_text

    def test_app_no_args_is_help(self):
        """Test that the app shows help when no arguments provided."""
        assert app.info.no_args_is_help is True



================================================
FILE: tests/run/test_github_issue.py
================================================
import re
from unittest.mock import patch

import pytest

from minisweagent.models.test_models import DeterministicModel
from minisweagent.run.github_issue import DEFAULT_CONFIG, main


def normalize_outputs(s: str) -> str:
    """Strip leading/trailing whitespace and normalize internal whitespace"""
    # Remove everything between <args> and </args>, because this contains docker container ids
    s = re.sub(r"<args>(.*?)</args>", "", s, flags=re.DOTALL)
    # Replace all lines that have root in them because they tend to appear with times
    s = "\n".join(l for l in s.split("\n") if "root root" not in l)
    return "\n".join(line.rstrip() for line in s.strip().split("\n"))


def assert_observations_match(expected_observations: list[str], messages: list[dict]) -> None:
    """Compare expected observations with actual observations from agent messages

    Args:
        expected_observations: List of expected observation strings
        messages: Agent conversation messages (list of message dicts with 'role' and 'content')
    """
    # Extract actual observations from agent messages
    # User messages (observations) are at indices 3, 5, 7, etc.
    actual_observations = []
    for i in range(len(expected_observations)):
        user_message_index = 3 + (i * 2)
        assert messages[user_message_index]["role"] == "user"
        actual_observations.append(messages[user_message_index]["content"])

    assert len(actual_observations) == len(expected_observations), (
        f"Expected {len(expected_observations)} observations, got {len(actual_observations)}"
    )

    for i, (expected_observation, actual_observation) in enumerate(zip(expected_observations, actual_observations)):
        normalized_actual = normalize_outputs(actual_observation)
        normalized_expected = normalize_outputs(expected_observation)

        assert normalized_actual == normalized_expected, (
            f"Step {i + 1} observation mismatch:\nExpected: {repr(normalized_expected)}\nActual: {repr(normalized_actual)}"
        )


def test_configure_if_first_time_called():
    """Test that configure_if_first_time is called when running github_issue main."""
    with (
        patch("minisweagent.run.github_issue.configure_if_first_time") as mock_configure,
        patch("minisweagent.run.github_issue.fetch_github_issue") as mock_fetch,
        patch("minisweagent.run.github_issue.InteractiveAgent") as mock_agent,
        patch("minisweagent.run.github_issue.get_model"),
        patch("minisweagent.run.github_issue.DockerEnvironment"),
        patch("minisweagent.run.github_issue.yaml.safe_load") as mock_yaml_load,
        patch("minisweagent.run.github_issue.get_config_path") as mock_get_config_path,
        patch("minisweagent.run.github_issue.save_traj"),
    ):
        mock_fetch.return_value = "Test issue"
        mock_yaml_load.return_value = {"agent": {}, "environment": {}, "model": {}}
        mock_get_config_path.return_value.read_text.return_value = "test config"
        mock_agent_instance = mock_agent.return_value
        mock_agent_instance.run.return_value = (0, "success")
        mock_agent_instance.env.execute.return_value = None

        main(issue_url="https://github.com/test/repo/issues/1", config=DEFAULT_CONFIG, model="test-model", yolo=True)

        mock_configure.assert_called_once()


@pytest.mark.slow
def test_github_issue_end_to_end(github_test_data):
    """Test the complete flow from CLI to final result using real environment but deterministic model"""

    model_responses = github_test_data["model_responses"]
    expected_observations = github_test_data["expected_observations"]

    with (
        patch("minisweagent.run.github_issue.configure_if_first_time"),
        patch("minisweagent.run.github_issue.get_model") as mock_get_model,
        patch("minisweagent.agents.interactive.prompt_session.prompt", return_value=""),  # No new task
    ):
        mock_get_model.return_value = DeterministicModel(outputs=model_responses)
        github_url = "https://github.com/SWE-agent/test-repo/issues/1"
        agent = main(issue_url=github_url, model="tardis", config=DEFAULT_CONFIG, yolo=True)  # type: ignore

    assert agent is not None
    messages = agent.messages

    # Verify we have the right number of messages
    # Should be: system + user (initial) + (assistant + user) * number_of_steps
    expected_total_messages = 2 + (len(model_responses) * 2)
    assert len(messages) == expected_total_messages, f"Expected {expected_total_messages} messages, got {len(messages)}"

    assert_observations_match(expected_observations, messages)

    assert agent.model.n_calls == len(model_responses), (
        f"Expected {len(model_responses)} steps, got {agent.model.n_calls}"
    )



================================================
FILE: tests/run/test_inspector.py
================================================
import json
import tempfile
from pathlib import Path
from unittest.mock import patch

import pytest
import typer

from minisweagent.run.inspector import TrajectoryInspector, main


def get_screen_text(app: TrajectoryInspector) -> str:
    """Extract all text content from the app's UI."""
    text_parts = []

    # Get all Static widgets in the main content container
    content_container = app.query_one("#content")
    for static_widget in content_container.query("Static"):
        if static_widget.display:
            if hasattr(static_widget, "renderable") and static_widget.renderable:  # type: ignore[attr-defined]
                text_parts.append(str(static_widget.renderable))  # type: ignore[attr-defined]

    return "\n".join(text_parts)


@pytest.fixture
def sample_simple_trajectory():
    """Sample trajectory in simple format (list of messages)."""
    return [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello, solve this problem."},
        {"role": "assistant", "content": "I'll help you solve this.\n\n```bash\nls -la\n```"},
        {"role": "user", "content": "Command output here."},
        {
            "role": "assistant",
            "content": "Now I'll finish.\n\n```bash\necho COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT\n```",
        },
    ]


@pytest.fixture
def sample_swebench_trajectory():
    """Sample trajectory in SWEBench format (dict with messages array)."""
    return {
        "instance_id": "test-instance-1",
        "info": {
            "exit_status": "Submitted",
            "submission": "Fixed the issue",
            "model_stats": {"instance_cost": 0.05, "api_calls": 3},
        },
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": [{"type": "text", "text": "Please solve this issue."}]},
            {"role": "assistant", "content": "I'll analyze the issue.\n\n```bash\ncat file.py\n```"},
            {"role": "user", "content": [{"type": "text", "text": "File contents here."}]},
            {"role": "assistant", "content": "Fixed!\n\n```bash\necho COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT\n```"},
        ],
    }


@pytest.fixture
def temp_trajectory_files(sample_simple_trajectory, sample_swebench_trajectory):
    """Create temporary trajectory files for testing."""
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)

        # Simple format trajectory
        simple_file = temp_path / "simple.traj.json"
        simple_file.write_text(json.dumps(sample_simple_trajectory, indent=2))

        # SWEBench format trajectory
        swebench_file = temp_path / "swebench.traj.json"
        swebench_file.write_text(json.dumps(sample_swebench_trajectory, indent=2))

        # Invalid JSON file
        invalid_file = temp_path / "invalid.traj.json"
        invalid_file.write_text("invalid json content")

        yield [simple_file, swebench_file, invalid_file]


@pytest.mark.slow
async def test_trajectory_inspector_basic_navigation(temp_trajectory_files):
    """Test basic step navigation in trajectory inspector."""
    valid_files = [f for f in temp_trajectory_files if f.name != "invalid.traj.json"]

    app = TrajectoryInspector(valid_files)

    async with app.run_test() as pilot:
        # Should start with first trajectory, first step
        await pilot.pause(0.1)
        assert "Trajectory 1/2 - simple.traj.json - Step 1/3" in app.title
        content = get_screen_text(app)
        assert "SYSTEM" in content
        assert "You are a helpful assistant" in content
        assert "solve this problem" in content

        # Navigate to next step
        await pilot.press("l")
        assert "Step 2/3" in app.title
        assert "MINI-SWE-AGENT" in get_screen_text(app)
        assert "I'll help you solve this" in get_screen_text(app)

        # Navigate to last step
        await pilot.press("$")
        assert "Step 3/3" in app.title
        assert "MINI-SWE-AGENT" in get_screen_text(app)
        assert "echo COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT" in get_screen_text(app)

        # Navigate back to first step
        await pilot.press("0")
        assert "Step 1/3" in app.title
        assert "SYSTEM" in get_screen_text(app)

        # Navigate with left/right arrows
        await pilot.press("right")
        assert "Step 2/3" in app.title
        await pilot.press("left")
        assert "Step 1/3" in app.title


@pytest.mark.slow
async def test_trajectory_inspector_trajectory_navigation(temp_trajectory_files):
    """Test navigation between different trajectory files."""
    valid_files = [f for f in temp_trajectory_files if f.name != "invalid.traj.json"]

    app = TrajectoryInspector(valid_files)

    async with app.run_test() as pilot:
        await pilot.pause(0.1)

        # Should start with first trajectory
        assert "Trajectory 1/2 - simple.traj.json" in app.title
        content = get_screen_text(app)
        assert "You are a helpful assistant" in content

        # Navigate to next trajectory
        await pilot.press("L")
        assert "Trajectory 2/2 - swebench.traj.json" in app.title
        await pilot.pause(0.1)
        content = get_screen_text(app)
        assert "You are a helpful assistant" in content

        # Navigate back to previous trajectory
        await pilot.press("H")
        assert "Trajectory 1/2 - simple.traj.json" in app.title

        # Try to navigate beyond bounds
        await pilot.press("H")  # Should stay at first
        assert "Trajectory 1/2 - simple.traj.json" in app.title

        await pilot.press("L")  # Go to second
        await pilot.press("L")  # Try to go beyond
        assert "Trajectory 2/2 - swebench.traj.json" in app.title  # Should stay at last


@pytest.mark.slow
async def test_trajectory_inspector_swebench_format(temp_trajectory_files):
    """Test that SWEBench format trajectories are handled correctly."""
    valid_files = [f for f in temp_trajectory_files if f.name != "invalid.traj.json"]

    app = TrajectoryInspector(valid_files)

    async with app.run_test() as pilot:
        # Navigate to SWEBench trajectory
        await pilot.press("L")
        await pilot.pause(0.1)

        assert "Trajectory 2/2 - swebench.traj.json" in app.title
        assert "Step 1/3" in app.title

        # Check that list content is properly rendered - step 1 should have the initial user message
        content = get_screen_text(app)
        assert "Please solve this issue" in content


@pytest.mark.slow
async def test_trajectory_inspector_scrolling(temp_trajectory_files):
    """Test scrolling behavior in trajectory inspector."""
    valid_files = [f for f in temp_trajectory_files if f.name != "invalid.traj.json"]

    app = TrajectoryInspector(valid_files)

    async with app.run_test() as pilot:
        await pilot.pause(0.1)

        # Test scrolling
        vs = app.query_one("VerticalScroll")
        initial_y = vs.scroll_target_y

        await pilot.press("j")  # scroll down
        assert vs.scroll_target_y >= initial_y

        await pilot.press("k")  # scroll up
        # Should scroll up (may not be exactly equal due to content constraints)


@pytest.mark.slow
async def test_trajectory_inspector_empty_trajectory():
    """Test inspector behavior with empty trajectory list."""
    app = TrajectoryInspector([])

    async with app.run_test() as pilot:
        await pilot.pause(0.1)

        assert "Trajectory Inspector - No Data" in app.title
        assert "No trajectory loaded" in get_screen_text(app)

        # Navigation should not crash
        await pilot.press("l")
        await pilot.press("h")
        await pilot.press("L")
        await pilot.press("H")


async def test_trajectory_inspector_invalid_file(temp_trajectory_files):
    """Test inspector behavior with invalid JSON file."""
    invalid_file = [f for f in temp_trajectory_files if f.name == "invalid.traj.json"][0]

    # Mock notify to capture error messages
    app = TrajectoryInspector([invalid_file])

    # Since this is not an async run_test, we need to manually trigger the load
    # The error should be captured when _load_current_trajectory is called
    app._load_current_trajectory()

    assert app.messages == []
    assert app.steps == []


def test_trajectory_inspector_load_trajectory_formats(sample_simple_trajectory, sample_swebench_trajectory):
    """Test loading different trajectory formats."""
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)

        # Test simple format
        simple_file = temp_path / "simple.traj.json"
        simple_file.write_text(json.dumps(sample_simple_trajectory))

        app = TrajectoryInspector([simple_file])
        assert len(app.messages) == 5
        assert len(app.steps) == 3

        # Test SWEBench format
        swebench_file = temp_path / "swebench.traj.json"
        swebench_file.write_text(json.dumps(sample_swebench_trajectory))

        app = TrajectoryInspector([swebench_file])
        assert len(app.messages) == 5
        assert len(app.steps) == 3


def test_trajectory_inspector_unrecognized_format():
    """Test inspector behavior with unrecognized trajectory format."""
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)

        # Create file with unrecognized format
        unrecognized_file = temp_path / "unrecognized.traj.json"
        unrecognized_file.write_text(json.dumps({"some": "other", "format": True}))

        app = TrajectoryInspector([unrecognized_file])

        # Should handle gracefully
        assert app.messages == []
        assert app.steps == []


def test_trajectory_inspector_current_trajectory_name():
    """Test current_trajectory_name property."""
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)
        test_file = temp_path / "test.traj.json"
        test_file.write_text(json.dumps([]))

        app = TrajectoryInspector([test_file])
        assert app.current_trajectory_name == "test.traj.json"

        # Test with empty trajectory list
        app = TrajectoryInspector([])
        assert app.current_trajectory_name == "No trajectories"


@pytest.mark.slow
async def test_trajectory_inspector_css_loading():
    """Test that CSS is properly loaded from config."""
    app = TrajectoryInspector([])

    # Verify CSS contains expected styles
    assert ".message-container" in app.CSS
    assert ".message-header" in app.CSS
    assert ".message-content" in app.CSS


@pytest.mark.slow
async def test_trajectory_inspector_quit_binding(temp_trajectory_files):
    """Test quit functionality."""
    valid_files = [f for f in temp_trajectory_files if f.name != "invalid.traj.json"]

    app = TrajectoryInspector(valid_files)

    async with app.run_test() as pilot:
        await pilot.pause(0.1)

        # Test quit functionality
        await pilot.press("q")
        await pilot.pause(0.1)

        # App should exit gracefully (the test framework handles this)


@patch("minisweagent.run.inspector.TrajectoryInspector.run")
def test_main_with_single_file(mock_run, temp_trajectory_files):
    """Test main function with a single trajectory file."""
    valid_file = temp_trajectory_files[0]  # simple.traj.json

    main(str(valid_file))

    mock_run.assert_called_once()
    # Verify the inspector was created with the correct file
    assert mock_run.call_count == 1


@patch("minisweagent.run.inspector.TrajectoryInspector.run")
def test_main_with_directory_containing_trajectories(mock_run, temp_trajectory_files):
    """Test main function with a directory containing trajectory files."""
    directory = temp_trajectory_files[0].parent

    main(str(directory))

    mock_run.assert_called_once()


@patch("minisweagent.run.inspector.TrajectoryInspector.run")
def test_main_with_directory_no_trajectories(mock_run):
    """Test main function with a directory containing no trajectory files."""
    with tempfile.TemporaryDirectory() as temp_dir:
        # Create some non-trajectory files
        temp_path = Path(temp_dir)
        (temp_path / "other.json").write_text('{"not": "trajectory"}')
        (temp_path / "readme.txt").write_text("some text")

        with pytest.raises(typer.BadParameter, match="No trajectory files found"):
            main(str(temp_dir))

        mock_run.assert_not_called()


@patch("minisweagent.run.inspector.TrajectoryInspector.run")
def test_main_with_nonexistent_path(mock_run):
    """Test main function with a path that doesn't exist."""
    nonexistent_path = "/this/path/does/not/exist"

    with pytest.raises(typer.BadParameter, match="Path .* does not exist"):
        main(nonexistent_path)

    mock_run.assert_not_called()


@patch("minisweagent.run.inspector.TrajectoryInspector.run")
def test_main_with_current_directory_default(mock_run, temp_trajectory_files):
    """Test main function with default argument (current directory)."""
    directory = temp_trajectory_files[0].parent

    # Change to the temp directory to test the default "." behavior
    import os

    original_cwd = os.getcwd()
    try:
        os.chdir(str(directory))
        main(".")  # Explicitly test with "." since default is handled by typer
        mock_run.assert_called_once()
    finally:
        os.chdir(original_cwd)



================================================
FILE: tests/run/test_local.py
================================================
from unittest.mock import patch

from minisweagent.models.test_models import DeterministicModel
from minisweagent.run.mini import DEFAULT_CONFIG, main
from tests.run.test_github_issue import assert_observations_match


def test_local_end_to_end(local_test_data):
    """Test the complete flow from CLI to final result using real environment but deterministic model"""

    model_responses = local_test_data["model_responses"]
    expected_observations = local_test_data["expected_observations"]

    with (
        patch("minisweagent.run.mini.configure_if_first_time"),
        patch("minisweagent.models.litellm_model.LitellmModel") as mock_model_class,
        patch("minisweagent.agents.interactive.prompt_session.prompt", return_value=""),  # No new task
    ):
        mock_model_class.return_value = DeterministicModel(outputs=model_responses)
        agent = main(
            model_name="tardis",
            config_spec=DEFAULT_CONFIG,
            yolo=True,
            task="Blah blah blah",
            output=None,
            visual=False,
            cost_limit=10,
        )  # type: ignore

    assert agent is not None
    messages = agent.messages

    # Verify we have the right number of messages
    # Should be: system + user (initial) + (assistant + user) * number_of_steps
    expected_total_messages = 2 + (len(model_responses) * 2)
    assert len(messages) == expected_total_messages, f"Expected {expected_total_messages} messages, got {len(messages)}"

    assert_observations_match(expected_observations, messages)

    assert agent.model.n_calls == len(model_responses), (
        f"Expected {len(model_responses)} steps, got {agent.model.n_calls}"
    )



================================================
FILE: tests/run/test_run_hello_world.py
================================================
from unittest.mock import patch

from minisweagent.models.test_models import DeterministicModel
from minisweagent.run.hello_world import main
from tests.run.test_github_issue import assert_observations_match


def test_run_hello_world_end_to_end(local_test_data):
    """Test the complete flow from CLI to final result using real environment but deterministic model"""

    model_responses = local_test_data["model_responses"]
    expected_observations = local_test_data["expected_observations"]

    with (
        patch("minisweagent.run.hello_world.LitellmModel") as mock_model_class,
        patch("os.environ", {"MSWEA_MODEL_NAME": "tardis"}),
    ):
        mock_model_class.return_value = DeterministicModel(outputs=model_responses)
        agent = main(task="Blah blah blah")

    assert agent is not None
    messages = agent.messages

    # Verify we have the right number of messages
    # Should be: system + user (initial) + (assistant + user) * number_of_steps
    expected_total_messages = 2 + (len(model_responses) * 2)
    assert len(messages) == expected_total_messages, f"Expected {expected_total_messages} messages, got {len(messages)}"

    assert_observations_match(expected_observations, messages)

    assert agent.model.n_calls == len(model_responses), (
        f"Expected {len(model_responses)} steps, got {agent.model.n_calls}"
    )



================================================
FILE: tests/run/test_save.py
================================================
import json
import tempfile
from pathlib import Path

from minisweagent.agents.default import DefaultAgent
from minisweagent.environments.local import LocalEnvironment
from minisweagent.models.test_models import DeterministicModel
from minisweagent.run.utils.save import save_traj


def test_save_traj_includes_class_names():
    """Test that save_traj includes the full class names with import paths."""
    # Create a simple agent setup
    model = DeterministicModel(outputs=["echo 'test'"])
    env = LocalEnvironment()
    agent = DefaultAgent(model, env)

    # Run a minimal task to populate the agent
    agent.add_message("system", "test system message")
    agent.add_message("user", "test user message")

    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir) / "test_trajectory.json"

        # Save the trajectory
        save_traj(agent, temp_path, exit_status="Submitted", result="test result", print_path=False)

        # Load the saved trajectory
        with temp_path.open() as f:
            saved_data = json.load(f)

        # Verify the structure
        assert "info" in saved_data
        assert "config" in saved_data["info"]

        config = saved_data["info"]["config"]

        # Verify all three class types are present with correct import paths
        assert "agent_type" in config
        assert "model_type" in config
        assert "environment_type" in config

        # Verify the actual class names with module paths
        assert config["agent_type"] == "minisweagent.agents.default.DefaultAgent"
        assert config["model_type"] == "minisweagent.models.test_models.DeterministicModel"
        assert config["environment_type"] == "minisweagent.environments.local.LocalEnvironment"

        # Verify other expected data is still present
        assert saved_data["info"]["exit_status"] == "Submitted"
        assert saved_data["info"]["submission"] == "test result"
        assert saved_data["trajectory_format"] == "mini-swe-agent-1"


def test_save_traj_with_none_agent():
    """Test that save_traj works correctly when agent is None."""
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir) / "test_trajectory.json"

        # Save with None agent
        save_traj(None, temp_path, exit_status="Failed", result="no agent", print_path=False)

        # Load the saved trajectory
        with temp_path.open() as f:
            saved_data = json.load(f)

        # Verify basic structure
        assert "info" in saved_data
        assert saved_data["info"]["exit_status"] == "Failed"
        assert saved_data["info"]["submission"] == "no agent"

        # Verify class types are not present when agent is None (since config is not present)
        # When agent is None, there should be no config section at all

        # Verify config is not present when agent is None
        assert "config" not in saved_data["info"]



================================================
FILE: tests/run/test_swebench.py
================================================
import json
from dataclasses import asdict, dataclass
from typing import Any
from unittest.mock import patch

import pytest

from minisweagent import package_dir
from minisweagent.models.test_models import DeterministicModel
from minisweagent.run.extra.swebench import (
    filter_instances,
    get_swebench_docker_image_name,
    main,
    remove_from_preds_file,
    update_preds_file,
)


@pytest.mark.slow
@pytest.mark.parametrize("workers", [1, 2])
def test_swebench_end_to_end(github_test_data, tmp_path, workers):
    """Test the complete SWEBench flow using the _test subset with deterministic model"""

    model_responses = github_test_data["model_responses"]

    with patch("minisweagent.run.extra.swebench.get_model") as mock_get_model:
        mock_get_model.return_value = DeterministicModel(outputs=model_responses, cost_per_call=0.1)

        main(
            subset="_test",
            split="test",
            slice_spec="0:1",
            output=str(tmp_path),
            workers=workers,
            filter_spec="swe-agent__test-repo-1",
            config_spec=package_dir / "config" / "extra" / "swebench.yaml",
            environment_class="docker",
        )

    traj_file_path = package_dir.parent.parent / "tests" / "test_data" / "github_issue.traj.json"
    trajectory = json.loads(traj_file_path.read_text())

    last_message = trajectory[-1]["content"]

    instance_id = "swe-agent__test-repo-1"
    expected_result = {
        instance_id: {
            "model_name_or_path": "deterministic",
            "instance_id": instance_id,
            "model_patch": last_message,
        }
    }

    with open(tmp_path / "preds.json") as f:
        actual_result = json.load(f)

    assert actual_result == expected_result

    traj_output_file = tmp_path / instance_id / f"{instance_id}.traj.json"
    output_trajectory = json.loads(traj_output_file.read_text())
    assert output_trajectory["messages"][-1]["content"] == last_message


def test_get_image_name_with_existing_image_name():
    """Test get_image_name when image_name is already provided"""
    instance = {"image_name": "custom/image:tag", "instance_id": "test__repo__1"}
    assert get_swebench_docker_image_name(instance) == "custom/image:tag"


def test_get_image_name_without_image_name():
    """Test get_image_name when image_name needs to be constructed"""
    instance = {"instance_id": "swe-agent__test-repo__1"}
    expected = "swebench/sweb.eval.x86_64.swe-agent_1776_test-repo_1776_1:latest"
    assert get_swebench_docker_image_name(instance) == expected


def test_get_image_name_with_none_image_name():
    """Test get_image_name when image_name is explicitly None"""
    instance = {"image_name": None, "instance_id": "django__django__4.0"}
    expected = "swebench/sweb.eval.x86_64.django_1776_django_1776_4.0:latest"
    assert get_swebench_docker_image_name(instance) == expected


def test_get_image_name_with_complex_instance_id():
    """Test get_image_name with complex instance_id containing multiple double underscores"""
    instance = {"instance_id": "project__sub__module__version__1.2.3"}
    expected = "swebench/sweb.eval.x86_64.project_1776_sub_1776_module_1776_version_1776_1.2.3:latest"
    assert get_swebench_docker_image_name(instance) == expected


def test_filter_instances_no_filters():
    """Test filter_instances with no filtering applied"""
    instances = [{"instance_id": "repo1__test1"}, {"instance_id": "repo2__test2"}, {"instance_id": "repo3__test3"}]
    result = filter_instances(instances, filter_spec="", slice_spec="")
    assert result == instances


def test_filter_instances_regex_filter():
    """Test filter_instances with regex filtering"""
    instances = [
        {"instance_id": "django__test1"},
        {"instance_id": "flask__test2"},
        {"instance_id": "django__test3"},
        {"instance_id": "requests__test4"},
    ]
    result = filter_instances(instances, filter_spec=r"django__.*", slice_spec="")
    expected = [{"instance_id": "django__test1"}, {"instance_id": "django__test3"}]
    assert result == expected


def test_filter_instances_slice_only():
    """Test filter_instances with slice specification"""
    instances = [{"instance_id": f"repo{i}__test{i}"} for i in range(10)]
    result = filter_instances(instances, filter_spec="", slice_spec="2:5")
    expected = [{"instance_id": "repo2__test2"}, {"instance_id": "repo3__test3"}, {"instance_id": "repo4__test4"}]
    assert result == expected


def test_filter_instances_slice_start_only():
    """Test filter_instances with slice start only"""
    instances = [{"instance_id": f"repo{i}__test{i}"} for i in range(5)]
    result = filter_instances(instances, filter_spec="", slice_spec="3:")
    expected = [{"instance_id": "repo3__test3"}, {"instance_id": "repo4__test4"}]
    assert result == expected


def test_filter_instances_slice_end_only():
    """Test filter_instances with slice end only"""
    instances = [{"instance_id": f"repo{i}__test{i}"} for i in range(5)]
    result = filter_instances(instances, filter_spec="", slice_spec=":2")
    expected = [{"instance_id": "repo0__test0"}, {"instance_id": "repo1__test1"}]
    assert result == expected


def test_filter_instances_filter_and_slice():
    """Test filter_instances with both filtering and slicing"""
    instances = [
        {"instance_id": "django__test1"},
        {"instance_id": "flask__test2"},
        {"instance_id": "django__test3"},
        {"instance_id": "django__test4"},
        {"instance_id": "requests__test5"},
    ]
    result = filter_instances(instances, filter_spec=r"django__.*", slice_spec="1:3")
    expected = [{"instance_id": "django__test3"}, {"instance_id": "django__test4"}]
    assert result == expected


def test_filter_instances_shuffle():
    """Test filter_instances with shuffle enabled produces deterministic results"""
    instances = [{"instance_id": f"repo{i:02d}__test{i}"} for i in range(10)]
    # Test that shuffle produces same result with same seed
    result1 = filter_instances(instances.copy(), filter_spec="", slice_spec="", shuffle=True)
    result2 = filter_instances(instances.copy(), filter_spec="", slice_spec="", shuffle=True)
    assert result1 == result2
    # Test that shuffled result is different from original order
    result_no_shuffle = filter_instances(instances.copy(), filter_spec="", slice_spec="", shuffle=False)
    assert result1 != result_no_shuffle


def test_filter_instances_empty_list():
    """Test filter_instances with empty input list"""
    result = filter_instances([], filter_spec=r".*", slice_spec="0:5", shuffle=True)
    assert result == []


def test_filter_instances_no_matches():
    """Test filter_instances when regex matches nothing"""
    instances = [{"instance_id": "django__test1"}, {"instance_id": "flask__test2"}]
    result = filter_instances(instances, filter_spec=r"nonexistent__.*", slice_spec="")
    assert result == []


def test_update_preds_file_new_file(tmp_path):
    """Test update_preds_file when output file doesn't exist"""
    output_path = tmp_path / "preds.json"
    update_preds_file(output_path, "test__instance__1", "test_model", "test_result")

    assert output_path.exists()
    result = json.loads(output_path.read_text())
    expected = {
        "test__instance__1": {
            "model_name_or_path": "test_model",
            "instance_id": "test__instance__1",
            "model_patch": "test_result",
        }
    }
    assert result == expected


def test_update_preds_file_existing_file(tmp_path):
    """Test update_preds_file when output file already exists"""
    output_path = tmp_path / "preds.json"

    # Create initial file with one instance
    initial_data = {
        "existing__instance": {
            "model_name_or_path": "old_model",
            "instance_id": "existing__instance",
            "model_patch": "old_result",
        }
    }
    output_path.write_text(json.dumps(initial_data))

    # Add new instance
    update_preds_file(output_path, "new__instance", "new_model", "new_result")

    result = json.loads(output_path.read_text())
    expected = {
        "existing__instance": {
            "model_name_or_path": "old_model",
            "instance_id": "existing__instance",
            "model_patch": "old_result",
        },
        "new__instance": {
            "model_name_or_path": "new_model",
            "instance_id": "new__instance",
            "model_patch": "new_result",
        },
    }
    assert result == expected


def test_update_preds_file_overwrite_existing(tmp_path):
    """Test update_preds_file overwrites existing instance"""
    output_path = tmp_path / "preds.json"

    # Create initial file
    initial_data = {
        "test__instance": {
            "model_name_or_path": "old_model",
            "instance_id": "test__instance",
            "model_patch": "old_result",
        }
    }
    output_path.write_text(json.dumps(initial_data))

    # Update existing instance
    update_preds_file(output_path, "test__instance", "new_model", "new_result")

    result = json.loads(output_path.read_text())
    expected = {
        "test__instance": {
            "model_name_or_path": "new_model",
            "instance_id": "test__instance",
            "model_patch": "new_result",
        }
    }
    assert result == expected


def test_remove_from_preds_file_existing(tmp_path):
    """Test remove_from_preds_file removes existing instance"""
    output_path = tmp_path / "preds.json"

    # Create file with multiple instances
    initial_data = {
        "instance1": {"model_name_or_path": "model1", "instance_id": "instance1", "model_patch": "result1"},
        "instance2": {"model_name_or_path": "model2", "instance_id": "instance2", "model_patch": "result2"},
    }
    output_path.write_text(json.dumps(initial_data))

    # Remove one instance
    remove_from_preds_file(output_path, "instance1")

    result = json.loads(output_path.read_text())
    expected = {"instance2": {"model_name_or_path": "model2", "instance_id": "instance2", "model_patch": "result2"}}
    assert result == expected


def test_remove_from_preds_file_nonexistent_instance(tmp_path):
    """Test remove_from_preds_file with nonexistent instance"""
    output_path = tmp_path / "preds.json"

    initial_data = {"instance1": {"model_name_or_path": "model1", "instance_id": "instance1", "model_patch": "result1"}}
    output_path.write_text(json.dumps(initial_data))

    # Try to remove nonexistent instance
    remove_from_preds_file(output_path, "nonexistent")

    # File should be unchanged
    result = json.loads(output_path.read_text())
    assert result == initial_data


def test_remove_from_preds_file_no_file(tmp_path):
    """Test remove_from_preds_file when file doesn't exist"""
    output_path = tmp_path / "preds.json"

    # Should not raise an error
    remove_from_preds_file(output_path, "any_instance")

    # File should still not exist
    assert not output_path.exists()


@pytest.mark.slow
def test_redo_existing_false_skips_existing(github_test_data, tmp_path):
    """Test that redo_existing=False skips instances that already have results"""
    model_responses = github_test_data["model_responses"]

    # Create existing preds.json with one instance
    preds_file = tmp_path / "preds.json"
    existing_data = {
        "swe-agent__test-repo-1": {
            "model_name_or_path": "previous_model",
            "instance_id": "swe-agent__test-repo-1",
            "model_patch": "previous_result",
        }
    }
    preds_file.write_text(json.dumps(existing_data))

    with patch("minisweagent.run.extra.swebench.get_model") as mock_get_model:
        mock_get_model.return_value = DeterministicModel(outputs=model_responses)

        main(
            subset="_test",
            split="test",
            slice_spec="0:1",
            output=str(tmp_path),
            workers=1,
            filter_spec="swe-agent__test-repo-1",
            redo_existing=False,
            config_spec=package_dir / "config" / "extra" / "swebench.yaml",
        )

    # Should still have the original result
    result = json.loads(preds_file.read_text())
    assert result == existing_data


@pytest.mark.slow
def test_redo_existing_true_overwrites_existing(github_test_data, tmp_path):
    """Test that redo_existing=True processes instances even if they already have results"""
    model_responses = github_test_data["model_responses"]

    # Create existing preds.json with one instance
    preds_file = tmp_path / "preds.json"
    existing_data = {
        "swe-agent__test-repo-1": {
            "model_name_or_path": "previous_model",
            "instance_id": "swe-agent__test-repo-1",
            "model_patch": "previous_result",
        }
    }
    preds_file.write_text(json.dumps(existing_data))

    with patch("minisweagent.run.extra.swebench.get_model") as mock_get_model:
        mock_get_model.return_value = DeterministicModel(outputs=model_responses, cost_per_call=0.1)

        main(
            subset="_test",
            split="test",
            slice_spec="0:1",
            output=str(tmp_path),
            workers=1,
            filter_spec="swe-agent__test-repo-1",
            redo_existing=True,
            config_spec=package_dir / "config" / "extra" / "swebench.yaml",
            environment_class="docker",
        )

    # Should have new result from deterministic model
    traj_file_path = package_dir.parent.parent / "tests" / "test_data" / "github_issue.traj.json"
    trajectory = json.loads(traj_file_path.read_text())
    expected_result = trajectory[-1]["content"]

    result = json.loads(preds_file.read_text())
    assert result["swe-agent__test-repo-1"]["model_patch"] == expected_result
    assert result["swe-agent__test-repo-1"]["model_name_or_path"] == "deterministic"


@dataclass
class ExceptionModelConfig:
    model_name: str = "exception_model"


class ExceptionModel:
    """Test model that raises exceptions during processing."""

    def __init__(self, exception_type: type[Exception] = RuntimeError, exception_message: str = "Test exception"):
        self.exception_type = exception_type
        self.exception_message = exception_message
        self.cost = 0.0
        self.n_calls = 0
        self.config = ExceptionModelConfig()

    def query(self, *args, **kwargs):
        self.n_calls += 1
        raise self.exception_type(self.exception_message)

    def get_template_vars(self) -> dict[str, Any]:
        return asdict(self.config) | {"n_model_calls": self.n_calls, "model_cost": self.cost}


@pytest.mark.slow
@pytest.mark.parametrize("workers", [1, 2])
def test_exception_handling_in_agent_run(tmp_path, workers):
    """Test that exceptions during agent.run() are properly handled and recorded"""
    with patch("minisweagent.run.extra.swebench.get_model") as mock_get_model:
        mock_get_model.return_value = ExceptionModel(RuntimeError, "Agent processing failed")

        with patch("minisweagent.run.extra.swebench.RunBatchProgressManager") as mock_progress_class:
            mock_progress_manager = mock_progress_class.return_value
            mock_progress_manager.render_group = None  # For Live context manager

            main(
                subset="_test",
                split="test",
                slice_spec="0:1",
                output=str(tmp_path),
                workers=workers,
                filter_spec="swe-agent__test-repo-1",
                config_spec=package_dir / "config" / "extra" / "swebench.yaml",
                environment_class="docker",
            )

    # Check that prediction file contains exception information
    preds_file = tmp_path / "preds.json"
    assert preds_file.exists()

    result = json.loads(preds_file.read_text())
    instance_id = "swe-agent__test-repo-1"
    assert instance_id in result
    assert result[instance_id]["model_patch"] == "Agent processing failed"
    assert result[instance_id]["model_name_or_path"] == "exception_model"

    # Check that trajectory file contains exception information
    traj_file = tmp_path / instance_id / f"{instance_id}.traj.json"
    assert traj_file.exists()

    traj_data = json.loads(traj_file.read_text())
    assert traj_data["instance_id"] == instance_id
    assert traj_data["info"]["exit_status"] == "RuntimeError"
    assert traj_data["info"]["submission"] == "Agent processing failed"


@pytest.mark.slow
@pytest.mark.parametrize("workers", [1, 2])
def test_different_exception_types(tmp_path, workers):
    """Test that different exception types are properly recorded"""
    with patch("minisweagent.run.extra.swebench.get_model") as mock_get_model:
        mock_get_model.return_value = ExceptionModel(ValueError, "Invalid input provided")

        with patch("minisweagent.run.extra.swebench.RunBatchProgressManager") as mock_progress_class:
            mock_progress_manager = mock_progress_class.return_value
            mock_progress_manager.render_group = None  # For Live context manager

            main(
                subset="_test",
                split="test",
                slice_spec="0:1",
                output=str(tmp_path),
                workers=workers,
                filter_spec="swe-agent__test-repo-1",
                config_spec=package_dir / "config" / "extra" / "swebench.yaml",
                environment_class="docker",
            )

    # Check trajectory file for correct exception type
    instance_id = "swe-agent__test-repo-1"
    traj_file = tmp_path / instance_id / f"{instance_id}.traj.json"
    traj_data = json.loads(traj_file.read_text())

    assert traj_data["info"]["exit_status"] == "ValueError"
    assert traj_data["info"]["submission"] == "Invalid input provided"


@pytest.mark.slow
def test_exception_handling_with_progress_manager(tmp_path):
    """Test that progress manager receives exception notifications in multithreaded mode"""
    with patch("minisweagent.run.extra.swebench.get_model") as mock_get_model:
        mock_get_model.return_value = ExceptionModel(ConnectionError, "Network timeout")

        with patch("minisweagent.run.extra.swebench.RunBatchProgressManager") as mock_progress_class:
            mock_progress_manager = mock_progress_class.return_value
            mock_progress_manager.render_group = None  # For Live context manager

            main(
                subset="_test",
                split="test",
                slice_spec="0:1",
                output=str(tmp_path),
                workers=2,  # Use multithreaded to test progress manager
                filter_spec="swe-agent__test-repo-1",
                config_spec=package_dir / "config" / "extra" / "swebench.yaml",
                environment_class="docker",
            )

            # Verify progress manager methods were called
            mock_progress_manager.on_instance_start.assert_called_once_with("swe-agent__test-repo-1")
            mock_progress_manager.on_instance_end.assert_called_once_with("swe-agent__test-repo-1", "ConnectionError")

            # on_uncaught_exception should not be called since exceptions are handled properly
            mock_progress_manager.on_uncaught_exception.assert_not_called()



================================================
FILE: tests/run/test_swebench_single.py
================================================
from unittest.mock import patch

import pytest

from minisweagent import package_dir
from minisweagent.models.test_models import DeterministicModel
from minisweagent.run.extra.swebench_single import main


@pytest.mark.slow
def test_swebench_single_end_to_end(github_test_data):
    """Test the swebench_single script using the _test subset with deterministic model.
    This mostly tests that no exception occurs.
    """

    model_responses = github_test_data["model_responses"]

    with patch("minisweagent.run.extra.swebench_single.get_model") as mock_get_model:
        with patch("minisweagent.agents.interactive.prompt_session.prompt", return_value=""):  # No new task
            mock_get_model.return_value = DeterministicModel(outputs=model_responses, cost_per_call=0.1)

            # Test with explicit instance ID
            main(
                subset="_test",
                split="test",
                instance_spec="swe-agent__test-repo-1",
                model_name="deterministic",
                config_path=package_dir / "config" / "extra" / "swebench.yaml",
                environment_class="docker",
                exit_immediately=False,
                output=package_dir / "test_output.json",
            )

        # Verify model was called with correct parameters
        mock_get_model.assert_called_once()


@pytest.mark.slow
def test_swebench_single_end_to_end_exit_immediately(github_test_data):
    """Test the swebench_single script using the _test subset with deterministic model.
    This mostly tests that no exception occurs.
    This test uses the --exit-immediately flag to exit immediately when the agent wants to finish instead of prompting.
    """

    model_responses = github_test_data["model_responses"]

    with patch("minisweagent.run.extra.swebench_single.get_model") as mock_get_model:
        mock_get_model.return_value = DeterministicModel(outputs=model_responses, cost_per_call=0.1)

        # Test with explicit instance ID
        main(
            subset="_test",
            split="test",
            instance_spec="swe-agent__test-repo-1",
            model_name="deterministic",
            config_path=package_dir / "config" / "extra" / "swebench.yaml",
            environment_class="docker",
            exit_immediately=True,
            output=package_dir / "test_output.json",
        )

        # Verify model was called with correct parameters
        mock_get_model.assert_called_once()



================================================
FILE: tests/test_data/__init__.py
================================================
[Empty file]


================================================
FILE: tests/test_data/github_issue.traj.json
================================================
[
  {
    "role": "system",
    "content": "You are a helpful assistant that can interact with a computer.\n\nYour response must contain exactly ONE bash code block with ONE command (or commands connected with && or ||).\nInclude a THOUGHT section before your command where you explain your reasoning process.\nFormat your response as shown in <format_example>.\n\n<format_example>\nYour reasoning and analysis here. Explain why you want to perform the action.\n\n```bash\nyour_command_here\n```\n</format_example>\n\nFailure to follow these rules will cause your response to be rejected.\nTo finish, issue the following command: `echo MINI_SWE_AGENT_FINAL_OUTPUT`\nwithout any other command.\n"
  },
  {
    "role": "user",
    "content": "Please solve this issue: GitHub Issue: SyntaxError: invalid syntax\n\nI'm running `missing_colon.py` as follows:\r\n\r\n```python\r\ndivision(23, 0)\r\n```\r\n\r\nbut I get the following error:\r\n\r\n```\r\n  File \"/Users/fuchur/Documents/24/git_sync/swe-agent-test-repo/tests/./missing_colon.py\", line 4\r\n    def division(a: float, b: float) -> float\r\n                                             ^\r\nSyntaxError: invalid syntax\r\n```\n\nYou can execute bash commands and edit files to implement the necessary changes.\n\n## Recommended Workflow\n1. Analyze the codebase by finding and reading relevant files\n2. Create a script to reproduce the issue\n3. Edit the source code to resolve the issue\n4. Verify your fix works by running your script again\n5. Test edge cases to ensure your fix is robust\n\n## Important Rules\n\n1. Every response must contain exactly one action\n2. The action must be enclosed in triple backticks\n3. Directory or environment variable changes are not persistent. Every action is executed in a new subshell.\n   However, you can prefix any action with `MY_ENV_VAR=MY_VALUE cd /path/to/working/dir && ...` or write/load environment variables from files\n4. To finish, issue the following command: `echo MINI_SWE_AGENT_FINAL_OUTPUT`.\n   Do not combine it with any other command.\n\n## Formatting your response\n\nHere is an example of a correct response:\n\n<example_response>\nTHOUGHT: I need to understand the structure of the repository first. Let me check what files are in the current directory to get a better understanding of the codebase.\n\n```bash\nls -la\n```\n</example_response>\n\n## Useful command examples\n\n### Create a new file:\n\n```bash\ncat <<'EOF' > newfile.py\nimport numpy as np\nhello = \"world\"\nprint(hello)\nEOF\n```\n\n### Edit files with sed:\n\n```bash\n# Replace all occurrences\nsed -i 's/old_string/new_string/g' filename.py\n\n# Replace only first occurrence\nsed -i 's/old_string/new_string/' filename.py\n\n# Replace first occurrence on line 1\nsed -i '1s/old_string/new_string/' filename.py\n\n# Replace all occurrences in lines 1-10\nsed -i '1,10s/old_string/new_string/g' filename.py\n```\n\n### View file content:\n\n```bash\n# View specific lines with numbers\nnl -ba filename.py | sed -n '10,20p'\n```\n\n### Any other command you want to run\n\n```bash\nanything\n```"
  },
  {
    "role": "assistant",
    "content": "I'll help you solve this issue. Let me first examine the content of the file that's causing the syntax error.\n\n```bash\ncat /Users/fuchur/Documents/24/git_sync/swe-agent-test-repo/tests/./missing_colon.py\n```"
  },
  {
    "role": "user",
    "content": "<returncode>1</returncode>\n<output>\ncat: /Users/fuchur/Documents/24/git_sync/swe-agent-test-repo/tests/./missing_colon.py: No such file or directory\n</output>"
  },
  {
    "role": "assistant",
    "content": "I need to find the correct path to the file. Let me check the current directory structure:\n\n```bash\nls -la\n```"
  },
  {
    "role": "user",
    "content": "<returncode>0</returncode>\n<output>\ntotal 44\ndrwxr-xr-x 6 root root 4096 Jul 17 01:31 .\ndrwxr-xr-x 1 root root 4096 Jul 17 01:31 ..\ndrwxr-xr-x 8 root root 4096 Jul 17 01:31 .git\n-rw-r--r-- 1 root root 3495 Jul 17 01:31 .gitignore\n-rw-r--r-- 1 root root   61 Jul 17 01:31 Dockerfile\n-rw-r--r-- 1 root root  183 Jul 17 01:31 README.md\n-rwxr-xr-x 1 root root  404 Jul 17 01:31 build_and_push_docker.sh\ndrwxr-xr-x 2 root root 4096 Jul 17 01:31 problem_statements\n-rw-r--r-- 1 root root  503 Jul 17 01:31 pyproject.toml\ndrwxr-xr-x 3 root root 4096 Jul 17 01:31 src\ndrwxr-xr-x 2 root root 4096 Jul 17 01:31 tests\n</output>"
  },
  {
    "role": "assistant",
    "content": "Let me check the tests directory where the missing_colon.py file should be:\n\n```bash\nls -la tests/\n```"
  },
  {
    "role": "user",
    "content": "<returncode>0</returncode>\n<output>\ntotal 24\ndrwxr-xr-x 2 root root 4096 Jul 17 01:31 .\ndrwxr-xr-x 6 root root 4096 Jul 17 01:31 ..\n-rwxr-xr-x 1 root root  368 Jul 17 01:31 existing_lint_error.py\n-rw-r--r-- 1 root root   65 Jul 17 01:31 filetoread.txt\n-rwxr-xr-x 1 root root  141 Jul 17 01:31 missing_colon.py\n-rw-r--r-- 1 root root  128 Jul 17 01:31 test_tribonaccy.py\n</output>"
  },
  {
    "role": "assistant",
    "content": "Great! I found the missing_colon.py file in the tests directory. Let me examine its content:\n\n```bash\ncat tests/missing_colon.py\n```"
  },
  {
    "role": "user",
    "content": "<returncode>0</returncode>\n<output>\n#!/usr/bin/env python3\n\n\ndef division(a: float, b: float) -> float\n    return a/b\n\n\nif __name__ == \"__main__\":\n    print(division(123, 15))\n\n</output>"
  },
  {
    "role": "assistant",
    "content": "I found the issue! In Python, a function definition needs to end with a colon `:` before the function body. The error message is pointing to line 4 where the colon is missing after the function return type annotation.\n\nLet me fix the file:\n\n```bash\nsed -i 's/def division(a: float, b: float) -> float/def division(a: float, b: float) -> float:/' tests/missing_colon.py\n```"
  },
  {
    "role": "user",
    "content": "<returncode>0</returncode>\n<output>\n</output>"
  },
  {
    "role": "assistant",
    "content": "Now let's verify that the file has been fixed correctly:\n\n```bash\ncat tests/missing_colon.py\n```"
  },
  {
    "role": "user",
    "content": "<returncode>0</returncode>\n<output>\n#!/usr/bin/env python3\n\n\ndef division(a: float, b: float) -> float:\n    return a/b\n\n\nif __name__ == \"__main__\":\n    print(division(123, 15))\n\n</output>"
  },
  {
    "role": "assistant",
    "content": "Let's test if our fix resolved the syntax error by running the script:\n\n```bash\npython3 tests/missing_colon.py\n```"
  },
  {
    "role": "user",
    "content": "<returncode>0</returncode>\n<output>\n8.2\n</output>"
  },
  {
    "role": "assistant",
    "content": "Great! The script now runs correctly without syntax errors and correctly outputs the result of 123 divided by 15, which is 8.2.\n\nLet me now check if running the division with the originally mentioned values (23, 0) would work:\n\n```bash\npython3 -c \"from tests.missing_colon import division; print(division(23, 0))\"\n```"
  },
  {
    "role": "user",
    "content": "<returncode>1</returncode>\n<output>\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/testbed/tests/missing_colon.py\", line 5, in division\n    return a/b\n           ~^~\nZeroDivisionError: division by zero\n</output>"
  },
  {
    "role": "assistant",
    "content": "As expected, when trying to divide by zero (23, 0), we get a ZeroDivisionError. While our syntax fix was successful, the function doesn't handle division by zero gracefully.\n\nLet me update the division function to handle the zero division case properly:\n\n```bash\ncat > tests/missing_colon.py << 'EOF'\n#!/usr/bin/env python3\n\n\ndef division(a: float, b: float) -> float:\n    if b == 0:\n        raise ValueError(\"Cannot divide by zero\")\n    return a/b\n\n\nif __name__ == \"__main__\":\n    print(division(123, 15))\nEOF\n```"
  },
  {
    "role": "user",
    "content": "<returncode>0</returncode>\n<output>\n</output>"
  },
  {
    "role": "assistant",
    "content": "Now let's verify that the updated file works correctly both for valid division and for the division by zero case:\n\n```bash\necho MINI_SWE_AGENT_FINAL_OUTPUT && git add -A && git diff --cached\n```"
  },
  {
    "role": "user",
    "content": "diff --git a/tests/missing_colon.py b/tests/missing_colon.py\nindex 20edef5..f55e657 100755\n--- a/tests/missing_colon.py\n+++ b/tests/missing_colon.py\n@@ -1,10 +1,11 @@\n #!/usr/bin/env python3\n \n \n-def division(a: float, b: float) -> float\n+def division(a: float, b: float) -> float:\n+    if b == 0:\n+        raise ValueError(\"Cannot divide by zero\")\n     return a/b\n \n \n if __name__ == \"__main__\":\n     print(division(123, 15))\n-\n"
  }
]


================================================
FILE: tests/test_data/local.traj.json
================================================
[
  {
    "role": "system",
    "content": "You are a helpful assistant that can interact with a computer.\n\nYour response must contain exactly ONE bash code block with ONE command (or commands connected with && or ||).\nInclude a THOUGHT section before your command where you explain your reasoning process.\nFormat your response as shown in <format_example>.\n\n<format_example>\nYour reasoning and analysis here. Explain why you want to perform the action.\n\n```bash\nyour_command_here\n```\n</format_example>\n\nFailure to follow these rules will cause your response to be rejected.\nTo finish, issue the following command: `echo MINI_SWE_AGENT_FINAL_OUTPUT`\nwithout any other command.\n"
  },
  {
    "role": "user",
    "content": "Please solve this issue: Blah blah blah\n\nYou can execute bash commands and edit files to implement the necessary changes.\n\n## Recommended Workflow\n1. Analyze the codebase by finding and reading relevant files\n2. Create a script to reproduce the issue\n3. Edit the source code to resolve the issue\n4. Verify your fix works by running your script again\n5. Test edge cases to ensure your fix is robust\n\n## Important Rules\n\n1. Every response must contain exactly one action\n2. The action must be enclosed in triple backticks\n3. Directory or environment variable changes are not persistent. Every action is executed in a new subshell.\n   However, you can prefix any action with `MY_ENV_VAR=MY_VALUE cd /path/to/working/dir && ...` or write/load environment variables from files\n4. To finish, issue the following command: `echo MINI_SWE_AGENT_FINAL_OUTPUT`.\n   Do not combine it with any other command.\n\n## Formatting your response\n\nHere is an example of a correct response:\n\n<example_response>\nTHOUGHT: I need to understand the structure of the repository first. Let me check what files are in the current directory to get a better understanding of the codebase.\n\n```bash\nls -la\n```\n</example_response>\n\n## Useful command examples\n\n### Create a new file:\n\n```bash\ncat <<'EOF' > newfile.py\nimport numpy as np\nhello = \"world\"\nprint(hello)\nEOF\n```\n\n### Edit files with sed:\n\n```bash\n# Replace all occurrences\nsed -i 's/old_string/new_string/g' filename.py\n\n# Replace only first occurrence\nsed -i 's/old_string/new_string/' filename.py\n\n# Replace first occurrence on line 1\nsed -i '1s/old_string/new_string/' filename.py\n\n# Replace all occurrences in lines 1-10\nsed -i '1,10s/old_string/new_string/g' filename.py\n```\n\n### View file content:\n\n```bash\n# View specific lines with numbers\nnl -ba filename.py | sed -n '10,20p'\n```\n\n### Any other command you want to run\n\n```bash\nanything\n```"
  },
  {
    "role": "assistant",
    "content": "Here be some dummy command:\n\n```bash\necho 'hello world'\n```"
  },
  {
    "role": "user",
    "content": "<returncode>0</returncode>\n<output>\nhello world\n</output>"
  },
  {
    "role": "assistant",
    "content": "OK, now let me end this run.\n\n```bash\necho MINI_SWE_AGENT_FINAL_OUTPUT\n```"
  },
  {
    "role": "user",
    "content": ""
  }
]


================================================
FILE: tests/test_data/results.json
================================================
{
  "swe-agent__test-repo-1": {
    "model_name_or_path": "claude-sonnet-4-20250514",
    "instance_id": "swe-agent__test-repo-1",
    "model_patch": "diff --git a/tests/missing_colon.py b/tests/missing_colon.py\nindex 20edef5..5857437 100755\n--- a/tests/missing_colon.py\n+++ b/tests/missing_colon.py\n@@ -1,7 +1,7 @@\n #!/usr/bin/env python3\n \n \n-def division(a: float, b: float) -> float\n+def division(a: float, b: float) -> float:\n     return a/b"
  }
}


================================================
FILE: tests/test_data/update_outputs_github_issue.py
================================================
#!/usr/bin/env python3

import json
import sys
from pathlib import Path
from unittest.mock import patch

from minisweagent.models.test_models import DeterministicModel
from minisweagent.run.github_issue import DEFAULT_CONFIG, main


def update_trajectory():
    traj_path = Path(__file__).parent / "github_issue.traj.json"
    trajectory = json.loads(traj_path.read_text())

    model_responses = [msg["content"] for msg in trajectory[2:] if msg["role"] == "assistant"]

    with patch("minisweagent.run.github_issue.get_model") as mock_get_model:
        mock_get_model.return_value = DeterministicModel(outputs=model_responses)
        github_url = "https://github.com/SWE-agent/test-repo/issues/1"
        agent = main(issue_url=github_url, model="tardis", config=DEFAULT_CONFIG)

    traj_path.write_text(json.dumps(agent.messages, indent=2))


if __name__ == "__main__":
    update_trajectory()


================================================
FILE: tests/test_data/update_outputs_local.py
================================================
#!/usr/bin/env python3

import json
import sys
from pathlib import Path
from unittest.mock import patch

from minisweagent.models.test_models import DeterministicModel
from minisweagent.run.mini import DEFAULT_CONFIG, main


def update_trajectory():
    traj_path = Path(__file__).parent / "local.traj.json"
    trajectory = json.loads(traj_path.read_text())

    task = "Blah blah blah"

    model_responses = [msg["content"] for msg in trajectory[2:] if msg["role"] == "assistant"]
    print(f"Got {len(model_responses)} model responses")

    with patch("minisweagent.run.mini.get_model") as mock_get_model:
        mock_get_model.return_value = DeterministicModel(outputs=model_responses)
        main(model_name="tardis", config_spec=DEFAULT_CONFIG, output=traj_path, task=task, yolo=True)

if __name__ == "__main__":
    update_trajectory()


================================================
FILE: .cursor/rules/project.mdc
================================================
---
description:
globs:
alwaysApply: true
---
# mini-SWE-agent overview

- mini-SWE-agent implements an AI software engineering agent that solves github issues and similar programming challenges
- The idea of this project is to write the simplest, smallest, most readable agent.

The project is structured as

```bash
minisweagent/__init__  # Protocols/interfaces for all base classes
minisweagent/agents  # Agent control flow & loop
minisweagent/environments  # Executing agent actions
minisweagent/models  # LM interfaces
minisweagent/run  # Run scripts that serve as an entry point
```

- The project embraces polymorphism: Every individual class should be simple, but we offer alternatives
- Every use case should start with a run script, that picks one agent, environment, and model class to run


================================================
FILE: .cursor/rules/style.mdc
================================================
---
description:
globs:
alwaysApply: true
---

# Style guide

1. Target python 3.10 or higher
2. Use python with type annotations. Use `list` instead of `List`.
3. Use `pathlib` instead of `os.path`. Use `Path.read_text()` over `with ...open()` constructs.
4. Use `typer` to add interfaces
5. Keep code comments to a minimum and only highlight particularly logically challenging things
6. Do not append to the README unless specifically requested
7. Use `jinja` for formatting templates
8. Use `dataclass` for keeping track config
9. Do not catch exceptions unless explicitly told to.
10. Write concise, short, minimal code.
11. In most cases, avoid initializing variables just to pass them to a function. Instead just pass the expression to the function directly.
12. Not every exception has to be caught. Exceptions are a good way to show problems to a user.
13. This repository rewards minimal code. Try to be as concise as possible.

Here's an example for rule 11:

```python
# bad
a = func()
Class(a)

# good
Class(func())
```

## Test style

1. Use `pytest`, not `unittest`.
2. <IMPORTANT>Do not mock/patch anything that you're not explicitly asked to do</IMPORTANT>
3. Avoid writing trivial tests. Every test should test for at least one, preferably multiple points of failure
4. Avoid splitting up code in multiple lines like this: `a=func()\n assert a=b`. Instead, just do `assert func() == b`
5. The first argument to `pytest.mark.parametrize` should be a tuple (not a string! not a list!), the second argument must be a list (not a tuple!).

Here's an example for rule 4:

```python
# bad
result = func()
assert result == b

# good
assert func() == b
```



================================================
FILE: .github/CODE_OF_CONDUCT.md
================================================

# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, caste, color, religion, or sexual
identity and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
* Focusing on what is best not just for us as individuals, but for the overall
  community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or advances of
  any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email address,
  without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official email address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at
[INSERT CONTACT METHOD].
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series of
actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or permanent
ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior, harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within the
community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.1, available at
[https://www.contributor-covenant.org/version/2/1/code_of_conduct.html][v2.1].

Community Impact Guidelines were inspired by
[Mozilla's code of conduct enforcement ladder][Mozilla CoC].

For answers to common questions about this code of conduct, see the FAQ at
[https://www.contributor-covenant.org/faq][FAQ].

[homepage]: https://www.contributor-covenant.org
[v2.1]: https://www.contributor-covenant.org/version/2/1/code_of_conduct.html
[Mozilla CoC]: https://github.com/mozilla/diversity
[FAQ]: https://www.contributor-covenant.org/faq



================================================
FILE: .github/copilot-instructions.md
================================================
# Style guide

1. Target python 3.10 or higher
2. Use python with type annotations. Use `list` instead of `List`.
3. Use `pathlib` instead of `os.path`. Use `Path.read_text()` over `with ...open()` constructs.
4. Use `typer` to add interfaces
5. Keep code comments to a minimum and only highlight particularly logically challenging things
6. Do not append to the README unless specifically requested
7. Use `jinja` for formatting templates
8. Use `dataclass` for keeping track config
9. Do not catch exceptions unless explicitly told to.
10. Write concise, short, minimal code.
11. In most cases, avoid initializing variables just to pass them to a function. Instead just pass the expression to the function directly.
12. Not every exception has to be caught. Exceptions are a good way to show problems to a user.
13. This repository rewards minimal code. Try to be as concise as possible.
14. Do NOT warn about print messages in tests, print statements in tests are OK!

Here's an example for rule 11:

```python
# bad
a = func()
Class(a)

# good
Class(func())
```

## Test style

1. Use `pytest`, not `unittest`.
2. <IMPORTANT>Do not mock/patch anything that you're not explicitly asked to do</IMPORTANT>
3. Avoid writing trivial tests. Every test should test for at least one, preferably multiple points of failure
4. Avoid splitting up code in multiple lines like this: `a=func()\n assert a=b`. Instead, just do `assert func() == b`
5. The first argument to `pytest.mark.parametrize` should be a tuple (not a string! not a list!), the second argument must be a list (not a tuple!).
6. Do NOT warn about print messages in tests, print statements in tests are OK!

Here's an example for rule 4:

```python
# bad
result = func()
assert result == b

# good
assert func() == b
```




================================================
FILE: .github/dependabot.yml
================================================
version: 2
updates:
  # Maintain dependencies for GitHub Actions
  - package-ecosystem: "github-actions"
    directory: "/"
    schedule:
      interval: "weekly"



================================================
FILE: .github/mlc_config.json
================================================
{
  "ignorePatterns": [
    {
      "pattern": "https://github.com/?.*"
    },
    {
      "pattern": "https://platform.openai.com/docs/.*"
    },
    {
      "pattern": ".*localhost.*"
    },
    {
      "pattern": "^(?!https).*(?<!\\.md)$"
    }
  ]
}



================================================
FILE: .github/PULL_REQUEST_TEMPLATE.md
================================================
<!--
Thanks for contributing a pull request, we appreciate you!

If this PR fixes an issue, please reference it, e.g., 'Fixes #1234'.

You can delete this comment.
-->



================================================
FILE: .github/.codecov.yml
================================================
# Configuration for codecov
coverage:
  status:
    project:
      default:
        # If we get < 50% coverage, codecov is gonna mark it a failure
        target: 50%
        threshold: null
    patch:
      default:
        # Codecov won't mark it as a failure if a patch is not covered well
        informational: true
github_checks:
  # Don't mark lines that aren't covered
  annotations: false




================================================
FILE: .github/ISSUE_TEMPLATE/bug.yml
================================================
name: Bug Report
description: Create a report to help us reproduce and correct the bug
labels: ['bug']

body:
- type: textarea
  attributes:
    label: Describe the bug
    description: >
      A clear and concise description of what the bug is.
      Please copy as much as possible from the output, in particular any error message.
      Any hint about how to reproduce your issue will help us help you!
      Please also include your operating system where relevant.
      Markdown formatting is appreciated 😉

      Hint: The full history of your last `mini` run is saved to your global config directory.
      Including it helps us understand the issue.
  validations:
    required: true
- type: markdown
  attributes:
    value: >
      Thanks for contributing 🎉!



================================================
FILE: .github/ISSUE_TEMPLATE/config.yml
================================================
contact_links:
  - name: Slack
    url: https://discord.gg/AVEFbBn2rH
    about: Open ended discussion (only if not suitable for a GitHub issue)



================================================
FILE: .github/ISSUE_TEMPLATE/question.yml
================================================
name: Question
description: Anything that's not a bug
labels: ['question']

body:
- type: textarea
  attributes:
    label: Describe the issue
    description: The more details the better!
  validations:
    required: true



================================================
FILE: .github/workflows/build-docs.yaml
================================================
name: build-docs

on:
  push:
    branches:
      - main
      - "build-docs-*"
  pull_request:
    branches:
      - main

# Prevent concurrent runs that could conflict when pushing to gh-pages
concurrency:
  group: build-docs-${{ github.ref }}
  cancel-in-progress: false

permissions:
  contents: write
jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v5
        with:
          fetch-depth: 0
      - name: Configure Git Credentials
        run: |
          git config user.name github-actions[bot]
          git config user.email 41898282+github-actions[bot]@users.noreply.github.com
      - uses: actions/setup-python@v5
        with:
          python-version: 3.x
      - run: echo "cache_id=$(date --utc '+%V')" >> $GITHUB_ENV
      - uses: actions/cache@v4
        with:
          key: mkdocs-material-${{ env.cache_id }}
          path: .cache
          restore-keys: |
            mkdocs-material-
      - name: Install uv
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
      - run: uv pip install --python ${Python_ROOT_DIR} '.[full]'
      - name: Build + Deploy Documentation
        if: github.ref == 'refs/heads/main'
        run: mike deploy --push 1.0 latest
      - name: Build Documentation (strict)
        # Do this even if we've already deployed, just so we can catch errors
        # and mark the build as failed.
        run: mkdocs build --strict


================================================
FILE: .github/workflows/check-links-periodic.yaml
================================================
name: Check Markdown links full

on:
  workflow_dispatch:
  push:
    branches:
      - main
      - check-links*
      - fix-links*
  schedule:
    - cron: "0 0,12 * * *"

jobs:
  markdown-link-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v5
      - uses: tcort/github-action-markdown-link-check@v1
        with:
          use-verbose-mode: 'yes'
          config-file: .github/mlc_config.json



================================================
FILE: .github/workflows/check-links-pr.yaml
================================================
on: [pull_request]
name: Check Markdown links in modified files
jobs:
  markdown-link-check:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v5
    - uses: tcort/github-action-markdown-link-check@v1
      with:
        use-verbose-mode: 'yes'
        check-modified-files-only: 'yes'
        base-branch: 'main'
        config-file: .github/mlc_config.json



================================================
FILE: .github/workflows/fire.yaml
================================================
name: Fire Test

on:
  workflow_dispatch:

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install package
      run: |
        pip install -e .

    - name: Configure mini-extra
      run: |
        mini-extra config set MSWEA_CONFIGURED 'true'

    - name: Clone test repository
      run: |
        git clone https://github.com/swe-agent/test-repo

    - name: Change to test repo directory
      run: cd test-repo
    - name: Run mini with task
      working-directory: test-repo
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        mini --exit-immediately -y --model openai/gpt-5-mini -t "$(cat << 'EOF'
        I'm running missing_colon.py as follows:

        division(23, 0)

        but I get the following error:

          File "/Users/fuchur/Documents/24/git_sync/swe-agent-test-repo/tests/./missing_colon.py", line 4
            def division(a: float, b: float) -> float
                                                     ^
        SyntaxError: invalid syntax
        EOF
        )"



================================================
FILE: .github/workflows/pylint.yaml
================================================
name: Pylint

env:
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

on:
  push:
    branches:
      - main
    paths-ignore:
      - 'docs/**'
      - 'README.md'
      - 'mkdocs.yml'
      - '.cursor/**'
      - '.github/workflows/build-docs.yaml'
      - '.github/workflows/release.yaml'
      - '.github/workflows/pytest.yaml'
  pull_request:
    branches:
      - main
    paths-ignore:
      - 'docs/**'
      - 'README.md'
      - 'mkdocs.yml'
      - '.cursor/**'
      - '.github/workflows/build-docs.yaml'
      - '.github/workflows/release.yaml'
      - '.github/workflows/pytest.yaml'

jobs:
  pylint:
    runs-on: ubuntu-latest
    defaults:
      run:
        shell: bash -l {0}
    steps:
      - name: Checkout code
        uses: actions/checkout@v5
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install uv
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
      - name: Install dependencies
        run: |
            uv pip install --python ${Python_ROOT_DIR} -e '.[full]'
            uv pip install --python ${Python_ROOT_DIR} pylint
      - name: Run pylint (fail only on errors)
        run: |
          # Run pylint with errors-only flag to only fail on real errors (E and F level)
          # This will show all messages but only exit with error code for errors, not warnings/advice
          pylint minisweagent/ --errors-only


================================================
FILE: .github/workflows/pytest.yaml
================================================

name: Pytest

env:
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

on:
  push:
    branches:
      - main
    paths-ignore:
      - 'docs/**'
      - 'README.md'
      - 'mkdocs.yml'
      - '.cursor/**'
      - '.github/workflows/build-docs.yaml'
      - '.github/workflows/release.yaml'
      - '.github/workflows/pylint.yaml'
  pull_request:
    branches:
      - main
    paths-ignore:
      - 'docs/**'
      - 'README.md'
      - 'mkdocs.yml'
      - '.cursor/**'
      - '.github/workflows/build-docs.yaml'
      - '.github/workflows/release.yaml'
      - '.github/workflows/pylint.yaml'

jobs:
  test:
    runs-on: ubuntu-latest
    defaults:
      run:
        shell: bash -l {0}
    steps:
      - name: Checkout code
        uses: actions/checkout@v5
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install podman and singularity
        run: |
            sudo apt-get update
            sudo apt-get install -y podman
            # Install singularity/apptainer
            sudo apt-get install -y wget
            cd /tmp
            wget https://github.com/apptainer/apptainer/releases/download/v1.3.4/apptainer_1.3.4_amd64.deb
            sudo dpkg -i apptainer_1.3.4_amd64.deb || sudo apt-get install -f -y
            # Create singularity symlink for compatibility
            sudo ln -sf /usr/bin/apptainer /usr/bin/singularity
      - name: Install uv
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
      - name: Install dependencies
        run: |
            uv pip install --python ${Python_ROOT_DIR} -e '.[full]'
      - name: Run pytest
        run: pytest -v --cov --cov-branch --cov-report=xml -n auto
      - name: Upload coverage reports to Codecov
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          slug: SWE-agent/mini-swe-agent


================================================
FILE: .github/workflows/release.yaml
================================================
name: Release to PyPI

on:
  workflow_dispatch:
    inputs:
      test_pypi:
        description: 'Upload to Test PyPI instead of PyPI'
        required: false
        default: false
        type: boolean
  release:
    types: [published]

jobs:
  build-and-publish:
    runs-on: ubuntu-latest
    environment: release

    steps:
    - name: Checkout code
      uses: actions/checkout@v5

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.13'

    - name: Install uv
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
    - name: Install build dependencies
      run: |
        uv pip install --python ${Python_ROOT_DIR} build twine

    - name: Build package
      run: python -m build

    - name: Check package
      run: python -m twine check dist/*

    - name: Upload to Test PyPI
      if: ${{ github.event.inputs.test_pypi == 'true' }}
      uses: pypa/gh-action-pypi-publish@release/v1
      with:
        password: ${{ secrets.TEST_PYPI_API_TOKEN }}
        repository-url: https://test.pypi.org/legacy/
        verbose: true

    - name: Upload to PyPI
      if: ${{ github.event.inputs.test_pypi != 'true' }}
      uses: pypa/gh-action-pypi-publish@release/v1
      with:
        password: ${{ secrets.PYPI_API_TOKEN }}
        verbose: true

  smoke-test:
    runs-on: ubuntu-latest
    needs: build-and-publish

    steps:
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.13'

    - name: Install uv and pipx
      run: |
        curl -LsSf https://astral.sh/uv/install.sh | sh
        uv pip install --python ${Python_ROOT_DIR} pipx

    - name: Wait for package availability
      run: |
        echo "Waiting for package to be available on PyPI..."
        sleep 60

    - name: Test pipx run mini-swe-agent --help (Test PyPI)
      if: ${{ github.event.inputs.test_pypi == 'true' }}
      run: pipx run --pip-args="--index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/" mini-swe-agent --help

    - name: Test pipx run mini-swe-agent --help (PyPI)
      if: ${{ github.event.inputs.test_pypi != 'true' }}
      run: pipx run mini-swe-agent --help

    - name: Test uvx mini-swe-agent --help (Test PyPI)
      if: ${{ github.event.inputs.test_pypi == 'true' }}
      run: uvx --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ mini-swe-agent --help

    - name: Test uvx mini-swe-agent --help (PyPI)
      if: ${{ github.event.inputs.test_pypi != 'true' }}
      run: uvx mini-swe-agent --help

    - name: Test pip install and CLI commands (Test PyPI)
      if: ${{ github.event.inputs.test_pypi == 'true' }}
      run: |
        uv pip install --python ${Python_ROOT_DIR} --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ mini-swe-agent
        mini --help
        mini-extra --help
        mini-e --help

    - name: Test pip install and CLI commands (PyPI)
      if: ${{ github.event.inputs.test_pypi != 'true' }}
      run: |
        uv pip install --python ${Python_ROOT_DIR} mini-swe-agent
        mini --help
        mini-extra --help
        mini-e --help

