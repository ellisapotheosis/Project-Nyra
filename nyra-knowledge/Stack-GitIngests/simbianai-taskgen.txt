Directory structure:
└── simbianai-taskgen/
    ├── README.md
    ├── build.sh
    ├── changelog.txt
    ├── LICENSE
    ├── myagent.pkl
    ├── pyproject.toml
    ├── requirements.txt
    ├── setup.py
    ├── .example.env
    ├── contrib/
    │   ├── README.md
    │   └── community/
    │       ├── HelpfulAssistant/
    │       │   └── main.py
    │       └── MathExpert/
    │           └── main.py
    ├── Paper/
    │   ├── README.md
    │   ├── math_dataset/
    │   │   ├── readme.md
    │   │   ├── math.ipynb
    │   │   └── selected_problems_level_5.json
    │   ├── naturalquestions_rag/
    │   │   └── readme.md
    │   ├── textworld/
    │   │   └── textworld_conversation.ipynb
    │   └── web_browsing_agent/
    │       ├── README.md
    │       ├── conversational_web_browsing_agent.py
    │       ├── conversational_web_browsing_agent_results.py
    │       └── requirements.txt
    ├── resources/
    │   └── README.md
    ├── taskgen/
    │   ├── __init__.py
    │   ├── base.py
    │   ├── base_async.py
    │   ├── function.py
    │   ├── memory.py
    │   ├── ranker.py
    │   ├── utils.py
    │   └── wrapper.py
    └── .github/
        └── workflows/
            └── build.yaml

================================================
FILE: README.md
================================================
# TaskGen (Simbian) v3.3.4
### A Task-based agentic framework building on StrictJSON outputs by LLM agents
### An Open Source Initiative Led by John Tan Chong Min, Supported by Simbian AI
### As of 15 Oct 2024, John Tan Chong Min has left Simbian and is maintaining his own version of TaskGen at https://github.com/tanchongmin/taskgen

![TaskGen Overview](./resources/TaskGen_Overview.png)

- Paper: https://web3.arxiv.org/pdf/2407.15734
- Video: https://www.youtube.com/watch?v=F3usuxs2p1Y
- Related Repositories: StrictJSON (https://github.com/tanchongmin/strictjson) [Do help star this as well!]

### Creator's Preamble
Happy to share that the task-based agentic framework I have been working on - TaskGen - is largely complete! 

Noteable features include:
- Splitting of Tasks into subtasks for bite-sized solutions for each subtask
- Single Agent with LLM Functions
- Single Agent with External Functions
- Meta Agent with Inner Agents as Functions
- Shared Variables for multi-modality support
- Retrieval Augmented Generation (RAG) over Function space
- Memory to provide additional task-based prompts for task
- Global Context for configuring your own prompts + add persistent variables
- Async mode for Agent, Function and `strict_json` added
- Community Uploading and Downloading of Agent and Functions

I am quite sure that this is the best open-source agentic framework for task-based execution out there! 
Existing frameworks like AutoGen rely too much on conversational text which is lengthy and not targeted.
TaskGen uses StrictJSON (JSON parser with type checking and more!) as the core, and agents are efficient and are able to do Chain of Thought natively using JSON keys and descriptions as a guide.

I can't wait to see what this new framework can do for you!

### Benefits of JSON messaging over agentic frameworks using conversational free-text like AutoGen
- JSON format helps do Chain-of-Thought prompting naturally and is less verbose than free text
- JSON format allows natural parsing of multiple output fields by agents
- StrictJSON helps to ensure all output fields are there and of the right format required for downstream processing

### Creator Info
- Created: 17 Feb 2024 - 15 Oct 2024 by [John Tan Chong Min](https://www.linkedin.com/in/john-chong-min-tan-94652288/)
- Co-developer / Lead Contributor: [Prince Saroj](https://www.linkedin.com/in/psaroj/)
- TaskGen Paper Research Staff: [Prince Saroj](https://www.linkedin.com/in/psaroj/), [Hardik Maheshwari](https://www.linkedin.com/in/hardik1496/), [Bharat Runwal](https://www.linkedin.com/in/bharat-runwal-673144196/), [Brian Lim](https://www.linkedin.com/in/brianlimyisheng/), [Richard Cottrill](https://www.linkedin.com/in/richardc/)
- TaskGen Mentors / Funders: [Ambuj Kumar](https://www.linkedin.com/in/ambujkumar/), [Alankrit Chona](https://www.linkedin.com/in/alankrit-chona-927875a7/), [Mehul Motani](https://www.linkedin.com/in/motani/)

## How do I use this? 
1. Download package via command line ```pip install taskgen-ai==3.3.4```
2. Set up your LLM and provide any API keys if needed
3. Import the required functions from ```taskgen``` and use them!

## Differences in LLM for Agentic Framework
- Default model is now gpt-4o-mini if you do not specify any LLM!
- Weaker models like ChatGPT (gpt-3.5-turbo) and Llama 3 8B are consistent only if you specify very clearly what you want the Agent to do and give examples of what you want
- gpt-4o-mini, gpt-4o, Llama 3 70B and more advanced models can perform better zero-shot without much examples
- TaskGen is compatible with ChatGPT and similar models, but for more robust use, consider using gpt-4o-mini and better models

# 1. Agent Basics
- Create an agent by entering your agent's name and description
- Agents are task-based, so they will help generate subtasks to fulfil your main task

- Agents are made to be non-verbose, so they will just focus only on task instruction (Much more efficient compared to conversational-based agentic frameworks like AutoGen)
- Agent's interactions will be stored into `subtasks_completed` by default, which will serve as a memory buffer for future interactions

- **Inputs for Agent**:
    - **agent_name**: String. Name of agent, hinting at what the agent does
    - **agent_description**: String. Short description of what the agent does
    - **max_subtasks**: Int. Default: 5. The maximum number of subtasks the agent can have
    - **verbose**: Bool. Default: True. Whether to print out agent's intermediate thoughts
    - **llm**: Function. The LLM to be used by the Agent
<br/><br/>

- **Agent Internal Parameters**:
    - **Task**: String. The task the agent has been assigned to - Defaults to "No task assigned"
    - **Subtasks Completed**: Dict. The keys are the subtask names and the values are the result of the respective subtask
    - **Is Task Completed**: Bool. Whether the current Task is completed
<br/><br/>

- **Task Running**
    - **reset()**: Resets the Agent Internal Parameters and Subtasks Completed. You should do this at the start of every new task assigned to the Agent to minimise potential confusion of what has been done for this task versus previous tasks
    - **run(task: str, num_subtasks: int = max_subtasks)**: Performs the task. Do note that agent's state will not be reset, so if you want to reset it, call reset() prior to running this. Runs the task for **num_subtasks** steps. If not specified, we will take the **max_subtasks**.
<br/><br/>

- **Give User Output**
    - **reply_user(query: str = '', stateful: bool = True)**: Using all information from subtasks, give a reply about the `query` to the user. If `query` is not given, then it replies based on the current task the agent is doing. If `stateful` is True, saves this query and reply into `subtasks_completed`
<br/><br/>
    
- **Check status of Agent**:
    - **status()**: Lists out Agent Name, Agent Description, Available Functions (default function is to use the LLM), Task, Subtasks Completed and Is Task Completed
    
## Example Agent Creation
```python
my_agent = Agent('Helpful assistant', 'You are a generalist agent', llm = llm)
```

## Example Agent Task Running - Split the assigned task into subtasks and execute each of them

```python
output = my_agent.run('Give me 5 words rhyming with cool, and make a 4-sentence poem using them')
```

`Subtask identified: Find 5 words that rhyme with 'cool'`

`Getting LLM to perform the following task: Find 5 words that rhyme with 'cool'`
> pool, rule, fool, tool, school

`Subtask identified: Compose a 4-sentence poem using the words 'pool', 'rule', 'fool', 'tool', and 'school'`

`Getting LLM to perform the following task: Compose a 4-sentence poem using the words 'pool', 'rule', 'fool', 'tool', and 'school'`
> In the school, the golden rule is to never be a fool. Use your mind as a tool, and always follow the pool.

`Task completed successfully!`

## Check Agent's Status
```python
my_agent.status()
```

`Agent Name: Helpful assistant`

`Agent Description: You are a generalist agent`

`Available Functions: ['use_llm', 'end_task']`

`Task: Give me 5 words rhyming with cool, and make a 4-sentence poem using them`

`Subtasks Completed:`

`Subtask: Find 5 words that rhyme with 'cool'`

`pool, rule, fool, tool, school`

`Subtask: Compose a 4-sentence poem using the words 'pool', 'rule', 'fool', 'tool', and 'school'`

`In the school, the golden rule is to never be a fool. Use your mind as a tool, and always follow the pool.`

`Is Task Completed: True`

## Example Agent Reply to User - Reference the subtasks' output to answer the user's query
```python
output = my_agent.reply_user()
```

`
Here are 5 words that rhyme with "cool": pool, rule, fool, tool, school. Here is a 4-sentence poem using these words: "In the school, the golden rule is to never be a fool. Use your mind as a tool, and always follow the pool."
`

# 2. Power Up your Agents - Bring in Functions (aka Tools)
- First define the functions, either using class `Function` (see Tutorial 0), or just any Python function with input and output types defined in the signature and with a docstring
- After creating your agent, use `assign_functions` to assign a list of functions of class `Function`, or general Python functions (which will be converted to AsyncFunction)
- Function names will be automatically inferred if not specified
- Proceed to run tasks by using `run()`

```python
# This is an example of an LLM-based function (see Tutorial 0)
sentence_style = Function(fn_description = 'Output a sentence with words <var1> and <var2> in the style of <var3>', 
                         output_format = {'output': 'sentence'},
                         fn_name = 'sentence_with_objects_entities_emotion',
                         llm = llm)

# This is an example of an external user-defined function (see Tutorial 0)
def binary_to_decimal(binary_number: str) -> int:
    '''Converts binary_number to integer of base 10'''
    return int(str(binary_number), 2)

# Initialise your Agent
my_agent = Agent('Helpful assistant', 'You are a generalist agent')

# Assign the functions
my_agent.assign_functions([sentence_style, binary_to_decimal])

# Run the Agent
output = my_agent.run('First convert binary string 1001 to a number, then generate me a happy sentence with that number and a ball')
```

`Subtask identified: Convert the binary number 1001 to decimal`
`Calling function binary_to_decimal with parameters {'x': '1001'}`

> {'output1': 9}

`Subtask identified: Generate a happy sentence with the decimal number and a ball`
`Calling function sentence_with_objects_entities_emotion with parameters {'obj': '9', 'entity': 'ball', 'emotion': 'happy'}`

> {'output': 'I am so happy with my 9 balls.'}

`Task completed successfully!`

- Approach 1: Automatically Run your agent using `run()`

- Approach 2: Manually select and use functions for your task
    - **select_function(task: str)**: Based on the task, output the next function name and input parameters
    - **use_function(function_name: str, function_params: dict, subtask: str = '', stateful: bool = True)**: Uses the function named `function_name` with `function_params`. `stateful` controls whether the output of this function will be saved to `subtasks_completed` under the key of `subtask`
<br/><br/>

- **Assign/Remove Functions**:
    - **assign_functions(function_list: list)**: Assigns a list of functions to the agent
    - **remove_function(function_name: str)**: Removes function named function_name from the list of assigned functions
<br/><br/>

- **Show Functions**:
    - **list_functions()**: Returns the list of functions of the agent
    - **print_functions()**: Prints the list of functions of the agent
<br/><br/>

# 3. AsyncAgent

- `AsyncAgent` works the same way as `Agent`, only much faster due to parallelisation of tasks
- It can only be assigned functions of class `AsyncFunction`, or general Python functions (which will be converted to AsyncFunction)
- If you define your own `AsyncFunction`, you should define the fn_name as well if it is not an External Function
- As a rule of thumb, just add the `await` keyword to any function that you run with the `AsyncAgent`

#### Example LLM in Async Mode
```python
async def llm_async(system_prompt: str, user_prompt: str):
    ''' Here, we use OpenAI for illustration, you can change it to your own LLM '''
    # ensure your LLM imports are all within this function
    from openai import AsyncOpenAI
    
    # define your own LLM here
    client = AsyncOpenAI()
    response = await client.chat.completions.create(
        model='gpt-4o-mini',
        temperature = 0,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
    )
    return response.choices[0].message.content
```

#### Example Agentic Workflow
```python
# This is an example of an LLM-based function (see Tutorial 0)
sentence_style = AsyncFunction(fn_description = 'Output a sentence with words <var1> and <var2> in the style of <var3>', 
                         output_format = {'output': 'sentence'},
                         fn_name = 'sentence_with_objects_entities_emotion', # you must define fn_name for LLM-based functions
                         llm = llm_async) # use an async LLM function

# This is an example of an external user-defined function (see Tutorial 0)
def binary_to_decimal(binary_number: str) -> int:
    '''Converts binary_number to integer of base 10'''
    return int(str(binary_number), 2)

# Initialise your Agent
my_agent = AsyncAgent('Helpful assistant', 'You are a generalist agent')

# Assign the functions
my_agent.assign_functions([sentence_style, binary_to_decimal])

# Run the Agent
output = await my_agent.run('Generate me a happy sentence with a number and a ball. The number is b1001 converted to decimal')
```

# 4. Shared Variables

*"Because text is not enough"* - Anonymous

- `shared_variables` is a dictionary, that is initialised in Agent (default empty dictionary), and can be referenced by any function of the agent (including Inner Agents and their functions)
- This can be useful for non-text modalities (e.g. audio, pdfs, image) and lengthy text modalities, which we do not want to output into `subtasks_completed` directly
- To use, simply define an External Function with `shared_variables` as the first input variable, from which you can access and modify `shared_variables` directly
- The agent will also be able to be self-referenced in the External Function via `shared_variables['agent']`, so you can change the agent's internal parameters via `shared_variables`
- If the function has no output because the output is stored in `shared_variables`, the default return value will be `{'Status': 'Completed'}`

### Example External Function using `shared_variables`
```python
# Use shared_variables as input to your external function to access and modify the shared variables
def generate_quotes(shared_variables, number_of_quotes: int, category: str):
    ''' Generates number_of_quotes quotes about category '''
    # Retrieve from shared variables
    my_quote_list = shared_variables['Quote List']
    
    # Generate the quotes
    res = strict_json(system_prompt = f'''Generate {number_of_quotes} sentences about {category}. 
Do them in the format "<Quote> - <Person>", e.g. "The way to get started is to quit talking and begin doing. - Walt Disney"
Ensure your quotes contain only ' within the quote, and are enclosed by " ''',
                      user_prompt = '',
                      output_format = {'Quote List': f'list of {number_of_quotes} quotes, type: List[str]'},
                      llm = llm)
    
    my_quote_list.extend([f'Category: {category}. '+ x for x in res['Quote List']])
    
    # Store back to shared variables
    shared_variables['Quote List'] = my_quote_list
```

# 5. Global Context

- `Global Context` is a very powerful feature in TaskGen, as it allows the Agent to be updated with the latest environmental state before every decision it makes
- It also allows for learnings in `shared_variables` to be carried across tasks, making the Agent teachable and learn through experiences
- A recommended practice is to always store the learnings of the Agent during the External Function call, and reset the Agent after each task, so that `subtasks_completed` will be as short as possible to avoid confusion to the Agent

- There are two ways to use `Global Context`, and both can be used concurrently:
    - 1. `global_context`
        - If all you need in the global context is `shared_variables` without any modification to it, then you can use `global_context`
        - `global_context` is a string with `<shared_variables_name>` enclosed with `<>`. These <> will be replaced with the actual variable in `shared_variables`
    - 2. `get_global_context` 
        - `get_global_context` is a function that takes in the agent's internal parameters (self) and outputs a string to the LLM to append to the prompts of any LLM-based calls internally, e.g. `get_next_subtask`, `use_llm`, `reply_to_user`
    - You have full flexibility to access anything the agent knows and process the `shared_variables` as required and configure a global prompt to the agent
    
## Example for `global_context` : Inventory Manager
- We can use `Global Context` to keep track of inventory state
- We simply get the functions `add_item_to_inventory` and `remove_item_from_inventory` to modify the `shared_variable` named `Inventory`
- Note we can also put rule-based checks like checking if item is in inventory before removing inside the function
- Even after task reset, the Agent still knows the inventory because of `Global Context`

```python
def add_item_to_inventory(shared_variables, item: str) -> str:
    ''' Adds item to inventory, and returns outcome of action '''
    shared_variables['Inventory'].append(item)
    return f'{item} successfully added to Inventory'
    
def remove_item_from_inventory(shared_variables, item: str) -> str:
    ''' Removes item from inventory and returns outcome of action '''
    if item in shared_variables['Inventory']:
        shared_variables['Inventory'].remove(item)
        return f'{item} successfully removed from Inventory'
    else:
        return f'{item} not found in Inventory, unable to remove'
    
agent = Agent('Inventory Manager', 
              'Adds and removes items in Inventory. Only able to remove items if present in Inventory',
              shared_variables = {'Inventory': []},
              global_context = 'Inventory: <Inventory>', # Add in Global Context here with shared_variables Inventory
              llm = llm).assign_functions([add_item_to_inventory, remove_item_from_inventory])
```
    
# Other Features
- There are other features like Memory (Tutorial 3), Hierarchical Agents (Tutorial 4), CodeGen and External Function Interfacing (Tutorial 5), Conversation Class (Tutorial 6)
- These extend the baseline features of TaskGen and you are encouraged to take a look at the Tutorials for more information.

# Known Limitations
- `gpt-3.5-turbo` is not that great with mathematical functions for Agents. Use `gpt-4o-mini` or better for more consistent results
- `gpt-3.5-turbo` is not that great with Memory (Tutorial 3). Use `gpt-4o-mini` or better for more consistent results
3. Other Known Limitations - Do test the framework out extensively and note its failure cases. We will see if we can address them, if not we will put them in Known Limitations.
4. (For the prompt engineer). If you could find a better way to make the prompts work, let us know directly - we do need to test this out across all Tutorial Jupyter Notebooks to make sure that it really works with existing datasets. Also, if you are using other LLMs beside OpenAI, and find the prompts do not work as well - try to rejig your own prompts and let us know as well!



================================================
FILE: build.sh
================================================
#!/bin/bash -e

rm -rf venv
python3.11 -m venv venv
source venv/bin/activate
pip install --upgrade pip
pip install --upgrade setuptools
pip install wheel
pip install toml-cli

VERSION=$(toml get --toml-path pyproject.toml project.version)

WHL_FILE=dist/taskgen_ai-${VERSION}-py3-none-any.whl
rm -f ${WHL_FILE}

python setup.py bdist_wheel

if [ ! -f ${WHL_FILE} ]; then
   echo "ERROR: ${WHL_FILE} was not generated"
   exit -1
fi

echo "-----------------------------------------------"
echo "File ${WHL_FILE} created"

# -- pyright --
pip install -r requirements.txt
pip check
#pip install pyright==1.1.394
#pyright



================================================
FILE: changelog.txt
================================================
6 Sep 2024 (v3.3.4)
- Added contribution guide for Wrappers, Memory class, Jupyter Notebook in contrib folder
- Removed `isempty` function usage for `Memory` class in `agent.py` because we do not need to check for this condition anymore if memory is always filled
- Removed `add_file` compulsory function for `MemoryTemplate` in `memory.py` because not all memory types need files
- Added `thoughts` variable to Agent, which is a list to capture the Thoughts, Observations, Current Subtask, Equipped Function for each step
- Updated Tutorial 6 to use `ConversationWrapper`, which superclasses the Agent and enables the ConversationWrapper to also perform the Agent's functions. Moving ahead, we will have various wrappers like ReflectionWrapper, PlannerWrapper and many more to augment Agent's capabilitieis

5 Aug 2024 (v3.3.3)
- Refined ChromaDB Memory (credit: Prince Saroj)
- Added asyncio and chromadb dependency
- Refined Tutorial 3 to better illustrated how to use the various memory types

29 Jul 2024 (v3.3.2)
- Added ChromaDB Memory to use VectorDB for insertion and retrieval of memories (credit: Prince Saroj)

25 Jul 2024 (v3.3.1)
- Fixed a missing filter for strict_json_async to get only the text within {} for json
- Added a MemoryTemplate in memory.py, to serve as foundation for VectorDBMemory and GraphMemory

25 Jul 2024 (v3.3.0)
- For strict_json, changed the output to be f'''\nOutput in the following json template: ```{new_output_format}```
Update values enclosed in <> and remove the <>. 
Your response must only be the updated json template beginning with {{ and ending with }}
Ensure the following output keys are present in the json: {' '.join(list(new_output_format.keys()))}'''

This helps by changing ['###key###'] into ###key### , which helps Llama 3 8B better understand how to format the json

- Fixed an error with assign_functions() when assigning Inner Agents, as it assigns the Meta Agent instead of the Inner Agent inside this function
- Updated Inner Agent in shared variables under 'agent' for Inner Agent calls

22 Jul 2024 (v3.2.1)
- Changed all default model parameters in TaskGen Tutorial Notebooks and code to be 'gpt-4o-mini' instead of 'gpt-3.5-turbo' for cheaper and better performance
- Works with existing notebooks with minimal changes (needed to specify "Only provides cooking guidance" instead of "provides cooking guidance" in agent description of TaskGen Ask Me Anything Notebook to make the reply\_user more factual and related to agent's description)
- Memory (Tutorial 3) now works perfectly with 'gpt-4o-mini' and not just with 'gpt-4o'

10 Jul 2024 (v3.2.0)
- Updated strict_json prompt to be f'''\nOutput in the following json template: ```{new_output_format}```
Update values enclosed in <> and remove the <>. 
Your response must only be the updated json template beginning with {{ and ending with }}
Ensure the following output keys are present in the json: {list(new_output_format.keys())}'''
This will help smaller models like Llama 3 8B generate more reliably.
- Added explicit mention of output field keys in Agent prompts to aid smaller models
- Changed get_next_subtask() to output Equipped Function Name instead of Equipped Function for better LLM understanding, and changed prompt to include Observation and Thoughts as well
- Updated retrieve_by_ranker() in memory.py to only retrieve when the memory items are more than top_k. This helps to save some costs by not performing the retrieval when not needed
- Fixed some issues with Tutorials with no llm variable, and missing imports

6 Jul 2024 (v3.1.0)
- Changed Subtask Completed's subtask to be the function name + function params in order to 1) reduce verbosity, 2) make the subtask accurate as it is what is actually done
- Made prompt to get input parameters of Equipped Function more concise by removing Thoughts and only focusing on Current Subtask
- made `contribute_agent()` specify the github user name as well in the agent in order to avoid conflicts between user
- Fixed a bug with AsyncAgent Function Calling in `run()` due to a missing `await` keyword
- For `use_llm` function prompt, added "Do not just state that you performed the task, generate the detailed outcome as well." to get it to actually generate stuff out
- Revamped all Tutorials to use llm variable as default for `Agent`, `Function`, `strict_json`, `strict_json_async`, `AsyncFunction`, `AsyncAgent`
- Added more examples for Shared Variables and Global Context, which is the bedrock of TaskGen for things like LLM OS. Also, reorganised the Tutorials in order of priority

3 Jul 2024 (v3.0.0)
- Edited the `strict_json` and `strict_json_async` prompt to list out all the json output keys, so that the LLM will be more grounded
- Edited the `convert_to_dict` function in `base.py` to make it more robust to variations of incorrect keys, edited error message for incorrect key to make it more robust
- Changed the `conversation.py` Conversable Class to make it output Action Done in text format (instead of Subtasks Completed), which could be understood by LLMs in a better way
- Ensured the examples in Tutorial 7 would work for ChatGPT, Claude Haiku and Llama3 70B
- Despite repeated attempts to ensure compatibility for Llama3 8B, the json generating abilities are not good enough for TaskGen purposes

2 Jul 2024 (v2.6.0)
- Removed Actual Subtasks in get_next_subtasks() of Agent and AsyncAgent. It was interferring with getting the functions for Function Calling.

2 Jul 2024 (v2.5.1)
- Added auto-chunking for memory with pdf, csv, excel, docx files using `add_file` method in Memory class (Credit: Prince Saroj)
- Updated Tutorial 4 to use docstring for external functions, and to showcase auto-chunking in memory (Credit: Prince Saroj)
- Added in llm from agent to Memory as default, so that Memory retrieve_by_llm could be done via the same llm as the agent
- Added in dependencies: langchain, PyPDF2, python-docx, pandas, xlrd

1 Jul 2024 (v2.5.0)
- Added `ConversableAgent` class in `conversation.py`
- For Agent's query() function, changed it to using user_prompt instead of system_prompt, as it caused ChatGPT some issues in identifying the user input when user_prompt was blank
- Changed Updated Subtask to Actual Subtask in get_next_subtask in agent.py, for robustness
- Changed `contribute_agent` code to be done without the need for git clone (Credit: Hardik)
- Only import OpenAI when necessary in ranker.py, base.py, base_async.py (Credit: JiangZhuo)

28 Jun 2024 (v2.4.1)
- Added `code_action` as an input variable for Agent, which when set to True, will by default use the `python_generate_and_run_code_tool` function all the time except for end_task (which you need to equip to the Agent yourself externally)
- Updated Tutorial 6 for Code as Action agent
- Fixed some minor issues for `contribute_agent` code

2 Jun 2024 (v2.4.0)
- Added Async Support for `Agent`, `Function` and `strict_json` (credit to Prince Saroj)
- New Async Classes: AsyncAgent, AsyncFunction, AsyncMemory, AsyncRanker
- New Async function: strict_json_async
- New Support: support for async external functions
- Added Tutorials 8 and 9 to show how to use TaskGen in Async mode
- Added `contribute_agent` and `load_community_agent` (credit to Hardik) so as to easily contribute an Agent in code form, and download a community agent by {{agent_class_name}}. Community agents will be in the path https://github.com/simbianai/taskgen/contrib/community/{{agent_class_name}}
- Added aliases `use_tool` for `use_function`, `list_tools` for `list_functions`, `print_tools` for `print_functions`, `assign_tools` for `assign_functions`, `remove_tool` for `remove_function`
- Added a smart check for `assign_functions` to see if you are assigning an Agent. If so, we will automatically call `assign_agents` for you

24 May 2024 (v2.3.1)
- Hierarchical Agents:
    - Inner agents will no longer be able to modify meta agent's subtasks
    - Inner agents will be outputting a summary of what they have done via reply_user() instead of everything that they have done
- Global Context: Added option to provide global context as a string with <var> that will be replaced with corresponding names in shared_variables at run-time (variable global_context)
- Shared Variables: Added an 'agent' shared variables which points to itself, so that functions can access anything the agent has access to
- Memory: Changed Memory prompt to Knowledge Reference for better understanding by LLM
- Next Subtask:
    - Changed End Task (end_task) description to "Passes the final output to the user", so as to help map the end task action to reply user action which the LLM understands better
    - Modified the prompt for `get_subtasks_completed()` to better reflect that no further user input is expected, and not to repeat previous subtasks unless really needed, and that no user output is needed
    - Current Subtask generated is now with greater detail and with all context needed
    - Better input variable generation for Equipped Function for Agents
    - Modified the prompt the `use_llm()` function to know it is part of Agent
    - Changed the `summarise_subtasks_count` to 5 so that it doesn't summarise too often
- StrictJSON: 
    - Fixed nested array issue whereby we did not process nested Array[] due to not including that as part of regex (List[] works)
    - type: code now converts python```code``` -> code, which makes it more versatile for LLM code generation

18 May 2024 (v2.3.0)
- Changed Agent main prompt in `use_llm()` and `get_subtasks_completed()` and `reply_user()` to better reflect Agent is helping User to complete Assigned Task without needing any input from User, or to output directly to User. ChatGPT's prompting is heavily biased to helping Users, so it helps the Agent output much better
- After generating parameters for the Equipped Functions, we will modify the Current Subtask to better match what is done in the Equipped Functions. This helps the Agent better understand what has been done
- Added back unicode_escape decoding in `strict_json`, but it doesn't decode for all unicode escapes, just \\t, \\n, \\' and \\". Using type: code will do for all unicode escapes

18 May 2024 (v2.2.0)
- Added color for output text of Agent
- Made `get_next_subtasks` and `use_llm` prompts more versatile
- Improved definitions of `use_llm` and `end_task` default functions for Agent
- Added `summarise_subtasks_count` for `Agent`, which summarises Subtasks Completed every `summarise_subtasks_count` iterations according to overall task
- Fixed issue with Hierarchical Agents whereby the memory_bank's `Function` key in `Agent` class was overwritten by instantating of new agents. Turns out that input variables of type dict will get carried over to all instantiations of the Class. We have to set it as None, and define the dictionary within the function to prevent this
- Fixed the dictionary input variables to None for `strict_json` and `Function` and `Ranker` as well
- Added a `retrieve_fn` in `Memory` class to enable vector database queries
- split function.py from base.py as it was getting large
- type: code introduced for `strict_json` so we won't force to unicode for all strings with \\

10 May 2024 (v2.1.1)
- Ensure that external_fn must be a Python function and not a Class method, as the latter does not encompass all the necessary input variables
- Better prompting for `get_next_subtask` to end task more effectively when Assigned Task is completed

10 May 2024 (v2.1.0)
- `Agent.assign_functions()` now takes in Python functions without need to parse into `Function` class first - output format, description and input variables will be inferred from function signature and docstring
- `Function()` now allows just parsing the entire thing from external function, using `Function(external_fn = fn)`
- Created Tutorial 6: External Function Interface to demonstrate the new function interfacing capabilities for TaskGen Agents and interfacing with LangChain and LlamaIndex
- Better prompting for `get_next_subtask` to be more detailed in the next subtask
- Made `use_llm()` aware that it is called `use_llm`, so it doesn't return an error saying it does not have that function

9 May 2024 (v2.0.1)
- Fixed a bug with `async_strict_json` to incorporate one `async_chat()` call instead of `chat()`

9 May 2024 (v2.0.0)
- HUGE: Revamped `get__next_subtask` to remove overall plan array, and make LLM only do single step lookahead (works much better without much hallucination of tasks)
- HUGE: Async support added. Added async_chat, async_strict_json, async Function call in base.py
- Reprompted `reply_user()` to respond to task completion status better if there is no query
- Reformatted `Global Context:` in prompt to make it enclose everything in `get_global_context()`
- Made context of `use_llm()` function better for better generation

5 May 2024 (v1.4.0)
- Changed prompt of strict_json to make it compatible with more LLMs and improve performance on ChatGPT: 
    '''\nOutput in the following json string format: {new_output_format}
    Update text enclosed in <>. Output only a valid json string beginning with {{ and ending with }}'''
- Fixed an error with input parameter mismatch in remove() in Memory
- Added more external LLM examples in Tutorial 0
- Updated all Tutorials

22 Apr 2024 (v1.3.2)
- Changes requirements.txt to dill>=0.3.7 so that it can run with colab
- Added return_as_json = True as an input variable to strict_json (default: False, so it will be a python dictionary), so that you can get the json string as output if needed. Works also for OpenAIJson mode

16 Apr 2024 (v1.3.1)
- Changed requirements.txt to openai>=1.3.6 so that it is compatible with newer versions of LangChain

26 Mar 2024 (v1.3.0)
- Changed `get_additional_context` to `get_global_context` to better reflect that it is information that carries over across tasks
- Refined Tutorial 5 Chatbot with Sherlock Holmes example to make it more performant with some prompt changes

22 Mar 2024 (v1.2.0)
- Changed StrictJSON prompt to make it better: Added "You must output valid json with all keys present." to system prompt of `strict_json` to make it more robust
- Added `is_compulsory` variable in `Function`. Default: `False`. When set to `True`, make them always appear for planning regardless of whether RAG is used
- Added more examples for memory

18 Mar 2024 (v1.1.0)
- Made the llm variable more robust by explicitly referencing it in the calls from `Agent` to `Function` to `strict_json`
- Added agent loading and saving using `save_agent` and `load_agent`
- Added `get_additional_context` for persistent variables and additional prompts to be given to `Agent`
- Ensure that in `Ranker`, the query and key are always string to prevent unhashable types from being referenced by dictionary
- If function takes in no input, in `next_subtasks_completed()` we will not force the LLM to output the parameter and skip the step for input parameter type conformity

3 Mar 2024 (v1.0.0)
- Better prompt engineering for `use_llm`, `reply_user`, `get_next_subtask`
- Subtasks completed index now a string so as to store unhashable datatypes too
- Added functionality for removing `use_llm` and `end_task` as functions of Agent (in case you want to do manual execution of functions)
- Now allowing for duplicate subtask names by adding (count) at the end of the subtask instruction -> needed for OS-like usage\
- Equipped Function in `get_next_subtask()` now uses an Enum to force the answer into a legitimate assigned function
- `get_next_subtask()` now generates the Equipped Function Inputs separately for non-instruction-based inputs, allowing for all input fields and input types to be enforced by strictjson
- Better distinguished between Subtask and Overall Task in `use_llm`, so that `use_llm` should only focus on the current Subtask and not solve Overall Task directly
- Fixed issue with nested outputs of nested dictionaries / lists after I added the true/false to True/False auto conversion

1 Mar 2024 (v0.0.8)
- Added automatic parsing of fn_description from external function's docstring if fn_description is not provided for `Function()`
- type checks now support `array`: same functionality as `list`, but LLM may understand `array` better due to JSON formatting type
- Auto type conversion from list to array after `type:` in `output_format`

29 Feb 2024 (v0.0.7)
- Throws an exception when `type:` is given in `output_format` for `Function()` when using External Functions as we do not do type checking there
- Throws a similar exception when `type:` is given in `output_format` for `openai_json_mode = True` in `strict_json` or `Function`

28 Feb 2024 (v0.0.6)
- Made list processing in StrictJSON more robust

26 Feb 2024 (v0.0.5)
- Added memory and RAG on functions and additional context based on task / overall plan

25 Feb 2024 (v0.0.4)
- Added shared variables, enabling a pool of shared variables to persist between functions for better handling of non-text modalities as well as long text
- Refined the prompt for get_next_subtask() in order to do element by element check of whether the Overall Plan is completed
- Changed the dictionary parsing of StrictJSON to ensure that we extract the leftmost and rightmost { and } respectively to do ast.literal_eval. Previously might have the quotation marks at the left and right which affected parsing
- Changed the bool parsing of StrictJSON to convert true/false to True/False first so as to make it parseable by ast.literal_eval

22 Feb 2024 (v0.0.3)
- Added hierarchical agents
- Refined prompts for use_llm and reply_user
- Agents now use subtasks_completed as a memory for context for future generation

17 Feb 2024 (v0.0.1)
- Creation of TaskGen, a task-based agentic framework building on StrictJSON outputs by LLM agents


================================================
FILE: LICENSE
================================================
Copyright (c) 2018 The Python Packaging Authority

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


================================================
FILE: myagent.pkl
================================================
[Binary file]


================================================
FILE: pyproject.toml
================================================
[build-system]
requires = ["setuptools"]
build-backend = "setuptools.build_meta"

[project]
name = "taskgen-ai"
version = "3.3.11"
authors = [
  { name="John Tan Chong Min", email="tanchongmin@gmail.com" },
]
description = "TaskGen Simbian. A Task-based agentic framework building on StrictJSON outputs by LLM agents. Cybersecurity-focused version."
readme = "README.md"
requires-python = ">=3.8"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]
dependencies = ["openai>=1.59.6",
"langchain", "dill>=0.3.9", "termcolor>=3.1.0", "requests",
"pypdf~=6.0.0", "python-docx", "pandas", "xlrd", "chromadb>=1.0.15",
"asyncio", "opentelemetry-sdk~=1.32.1"]

[project.urls]
Homepage = "https://github.com/simbianai/taskgen"
Issues = "https://github.com/simbianai/taskgen/issues"



================================================
FILE: requirements.txt
================================================
openai>=1.59.6
langchain
dill>=0.3.9
termcolor>=3.1.0
requests
pypdf~=6.0.0
python-docx
pandas
xlrd
chromadb>=1.0.15
asyncio
#this specific version due to pycti in sa
opentelemetry-sdk~=1.32.1



================================================
FILE: setup.py
================================================
from setuptools import setup, find_packages

setup(
    name="taskgen",
    version="3.3.11",
    packages=find_packages(),
    install_requires=[
        "openai>=1.59.6",
        "langchain",
        "dill>=0.3.9",
        "termcolor>=3.1.0",
        "requests",
        "pypdf~=6.0.0",
        "python-docx",
        "pandas",
        "chromadb",
        "xlrd",
        "chromadb>=1.0.15",
        "asyncio",
    ],
)



================================================
FILE: .example.env
================================================
OPENAI_API_KEY=


================================================
FILE: contrib/README.md
================================================
This will be the folder for contributions. 

- community - For Agent code that are submitted via `contribute_agent`




================================================
FILE: contrib/community/HelpfulAssistant/main.py
================================================
from taskgen import Agent, Function, Memory, Ranker
import math


# Author: @hardikmaheshwari
class HelpfulAssistant(Agent):
    def __init__(self):
        var_binary_to_decimal = Function(
            fn_name="binary_to_decimal",
            fn_description='''Convert input <x: a binary number in base 2> to base 10''',
            output_format={'output1': 'x in base 10'},
            examples=None,
            external_fn=binary_to_decimal,
            is_compulsory=False)
        
        var_sentence_with_objects_entities_emotion = Function(
            fn_name="sentence_with_objects_entities_emotion",
            fn_description='''Output a sentence with <obj> and <entity> in the style of <emotion>''',
            output_format={'output': 'sentence'},
            examples=None,
            external_fn=None,
            is_compulsory=False)
        


        super().__init__(
            agent_name="Helpful assistant",
            agent_description='''You are a generalist agent''',
            max_subtasks=5,
            summarise_subtasks_count=5,
            memory_bank={'Function': Memory(memory=[], top_k=5, mapper=lambda x: x.fn_name + ': ' + x.fn_description, approach='retrieve_by_ranker', ranker=Ranker(model='text-embedding-3-small', ranking_fn=None)),},
            shared_variables={},
            get_global_context=None,
            global_context='''''',
            default_to_llm=True,
            verbose=True,
            debug=False
        )

        self.assign_functions(
            [var_binary_to_decimal,var_sentence_with_objects_entities_emotion]
        )

        self.assign_agents(
            []
        )
                        
# Supporting Functions
def binary_to_decimal(x):
    return int(str(x), 2)





================================================
FILE: contrib/community/MathExpert/main.py
================================================
from taskgen import Agent, Function, Memory, Ranker
import math


# Author: @tanchongmin
# Author Comments: This agent should be used for any addition-based calculation
class MathExpert(Agent):
    def __init__(self):
        var_add = Function(
            fn_name="add",
            fn_description='''Takes in <x: int> and <y: int> and returns the sum''',
            output_format={'output_1': 'int'},
            examples=None,
            external_fn=add,
            is_compulsory=False)
        


        super().__init__(
            agent_name="Math Expert",
            agent_description='''Does Math very well''',
            max_subtasks=5,
            summarise_subtasks_count=5,
            memory_bank={'Function': Memory(memory=[], top_k=5, mapper=lambda x: x.fn_name + ': ' + x.fn_description, approach='retrieve_by_ranker', ranker=Ranker(model='text-embedding-3-small', ranking_fn=None)),},
            shared_variables={},
            get_global_context=None,
            global_context='''''',
            default_to_llm=True,
            code_action=False,
            verbose=True,
            debug=False
        )

        self.assign_functions(
            [var_add]
        )

        self.assign_agents(
            []
        )
                        
# Supporting Functions
def add(x: int, y: int) -> int:
    '''Takes in x and y and returns the sum'''
    return x+y





================================================
FILE: Paper/README.md
================================================
This section is for the code for the experiments in the TaskGen Paper.

- maze_navigator is for Dynamic Maze Environment (Appendix D)
- textworld is for TextWorld (Appendix E)
- web_browsing_agent is for Web Browsing Agent (Appendix F)
- math_dataset is for MATH Level 5 problems (Appendix G)
- naturalquestions_rag is for Question Answer on NaturalQuestions dataset (Appendix H)




================================================
FILE: Paper/math_dataset/readme.md
================================================
# MATH DATASET

![math agent](math_agent.png)


- We provide the selected Level-5 problems on 5 categories (20x5 = 100 problems) in file `selected_problems_level_5.json` that we used in our paper.
- In order to run evaluation, first download and extract the MATH dataset from the main [repo](https://github.com/hendrycks/math). 


================================================
FILE: Paper/math_dataset/math.ipynb
================================================
# Jupyter notebook converted to Python script.

# Set up API key and do the necessary imports
import os
from taskgen import *

os.environ['OPENAI_API_KEY'] = ""

"""
## DATA LOADING
"""

import json
with open('selected_problems_level_5.json', 'r') as file:
    selected_problems = json.load(file)

"""
## TaskGen Agent And Functions 
"""


def python_run_tool(code_snippet: str) -> str:
    '''Runs code_snippet and outputs the result of all print statements'''
    import sys
    import io
    import math
    import numpy
    import random
    import datetime
    import re
    import matplotlib
    import pandas
    import sympy
    import plotly
    import numpy as np
    # Disable file access
    def restricted_open(*args, **kwargs):
        raise PermissionError("File access is restricted")
    
    # Capture the output
    old_stdout = sys.stdout
    sys.stdout = io.StringIO()

    try:
        # Safe environment to execute the user code
        allowed_globals = {
            '__builtins__': {
                'print': print,
                'range': range,
                'len': len,
                'int': int,
                'float': float,
                'str': str,
                'list': list,
                'dict': dict,
                'set': set,
                'tuple': tuple,
                'abs': abs,
                'min': min,
                'max': max,
                'sum': sum,
                'any': any,
                'all': all,
                'sorted': sorted,
                'zip': zip,
                'map': map,
                'filter': filter,
                '__import__': __import__,
                'math': math,  # Allow access to the math module
                'datetime': datetime, # Allow access to datetime module
                'random': random, # Allow access to random module
                'numpy': numpy, # Allow access to numpy module
                're': re,
                'pandas': pandas,
                'open': restricted_open,  # Override open to restrict file access
                'sympy': sympy,                
            }
        }

        safe_locals = {}

        exec(code_snippet, allowed_globals, safe_locals)
        output = sys.stdout.getvalue()
    except Exception as e:
        output = f"Error: {e}"
    finally:
        # Restore the original stdout
        sys.stdout = old_stdout

    return output

python_generator_tool = Function('''Generate code based only on <instruction: str> without additional context.
    Ensure that you define all variables and list out all imports.
    You can only import the following modules: math, numpy, random, datetime, re, matplotlib, pandas, sympy, plotly
    ***Do not use short forms for modules for example use import numpy instead of import numpy as np***
    Do not define any functions
    You are not able to use the Equipped Functions using this tool.
    Ensure all required output are in print statements''',
                                     output_format = {'Output Code': 'type: code'}, fn_name = 'python_generator_tool',model="gpt-4o")



python_debug_tool = Function('''Debugs Python Code and returns corrected code.
Instruction: <instruction: str>
Current Code: <python_code: str>
Error Message: <error_msg: str>''',
                                 output_format = {'Thoughts': 'How to correct code', 'Corrected Code': 'type: code'}, fn_name = 'python_debug_tool',model="gpt-4o")




# Uses LLM to generate Code
def python_generate_and_run_code_tool(shared_variables, instruction: str) -> str:
    ''' Generates and runs code based on instruction. Returns 1) the result of all print statements in code, or error messages, and 2) the code '''
    # from termcolor import colored
    
    # Append context to tool
    if shared_variables and 'agent' in shared_variables:
        instruction = f"Context: {shared_variables['agent'].overall_task}\nPrevious Subtasks: {shared_variables['agent'].subtasks_completed}\nInstruction: {instruction}"
    # Generate Code
    python_code = python_generator_tool(instruction)['Output Code']
    
    # Run and Debug Code
    for _ in range(3):
        output = python_run_tool(python_code)

        if output[:5] == "Error":
            debugged_code = python_debug_tool(instruction, python_code, output)
            python_code = debugged_code['Corrected Code']

        else:
            break
            
    return output, python_code

# Agent for Math Problem Solver

agent = Agent('Math Problem Solver', 
'''You are a math expert skilled in solving a wide range of math problems, including symbolic, algebraic, and numerical challenges. Your task is to analyze the given problem, break it down into manageable steps, and provide a comprehensive solution with clear, logical reasoning. Ensure that your explanations are thorough, using relevant mathematical principles and terminology. Importantly, present the final answer clearly and explicitly, such that no further calculations are required from the user. Please note: do not use any large language models to derive the solution, you are free to use the tools given to you for the solution like sympy library; all steps should be manually explainable and grounded in mathematical theory. When using code to solve the problem, only print out the final answer. You should only use code to solve the problem if it is necessary and should not be the primary method of solving the problem.
''',model="gpt-4o").assign_functions(
    [python_generate_and_run_code_tool])


# Assign the shared variables for agent so code tool can get context of the task
agent.shared_variables['agent'] = agent


"""
### Example : Counting and Probability
"""

example_prob = selected_problems['counting_and_probability'][0]

with open(example_prob, 'r') as file:
    example_data = json.load(file)

example_data["problem"]

"""
#### Simple example of running the agent on the problem
"""

agent.reset()    
soln = agent.run(example_data["problem"])



================================================
FILE: Paper/math_dataset/selected_problems_level_5.json
================================================
{
    "counting_and_probability": [
        "MATH/test/counting_and_probability/212.json",
        "MATH/test/counting_and_probability/650.json",
        "MATH/test/counting_and_probability/936.json",
        "MATH/test/counting_and_probability/545.json",
        "MATH/test/counting_and_probability/903.json",
        "MATH/test/counting_and_probability/1043.json",
        "MATH/test/counting_and_probability/951.json",
        "MATH/test/counting_and_probability/392.json",
        "MATH/test/counting_and_probability/1102.json",
        "MATH/test/counting_and_probability/956.json",
        "MATH/test/counting_and_probability/1081.json",
        "MATH/test/counting_and_probability/860.json",
        "MATH/test/counting_and_probability/792.json",
        "MATH/test/counting_and_probability/1062.json",
        "MATH/test/counting_and_probability/279.json",
        "MATH/test/counting_and_probability/216.json",
        "MATH/test/counting_and_probability/295.json",
        "MATH/test/counting_and_probability/263.json",
        "MATH/test/counting_and_probability/485.json",
        "MATH/test/counting_and_probability/1038.json"
    ],
    "intermediate_algebra": [
        "MATH/test/intermediate_algebra/1008.json",
        "MATH/test/intermediate_algebra/1508.json",
        "MATH/test/intermediate_algebra/1784.json",
        "MATH/test/intermediate_algebra/2152.json",
        "MATH/test/intermediate_algebra/1146.json",
        "MATH/test/intermediate_algebra/522.json",
        "MATH/test/intermediate_algebra/946.json",
        "MATH/test/intermediate_algebra/773.json",
        "MATH/test/intermediate_algebra/1403.json",
        "MATH/test/intermediate_algebra/662.json",
        "MATH/test/intermediate_algebra/1297.json",
        "MATH/test/intermediate_algebra/2142.json",
        "MATH/test/intermediate_algebra/1440.json",
        "MATH/test/intermediate_algebra/1886.json",
        "MATH/test/intermediate_algebra/1354.json",
        "MATH/test/intermediate_algebra/1850.json",
        "MATH/test/intermediate_algebra/117.json",
        "MATH/test/intermediate_algebra/206.json",
        "MATH/test/intermediate_algebra/2015.json",
        "MATH/test/intermediate_algebra/575.json",
        "MATH/test/intermediate_algebra/960.json",
        "MATH/test/intermediate_algebra/39.json"
    ],
    "number_theory": [
        "MATH/test/number_theory/427.json",
        "MATH/test/number_theory/780.json",
        "MATH/test/number_theory/221.json",
        "MATH/test/number_theory/1000.json",
        "MATH/test/number_theory/274.json",
        "MATH/test/number_theory/1121.json",
        "MATH/test/number_theory/1281.json",
        "MATH/test/number_theory/530.json",
        "MATH/test/number_theory/937.json",
        "MATH/test/number_theory/340.json",
        "MATH/test/number_theory/764.json",
        "MATH/test/number_theory/892.json",
        "MATH/test/number_theory/947.json",
        "MATH/test/number_theory/1207.json",
        "MATH/test/number_theory/43.json",
        "MATH/test/number_theory/114.json",
        "MATH/test/number_theory/1229.json",
        "MATH/test/number_theory/1181.json",
        "MATH/test/number_theory/60.json",
        "MATH/test/number_theory/1038.json"
    ],
    "prealgebra": [
        "MATH/test/prealgebra/387.json",
        "MATH/test/prealgebra/1608.json",
        "MATH/test/prealgebra/849.json",
        "MATH/test/prealgebra/1475.json",
        "MATH/test/prealgebra/1877.json",
        "MATH/test/prealgebra/1287.json",
        "MATH/test/prealgebra/1920.json",
        "MATH/test/prealgebra/980.json",
        "MATH/test/prealgebra/224.json",
        "MATH/test/prealgebra/1727.json",
        "MATH/test/prealgebra/393.json",
        "MATH/test/prealgebra/1008.json",
        "MATH/test/prealgebra/795.json",
        "MATH/test/prealgebra/1762.json",
        "MATH/test/prealgebra/2049.json",
        "MATH/test/prealgebra/1667.json",
        "MATH/test/prealgebra/2061.json",
        "MATH/test/prealgebra/1426.json",
        "MATH/test/prealgebra/1090.json",
        "MATH/test/prealgebra/1363.json"
    ],
    "algebra": [
        "MATH/test/algebra/2742.json",
        "MATH/test/algebra/1447.json",
        "MATH/test/algebra/1863.json",
        "MATH/test/algebra/1396.json",
        "MATH/test/algebra/1042.json",
        "MATH/test/algebra/1282.json",
        "MATH/test/algebra/1051.json",
        "MATH/test/algebra/2626.json",
        "MATH/test/algebra/1063.json",
        "MATH/test/algebra/1227.json",
        "MATH/test/algebra/1757.json",
        "MATH/test/algebra/1084.json",
        "MATH/test/algebra/69.json",
        "MATH/test/algebra/2531.json",
        "MATH/test/algebra/1372.json",
        "MATH/test/algebra/1810.json",
        "MATH/test/algebra/611.json",
        "MATH/test/algebra/962.json",
        "MATH/test/algebra/313.json",
        "MATH/test/algebra/510.json"
    ]
}


================================================
FILE: Paper/naturalquestions_rag/readme.md
================================================
To run eval use https://github.com/google-research-datasets/natural-questions

# Interactive mode
```python nq_eval.py --gold_path=/Users/princesaroj/simbian_work/taskgen-benchmark/gold_entries_interactive.jsonl.gz --predictions_path=/Users/princesaroj/simbian_work/taskgen-benchmark/predictions_interactive.json --pretty_print```


# Non-nteractive mode
```python nq_eval.py --gold_path=/Users/princesaroj/simbian_work/taskgen-benchmark/gold_entries_non_interactive.jsonl.gz --predictions_path=/Users/princesaroj/simbian_work/taskgen-benchmark/predictions_non_interactive.json --pretty_print```


================================================
FILE: Paper/textworld/textworld_conversation.ipynb
================================================
# Jupyter notebook converted to Python script.

%pip install taskgen-ai textworld gym torch numpy

# Set up API key and do the necessary imports
from taskgen import *
import os
from google.colab import userdata

# this is only if you use OpenAI as your LLM
keyname = 'OPENAI_API_KEY'
os.environ['OPENAI_API_KEY'] = userdata.get(keyname)

!tw-make tw-simple --rewards dense --goal detailed --seed 18 --test --silent -f --output games/tw-rewardsDense_goalDetailed.z8
!tw-make tw-simple --rewards sparse --goal brief --seed 18 --test --silent -f --output games/tw-rewardsSparse_goalBrief.z8
!tw-make tw-simple --rewards sparse --goal none --seed 18 --test --silent -f --output games/tw-rewardsSparse_goalNone.z8

def llm(system_prompt: str, user_prompt: str) -> str:
    ''' Here, we use OpenAI for illustration, you can change it to your own LLM '''
    # ensure your LLM imports are all within this function
    from openai import OpenAI

    # define your own LLM here
    client = OpenAI()
    response = client.chat.completions.create(
        model='gpt-4o',
        seed=42,
        temperature = 0,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
    )
    return response.choices[0].message.content

import os
from glob import glob
import numpy as np
import textworld.gym
from typing import Mapping, Any
import torch


def play(agent, path, max_step=100, nb_episodes=10, verbose=True):
    torch.manual_seed(20211021)  # For reproducibility when using action sampling.

    infos_to_request = agent.infos_to_request
    infos_to_request.max_score = True  # Needed to normalize the scores.

    gamefiles = [path]
    if os.path.isdir(path):
        gamefiles = glob(os.path.join(path, "*.z8"))

    env_id = textworld.gym.register_games(gamefiles,
                                          request_infos=infos_to_request,
                                          max_episode_steps=max_step)
    env = textworld.gym.make(env_id)  # Create a Gym environment to play the text game.
    if verbose:
        if os.path.isdir(path):
            print(os.path.dirname(path), end="")
        else:
            print(os.path.basename(path), end="")

    # Collect some statistics: nb_steps, final reward.
    avg_moves, avg_scores, avg_norm_scores = [], [], []
    for no_episode in range(nb_episodes):
        obs, infos = env.reset()  # Start new episode.

        score = 0
        done = False
        nb_moves = 0
        while not done:
            command = agent.act(obs, score, done, infos)
            obs, score, done, infos = env.step(command)
            nb_moves += 1

        agent.act(obs, score, done, infos)  # Let the agent know the game is done.

        if verbose:
            print(".", end="")
        avg_moves.append(nb_moves)
        avg_scores.append(score)
        avg_norm_scores.append(score / infos["max_score"])

    env.close()
    if verbose:
        if os.path.isdir(path):
            msg = "  \tavg. steps: {:5.1f}; avg. normalized score: {:4.1f} / {}."
            print(msg.format(np.mean(avg_moves), np.mean(avg_norm_scores), 1))
        else:
            msg = "  \tavg. steps: {:5.1f}; avg. score: {:4.1f} / {}."
            print(msg.format(np.mean(avg_moves), np.mean(avg_scores), infos["max_score"]))

def remove_ascii_art(text):
    import re
    new_message = ""
    for line in text.splitlines():
        x = re.findall("[:alphanum:]", line)
        if x:
            new_message += line
    return new_message

class TaskgenAgent(textworld.gym.Agent):
    def __init__(self, seed=1234):
        self._seed = seed
        self._player = Agent('interactive fiction player',
                            'You are the player of an interactive fiction game. \
                            Consider which comands have been successful and try variations throughout the game. \
                            As the player, you may only issue commands in this game. \
                            complete game objectives then explore the game world. \
                            Consider whether the previous command was poorly formed or referred to an object not present. \
                            Commands are like "LOOK AT BOB"; "EAST"; "EXAMINE KNIFE". \
                            Respond only with a single command.',
                            llm = llm,
                            debug=False)
        self.new_conversation()

    def new_conversation(self):
      self._agent = ConversableAgent(self._player,
            persistent_memory = {'game objectives': 'an array of remaining game objectives. exploration may inform new objectives. array excludes objectives achieved.',
                                'successful commands': "an array of commands that succeeded in the game.",
                                'objects' : "an array of objects discovered and where they were last seen e.g.: 'red car in the northern carpark'; 'cup on the low table'; 'cardboard box in the cellar'.",
                                'rooms': "an array of rooms discovered and how they are organised e.g.: 'northern carpark east of the kitchen'; 'landing at the top of the stairs'."
                                },
            person = 'Game',
            verbose=False)

    @property
    def infos_to_request(self) -> textworld.EnvInfos:
        return textworld.EnvInfos(
            #admissible_commands=True,
            inventory=True)

    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> str:
        if done:
          self.new_conversation()
          return "quit"
        else:
          # result must be shorter than 200 chars https://gitlab.com/DavidGriffith/frotz/-/blob/master/Makefile#L120
          obs = remove_ascii_art(obs)
          #[ print(f'CHT: {x}') for x in infos]
          chat_input = f"{obs}\n\n{infos['inventory']}\n"
          if 'admissible_commands' in infos :
            commands = "\n".join([ f"- {x}" for x in infos['admissible_commands'] ])
            chat_input += f"\npossible commands:\n{commands}"
          #print(f'INP: {chat_input}')
          out_str = self._agent.chat(chat_input)
          if len(out_str) > 99:
              out_str = out_str[:99]
          self._last_command = out_str
          #print(f' OUT: {out_str}')
          #foo = input('do anything to continue')
          return out_str

from openai import OpenAI
client = OpenAI()

class LlmAgent(textworld.gym.Agent):
    def __init__(self, seed=42):
        self.seed = seed
        self.previous_messages = []

    @property
    def infos_to_request(self) -> textworld.EnvInfos:
        return textworld.EnvInfos(
            admissible_commands=True,
            inventory=True)

    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> str:
        # result should be shorter than 200 chars https://gitlab.com/DavidGriffith/frotz/-/blob/master/Makefile#L120
        messages = [{"role": "system", "content": f"You are the player of an interactive fiction game. As the player, you may only issue certain commands in this game. You must play the game by issuing commands of up to 200 characters."}]
        [ messages.append( {"role": msg[0], "content": msg[1]}) for msg in self.previous_messages[-10:]]
        obs = remove_ascii_art(obs)
        chat_input = f"{obs}\n\n{infos['inventory']}\n"
        if 'admissible_commands' in infos :
          commands = "\n".join([ f"- {x}" for x in infos['admissible_commands'] ])
          chat_input += f"\npossible commands:\n{commands}"
        #print(f'INP: {chat_input}')
        current_state = "\n\n".join([chat_input, "What is your command?"])
        messages.append( {"role": "user", "content": current_state})
        completion = client.chat.completions.create(model="gpt-4o", max_tokens=200, seed=self.seed, messages=messages)
        out_str = completion.choices[0].message.content
        self.previous_messages.append( ("user", obs))
        self.previous_messages.append( ("assistant", out_str) )
        if len(out_str) > 200:
            out_str = out_str[:200]
        return out_str

class RandomAgent(textworld.gym.Agent):
    """ Agent that randomly selects a command from the admissible ones. """
    def __init__(self, seed=1234):
        self.seed = seed
        self.rng = np.random.RandomState(self.seed)

    @property
    def infos_to_request(self) -> textworld.EnvInfos:
        return textworld.EnvInfos(admissible_commands=True)

    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> str:
        return self.rng.choice(infos["admissible_commands"])

play(TaskgenAgent(), "./games/tw-rewardsDense_goalDetailed.z8")    # Dense rewards, detailed goals
play(TaskgenAgent(), "./games/tw-rewardsSparse_goalBrief.z8")    # Sparse rewards, brief goals
play(TaskgenAgent(), "./games/tw-rewardsSparse_goalNone.z8")    # Sparse rewards, no goal

play(LlmAgent(), "./games/tw-rewardsDense_goalDetailed.z8")    # Dense rewards, detailed goals
play(LlmAgent(), "./games/tw-rewardsSparse_goalBrief.z8")    # Sparse rewards, brief goals
play(LlmAgent(), "./games/tw-rewardsSparse_goalNone.z8")    # Sparse rewards, no goal

play(RandomAgent(), "./games/tw-rewardsDense_goalDetailed.z8")    # Dense rewards, detailed goals
play(RandomAgent(), "./games/tw-rewardsSparse_goalBrief.z8")    # Sparse rewards, brief goals
play(RandomAgent(), "./games/tw-rewardsSparse_goalNone.z8")    # Sparse rewards, no goal



================================================
FILE: Paper/web_browsing_agent/README.md
================================================
# Web Browser Implementation
This example uses the Taskgen framework to create a web browser agent that can interact with web pages.     
The agent can take screenshots, extract text, and interact with interactive elements on the page.

## Introduction

## Features
The agent will be able to perform the following tasks:
- Open a web page
- Going to a specific URL
- Extract relevant content from the page
- Saves context in context.txt file

For more details, refer to the tasks list:

## Prerequisites
- Python 3.10
- taskgen library
- python-dotenv library
- OpenAI API key

## Setup
Create a `.env` file in the project root directory and add your OpenAI API key:
```bash
OPENAI_API_KEY=your_api_key_here
```

Install the required dependencies:
```bash
# Create a venv file in the root project directory
python -m venv venv

# Activate the venv environment
source venv/bin/activate

# Install the required dependencies
cd Paper/web_browsing_agent && pip install -r requirements.txt

# Run the web browsing agent
python conversational_web_browsing_agent.py
```

## Tasks given:

| Queries Completed | Task Description |
|-------------------|------------------|
| 0/5 | Search 'impact of social media on mental health' and summarize the academic studies. |
| 5/5 | Visit 'https://www.who.int' and summarize the latest health advisories. |
| 5/5 | Open the browser, search 'weather forecast New York', and save the first result. |
| 5/5 | Navigate to 'https://www.bbc.com/news', extract the top news headlines, and summarize them. |
| 0/5 | Search 'global warming statistics 2024' and provide a summary of the data trends. |
| 5/5 | Visit 'https://www.finance.yahoo.com', gather the latest stock market updates, and summarize. |
| 5/5 | Search 'Shakespeare's influence on modern literature' and summarize the academic articles. |
| 5/5 | Search 'quantum computing vs classical computing' and summarize the differences from multiple sources. |
| 5/5 | Visit 'https://www.nasa.gov', gather the latest Mars mission updates, and cross-reference with Wikipedia. |
| 1/5 | Search 'key events in AI development 2024' and summarize the timeline. |
| 3/5 | Search 'market analysis of electric vehicles 2024' and summarize the findings. |
| 5/5 | Search 'best noise-canceling headphones 2024' and summarize the top reviews. |
| 5/5 | Navigate to 'https://www.consumerreports.org', gather information on washing machines, and summarize the best options. |
| 5/5 | Visit 'https://docs.python.org', find information on Python decorators, and summarize. |
| 2/5 | Search 'workplace safety measures during COVID-19' and summarize the guidelines. |
| 4/5 | Visit 'https://www.cdc.gov', find information on flu prevention, and summarize. |
| 0/5 | Search 'latest trends in renewable energy 2024' and summarize the key developments. |
| 5/5 | Visit 'https://www.techcrunch.com', gather the latest technology news, and summarize. |
| 4/5 | Search 'evolution of jazz music' and summarize its impact on modern genres. |
| 5/5 | Visit 'https://www.metmuseum.org', explore the latest exhibits, and summarize. |

## User guide:
1. Run `python conversational_web_browsing_agent.py`
2. Users are able to ask questions on the search after completion from agent's memory.
3. When performing a new search, the agent should be exited by typing 'exit' and then re-entering the search query.
4. If having problems with reliability start with "Open the browser" followed by "search 'query'" and action to be taken.


================================================
FILE: Paper/web_browsing_agent/conversational_web_browsing_agent.py
================================================
import os
from typing import Tuple

from bs4 import BeautifulSoup
from dotenv import load_dotenv
from helium import get_driver, go_to, kill_browser, start_chrome
from openai import OpenAI
from selenium.webdriver.common.keys import Keys

from taskgen import Agent


# Load the API key from the .env file
def load_api_key(dotenv_path="../../.env"):
    load_dotenv(dotenv_path)
    return os.getenv("OPENAI_API_KEY")


load_api_key()


# Define the functions for browser control
def open_browser() -> str:
    """Opens the web browser using Helium"""
    start_chrome()
    return {"Output": "Web browser opened. Currently on empty page."}


def close_browser() -> str:
    """Closes the web browser using Helium"""
    kill_browser()
    return {"Output": "Web browser closed."}


def _browser_state() -> Tuple[str, str]:
    driver = get_driver()
    header = f"Address: {driver.current_url}\n"
    header += f"Title: {driver.title}\n"
    content = driver.page_source
    return (header, content)


def clean_html(content: str) -> str:
    soup = BeautifulSoup(content, "lxml")
    for script in soup(["script", "style"]):
        script.decompose()
    return soup.get_text(separator="\n")


def extract_relevant_content(html_content: str) -> str:
    soup = BeautifulSoup(html_content, "lxml")

    # Remove unwanted tags
    for script in soup(["script", "style", "nav", "footer", "header", "aside"]):
        script.decompose()

    # Extract headings and paragraphs
    headings = soup.find_all(["h1", "h2", "h3", "h4", "h5", "h6"])
    paragraphs = soup.find_all("p")

    # Combine the text from headings and paragraphs
    content = "\n".join(
        [heading.get_text() for heading in headings]
        + [para.get_text() for para in paragraphs]
    )

    # Optionally, limit the length of the content
    max_length = 2000  # Adjust as needed
    if len(content) > max_length:
        content = content[:max_length] + "..."

    return content


def save_context_to_file(header: str, content: str, filename: str = "context.txt"):
    clean_content = extract_relevant_content(content)
    with open(filename, "w", encoding="utf-8") as file:
        file.write(header + "\n=======================\n" + clean_content)


def informational_search(query: str) -> str:
    go_to(f"https://www.bing.com/search?q={query}")
    header, content = _browser_state()
    save_context_to_file(header, content)
    return {
        "Output": f"Performed informational search for '{query}' and saved context."
    }


def navigational_search(query: str) -> str:
    go_to(f"https://www.bing.com/search?q={query}")
    driver = get_driver()
    first_result = driver.find_element_by_css_selector("a")
    first_result.send_keys(Keys.RETURN)
    header, content = _browser_state()
    save_context_to_file(header, content)
    return {"Output": f"Performed navigational search for '{query}' and saved context."}


def visit_page(url: str) -> str:
    go_to(url)
    header, content = _browser_state()
    save_context_to_file(header, content)
    return {"Output": f"Visited page at {url} and saved context."}


def page_up() -> str:
    driver = get_driver()
    driver.execute_script("window.scrollBy(0, -window.innerHeight);")
    header, content = _browser_state()
    save_context_to_file(header, content)
    return {"Output": "Scrolled up and saved context."}


def page_down() -> str:
    driver = get_driver()
    driver.execute_script("window.scrollBy(0, window.innerHeight);")
    header, content = _browser_state()
    save_context_to_file(header, content)
    return {"Output": "Scrolled down and saved context."}


def summarize_context(filename: str = "context.txt") -> str:
    with open(filename, "r", encoding="utf-8") as file:
        content = file.read()

    client = OpenAI()

    response = client.chat.completions.create(
        model="gpt-3.5-turbo-0125",
        messages=[
            {
                "role": "system",
                "content": "You are a helpful assistant designed to summarize web pages.",
            },
            {
                "role": "user",
                "content": "Please summarize the following content:\n\n" + content,
            },
        ],
        max_tokens=150,
    )

    summary = response.choices[0].message.content.strip()
    return {"Output": f"Summary: {summary}"}


# Define the functions for web surfing
fn_list_3 = [
    informational_search,
    navigational_search,
    visit_page,
    open_browser,
    close_browser,
    summarize_context,
]

WebSurfer = Agent(
    "WebSurfer",
    "Performs web searches and navigates web pages. Always open the browser at the start of the task and close the browser at the end.",
    model="gpt-4o",
    default_to_llm=False,
).assign_functions(fn_list_3)

# Define the boss agent (meta agent) that controls other agents
bossagent = Agent(
    "WebNavigator",
    "Assists user to navigate the web. Always open the browser at the start of the task and close the browser at the end.",
    model="gpt-4o",
    default_to_llm=False,
)

# Update the boss agent to include the new WebSurfer agent
agent_list = [WebSurfer]
bossagent.assign_agents(agent_list)


# Define the help function
def display_help():
    help_text = """
Available commands:
  - open_browser: Opens the web browser
  - close_browser: Closes the web browser
  - type_text <text>: Types the given text
  - enter_key: Presses the Enter key
  - navigate_to_url_via_address_bar <url>: Navigates to the specified URL
  - exit: Exits the conversation
  - help: Displays this help message
"""
    print(help_text)


def main():
    print("Welcome to the WebNavigator CLI!")
    print("Type 'help' for a list of commands or 'exit' to quit.")
    print("Note: This CLI uses Bing for search queries.")
    print("Example query: Open browser, Search 'hello world !' and summarise the content.")

    while True:
        query = input("User: ")
        if query.lower() == "exit":
            print("Exiting the conversation.")
            break
        elif query.lower() == "help":
            display_help()
        else:
            output = bossagent.run(query)
            print(output)


if __name__ == "__main__":
    main()



================================================
FILE: Paper/web_browsing_agent/conversational_web_browsing_agent_results.py
================================================
import matplotlib.pyplot as plt

# List of queries
queries = [
    "Impact of social media on mental health",
    "WHO health advisories",
    "Weather forecast New York",
    "BBC news headlines",
    "Global warming statistics 2024",
    "Yahoo Finance stock market updates",
    "Shakespeare's influence on modern literature",
    "Quantum computing vs classical computing",
    "NASA Mars mission updates",
    "Key events in AI development 2024",
    "Market analysis of electric vehicles 2024",
    "Best noise-canceling headphones 2024",
    "Consumer Reports washing machines",
    "Python decorators",
    "Workplace safety measures during COVID-19",
    "CDC flu prevention",
    "Latest trends in renewable energy 2024",
    "TechCrunch technology news",
    "Evolution of jazz music",
    "MetMuseum latest exhibits",
]

# Number of successful results for each query
successful_results = [
    0, 5, 5, 5, 0, 5, 5, 5, 5, 1, 3, 5, 5, 5, 2, 4, 0, 5, 4, 5
]

# Calculate the percentage of successful results
total_tests = 5
success_rate = [(result / total_tests) * 100 for result in successful_results]

# Create the plot
fig, ax = plt.subplots(figsize=(12, 10))

# Plotting the bar chart
bars = ax.barh(queries, success_rate, color="skyblue")

# Adding labels and title
ax.set_xlabel("Success Rate (%)")
ax.set_title("Success Rate for Each Query (Based on 5 Test Attempts)")

# Adding text annotations
for bar in bars:
    width = bar.get_width()
    label_x_pos = width - 10 if width > 10 else width + 1
    ax.text(
        label_x_pos,
        bar.get_y() + bar.get_height() / 2,
        f"{width:.1f}%",
        ha="center",
        va="center",
        color="black",
        fontsize=8,
    )

# Customize the tick labels for better readability
plt.yticks(fontsize=10)
plt.xticks(range(0, 101, 10))

# Adding figure note
description_text = (
    "Figure H2: Graphical representation of the success rates for each query tested. "
    "Each query was tested 5 times, and the success rate is calculated as the percentage of successful "
    "attempts out of these 5 tests. The chart compares the effectiveness of different queries, providing "
    "a clear visualization of the success rate for each query."
)
plt.figtext(
    0.5, -0.1, description_text, wrap=True, horizontalalignment="center", fontsize=10
)

# Display the chart
plt.tight_layout()
plt.show()





================================================
FILE: Paper/web_browsing_agent/requirements.txt
================================================
annotated-types==0.6.0
anyio==3.7.1
attrs==23.2.0
beautifulsoup4==4.12.3
cachetools==5.3.3
certifi==2024.2.2
charset-normalizer==3.3.2
cloudpickle==3.0.0
contourpy==1.2.1
cycler==0.12.1
dill==0.3.8
distro==1.9.0
exceptiongroup==1.2.0
Farama-Notifications==0.0.4
fonttools==4.53.1
google-api-core==2.18.0
google-auth==2.29.0
google-cloud-vision==3.7.2
googleapis-common-protos==1.63.0
grpcio==1.62.2
grpcio-status==1.62.2
gymnasium==0.29.0
h11==0.14.0
helium==5.0.0
httpcore==1.0.4
httpx==0.27.0
idna==3.6
kiwisolver==1.4.5
lxml==5.2.2
matplotlib==3.9.1
miniwob==1.0
MouseInfo==0.1.3
numpy==1.26.4
openai==1.3.6
outcome==1.3.0.post0
packaging==24.1
pillow==10.2.0
proto-plus==1.23.0
protobuf==4.25.3
pyasn1==0.6.0
pyasn1_modules==0.4.0
PyAutoGUI==0.9.54
pydantic==2.6.4
pydantic_core==2.16.3
PyGetWindow==0.0.9
PyMsgBox==1.0.9
pyobjc-core==10.1
pyobjc-framework-Cocoa==10.1
pyobjc-framework-Quartz==10.1
pyparsing==3.1.2
pyperclip==1.8.2
PyRect==0.2.0
PyScreeze==0.1.30
PySocks==1.7.1
python-dateutil==2.9.0.post0
python-dotenv==1.0.1
pytweening==1.2.0
requests==2.31.0
rsa==4.9
rubicon-objc==0.4.7
selenium==4.18.1
six==1.16.0
sniffio==1.3.1
sortedcontainers==2.4.0
soupsieve==2.5
taskgen-ai==2.3.1
termcolor==2.4.0
tqdm==4.66.2
trio==0.24.0
trio-websocket==0.11.1
typing_extensions==4.10.0
urllib3==2.2.1
wsproto==1.2.0



================================================
FILE: resources/README.md
================================================
This has resources of how TaskGen Pro works, as well as public sharings of TaskGen Pro.



================================================
FILE: taskgen/__init__.py
================================================
from .base import *
from .base_async import *
from .memory import *
from .ranker import *
from .function import *
from .agent import *
from .wrapper import *

__all__ = [
    "strict_json",
    "strict_json_async",
    "strict_text",
    "strict_output",
    "strict_function",
    "Function",
    "AsyncFunction",
    "chat",
    "chat_async",
    "Ranker",
    "ConversableAgent",
    "ConversationWrapper",
    "AsyncRanker",
    "Memory",
    "MemoryTemplate",
    "AsyncMemory",
    "ChromaDbMemory",
    "AsyncChromaDbMemory",
    "Agent",
    "AsyncAgent",
]



================================================
FILE: taskgen/base.py
================================================
import json
import re
import ast
from typing import Tuple


### Helper Functions ###

def convert_to_list(field: str, **kwargs) -> list:
    '''Converts the string field into a list using the LLM (with **kwargs) to list out elements line by line'''
    
    system_msg = '''Output each element of the list in a new line starting with (%item) and ending with \n, e.g. ['hello', 'world'] -> (%item) hello\n(%item) world\nStart your response with (%item) and do not provide explanation'''
    user_msg = str(field)
    res = chat(system_msg, user_msg, **kwargs)

    # Extract out list items
    field = re.findall(r'\(%item\)\s*(.*?)\n*(?=\(%item\)|$)', res, flags=re.DOTALL)
    return field


def convert_to_dict(field: str, keys: dict, delimiter: str) -> dict:
    '''Converts the string field into a dictionary with keys by splitting on '{delimiter}{key}{delimiter}' '''
    output_d = {}
    for key in keys:
        # if output field missing, raise an error
        if f"'{delimiter}{key}{delimiter}':" not in field and f'"{delimiter}{key}{delimiter}":' not in field: 
            # try to fix it if possible
            ## Cases with no delimiter but with key and/or incomplete quotations
            if field.count(f"'{key}':") == 1:
                field = field.replace(f"'{key}':", f"'{delimiter}{key}{delimiter}':")
            elif field.count(f"'{key}:") == 1:
                field = field.replace(f"'{key}:", f"'{delimiter}{key}{delimiter}':")
            elif field.count(f"{key}':") == 1:
                field = field.replace(f"{key}':", f"'{delimiter}{key}{delimiter}':")
            elif field.count(f'"{key}":') == 1:
                field = field.replace(f'"{key}":', f'"{delimiter}{key}{delimiter}":')
            elif field.count(f'"{key}:') == 1:
                field = field.replace(f'"{key}:', f'"{delimiter}{key}{delimiter}":')
            elif field.count(f'{key}":') == 1:
                field = field.replace(f'{key}":', f'"{delimiter}{key}{delimiter}":')
                
            ## Cases with delimiter but with incomplete quotations
            elif field.count(f'{delimiter}{key}{delimiter}:') == 1:
                field = field.replace(f'{delimiter}{key}{delimiter}:', f'"{delimiter}{key}{delimiter}":')
            elif field.count(f'"{delimiter}{key}{delimiter}:') == 1:
                field = field.replace(f'"{delimiter}{key}{delimiter}:', f'"{delimiter}{key}{delimiter}":')
            elif field.count(f'{delimiter}{key}{delimiter}":') == 1:
                field = field.replace(f'{delimiter}{key}{delimiter}":', f'"{delimiter}{key}{delimiter}":')
            elif field.count(f"'{delimiter}{key}{delimiter}:") == 1:
                field = field.replace(f"'{delimiter}{key}{delimiter}:", f"'{delimiter}{key}{delimiter}':")
            elif field.count(f"{delimiter}{key}{delimiter}':") == 1:
                field = field.replace(f"{delimiter}{key}{delimiter}':", f"'{delimiter}{key}{delimiter}':")
            else:
                raise Exception(f'''The key "{delimiter}{key}{delimiter}" is not present in json output. Ensure that you include this key in the json output.''')
                
    # if all is good, we then extract out the fields
    # Use regular expressions to extract keys and values
    pattern = fr",*\s*['|\"]{delimiter}([^#]*){delimiter}['|\"]:\s*"

    matches = re.split(pattern, str(field[1:-1]).strip())

    # remove null matches
    my_matches = [match for match in matches if match !='']

    # remove the ' from the value matches
    curated_matches = [match[1:-1] if match[0] in '\'"' else match for match in my_matches]

    # create a dictionary
    for i in range(0, len(curated_matches), 2):
        output_d[curated_matches[i]] = curated_matches[i+1]
        
    return output_d

def llm_check(field, llm_check_msg: str, **kwargs) -> Tuple[bool, str]:
    ''' Uses the LLM to check if the field adheres to the llm_check_msg.
    Outputs whether requirement is met (True or False) and the action needed'''
    system_msg = f'''Check whether output field meets this requirement: {llm_check_msg}
Output in the following format:
```
# Thoughts: <Thoughts about whether output field meets requirement>
# Requirement Met: <Yes or No>
# Action Needed: <If Requirement Met is No, state in one sentence how to meet requirement. Otherwise, output NA>"
```
Update text enclosed in <>. Be concise.
'''
    user_msg = str(field)
    res = chat(system_msg, user_msg, **kwargs)
    requirement_met, action_needed = parse_response_llm_check(res)
    return requirement_met, action_needed


def parse_response_llm_check(res: str) -> Tuple[bool, str]:
    pattern = r"# Thoughts: (.+)\n# Requirement Met: (.+)\n# Action Needed: (.+)"
    matches = re.findall(pattern, res)

    if matches:
        _, requirement_met, action_needed = matches[0]
        requirement_met = 'yes' in requirement_met.lower()
        action_needed = action_needed.strip() if not requirement_met else ''
    else:
        requirement_met = 'yes' in res.lower()
        action_needed = res.strip() if not requirement_met else ''

    return requirement_met, action_needed




def type_check_and_convert(field, key, data_type, **kwargs):
    if data_type.lower() == 'str':
        try:
            # convert to string and remove escape characters from indents and quotations
            field = str(field)
        except Exception as e:
            pass
        if not isinstance(field, str):
            raise Exception(f'''Output field of "{key}" not of data type {data_type}. If not possible to match, output '' ''')
    # special case of str is code, where we remove double backslashes
    if data_type.lower() == 'code':
        try:
            field = str(field)
        except Exception as e:
            pass
        if not isinstance(field, str):
            raise Exception(f'''Output field of "{key}" not of data type {data_type}. If not possible to match, output '' ''')
        # do decoding for double backslashes
        field = bytes(field, "utf-8").decode("unicode_escape")
        # replace aprostrophes
        field = field.replace("â\x80\x99", "'")
        # remove the python and ```, whitespace at the front and back of code if any
        field = re.sub(r"^(\s|`)*(?i:python)?\s*", "", field)
        # Removes whitespace & ` from end
        field = re.sub(r"(\s|`)*$", "", field)
            
    # check for int
    if data_type.lower() == 'int':
        try:
            field = int(field)
        except Exception as e:
            pass
        if not isinstance(field, int):
            raise Exception(f'Output field of "{key}" not of data type {data_type}. If not possible to match, output 0')
    
    # check for float
    if data_type.lower() == 'float':
        try:
            field = float(field)
        except Exception as e:
            pass
        if not isinstance(field, float):
            raise Exception(f'Output field of "{key}" not of data type {data_type}. If not possible to match, output 0.0')
            
    # check for bool
    if data_type.lower() == 'bool':
        field = str(field)
        if 'true' == field[:4].lower():
            field = True
        elif 'false' == field[:5].lower():
            field = False
        else:
            raise Exception(f'Output field of "{key}" not of data type {data_type}. If not possible to match, output True')

    # check for dict
    if data_type[:4].lower() == 'dict':
        if not isinstance(field, dict):
            # first try to see if we can do ast.literal_eval with { and }
            try:
                field = str(field)
                startindex = field.find('{')
                endindex = field.rfind('}')
                field = field[startindex: endindex+1]
                field = ast.literal_eval(field)
                assert(isinstance(field, dict))
            except Exception as e:
                raise Exception(f"Output field of {key} not of data type dict. If not possible to match, rephrase output field into dictionary with attribute names as key and attribute description as value")
            
        # if we define more things in dict, evaluate those
        if len(data_type) > 4:
            try:
                attribute_checks = ast.literal_eval(data_type[4:])
                assert(isinstance(attribute_checks, list) == True)
            except Exception as e:
                raise Exception(f'Dictionary keys {data_type[4:]} of output field of "{key}" are not properly defined. Ensure that it is a proper list')
                
            # if data_type is a valid list, check if elements of list are present in dictionary
            if isinstance(attribute_checks, list):
                for item in attribute_checks:
                    if item not in field.keys():
                        raise Exception(f'Output field of "{key}" of type dict does not contain the key "{item}". The dict should contain keys {attribute_checks}')
            
    # check for enum
    if data_type[:4].lower() == 'enum':
        try:
            values = ast.literal_eval(data_type[4:])  
            assert(isinstance(values, list) == True)
        except Exception as e:
            raise Exception(f'Enumeration values {data_type[4:]} of output field of "{key}" are not properly defined. Ensure that it is a proper list')
        if field not in values:
            raise Exception(f'Output field of "{key}" ({field}) not one of {values}. If not possible to match, output {values[0]}')
    return field



def check_datatype(field, key: dict, data_type: str, **kwargs):
    ''' Ensures that output field of the key of JSON dictionary is of data_type 
    Currently supports int, float, str, code, enum, lists, nested lists, dict, dict with keys
    Takes in **kwargs for the LLM model
    Returns corrected output field that matches the datatype'''
    data_type = data_type.strip()
    
    # check if we want an LLM-based correction
    if data_type.lower()[:6] == 'ensure':
        llm_check_msg = data_type[6:].strip()
        print(f'Using LLM to check "{field}" to see if it adheres to "{llm_check_msg}"')
        requirement_met, action_needed = llm_check(field, llm_check_msg, **kwargs)
        # if check failed, raise error
        if not requirement_met:
            raise Exception(f'''Output field of "{key}" does not meet requirement "{llm_check_msg}". Action needed: "{action_needed}"''')
            
    # check for list at beginning of datatype
    # or the output field begins with [ and ends with ] but it is not a list, indicating an error with ast.literal_eval
    if data_type.lower()[:4] == 'list' or data_type.lower()[:5] == 'array' or (str(field)[0]=='[' and str(field)[-1]==']'):
        # first try to see if we can do ast.literal_eval with { and }
        try:
            field = str(field)
            startindex = field.find('[')
            endindex = field.rfind(']')
            field = field[startindex: endindex+1]
            field = ast.literal_eval(field)
        except Exception as e:
            pass
        if not isinstance(field, list):
            # if it is already in a datatype that is a list, ask LLM to fix it (1 LLM call)
            if '[' in field and ']' in field:
                print(f'Attempting to use LLM to fix {field} as it is not a proper array')
                field = convert_to_list(field, **kwargs)   
                print(f'Fixed list: {field}\n\n')
            else:
                raise Exception(f'''Output field of "{key}" not of data type array. If not possible to match, split output field into parts for elements of the array''')
            
    # check for nested list
    # Regex pattern to match content inside square brackets
    match = re.search(r"list\[(.*)\]", data_type, re.IGNORECASE)
    if match:
        internal_data_type = match.group(1)  # Extract the content inside the brackets
        # do processing for internal elements
        for num in range(len(field)):
            field[num] = check_datatype(field[num], 'array element of '+key, internal_data_type, **kwargs)
            
    match = re.search(r"array\[(.*)\]", data_type, re.IGNORECASE)
    if match:
        internal_data_type = match.group(1)  # Extract the content inside the brackets
        # do processing for internal elements
        for num in range(len(field)):
            field[num] = check_datatype(field[num], 'array element of '+key, internal_data_type, **kwargs)
            
    # if it is not nested, check individually
    else:
        field = type_check_and_convert(field, key, data_type, **kwargs)
    return field







def check_key(field: str, output_format, new_output_format, delimiter: str, delimiter_num: int, **kwargs):
    ''' Check whether each key in dict, or elements in list of new_output_format is present in field, and whether they meet the right data type requirements, then convert field to the right data type
    If needed, calls LLM model with parameters **kwargs to correct the output format for improperly formatted list
    output_format is user-given output format at each level, new_output_format is with delimiters in keys, and angle brackets surrounding values
    If output_format is a string, decode escape characters, so that code can run
    Returns field that is converted to a dictionary if able to. Otherwise, raises Exception errors for missing keys or wrong output format'''
    
    cur_delimiter = delimiter*delimiter_num
    
    if isinstance(output_format, dict):   
        # this is the processed output dictionary for that particular layer in the output structure
        output_d = {}
        # check key appears for each element in the output
        output_d = convert_to_dict(field, output_format.keys(), cur_delimiter)
            
        # after creating dictionary, step into next layer
        for key, value in output_d.items():
            # # if the output is a bool type, convert true and false into True and False for ast.literal_eval parsing
            if isinstance(output_format[key], str) and 'type:' in output_format[key] and 'bool' in output_format[key].split('type:')[-1]:
                value = value.replace('true','True').replace('false','False')
            output_d[key] = check_key(value, output_format[key], new_output_format[cur_delimiter+key+cur_delimiter], delimiter, delimiter_num+1)
            # after stepping back from the later layers back to present layer, check for types
            if isinstance(output_format[key], str) and 'type:' in output_format[key]:             
                # extract out data type
                data_type = str(output_format[key]).split('type:')[-1]
                # check the data type, perform type conversion as necessary
                output_d[key] = check_datatype(output_d[key], key, data_type, **kwargs)   
                
        return output_d

    # if list, step into each element
    elif isinstance(output_format, list):
        try:
            field = ast.literal_eval(field)
        except Exception as e:
            # if there is an error in literal processing, use LLM to split field into list
            field = convert_to_list(field, **kwargs)
            
        # check that list has at least same number of elements as the input
        if len(field) < len(output_format):
            raise Exception(f'''Output "{field}" has fewer elements than required by "{output_format}". Add in more list elements.''')
        
        return [check_key(str(field[num]), output_format[num], new_output_format[num], delimiter, delimiter_num+1) for num in range(len(output_format))]
    
    # if string, then do literal eval to convert output field for further processing
    elif isinstance(output_format, str):
        # if literal eval fails, just leave it as string, no need to raise error
        try:
            field = ast.literal_eval(field)
        except Exception as e:
            pass
        return remove_unicode_escape(field)
    
    # otherwise just return the value
    else:
        return field
    
    
    

    
def remove_unicode_escape(my_datatype):
    ''' Removes the unicode escape character from the ending string in my_datatype (can be nested) '''
    if isinstance(my_datatype, dict):
        output_d = {}
        # wrap keys with delimiters
        for key, value in my_datatype.items():
            output_d[key] = remove_unicode_escape(value)
        return output_d
    elif isinstance(my_datatype, list):
        return [remove_unicode_escape(item) for item in my_datatype]
    # if it is a string, remove the unicode escape characters from it, so code can be run
    elif isinstance(my_datatype, str):
        # only do decoding for code if backslash present
        if '\\' in my_datatype:
            my_datatype = my_datatype.replace('\\n','\n').replace('\\t','\t').replace('\\"','\"').replace("\\'","\'").replace("â\x80\x99", "'")
        return my_datatype
    else:
        return my_datatype
    
def wrap_with_angle_brackets(d: dict, delimiter: str, delimiter_num: int) -> dict:
    ''' Changes d to output_d by wrapping delimiters over the keys, and putting angle brackets on the values 
    Also changes all mention of `list` after type: to `array` for better processing '''
    if isinstance(d, dict):
        output_d = {}
        # wrap keys with delimiters
        for key, value in d.items():
            new_key = f'{delimiter}'*delimiter_num + str(key) + f'{delimiter}'*delimiter_num
            output_d[new_key] = wrap_with_angle_brackets(value, delimiter, delimiter_num+1)
        return output_d
    elif isinstance(d, list):
        return [wrap_with_angle_brackets(item, delimiter, delimiter_num+1) for item in d]
    elif isinstance(d, str):
        if 'type:' in d:
            type_part = d.split('type:')[1]
            original_type_part = type_part
            type_part = re.sub(r'\blist\b', 'array', type_part)
            # replace any mention of the word list with array at the later part
            d.replace(original_type_part, type_part)
        return f'<{d}>'
    else:
        return d
    
def chat(system_prompt: str, user_prompt: str, model: str = 'gpt-4o-mini', temperature: float = 0, verbose: bool = False, host: str = 'openai', llm = None, **kwargs):
    r"""Performs a chat with the host's LLM model with system prompt, user prompt, model, verbose and kwargs
    Returns the output string res
    - system_prompt: String. Write in whatever you want the LLM to become. e.g. "You are a \<purpose in life\>"
    - user_prompt: String. The user input. Later, when we use it as a function, this is the function input
    - model: String. The LLM model to use for json generation
    - verbose: Boolean (default: False). Whether or not to print out the system prompt, user prompt, GPT response
    - host: String. The provider of the LLM
    - llm: User-made llm function.
        - Inputs:
            - system_prompt: String. Write in whatever you want the LLM to become. e.g. "You are a \<purpose in life\>"
            - user_prompt: String. The user input. Later, when we use it as a function, this is the function input
        - Output:
            - res: String. The response of the LLM call
    - **kwargs: Dict. Additional arguments for LLM chat
    """
    if llm is not None:
        ''' If you specified your own LLM, then we just feed in the system and user prompt 
        LLM function should take in system prompt (str) and user prompt (str), and output a response (str) '''
        res = llm(system_prompt = system_prompt, user_prompt = user_prompt)
    
    ## This part here is for llms that are OpenAI based
    elif host == 'openai':
        # additional checks for openai json mode
        if 'response_format' in kwargs and kwargs['response_format'] == {"type": "json_object"}:
            # if model fails, default to gpt-3.5-turbo-1106
            try:
                assert(model in ['gpt-4-1106-preview', 'gpt-3.5-turbo-1106'])
            except Exception as e:
                model = 'gpt-3.5-turbo-1106'

        from openai import OpenAI
        client = OpenAI()
        response = client.chat.completions.create(
            model=model,
            temperature = temperature,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            **kwargs
        )
        res = response.choices[0].message.content

    if verbose:
        print('System prompt:', system_prompt)
        print('\nUser prompt:', user_prompt)
        print('\nGPT response:', res)
            
    return res


### Main Functions ###
def strict_json(system_prompt: str, user_prompt: str, output_format: dict, return_as_json = False, custom_checks: dict = None, check_data = None, delimiter: str = '###', num_tries: int = 3, openai_json_mode: bool = False, **kwargs):
    r""" Ensures that OpenAI will always adhere to the desired output JSON format defined in output_format.
    Uses rule-based iterative feedback to ask GPT to self-correct.
    Keeps trying up to num_tries it it does not. Returns empty JSON if unable to after num_tries iterations.
    
    Inputs (compulsory):
    - system_prompt: String. Write in whatever you want GPT to become. e.g. "You are a \<purpose in life\>"
    - user_prompt: String. The user input. Later, when we use it as a function, this is the function input
    - output_format: Dict. JSON format with the key as the output key, and the value as the output description
    
    Inputs (optional):
    - return_as_json: Bool. Default: False. Whether to return the output as a json. If False, returns as Python dict. If True, returns as json string
    - custom_checks: Dict. Key is output key, value is function which does checking of content for output field
    - check_data: Any data type. The additional data for custom_checks to use if required
    - delimiter: String (Default: '###'). This is the delimiter to surround the keys. With delimiter ###, key becomes ###key###
    - num_tries: Integer (default: 3). The number of tries to iteratively prompt GPT to generate correct json format
    - openai_json_mode: Boolean (default: False). Whether or not to use OpenAI JSON Mode
    - **kwargs: Dict. Additional arguments for LLM chat
    
    Output:
    - res: Dict. The JSON output of the model. Returns {} if JSON parsing failed.
    """
    # default initialise custom_checks to {}
    if custom_checks is None:
        custom_checks = {}
        
    # If OpenAI JSON mode is selected, then just let OpenAI do the processing
    if openai_json_mode:
        # add in code to warn user if type is defined for external function
        type_check = False
        for value in output_format.values():
            if 'type:' in str(value):
                type_check = True
        if type_check:
            print('Note: Type checking (type:) not done for OpenAI JSON Mode')
        
        output_format_prompt = "\nOutput in the following json string format: " + str(output_format) + "\nBe concise."
            
        my_system_prompt = str(system_prompt) + output_format_prompt
        my_user_prompt = str(user_prompt) 
            
        res = chat(my_system_prompt, my_user_prompt, response_format = {"type": "json_object"}, **kwargs)
        
        if return_as_json:
            return res
        else:
            try:
                loaded_json = json.loads(res)
            except Exception as e:
                loaded_json = {}
            return loaded_json
        
    # Otherwise, implement JSON parsing using Strict JSON
    else:
        # start off with no error message
        error_msg = ''

        # wrap the values with angle brackets and wrap keys with delimiter to encourage LLM to modify it
        new_output_format = wrap_with_angle_brackets(output_format, delimiter, 1)
        
        output_format_prompt = f'''\nOutput in the following json template: ```{new_output_format}```
Update values enclosed in <> and remove the <>. 
Your response must only be the updated json template beginning with {{ and ending with }}
Ensure the following output keys are present in the json: {' '.join(list(new_output_format.keys()))}'''

        for i in range(num_tries):
            my_system_prompt = str(system_prompt) + output_format_prompt + error_msg
            my_user_prompt = str(user_prompt) 

            # Use OpenAI to get a response
            res = chat(my_system_prompt, my_user_prompt, **kwargs)
            
            # extract only the chunk including the opening and closing braces
            # generate the { or } if LLM has forgotten to do so
            startindex = res.find('{')
            if startindex == -1:
                startindex = 0
                res = '{' + res
            endindex = res.rfind('}')
            if endindex == -1:
                res = res + '}'
                endindex = len(res) - 1
            
            res = res[startindex: endindex+1]

            # try-catch block to ensure output format is adhered to
            try:
                # check that res is a json string
                if res[0] != '{' or res[-1] != '}':
                    raise Exception('Ensure output must be a json string beginning with { and ending with }')
                
                # do checks for keys and output format, remove escape characters so code can be run
                end_dict = check_key(res, output_format, new_output_format, delimiter, delimiter_num = 1, **kwargs)
                
                # run user defined custom checks now
                for key in end_dict:
                    if key in custom_checks:
                        for check in custom_checks[key]:
                            requirement, requirement_met, action_needed = check(end_dict[key], check_data)
                            print(f'Running check for "{requirement}" on output field of "{key}"')
                            if not requirement_met:
                                print(f'Requirement not met. Action needed: "{action_needed}"\n\n')
                                raise Exception(f'Output field of "{key}" does not meet requirement "{requirement}". Action needed: "{action_needed}"')
                            else:
                                print('Requirement met\n\n')
                if return_as_json:
                    return json.dumps(end_dict, ensure_ascii=False)
                else:
                    return end_dict

            except Exception as e:
                error_msg = f"\n\nPrevious json: {res}\njson error: {str(e)}\nFix the error."                
                print("An exception occurred:", str(e))
                print("Current invalid json format:", res)

        return {}

### Legacy Support ###
# alternative names for strict_json
strict_text = strict_json
strict_output = strict_json


================================================
FILE: taskgen/base_async.py
================================================
import asyncio
import json
import re
import ast
from typing import Tuple
from taskgen.base import convert_to_dict, parse_response_llm_check, remove_unicode_escape, type_check_and_convert, wrap_with_angle_brackets

from taskgen.utils import ensure_awaitable

### Helper Functions ###


async def convert_to_list_async(field: str, **kwargs) -> list:
    '''Converts the string field into a list using the LLM (with **kwargs) to list out elements line by line'''
    
    system_msg = '''Output each element of the list in a new line starting with (%item) and ending with \n, e.g. ['hello', 'world'] -> (%item) hello\n(%item) world\nStart your response with (%item) and do not provide explanation'''
    user_msg = str(field)
    res = await chat_async(system_msg, user_msg, **kwargs)

    # Extract out list items
    field = re.findall(r'\(%item\)\s*(.*?)\n*(?=\(%item\)|$)', res, flags=re.DOTALL)
    return field



async def llm_check_async(field, llm_check_msg: str, **kwargs) -> Tuple[bool, str]:
    ''' Uses the LLM to check if the field adheres to the llm_check_msg.
    Outputs whether requirement is met (True or False) and the action needed'''
    system_msg = f'''Check whether output field meets this requirement: {llm_check_msg}
Output in the following format:
```
# Thoughts: <Thoughts about whether output field meets requirement>
# Requirement Met: <Yes or No>
# Action Needed: <If Requirement Met is No, state in one sentence how to meet requirement. Otherwise, output NA>"
```
Update text enclosed in <>. Be concise.
'''
    user_msg = str(field)
    res = await chat_async(system_msg, user_msg, **kwargs)
    requirement_met, action_needed = parse_response_llm_check(res)
    return requirement_met, action_needed


async def check_datatype_async(field, key: dict, data_type: str, **kwargs):
    ''' Ensures that output field of the key of JSON dictionary is of data_type 
    Currently supports int, float, str, code, enum, lists, nested lists, dict, dict with keys
    Takes in **kwargs for the LLM model
    Returns corrected output field that matches the datatype'''
    data_type = data_type.strip()
    
    # check if we want an LLM-based correction
    if data_type.lower()[:6] == 'ensure':
        llm_check_msg = data_type[6:].strip()
        print(f'Using LLM to check "{field}" to see if it adheres to "{llm_check_msg}"')
        requirement_met, action_needed = await llm_check_async(field, llm_check_msg, **kwargs)
        # if check failed, raise error
        if not requirement_met:
            raise Exception(f'''Output field of "{key}" does not meet requirement "{llm_check_msg}". Action needed: "{action_needed}"''')
            
    # check for list at beginning of datatype
    # or the output field begins with [ and ends with ] but it is not a list, indicating an error with ast.literal_eval
    if data_type.lower()[:4] == 'list' or data_type.lower()[:5] == 'array' or (str(field)[0]=='[' and str(field)[-1]==']'):
        # first try to see if we can do ast.literal_eval with { and }
        try:
            field = str(field)
            startindex = field.find('[')
            endindex = field.rfind(']')
            field = field[startindex: endindex+1]
            field = ast.literal_eval(field)
        except Exception as e:
            pass
        if not isinstance(field, list):
            # if it is already in a datatype that is a list, ask LLM to fix it (1 LLM call)
            if '[' in field and ']' in field:
                print(f'Attempting to use LLM to fix {field} as it is not a proper array')
                field = await convert_to_list_async(field, **kwargs)   
                print(f'Fixed list: {field}\n\n')
            else:
                raise Exception(f'''Output field of "{key}" not of data type array. If not possible to match, split output field into parts for elements of the array''')
            
    # check for nested list
    # Regex pattern to match content inside square brackets
    match = re.search(r"list\[(.*)\]", data_type, re.IGNORECASE)
    if match:
        internal_data_type = match.group(1)  # Extract the content inside the brackets
        # do processing for internal elements
        for num in range(len(field)):
            field[num] = await check_datatype_async(field[num], 'array element of '+key, internal_data_type, **kwargs)
            
    match = re.search(r"array\[(.*)\]", data_type, re.IGNORECASE)
    if match:
        internal_data_type = match.group(1)  # Extract the content inside the brackets
        # do processing for internal elements
        for num in range(len(field)):
            field[num] = await check_datatype_async(field[num], 'array element of '+key, internal_data_type, **kwargs)
            
    # if it is not nested, check individually
    else:
        field = type_check_and_convert(field, key, data_type, **kwargs)
    return field



    
    
    
async def check_key_async(field: str, output_format, new_output_format, delimiter: str, delimiter_num: int, **kwargs):
    ''' Check whether each key in dict, or elements in list of new_output_format is present in field, and whether they meet the right data type requirements, then convert field to the right data type
    If needed, calls LLM model with parameters **kwargs to correct the output format for improperly formatted list
    output_format is user-given output format at each level, new_output_format is with delimiters in keys, and angle brackets surrounding values
    If output_format is a string, decode escape characters, so that code can run
    Returns field that is converted to a dictionary if able to. Otherwise, raises Exception errors for missing keys or wrong output format'''
    
    cur_delimiter = delimiter*delimiter_num
    
    if isinstance(output_format, dict):   
        # this is the processed output dictionary for that particular layer in the output structure
        output_d = {}
        # check key appears for each element in the output
        output_d = convert_to_dict(field, output_format.keys(), cur_delimiter)
            
        # after creating dictionary, step into next layer
        for key, value in output_d.items():
            # # if the output is a bool type, convert true and false into True and False for ast.literal_eval parsing
            if isinstance(output_format[key], str) and 'type:' in output_format[key] and 'bool' in output_format[key].split('type:')[-1]:
                value = value.replace('true','True').replace('false','False')
            output_d[key] = await check_key_async(value, output_format[key], new_output_format[cur_delimiter+key+cur_delimiter], delimiter, delimiter_num+1)
            # after stepping back from the later layers back to present layer, check for types
            if isinstance(output_format[key], str) and 'type:' in output_format[key]:             
                # extract out data type
                data_type = str(output_format[key]).split('type:')[-1]
                # check the data type, perform type conversion as necessary
                output_d[key] = await check_datatype_async(output_d[key], key, data_type, **kwargs)   
                
        return output_d

    # if list, step into each element
    elif isinstance(output_format, list):
        try:
            field = ast.literal_eval(field)
        except Exception as e:
            # if there is an error in literal processing, use LLM to split field into list
            field = await convert_to_list_async(field, **kwargs)
            
        # check that list has at least same number of elements as the input
        if len(field) < len(output_format):
            raise Exception(f'''Output "{field}" has fewer elements than required by "{output_format}". Add in more list elements.''')
        
        coroutines = [check_key_async(str(field[num]), output_format[num], new_output_format[num], delimiter, delimiter_num+1) for num in range(len(output_format))]
        results = await asyncio.gather(*coroutines)
        return results
    
    # if string, then do literal eval to convert output field for further processing
    elif isinstance(output_format, str):
        # if literal eval fails, just leave it as string, no need to raise error
        try:
            field = ast.literal_eval(field)
        except Exception as e:
            pass
        return remove_unicode_escape(field)
    
    # otherwise just return the value
    else:
        return field
    


    



async def chat_async(system_prompt: str, user_prompt: str, model: str = 'gpt-4o-mini', temperature: float = 0, verbose: bool = False, host: str = 'openai', llm= None, **kwargs):
    r"""Performs a chat with the host's LLM model with system prompt, user prompt, model, verbose and kwargs
    Returns the output string res
    - system_prompt: String. Write in whatever you want the LLM to become. e.g. "You are a \<purpose in life\>"
    - user_prompt: String. The user input. Later, when we use it as a function, this is the function input
    - model: String. The LLM model to use for json generation
    - verbose: Boolean (default: False). Whether or not to print out the system prompt, user prompt, GPT response
    - host: String. The provider of the LLM
    - llm: User-made llm function.
        - Inputs:
            - system_prompt: String. Write in whatever you want the LLM to become. e.g. "You are a \<purpose in life\>"
            - user_prompt: String. The user input. Later, when we use it as a function, this is the function input
        - Output:
            - res: String. The response of the LLM call
    - **kwargs: Dict. Additional arguments for LLM chat
    """
    if llm is not None:
        ensure_awaitable(llm, 'llm')
        ''' If you specified your own LLM, then we just feed in the system and user prompt 
        LLM function should take in system prompt (str) and user prompt (str), and output a response (str) '''
        res = await llm(system_prompt = system_prompt, user_prompt = user_prompt)
    
    ## This part here is for llms that are OpenAI based
    elif host == 'openai':
        # additional checks for openai json mode
        if 'response_format' in kwargs and kwargs['response_format'] == {"type": "json_object"}:
            # if model fails, default to gpt-3.5-turbo-1106
            try:
                assert(model in ['gpt-4-1106-preview', 'gpt-3.5-turbo-1106'])
            except Exception as e:
                model = 'gpt-3.5-turbo-1106'

        from openai import AsyncOpenAI
        client = AsyncOpenAI()
        response = await client.chat.completions.create(
            model=model,
            temperature = temperature,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            **kwargs
        )
        res = response.choices[0].message.content

    if verbose:
        print('System prompt:', system_prompt)
        print('\nUser prompt:', user_prompt)
        print('\nGPT response:', res)
            
    return res



### Main Functions ###
    
    
async def strict_json_async(system_prompt: str, user_prompt: str, output_format: dict, return_as_json = False, custom_checks: dict = None, check_data = None, delimiter: str = '###', num_tries: int = 3, openai_json_mode: bool = False, **kwargs):
    r""" Ensures that OpenAI will always adhere to the desired output JSON format defined in output_format.
    Uses rule-based iterative feedback to ask GPT to self-correct.
    Keeps trying up to num_tries it it does not. Returns empty JSON if unable to after num_tries iterations.
    
    Inputs (compulsory):
    - system_prompt: String. Write in whatever you want GPT to become. e.g. "You are a \<purpose in life\>"
    - user_prompt: String. The user input. Later, when we use it as a function, this is the function input
    - output_format: Dict. JSON format with the key as the output key, and the value as the output description
    
    Inputs (optional):
    - return_as_json: Bool. Default: False. Whether to return the output as a json. If False, returns as Python dict. If True, returns as json string
    - custom_checks: Dict. Key is output key, value is function which does checking of content for output field
    - check_data: Any data type. The additional data for custom_checks to use if required
    - delimiter: String (Default: '###'). This is the delimiter to surround the keys. With delimiter ###, key becomes ###key###
    - num_tries: Integer (default: 3). The number of tries to iteratively prompt GPT to generate correct json format
    - openai_json_mode: Boolean (default: False). Whether or not to use OpenAI JSON Mode
    - **kwargs: Dict. Additional arguments for LLM chat
    
    Output:
    - res: Dict. The JSON output of the model. Returns {} if JSON parsing failed.
    """
    # default initialise custom_checks to {}
    if custom_checks is None:
        custom_checks = {}
        
    # If OpenAI JSON mode is selected, then just let OpenAI do the processing
    if openai_json_mode:
        # add in code to warn user if type is defined for external function
        type_check = False
        for value in output_format.values():
            if 'type:' in str(value):
                type_check = True
        if type_check:
            print('Note: Type checking (type:) not done for OpenAI JSON Mode')
        
        output_format_prompt = "\nOutput in the following json string format: " + str(output_format) + "\nBe concise."
            
        my_system_prompt = str(system_prompt) + output_format_prompt
        my_user_prompt = str(user_prompt) 
            
        res = await chat_async(my_system_prompt, my_user_prompt, response_format = {"type": "json_object"}, **kwargs)
        
        if return_as_json:
            return res
        else:
            try:
                loaded_json = json.loads(res)
            except Exception as e:
                loaded_json = {}
            return loaded_json
        
    # Otherwise, implement JSON parsing using Strict JSON
    else:
        # start off with no error message
        error_msg = ''

        # wrap the values with angle brackets and wrap keys with delimiter to encourage LLM to modify it
        new_output_format = wrap_with_angle_brackets(output_format, delimiter, 1)
        
        output_format_prompt = f'''\nOutput in the following json template: ```{new_output_format}```
Update values enclosed in <> and remove the <>. 
Your response must only be the updated json template beginning with {{ and ending with }}
Ensure the following output keys are present in the json: {' '.join(list(new_output_format.keys()))}'''

        for i in range(num_tries):
            my_system_prompt = str(system_prompt) + output_format_prompt + error_msg
            my_user_prompt = str(user_prompt) 

            # Use OpenAI to get a response
            res = await chat_async(my_system_prompt, my_user_prompt, **kwargs)
            
            # extract only the chunk including the opening and closing braces
            # generate the { or } if LLM has forgotten to do so
            startindex = res.find('{')
            if startindex == -1:
                startindex = 0
                res = '{' + res
            endindex = res.rfind('}')
            if endindex == -1:
                res = res + '}'
                endindex = len(res) - 1
                
            res = res[startindex: endindex+1]

            # try-catch block to ensure output format is adhered to
            try:
                # check that res is a json string
                if res[0] != '{' or res[-1] != '}':
                    raise Exception('Ensure output must be a json string beginning with { and ending with }')
                
                # do checks for keys and output format, remove escape characters so code can be run
                end_dict = await check_key_async(res, output_format, new_output_format, delimiter, delimiter_num = 1, **kwargs)
                
                # run user defined custom checks now
                for key in end_dict:
                    if key in custom_checks:
                        for check in custom_checks[key]:
                            requirement, requirement_met, action_needed = check(end_dict[key], check_data)
                            print(f'Running check for "{requirement}" on output field of "{key}"')
                            if not requirement_met:
                                print(f'Requirement not met. Action needed: "{action_needed}"\n\n')
                                raise Exception(f'Output field of "{key}" does not meet requirement "{requirement}". Action needed: "{action_needed}"')
                            else:
                                print('Requirement met\n\n')
                if return_as_json:
                    return json.dumps(end_dict, ensure_ascii=False)
                else:
                    return end_dict

            except Exception as e:
                error_msg = f"\n\nPrevious json: {res}\njson error: {str(e)}\nFix the error."                
                print("An exception occurred:", str(e))
                print("Current invalid json format:", res)

        return {}

### Legacy Support ###
# alternative names for strict_json
strict_text_async = strict_json_async
strict_output_async = strict_json_async


================================================
FILE: taskgen/function.py
================================================
import re
import inspect
from typing import get_type_hints

from taskgen.base import strict_json
from taskgen.base_async import strict_json_async

from taskgen.utils import ensure_awaitable, get_source_code_for_func

### Helper Functions ###

def get_clean_typename(typ) -> str:
    """Returns a clean, readable name for a type, including handling generics."""
    if hasattr(typ, '__origin__'):  # Check for generic types
        if typ.__origin__ is not None:  # Generic types, e.g., List, Dict
            base_name = typ.__origin__.__name__
            if hasattr(typ, '__args__') and typ.__args__ is not None:
                args = [get_clean_typename(arg) for arg in typ.__args__]
                return f"{base_name}[{', '.join(args)}]"
            else:
                return base_name  # Handle cases like `Dict` without specified parameters
        else:  # Non-generic but special types, e.g., typing.List without parameters
            return typ._name if hasattr(typ, '_name') else str(typ)
    elif hasattr(typ, '__name__'):
        return typ.__name__  # Simple types, e.g., int, str
    else:
        return str(typ)  # Fallback, should rarely be used

def get_fn_description(my_function):
    ''' Returns the modified docstring of my_function, that takes into account input variable names and types in angle brackets
    Also returns the list of input parameters to the function in sequence
    e.g.: Adds numbers x and y -> Adds numbers <x: int> and <y: int>
    Input variables that are optional (already assigned a default value) need not be in the docstring
    args and kwargs variables are not parsed '''
     
    if not inspect.isfunction(my_function):
        raise Exception(f'{my_function} is not a Python function')
        
    # Get the signature and type hints of the function
    # if my_function.__doc__ == None:
    #     return '', []

    signature = inspect.signature(my_function)
    full_type_hints = get_type_hints(my_function)
    my_fn_description = my_function.__doc__ if my_function.__doc__ else ''

    param_list = []
    # Access parameters and their types
    parameters = signature.parameters
    for param_name, param in parameters.items():
        # skip args and kwargs and shared variables
        if param_name in ['shared_variables', 'args', 'kwargs']:
            continue
        
        param_type = param.annotation.__name__ if param.annotation != inspect.Parameter.empty else "unannotated"
        # Handle specific typing
        if param_name in full_type_hints:
            param_type = get_clean_typename(full_type_hints[param_name])

        # Create new_param representation
        new_param = f'<{param_name}: {param_type}>' if param_type != "unannotated" else f'<{param_name}>'

        # Pattern to find the parameter in the docstring
        pattern = re.compile(fr'\b({param_name})\b')
        
        # Substitute the parameter in the docstring
        if pattern.search(my_fn_description):
            my_fn_description = pattern.sub(new_param, my_fn_description)
            param_list.append(param_name)
            
        # otherwise, function description will just be the function signature
        else:
            # add a continuation if description is current empty
            if my_fn_description != '':
                my_fn_description += ', '
                
            param_list.append(param_name)
            print(f'Input variable "{param_name}" not in docstring of "{my_function.__name__}". Adding it to docstring')
            my_fn_description += f'Input: {new_param}'
            
            if param.default != inspect.Parameter.empty:
                my_fn_description += f', default: {param.default}'

    return my_fn_description, param_list

def get_fn_output(my_function) -> dict:
    ''' Returns the dictionary of output parameters and types of the form {"Output 1": "Type", "Output 2": "Type"}'''
     
    if not inspect.isfunction(my_function):
        raise Exception(f'{my_function} is not a Python function')
        
    # Initialize the output format dictionary
    output_format = {}

    full_type_hints = get_type_hints(my_function)
    my_fn_description = my_function.__doc__

    # Check for return annotation
    if 'return' in full_type_hints:
        return_type = full_type_hints['return']
        # Adjust dictionary according to the return type
        if isinstance(return_type, tuple):
            for idx, type_hint in enumerate(return_type):
                output_format[f"output_{idx + 1}"] = get_clean_typename(type_hint)
        else:
            output_format["output_1"] = get_clean_typename(return_type)

    return output_format

### Main Class ###

class BaseFunction:
    def __init__(self,
                 fn_description: str = '', 
                 output_format: dict = None,
                 examples = None,
                 external_fn = None,
                 is_compulsory = False,
                 fn_name = None,
                 llm = None,
                 **kwargs):
        ''' 
        Creates an LLM-based function or wraps an external function using fn_description and outputs JSON based on output_format. 
        (Optional) Can define the function based on examples (list of Dict containing input and output variables for each example)
        (Optional) If you would like greater specificity in your function's input, you can describe the variable after the : in the input variable name, e.g. `<var1: an integer from 10 to 30`. Here, `var1` is the input variable and `an integer from 10 to 30` is the description.
        
        Inputs (primary):
        - fn_description: String. Function description to describe process of transforming input variables to output variables. Variables must be enclosed in <> and listed in order of appearance in function input.
Can also be done automatically by providing docstring with input variable names in external_fn
        - output_format: Dict. Dictionary containing output variables names and description for each variable.
           
        Inputs (optional):
        - examples: Dict or List[Dict]. Examples in Dictionary form with the input and output variables (list if more than one)
        - external_fn: Python Function. If defined, instead of using LLM to process the function, we will run the external function. 
            If there are multiple outputs of this function, we will map it to the keys of `output_format` in a one-to-one fashion
        - is_compulsory: Bool. Default: False. This is whether to always use the Function when doing planning in Agents
        - fn_name: String. If provided, this will be the name of the function. Otherwise, if `external_fn` is provided, it will be the name of `external_fn`. Otherwise, we will use LLM to generate a function name from the `fn_description`
        - llm: Function. The llm parameter to pass into strict_json
        - **kwargs: Dict. Additional arguments you would like to pass on to the strict_json function (such as llm)
        
        ## Example
        fn_description = 'Output the sum of <num1> and <num2>'
        output_format = {'output': 'sum of two numbers'}
        examples = [{'num1': 5, 'num2': 6, 'output': 11}, {'num1': 2, 'num2': 4, 'output': 6}]
        '''
        
        self.fn_description = ''
        self.output_format = {}
        
        # this is only for external functions
        self.external_param_list = [] 
        if external_fn is not None:
            # add in code to warn user if type is defined for external function
            type_check = False
            if output_format is not None:
                for value in output_format.values():
                    if 'type:' in str(value):
                        type_check = True
                if type_check:
                    print('Note: Type checking (type:) not done for External Functions')
            
            # get details from docstring of external function only if fn_description is not given
            if fn_description == '':
                self.fn_description, self.external_param_list = get_fn_description(external_fn)
            
            # get the output format from the function signature if output format is not given
            if output_format is None:
                self.output_format = get_fn_output(external_fn)             
            
        # if function description provided, use it to update the function description
        if fn_description != '':
            self.fn_description = fn_description

        # if output format is provided, use it to update the function output format
        if output_format is not None:
            self.output_format = output_format
            
        self.examples = examples
        self.external_fn = external_fn
        self.is_compulsory = is_compulsory
        self.fn_name = fn_name
        self.llm = llm
        self.kwargs = kwargs
        
        self.variable_names = []
        self.shared_variable_names = []
        # use regex to extract variables from function description
        matches = re.findall(r'<(.*?)>', self.fn_description)
            
        for match in matches:
            first_half = match.split(':')[0]
            if first_half not in self.variable_names:
                # if the first two characters of variable are s_, means take from shared_variables
                if first_half[:2] != 's_':
                    self.variable_names.append(first_half)
                # otherwise we take from shared_variables
                else:
                    self.shared_variable_names.append(first_half)
                    # replace the original variable without the <> so as not to confuse the LLM
                    self.fn_description = self.fn_description.replace(f'<{match}>', first_half)
                    
        # make it such that we follow the same order for variable names as per the external function only if there are external function params
        if self.external_param_list != []:
            self.variable_names = [x for x in self.external_param_list if x in self.variable_names]
             

        # Append examples to description
        if self.examples is not None:
            self.fn_description += '\nExamples:\n' + str(examples) 
            
    def __str__(self):
        ''' Prints out the function's parameters '''
        return f'Description: {self.fn_description}\nInput: {self.variable_names}\nOutput: {self.output_format}\n'
    
    def get_python_representation(self):
        """Returns a Python representation of the Function object, including the external function code if available."""
        external_fn_code = None
        external_fn_ref = None

        if self.external_fn:
            if inspect.isfunction(self.external_fn) and self.external_fn.__name__ == "<lambda>":
                external_fn_ref = get_source_code_for_func(self.external_fn)
            else:
                external_fn_ref = self.external_fn.__name__
                external_fn_code = get_source_code_for_func(self.external_fn)

        fn_initialization = f"""Function(
            fn_name="{self.fn_name}",
            fn_description='''{self.fn_description}''',
            output_format={self.output_format},
            examples={self.examples},
            external_fn={external_fn_ref},
            is_compulsory={self.is_compulsory})
        """
        return (fn_initialization, external_fn_code)
    
    
    def _prepare_function_kwargs(self, *args, **kwargs):
         # get the shared_variables if there are any
        shared_variables = kwargs.get('shared_variables', {})
        # remove the mention of shared_variables in kwargs
        if 'shared_variables' in kwargs:
            del kwargs['shared_variables']
        # extract out only variables listed in variable_list from kwargs
        function_kwargs = {key: value for key, value in kwargs.items() if key in self.variable_names}
        # additionally, if function references something in shared_variables, add that in
        for variable in self.shared_variable_names:
            if variable in shared_variables:
                function_kwargs[variable] = shared_variables[variable]
        # Do the auto-naming of variables as var1, var2, or as variable names defined in variable_names
        for num, arg in enumerate(args):
            if len(self.variable_names) > num:
                function_kwargs[self.variable_names[num]] = arg
            else:
                function_kwargs[f'var{num+1}'] = arg
                
        return function_kwargs, shared_variables

    def _prepare_strict_json_kwargs(self, **kwargs):
        return {key: value for key, value in kwargs.items() if key not in self.variable_names}

    def _update_shared_variables(self, results, shared_variables):
        keys_to_delete = []
        for key in results:
            if key.startswith('s_'):
                shared_variables[key] = results[key]
                keys_to_delete.append(key)

        for key in keys_to_delete:
            del results[key]

    def _generate_function_name(self):
        if self.fn_name is None:
            if self.external_fn is not None and hasattr(self.external_fn, '__name__') and self.external_fn.__name__ != '<lambda>':
                self.fn_name = self.external_fn.__name__
            else:
                self.fn_name = 'generated_function_name'  # Replace with actual function name generation logic.
            self.__name__ = self.fn_name
    
    
            
            
class Function(BaseFunction):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if self.fn_name is None:
            # if external function has a name, use it
            if self.external_fn is not None and hasattr(self.external_fn, '__name__') and self.external_fn.__name__ != '<lambda>':
                self.fn_name = self.external_fn.__name__
            # otherwise, generate name out
            else:
                res = strict_json(system_prompt = "Output a function name to summarise the usage of this function.",
                                  user_prompt = str(self.fn_description),
                                  output_format = {"Thoughts": "What function does", "Name": "Function name with _ separating words that summarises what function does"},
                                 llm = self.llm,
                                 **self.kwargs)
                self.fn_name = res['Name']
         # change instance's name to function's name
        self.__name__ = self.fn_name
        
    
    def __call__(self, *args, **kwargs):
        ''' Describes the function, and inputs the relevant parameters as either unnamed variables (args) or named variables (kwargs)
        
        Inputs:
        - shared_varables: Dict. Default: empty dict. The variables which will be shared between functions. Only passed in if required by function 
        - *args: Tuple. Unnamed input variables of the function. Will be processed to var1, var2 and so on based on order in the tuple
        - **kwargs: Dict. Named input variables of the function. Can also be variables to pass into strict_json
        
        Output:
        - res: Dict. JSON containing the output variables'''
        
        # get the shared_variables if there are any
        function_kwargs, shared_variables = self._prepare_function_kwargs(*args, **kwargs)

        # extract out only variables not listed in variable list
        strict_json_kwargs = {
            my_key: kwargs[my_key] for my_key in kwargs 
            if my_key not in self.variable_names and my_key != 'shared_variables'
        }
                
        # If strict_json function, do the function. 
        if self.external_fn is None:
            res = strict_json(system_prompt = self.fn_description,
                            user_prompt = function_kwargs,
                            output_format = self.output_format,
                            llm = self.llm,
                            **self.kwargs, **strict_json_kwargs)
            
        # Else run the external function
        else:
            res = {}
            # if external function uses shared_variables, pass it in
            argspec = inspect.getfullargspec(self.external_fn)
            if 'shared_variables' in argspec.args:
                fn_output = self.external_fn(shared_variables = shared_variables, **function_kwargs)
            else:
                fn_output = self.external_fn(**function_kwargs)
                
            # if there is nothing in fn_output, skip this part
            if fn_output is not None:
                output_keys = list(self.output_format.keys())
                # convert the external function into a tuple format to parse it through the JSON dictionary output format
                if not isinstance(fn_output, tuple):
                    fn_output = [fn_output]

                for i in range(len(fn_output)):
                    if len(output_keys) > i:
                        res[output_keys[i]] = fn_output[i]
                    else:
                        res[f'output_{i+1}'] = fn_output[i]
        
        # check if any of the output variables have a s_, which means we update the shared_variables and not output it
        self._update_shared_variables(res, shared_variables)
                
        if res == {}:
            res = {'Status': 'Completed'}

        return res
        
        
            
class AsyncFunction(BaseFunction):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        ensure_awaitable(self.llm, 'llm')
        if self.fn_name is None:
            # if external function has a name, use it
            if self.external_fn is not None and hasattr(self.external_fn, '__name__') and self.external_fn.__name__ != '<lambda>':
                self.fn_name = self.external_fn.__name__
        self.__name__ = self.fn_name
        
    async def async_init(self): 
        ''' This generates the name for the function using strict_json_async '''
        if self.fn_name is None:
            res = await strict_json_async(system_prompt = "Output a function name to summarise the usage of this function.",
                              user_prompt = str(self.fn_description),
                              output_format = {"Thoughts": "What function does", "Name": "Function name with _ separating words that summarises what function does"},
                             llm = self.llm,
                             **self.kwargs)
            self.fn_name = res['Name']

            # change instance's name to function's name
            self.__name__ = self.fn_name
        
    async def __call__(self, *args, **kwargs):
        ''' Describes the function, and inputs the relevant parameters as either unnamed variables (args) or named variables (kwargs)
        
        Inputs:
        - shared_varables: Dict. Default: empty dict. The variables which will be shared between functions. Only passed in if required by function 
        - *args: Tuple. Unnamed input variables of the function. Will be processed to var1, var2 and so on based on order in the tuple
        - **kwargs: Dict. Named input variables of the function. Can also be variables to pass into strict_json
        
        Output:
        - res: Dict. JSON containing the output variables'''
        
        # get the shared_variables if there are any
      
        if self.fn_name is None:
            await self.async_init()
        
        function_kwargs, shared_variables = self._prepare_function_kwargs(*args, **kwargs)

        # extract out only variables not listed in variable list
        strict_json_kwargs = {
                    my_key: kwargs[my_key] for my_key in kwargs 
                    if my_key not in self.variable_names and my_key != 'shared_variables'
                }
               
                
        # If strict_json function, do the function. 
        if self.external_fn is None:
            res = await strict_json_async(system_prompt = self.fn_description,
                            user_prompt = function_kwargs,
                            output_format = self.output_format,
                            llm = self.llm,
                            **self.kwargs, **strict_json_kwargs)
            
        # Else run the external function
        else:
            res = {}
            # if external function uses shared_variables, pass it in
            argspec = inspect.getfullargspec(self.external_fn)
            if 'shared_variables' in argspec.args:
                if  inspect.iscoroutinefunction(self.external_fn):
                    fn_output = await self.external_fn(shared_variables = shared_variables, **function_kwargs)
                else: 
                    fn_output = self.external_fn(shared_variables = shared_variables, **function_kwargs)
            else:
                if  inspect.iscoroutinefunction(self.external_fn):
                    fn_output = await self.external_fn(**function_kwargs)
                else:
                    fn_output = self.external_fn(**function_kwargs)
                
            # if there is nothing in fn_output, skip this part
            if fn_output is not None:
                output_keys = list(self.output_format.keys())
                # convert the external function into a tuple format to parse it through the JSON dictionary output format
                if not isinstance(fn_output, tuple):
                    fn_output = [fn_output]

                for i in range(len(fn_output)):
                    if len(output_keys) > i:
                        res[output_keys[i]] = fn_output[i]
                    else:
                        res[f'output_{i+1}'] = fn_output[i]
        
         
        # check if any of the output variables have a s_, which means we update the shared_variables and not output it
        self._update_shared_variables(res, shared_variables)
                
        if res == {}:
            res = {'Status': 'Completed'}

        return res


    
# alternative name for strict_function (it is now called Function)
strict_function = Function


================================================
FILE: taskgen/memory.py
================================================
from ast import List
import asyncio
import hashlib
import os
import time
from typing import Any
import pypdf
from docx import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
import chromadb
from chromadb.api.async_client import AsyncClient
from chromadb.api.client import Client
from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction
import copy

import pandas as pd
from abc import ABC, abstractmethod

from taskgen.base import strict_json
from taskgen.base_async import strict_json_async

from taskgen.ranker import AsyncRanker, Ranker
from taskgen.utils import ensure_awaitable, get_source_code_for_func, top_k_index

###################
## Base Template ##
###################


class MemoryTemplate(ABC):
    """A generic template provided for all memories"""

    @abstractmethod
    def append(self, memory_list, mapper=None):
        """Appends multiple new memories"""
        pass

    # TODO Should this be deleted based on metadata key - value filter
    @abstractmethod
    def remove(self, existing_memory):
        """Removes an existing_memory. existing_memory can be str, or triplet if it is a Knowledge Graph"""
        pass

    @abstractmethod
    def reset(self):
        """Clears all memories"""

    @abstractmethod
    def retrieve(self, task: str):
        """Retrieves some memories according to task"""
        pass

    ## Some utility functions
    def read_file(self, filepath, text_splitter=None):
        if ".xls" in filepath:
            text = pd.read_excel(filepath).to_string()
        elif ".csv" in filepath:
            text = pd.read_csv(filepath).to_string()
        elif ".docx" in filepath:
            text = self.read_docx(filepath)
        elif ".pdf" in filepath:
            text = self.read_pdf(filepath)
        else:
            raise ValueError(
                "File type not spported, supported file types: pdf, docx, csv, xls"
            )

        if not text_splitter:
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=100,
                length_function=len,
                is_separator_regex=False,
                separators=[".\n", "\n"],
            )

        texts = text_splitter.split_text(text)
        memories = [{"content": text, "filepath": filepath} for text in texts]
        return memories

    def read_pdf(self, filepath):
        # Open the PDF file
        text_list = []
        with open(filepath, "rb") as file:
            pdf_reader = pypdf.PdfReader(file)
            for page in pdf_reader.pages:
                page_text = page.extract_text()
                if page_text:  # Ensure there's text on the page
                    text_list.append(page_text)
                else:
                    print("No text found on page")
        return "\n".join(text_list)

    def read_docx(self, filepath):
        doc = Document(filepath)
        text_list = []
        for para in doc.paragraphs:
            text_list.append(para.text)
        return "\n".join(text_list)


########################
## In-house vector db ##
########################

### BaseMemory is VectorDB that is natively implemented, but not optimised. This will be used for function-based RAG and other kinds of RAG that are not natively text-based. For more optimised memory, check out ChromaDbMemory


class BaseMemory(MemoryTemplate):
    """Retrieves top k memory items based on task. This is an in-house, unoptimised, vector db
    - Inputs:
        - `memory`: List. Default: None. The list containing the memory items
        - `top_k`: Int. Default: 3. The number of memory list items to retrieve
        - `mapper`: Function. Maps the memory item to another form for comparison by ranker or LLM. Default: `lambda x: x`
            - Example mapping: `lambda x: x.fn_description` (If x is a Class and the string you want to compare for similarity is the fn_description attribute of that class)
        - `approach`: str. Either `retrieve_by_ranker` or `retrieve_by_llm` to retrieve memory items
            - Ranker is faster and cheaper as it compares via embeddings, but are inferior to LLM-based methods for contextual information
        - `llm`: Function. The llm to use for `strict_json` llm retriever
        - `retrieve_fn`: Default: None. Takes in task and outputs top_k similar memories in a list. Does away with the Ranker() altogether
        - `ranker`: `Ranker`. The Ranker which defines a similarity score between a query and a key. Default: OpenAI `text-embedding-3-small` model.
            - Can be replaced with a function which returns similarity score from 0 to 1 when given a query and key
    """

    def __init__(
        self,
        memory: list = None,
        top_k: int = 3,
        mapper=lambda x: x,
        approach="retrieve_by_ranker",
        llm=None,
        retrieve_fn=None,
        ranker=None,
    ):
        if memory is None:
            self.memory = []
        else:
            self.memory = memory
        self.top_k = top_k
        self.mapper = mapper
        self.approach = approach
        self.ranker = ranker
        self.retrieve_fn = retrieve_fn
        self.llm = llm

    def add_file(self, filepath, text_splitter=None):
        memories = self.read_file(filepath, text_splitter)
        texts = [memory["content"] for memory in memories]
        self.append(texts)

    def append(self, memory_list, mapper=None):
        """Adds a list of memories"""
        if not isinstance(memory_list, list):
            memory_list = [memory_list]
        self.memory.extend(memory_list)

    def remove(self, memory_to_remove):
        """Removes a memory"""
        self.memory.remove(memory_to_remove)

    def reset(self):
        """Clears all memory"""
        self.memory = []

    def isempty(self) -> bool:
        """Returns whether or not the memory is empty"""
        return not self.memory

    def get_python_representation(self, include_memory_elements) -> str:
        """Returns a string representation of the object for debugging"""
        return f"Memory(memory={self.memory if include_memory_elements else []}, top_k={self.top_k}, mapper={get_source_code_for_func(self.mapper)}, approach='{self.approach}', ranker={self.ranker.get_python_representation() if hasattr(self.ranker, 'get_python_representation') else 'None'})"


class Memory(BaseMemory):
    """Retrieves top k memory items based on task
    - Inputs:
        - `memory`: List. Default: Empty List. The list containing the memory items
        - `top_k`: Int. Default: 3. The number of memory list items to retrieve
        - `mapper`: Function. Maps the memory item to another form for comparison by ranker or LLM. Default: `lambda x: x`
            - Example mapping: `lambda x: x.fn_description` (If x is a Class and the string you want to compare for similarity is the fn_description attribute of that class)
        - `approach`: str. Either `retrieve_by_ranker` or `retrieve_by_llm` to retrieve memory items
            - Ranker is faster and cheaper as it compares via embeddings, but are inferior to LLM-based methods for contextual information
        - `llm`: Function. The llm to use for `strict_json` llm retriever
        - `retrieve_fn`: Default: None. Takes in task and outputs top_k similar memories in a list
        - `ranker`: `Ranker`. The Ranker which defines a similarity score between a query and a key. Default: OpenAI `text-embedding-3-small` model.
            - Can be replaced with a function which returns similarity score from 0 to 1 when given a query and key
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if self.ranker is None:
            self.ranker = (
                Ranker()
            )  # Assuming Ranker needs to be initialized if not provided

    def retrieve(self, task: str) -> list:
        """Performs retrieval of top_k similar memories according to approach stated"""
        # if you have your own vector search function, implement it in retrieve_fn. Takes in a task and outputs the top-k results
        if self.retrieve_fn is not None:
            return self.retrieve_fn(task)
        else:
            if self.approach == "retrieve_by_ranker":
                return self.retrieve_by_ranker(task)
            else:
                return self.retrieve_by_llm(task)

    def retrieve_by_ranker(self, task: str) -> list:
        """Performs retrieval of top_k similar memories
        Returns the memory list items corresponding to top_k matches"""
        # if there is no need to filter because top_k is already more or equal to memory size, just return memory
        if self.top_k >= len(self.memory):
            return copy.deepcopy(self.memory)

        # otherwise, perform filtering
        else:
            memory_score = [
                self.ranker(self.mapper(memory_chunk), task)
                for memory_chunk in self.memory
            ]
            top_k_indices = top_k_index(memory_score, self.top_k)
            return [self.memory[index] for index in top_k_indices]

    def retrieve_by_llm(self, task: str) -> list:
        """Performs retrieval via LLMs
        Returns the key list as well as the value list"""
        res = strict_json(
            f'You are to output the top {self.top_k} most similar list items in Memory relevant to this: ```{task}```\nMemory: {[f"{i}. {self.mapper(mem)}" for i, mem in enumerate(self.memory)]}',
            "",
            output_format={
                f"top_{self.top_k}_list": f"Indices of top {self.top_k} most similar list items in Memory, type: list[int]"
            },
            llm=self.llm,
        )
        top_k_indices = res[f"top_{self.top_k}_list"]
        return [self.memory[index] for index in top_k_indices]


class AsyncMemory(BaseMemory):
    """Retrieves top k memory items based on task
    - Inputs:
        - `memory`: List. Default: Empty List. The list containing the memory items
        - `top_k`: Int. Default: 3. The number of memory list items to retrieve
        - `mapper`: Function. Maps the memory item to another form for comparison by ranker or LLM. Default: `lambda x: x`
            - Example mapping: `lambda x: x.fn_description` (If x is a Class and the string you want to compare for similarity is the fn_description attribute of that class)
        - `approach`: str. Either `retrieve_by_ranker` or `retrieve_by_llm` to retrieve memory items
            - Ranker is faster and cheaper as it compares via embeddings, but are inferior to LLM-based methods for contextual information
        - `llm`: Function. The llm to use for `strict_json` llm retriever
        - `retrieve_fn`: Default: None. Takes in task and outputs top_k similar memories in a list
        - `ranker`: `Ranker`. The Ranker which defines a similarity score between a query and a key. Default: OpenAI `text-embedding-3-small` model.
            - Can be replaced with a function which returns similarity score from 0 to 1 when given a query and key
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        if self.ranker is None:
            self.ranker = (
                AsyncRanker()
            )  # Assuming Ranker needs to be initialized if not provided
        if not isinstance(self.ranker, AsyncRanker):
            raise Exception("Sync Ranker not allowed in AsyncMemory")
        ensure_awaitable(self.retrieve_fn, "retrieve_fn")
        ensure_awaitable(self.llm, "llm")

    async def retrieve(self, task: str) -> list:
        """Performs retrieval of top_k similar memories according to approach stated"""
        # if you have your own vector search function, implement it in retrieve_fn. Takes in a task and outputs the top-k results
        if self.retrieve_fn is not None:
            return await self.retrieve_fn(task)
        else:
            if self.approach == "retrieve_by_ranker":
                return await self.retrieve_by_ranker(task)
            else:
                return await self.retrieve_by_llm(task)

    async def retrieve_by_ranker(self, task: str) -> list:
        """Performs retrieval of top_k similar memories
        Returns the memory list items corresponding to top_k matches"""
        # if there is no need to filter because top_k is already more or equal to memory size, just return memory
        if self.top_k >= len(self.memory):
            return copy.copy(self.memory)

        # otherwise, perform filtering
        else:
            tasks = [
                self.ranker(self.mapper(memory_chunk), task)
                for memory_chunk in self.memory
            ]
            memory_score = await asyncio.gather(*tasks)
            top_k_indices = top_k_index(memory_score, self.top_k)
            return [self.memory[index] for index in top_k_indices]

    async def retrieve_by_llm(self, task: str) -> list:
        """Performs retrieval via LLMs
        Returns the key list as well as the value list"""
        res = await strict_json_async(
            f'You are to output the top {self.top_k} most similar list items in Memory relevant to this: ```{task}```\nMemory: {[f"{i}. {self.mapper(mem)}" for i, mem in enumerate(self.memory)]}',
            "",
            output_format={
                f"top_{self.top_k}_list": f"Indices of top {self.top_k} most similar list items in Memory, type: list[int]"
            },
            llm=self.llm,
        )
        top_k_indices = res[f"top_{self.top_k}_list"]
        return [self.memory[index] for index in top_k_indices]


######################
## Chroma vector db ##
######################

## TODO: Make this non-OpenAI dependent


class BaseChromaDbMemory(MemoryTemplate):
    """Takes in the following parameters:
    `collection_name`: str. Compulsory. Name of the memory. Need to provide a unique name so that we can disambiguate between collections
    `client` - Default: None. ChromaDB client to use, if any
    `embedding_model`: Name of OpenAI's embedding_model to use with ChromaDB. Default OpenAI "text-embedding-3-small"
    `top_k`: Number of elements to retrieve. Default: 3
    `mapper`: Function. Maps the memory value to the embedded value. We do not need to embed the whole value, so this will serve as a way to tell us what to embed. Default: lambda x: x
    `pre_delete`: Bool. Default: False. If set to True, delete collection with all data inside it when initialising
    """

    def __init__(
        self,
        collection_name,
        client=None,
        embedding_model="text-embedding-3-small",
        top_k=3,
        mapper=lambda x: x,
        pre_delete=False,
    ):
        # Evaluate async client for chroma db for storage
        self.client = client or chromadb.PersistentClient()
        self.embedding_model = embedding_model
        self.top_k = top_k
        self.embedding_function = OpenAIEmbeddingFunction(
            api_key=os.environ.get("OPENAI_API_KEY"), model_name=self.embedding_model
        )
        self.salt = os.urandom(16).hex()
        self.collection_name = collection_name
        self.mapper = mapper
        self.collection = None
        if pre_delete:
            self.reset()
        if isinstance(self.client, Client):
            self.collection = self.client.get_or_create_collection(
                self.collection_name, embedding_function=self.embedding_function
            )

    @abstractmethod
    def get_openai_client(self):
        pass

    @abstractmethod
    def create_embedding(self, text):
        pass

    @abstractmethod
    def get_or_create_collection(self):
        pass

    def remove(self, memories: list[str]):
        if not isinstance(memories, list):
            memories = [memories]
        max_count = self.collection.count()
        for memory in memories:
            # retrieve up to top_k memories to remove
            retrieved_ = self.collection.query(
                query_texts=[memory], n_results=min(self.top_k, max_count)
            )
            removed = False
            for document, retrieved_id in zip(
                retrieved_["documents"][0], retrieved_["ids"][0]
            ):
                # only remove on exact match
                if document == memory:
                    self.collection.delete([retrieved_id])
                    print(f"Removing memory: {memory}")
                    removed = True
            if not removed:
                print(f"No memory to remove: {memory}")

    def remove_by_id(self, ids):
        if not isinstance(ids, list):
            ids = [ids]
        return self.collection.delete(ids)

    def generate_id(self, embedding):
        # Generate a unique ID based on the embedding with added salt
        current_time = str(time.time()).encode()
        salted_embedding = str(embedding).encode() + self.salt.encode() + current_time
        return hashlib.sha256(salted_embedding).hexdigest()

    def reset(self):
        return self.client.delete_collection(name=self.collection.name)

    def retrieve(self, task: str, filter=[]):
        max_count = self.collection.count()
        results = self.collection.query(
            query_texts=[task], n_results=min(self.top_k, max_count), where=filter
        )["metadatas"][0]

        return [
            mem_item["taskgen_content"] if "taskgen_content" in mem_item else mem_item
            for mem_item in results
        ]
        # return self.collection.query(
        #     query_texts=[task], n_results=self.top_k, where=filter
        # )


class ChromaDbMemory(BaseChromaDbMemory):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.openai_client = self.get_openai_client()
        self.collection = self.get_or_create_collection()

    def get_openai_client(self):
        from openai import OpenAI

        return OpenAI()

    def get_or_create_collection(self):
        return self.client.get_or_create_collection(
            self.collection_name, embedding_function=self.embedding_function
        )

    def add_file(self, filepath, text_splitter=None):
        new_memories = self.read_file(filepath, text_splitter)
        return self.append(new_memories, mapper=lambda x: x["content"])

    def create_embedding(self, text):
        return (
            self.openai_client.embeddings.create(
                input=[text], model=self.embedding_model
            )
            .data[0]
            .embedding
        )

    def append(self, new_memories, mapper=None):
        if not isinstance(new_memories, list):
            new_memories = [new_memories]

        if mapper:
            memory_strings = [mapper(memory) for memory in new_memories]
        else:
            memory_strings = [self.mapper(memory) for memory in new_memories]

        if all(isinstance(item, dict) for item in new_memories):
            metadatas = new_memories
        else:
            metadatas = [{"taskgen_content": item} for item in new_memories]

        return self.append_memory_list(memory_strings, metadatas)

    def append_memory_list(self, new_memories: list[str], metadatas: list[dict] = None):
        embeddings = [self.create_embedding(text) for text in new_memories]
        if metadatas is None:
            metadatas = [{} for _ in new_memories]
        ids = [
            metadata.get("id") or self.generate_id(embedding)
            for metadata, embedding in zip(metadatas, embeddings)
        ]
        return self.collection.upsert(
            ids=ids,
            embeddings=embeddings,
            documents=new_memories,
            metadatas=metadatas,
        )


class AsyncChromaDbMemory(BaseChromaDbMemory):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.openai_client = self.get_openai_client()

    def get_openai_client(self):
        from openai import AsyncOpenAI

        return AsyncOpenAI()

    async def get_or_create_collection(self):
        if self.collection:
            return self.collection
        if isinstance(self.client, Client):
            return self.client.get_or_create_collection(
                self.collection_name, embedding_function=self.embedding_function
            )
        elif isinstance(self.client, AsyncClient):
            return await self.client.get_or_create_collection(
                self.collection_name, embedding_function=self.embedding_function
            )

    async def add_file(self, filepath, text_splitter=None):
        new_memories = self.read_file(filepath, text_splitter)
        return await self.append(new_memories, mapper=lambda x: x["content"])

    async def create_embedding(self, text):
        embedding = await self.openai_client.embeddings.create(
            input=[text], model=self.embedding_model
        )
        return embedding.data[0].embedding

    async def append(self, new_memories=[], mapper=None):
        if not isinstance(new_memories, list):
            new_memories = [new_memories]

        if mapper:
            memory_strings = [mapper(memory) for memory in new_memories]
        else:
            memory_strings = [self.mapper(memory) for memory in new_memories]

        if all(isinstance(item, dict) for item in new_memories):
            metadatas = new_memories
        else:
            metadatas = [{"taskgen_content": item} for item in new_memories]

        return await self.append_memory_list(memory_strings, metadatas)

    async def append_memory_list(
        self, new_memories: list[str], metadatas: list[dict] = None
    ):
        self.collection = await self.get_or_create_collection()
        embeddings = await asyncio.gather(
            *[self.create_embedding(text) for text in new_memories]
        )
        if metadatas is None:
            metadatas = [{} for _ in new_memories]
        ids = [
            metadata.get("id") or self.generate_id(embedding)
            for metadata, embedding in zip(metadatas, embeddings)
        ]
        if isinstance(self.client, AsyncClient):
            return await self.collection.upsert(
                ids=ids,
                embeddings=embeddings,
                documents=new_memories,
                metadatas=metadatas,
            )
        elif isinstance(self.client, Client):
            return self.collection.upsert(
                ids=ids,
                embeddings=embeddings,
                documents=new_memories,
                metadatas=metadatas,
            )
        else:
            raise Exception("Unknown client type")

    async def remove(self, memories: list[str]):
        self.collection = await self.get_or_create_collection()
        if isinstance(self.client, AsyncClient):
            if not isinstance(memories, list):
                memories = [memories]
            max_count = self.collection.count()
            n_results_num = min(self.top_k, max_count)
            for memory in memories:
                # retrieve up to top_k memories to remove
                retrieved_ = await self.collection.query(
                    query_texts=[memory], n_results=n_results_num
                )
                removed = False
                for document, retrieved_id in zip(
                    retrieved_["documents"][0], retrieved_["ids"][0]
                ):
                    # only remove on exact match
                    if document == memory:
                        await self.collection.delete([retrieved_id])
                        print(f"Removing memory: {memory}")
                        removed = True
                if not removed:
                    print(f"No memory to remove: {memory}")
        else:
            return super().remove(memories)

    async def remove_by_id(self, ids=[]):
        self.collection = await self.get_or_create_collection()
        if isinstance(self.client, AsyncClient):
            return await self.collection.delete(ids)
        else:
            return super().remove(ids)

    async def reset(self):
        self.collection = await self.get_or_create_collection()
        if isinstance(self.client, AsyncClient):
            return await self.client.delete_collection(name=self.collection.name)
        return super().reset()

    async def retrieve(self, task: str, filter=[]):
        self.collection = await self.get_or_create_collection()
        if isinstance(self.client, AsyncClient):
            max_count = self.collection.count()
            n_results_num = min(self.top_k, max_count)
            results = await self.collection.query(
                query_texts=[task], n_results=n_results_num, where=filter
            )["metadatas"][0]

            return [
                (
                    mem_item["taskgen_content"]
                    if "taskgen_content" in mem_item
                    else mem_item
                )
                for mem_item in results
            ]

        else:
            return super().retrieve(task, filter)



================================================
FILE: taskgen/ranker.py
================================================
import inspect
import numpy as np

from taskgen.utils import ensure_awaitable


class BaseRanker:
    ''' Base class that defines shared properties for Rankers '''
    def __init__(self, model="text-embedding-3-small", ranking_fn=None, database=None):
        if database is None:
            database = {}
        self.model = model
        # For AsyncRanker ranking_fn must be async if provided
        self.ranking_fn = ranking_fn
        self.database = database

    def get_python_representation(self):
        return f"{self.__class__.__name__}(model='{self.model}', ranking_fn={inspect.getsource(self.ranking_fn) if self.ranking_fn else None})"
    
    
class Ranker(BaseRanker):
    def __call__(self, query, key) -> float:
        query, key = str(query), str(key)
        if self.ranking_fn is None:
            from openai import OpenAI
            client = OpenAI()
            query_embedding = self.get_or_create_embedding(query, client)
            key_embedding = self.get_or_create_embedding(key, client)
            return np.dot(query_embedding, key_embedding)
        else:
            return self.ranking_fn(query, key)

    def get_or_create_embedding(self, text, client):
        if text in self.database:
            return self.database[text]
        else:
            cleaned_text = text.replace("\n", " ")
            embedding = client.embeddings.create(input=[cleaned_text], model=self.model).data[0].embedding
            self.database[text] = embedding
            return embedding
        
class AsyncRanker(BaseRanker):
    async def __call__(self, query, key) -> float:
        query, key = str(query), str(key)
        if self.ranking_fn is None:

            from openai import AsyncOpenAI
            client = AsyncOpenAI()
            query_embedding = await self.get_or_create_embedding_async(query, client)
            key_embedding = await self.get_or_create_embedding_async(key, client)
            return np.dot(query_embedding, key_embedding)
        else:
            ensure_awaitable(self.ranking_fn, 'ranking_fn')
            return await self.ranking_fn(query, key)

    async def get_or_create_embedding_async(self, text, client):
        if text in self.database:
            return self.database[text]
        else:
            cleaned_text = text.replace("\n", " ")
            response = await client.embeddings.create(input=[cleaned_text], model=self.model)
            embedding = response.data[0].embedding
            self.database[text] = embedding
            return embedding


================================================
FILE: taskgen/utils.py
================================================
import ast
import heapq
import inspect
import re


def get_source_code_for_func(fn):
    if fn.__name__ == "<lambda>":
        source_line = inspect.getsource(fn)
        source_line = source_line.split('#')[0]
        match = re.search(r"\blambda\b[^:]+:.*", source_line).group(0)
        splits = [s for s in match.split(",") if s != ""]
        fn_code = splits[0]
        idx = 1
        while idx < len(splits):
            try:
                ast.parse(fn_code)
                break
            except SyntaxError as _:
                fn_code = fn_code + "," + splits[idx]
                idx = idx + 1
        while True:
            try:
                ast.parse(fn_code)
                break
            except SyntaxError as _:
                fn_code = fn_code[:-1]

        return fn_code
    else:
        return inspect.getsource(fn)
    
    
def ensure_awaitable(func, name):
    """ Utility function to check if the function is an awaitable coroutine function """
    if func is not None and not inspect.iscoroutinefunction(func):
        raise TypeError(f"{name} must be an awaitable coroutine function")
    
    
### Helper Functions
def top_k_index(lst, k):
    ''' Given a list lst, find the top k indices corresponding to the top k values '''
    indexed_lst = list(enumerate(lst))
    top_k_values_with_indices = heapq.nlargest(k, indexed_lst, key=lambda x: x[1])
    top_k_indices = [index for index, _ in top_k_values_with_indices]
    return top_k_indices








================================================
FILE: taskgen/wrapper.py
================================================
from taskgen.agent import Agent
from taskgen.base import strict_json

from termcolor import colored

### Wrapper for Conversation that superclasses Agent
class ConversationWrapper(Agent):
    ''' This class takes an Agent and allows for conversational-based interactions with User / another Agent / Environment. Also updates persistent memory with latest information in conversation
    
    - Inputs:
        - **agent (compulsory)**: Agent. The agent we want to interact with
        - **persistent_memory**: dict. What kinds of memory the agent should have that persist over the entire conversation and their descriptions. Uses the same format as `output_format` of `strict_json`.
        - **person**: str. The name of the person you are talking to
        - **conversation**: List. The current existing conversation. Default: None
        - **num_past_conversation**: int. The number of past conversations to use for the agent
        - **verbose**: bool. Default: True. Whether to print the Agent's inner states
        
    - ConversationWrapper will automatically implement 3 new variables in `agent.shared_variables`:
        - **Persistent Memory**: The memory that will be updated as the conversation goes along, defined in persistent_dict
        - **Conversation**: The entire history of the conversation
        - **Summary of Conversation**: A summary of the current conversation
        
    - ConversationWrapper uses `chat()` which chats with the Agent and the Agent will perform actions and reply the chat message'''
    
    def __init__(self, agent: Agent, persistent_memory: dict = None, person = 'User', conversation = None, num_past_conversation: int = 5, verbose: bool = True):
        # Initialize the parent Agent
        super().__init__(**agent.__dict__)  # Inherit all of the attributes of the passed agent
        # Initiatlize the functions
        self.assign_functions(list(agent.function_map.values()))
        
        ## Define Additional Variables as Needed
        self.persistent_memory = persistent_memory
        self.num_past_conversation = num_past_conversation
        self.person = person
        self.verbose = verbose
        
        ''' Define some external variables for the Agent '''
        # add in the various types of memory
        self.shared_variables['Persistent Memory'] = {}
        # add in the conversation
        if conversation is None:
            self.shared_variables['Conversation'] = []
        else:
            self.shared_variables['Conversation'] = conversation
        # add in the summary of conversation
        self.shared_variables['Summary of Conversation'] = ''
    
    ## Reply the person
    def chat(self, cur_msg):
        ''' This does one chat with the person, firstly performing actions then replying the person, while updating the important memory '''
        actions_done = []
        
        ## Do actions before replying person only if there are actions other than use_llm and end_task
        my_actions = list(self.function_map.keys()) 
        if 'use_llm' in my_actions: my_actions.remove('use_llm')
        if 'end_task' in my_actions: my_actions.remove('end_task')
        if len(my_actions) > 0:
            self.reset()
            self.run(f'''Summary of Past Conversation: ```{self.shared_variables['Summary of Conversation']}```
Past Conversation: ```{self.shared_variables['Conversation'][-self.num_past_conversation:]}```
Latest input from {self.person}: ```{cur_msg}```
Use Equipped Functions other than use_llm to help answer the latest input from {self.person}''',
            )
            
            if len(self.subtasks_completed) > 0:
                actions_done = self.reply_user('Summarise Subtasks Completed in one line', verbose = False)
                print(colored(f'Actions Done: {actions_done}', 'red', attrs = ['bold']))
                print()
                self.reset()

        ## Replies the person
        res = self.query(f'''Summary of Past Conversation: ```{self.shared_variables['Summary of Conversation']}```
Past Conversation: ```{self.shared_variables['Conversation'][-self.num_past_conversation:]}```
Latest Input from {self.person}: ```{cur_msg}```
Actions Done for Latest Input: ```{actions_done}```
Persistent Memory: ```{self.shared_variables['Persistent Memory']}```
Use Global Context and Conversation and Actions Done for Latest Input and and Persistent Memory as context when replying.

First think through how to reply the latest message by {self.person}, before drafting the reply.
{self.person} is not aware of Actions Done for Latest Input - include relevant information in your reply to {self.person}. Do not hallucinate actions.
Thereafter, update the Summary of Conversation''', 
                          
output_format = {"Thoughts": f"How to reply",
                 f"Reply to {self.person}": f"Your reply as {self.agent_name}",
                 "Summary of Conversation": "Summarise key points of entire conversation in at most two sentences, building on previous Summary"})
        
        # Update the Summary of Conversation and Append the conversation
        self.shared_variables['Summary of Conversation'] = res['Summary of Conversation']
        # Append information about user and actions to conversation
        self.shared_variables['Conversation'].append(f'{self.person}: {cur_msg}')
        self.shared_variables['Conversation'].append(f'{self.agent_name}: {res[f"Reply to {self.person}"]}')
        
        ## Update Persistent Memory
        if self.persistent_memory is not None and self.persistent_memory != {}:
            persistent_memory = strict_json(f'Update all fields of Persistent Memory based on information in Additional Conversation. Current value: ```{self.shared_variables["Persistent Memory"]}```',
               f'Additional Conversation\n{self.person}: {cur_msg}\n{self.agent_name}: {res[f"Reply to {self.person}"]}',
               output_format = self.persistent_memory,
               model = self.kwargs.get('model', 'gpt-4o-mini'),
               llm = self.llm,
               verbose = self.debug)
                                                           
            self.shared_variables["Persistent Memory"] = persistent_memory
        
        if self.verbose:
            print(colored(f'Thoughts: {res["Thoughts"]}', 'green', attrs = ['bold']))
            print(colored(f'Persistent Memory: {self.shared_variables["Persistent Memory"]}', 'blue', attrs = ['bold']))
            print(colored(f'Summary of Conversation: {res["Summary of Conversation"]}', 'magenta', attrs = ['bold']))
        
        return res[f'Reply to {self.person}']

### Alternative form that does not superclass Agent. Kept here for legacy purposes ###
class ConversableAgent:
    ''' This class takes an Agent and allows for conversational-based interactions with User / another Agent / Environment. Also updates persistent memory with latest information in conversation
    
    - Inputs:
        - **agent (compulsory)**: Agent. The agent we want to interact with
        - **persistent_memory**: dict. What kinds of memory the agent should have that persist over the entire conversation and their descriptions. Uses the same format as `output_format` of `strict_json`.
        - **person**: str. The name of the person you are talking to
        - **conversation**: List. The current existing conversation. Default: None
        - **num_past_conversation**: int. The number of past conversations to use for the agent
        - **verbose**: bool. Default: True. Whether to print the Agent's inner states
        
- ConversableAgent will automatically implement 3 new variables in `agent.shared_variables`:
    - **Persistent Memory**: The memory that will be updated as the conversation goes along, defined in persistent_dict
    - **Conversation**: The entire history of the conversationn
    - **Summary of Conversation**: A summary of the current conversation
    
- ConversableAgent uses `chat()` which chats with the Agent and the Agent will perform actions and reply the chat message'''
    def __init__(self, agent: Agent, persistent_memory: dict = None, person = 'User', conversation = None, num_past_conversation: int = 5, verbose: bool = True):
        self.agent = agent
        self.persistent_memory = persistent_memory
        self.num_past_conversation = num_past_conversation
        self.person = person
        self.verbose = verbose
        
        ''' Define some external variables for the Agent '''
        # add in the various types of memory
        self.agent.shared_variables['Persistent Memory'] = {}
        # add in the conversation
        if conversation is None:
            self.agent.shared_variables['Conversation'] = []
        else:
            self.agent.shared_variables['Conversation'] = conversation
        # add in the summary of conversation
        self.agent.shared_variables['Summary of Conversation'] = ''
    
    ## Reply the person
    def chat(self, cur_msg):
        ''' This does one chat with the person, firstly performing actions then replying the person, while updating the important memory '''
        actions_done = []
        
        ## Do actions before replying person only if there are actions other than use_llm and end_task
        my_actions = list(self.agent.function_map.keys()) 
        if 'use_llm' in my_actions: my_actions.remove('use_llm')
        if 'end_task' in my_actions: my_actions.remove('end_task')
        if len(my_actions) > 0:
            self.agent.reset()
            self.agent.run(f'''Summary of Past Conversation: ```{self.agent.shared_variables['Summary of Conversation']}```
Past Conversation: ```{self.agent.shared_variables['Conversation'][-self.num_past_conversation:]}```
Latest input from {self.person}: ```{cur_msg}```
Use Equipped Functions other than use_llm to help answer the latest input from {self.person}''',
            )
            
            if len(self.agent.subtasks_completed) > 0:
                actions_done = self.agent.reply_user('Summarise Subtasks Completed in one line', verbose = False)
                print(colored(f'Actions Done: {actions_done}', 'red', attrs = ['bold']))
                print()
                self.agent.reset()

        ## Replies the person
        res = self.agent.query(f'''Summary of Past Conversation: ```{self.agent.shared_variables['Summary of Conversation']}```
Past Conversation: ```{self.agent.shared_variables['Conversation'][-self.num_past_conversation:]}```
Latest Input from {self.person}: ```{cur_msg}```
Actions Done for Latest Input: ```{actions_done}```
Persistent Memory: ```{self.agent.shared_variables['Persistent Memory']}```
Use Global Context and Conversation and Actions Done for Latest Input and and Persistent Memory as context when replying.

First think through how to reply the latest message by {self.person}, before drafting the reply.
{self.person} is not aware of Actions Done for Latest Input - include relevant information in your reply to {self.person}. Do not hallucinate actions.
Thereafter, update the Summary of Conversation''', 
                          
output_format = {"Thoughts": f"How to reply",
                 f"Reply to {self.person}": f"Your reply as {self.agent.agent_name}",
                 "Summary of Conversation": "Summarise key points of entire conversation in at most two sentences, building on previous Summary"})
        
        # Update the Summary of Conversation and Append the conversation
        self.agent.shared_variables['Summary of Conversation'] = res['Summary of Conversation']
        # Append information about user and actions to conversation
        self.agent.shared_variables['Conversation'].append(f'{self.person}: {cur_msg}')
        self.agent.shared_variables['Conversation'].append(f'{self.agent.agent_name}: {res[f"Reply to {self.person}"]}')
        
        ## Update Persistent Memory
        if self.persistent_memory is not None and self.persistent_memory != {}:
            persistent_memory = strict_json(f'Update all fields of Persistent Memory based on information in Additional Conversation. Current value: ```{self.agent.shared_variables["Persistent Memory"]}```',
               f'Additional Conversation\n{self.person}: {cur_msg}\n{self.agent.agent_name}: {res[f"Reply to {self.person}"]}',
               output_format = self.persistent_memory,
               model = self.agent.kwargs.get('model', 'gpt-4o-mini'),
               llm = self.agent.llm,
               verbose = self.agent.debug)
                                                           
            self.agent.shared_variables["Persistent Memory"] = persistent_memory
        
        if self.verbose:
            print(colored(f'Thoughts: {res["Thoughts"]}', 'green', attrs = ['bold']))
            print(colored(f'Persistent Memory: {self.agent.shared_variables["Persistent Memory"]}', 'blue', attrs = ['bold']))
            print(colored(f'Summary of Conversation: {res["Summary of Conversation"]}', 'magenta', attrs = ['bold']))
        
        return res[f'Reply to {self.person}']


================================================
FILE: .github/workflows/build.yaml
================================================
name: check & build

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  workflow_dispatch:

concurrency:
    # Use github.run_id on main branch
    # Use github.event.pull_request.number on pull requests, so it's unique per pull request
    # Use github.ref on other branches, so it's unique per branch
    group: ${{ github.workflow }}-${{ github.ref_protected && github.run_id || github.event.pull_request.number || github.ref }}
    cancel-in-progress: true

jobs:
  checkstyle:
    runs-on: ubuntu-latest
    env:
      EXCLUDE: '^static/.*|assets/.*|/migrations/.*|\.min\.js$|\.min\.css$|\.css\.map$|\.min\.js$|\.js\.map$|\.svg$'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      #- name: Install isort and black
      #  run: pip install isort==5.13.2 black==24.10.0

      #- name: Run isort
      #  run: |
      #    git ls-files -- '*.py' | grep -Ev $EXCLUDE | xargs isort --profile black --check-only --diff
     
      #- name: Run black
      #  run: |
      #    git ls-files -- '*.py' | grep -Ev $EXCLUDE | xargs black --check --diff

      #- name: Run ruff
      #  run: |
      #    make ruff

  build:
    needs: checkstyle
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Run build
        run: |
          ./build.sh


