Directory structure:
└── openbmb-repoagent/
    ├── README.md
    ├── LICENSE
    ├── pyproject.toml
    ├── README_CN.md
    ├── display/
    │   ├── Makefile
    │   ├── README_DISPLAY.md
    │   ├── book_template/
    │   │   └── book.json
    │   ├── book_tools/
    │   │   ├── generate_repoagent_books.py
    │   │   └── generate_summary_from_book.py
    │   ├── books/
    │   │   └── BOOKS.md
    │   └── scripts/
    │       └── install_nodejs.sh
    ├── markdown_docs/
    │   ├── display/
    │   │   └── book_tools/
    │   │       ├── generate_repoagent_books.md
    │   │       └── generate_summary_from_book.md
    │   ├── repo_agent/
    │   │   ├── change_detector.md
    │   │   ├── chat_engine.md
    │   │   ├── file_handler.md
    │   │   ├── log.md
    │   │   ├── main.md
    │   │   ├── multi_task_dispatch.md
    │   │   ├── project_manager.md
    │   │   ├── runner.md
    │   │   ├── settings.md
    │   │   └── utils/
    │   │       ├── gitignore_checker.md
    │   │       └── meta_info_utils.md
    │   └── tests/
    │       ├── test_change_detector.md
    │       ├── test_json_handler.md
    │       └── test_structure_tree.md
    ├── repo_agent/
    │   ├── __init__.py
    │   ├── __main__.py
    │   ├── change_detector.py
    │   ├── chat_engine.py
    │   ├── doc_meta_info.py
    │   ├── file_handler.py
    │   ├── log.py
    │   ├── main.py
    │   ├── multi_task_dispatch.py
    │   ├── project_manager.py
    │   ├── prompt.py
    │   ├── runner.py
    │   ├── settings.py
    │   ├── chat_with_repo/
    │   │   ├── __init__.py
    │   │   ├── __main__.py
    │   │   ├── gradio_interface.py
    │   │   ├── json_handler.py
    │   │   ├── main.py
    │   │   ├── prompt.py
    │   │   ├── rag.py
    │   │   ├── text_analysis_tool.py
    │   │   └── vector_store_manager.py
    │   └── utils/
    │       ├── gitignore_checker.py
    │       └── meta_info_utils.py
    ├── tests/
    │   ├── __init__.py
    │   ├── test_change_detector.py
    │   ├── test_json_handler.py
    │   └── test_structure_tree.py
    └── .github/
        └── workflows/
            └── release.yml

================================================
FILE: README.md
================================================
<h1 align="center">
  <img src="https://github.com/OpenBMB/RepoAgent/assets/138990495/06bc2449-c82d-4b9e-8c83-27640e541451" width="50" alt="RepoAgent logo"/> <em>RepoAgent: An LLM-Powered Framework for Repository-level Code Documentation Generation.</em>
</h1>

<p align="center">
  <img src="https://img.shields.io/pypi/dm/repoagent" alt="PyPI - Downloads"/>
  <a href="https://pypi.org/project/repoagent/">
    <img src="https://img.shields.io/pypi/v/repoagent" alt="PyPI - Version"/>
  </a>
  <a href="Pypi">
    <img src="https://img.shields.io/pypi/pyversions/repoagent" alt="PyPI - Python Version"/>
  </a>
  <img alt="GitHub License" src="https://img.shields.io/github/license/LOGIC-10/RepoAgent">
  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LOGIC-10/RepoAgent?style=social">
  <img alt="GitHub issues" src="https://img.shields.io/github/issues/LOGIC-10/RepoAgent">
  <a href="https://arxiv.org/abs/2402.16667v1">
    <img src="https://img.shields.io/badge/cs.CL-2402.16667-b31b1b?logo=arxiv&logoColor=red" alt="arXiv"/>
  </a>
</p>

<p align="center">
  <img src="https://raw.githubusercontent.com/OpenBMB/RepoAgent/main/assets/images/RepoAgent.png" alt="RepoAgent"/>
</p>

<p align="center">
  <a href="https://github.com/LOGIC-10/RepoAgent/blob/main/README.md">English readme</a>
   • 
  <a href="https://github.com/LOGIC-10/RepoAgent/blob/main/README_CN.md">简体中文 readme</a>
</p>

## :tv: Demo

[![Watch the video](https://img.youtube.com/vi/YPPJBVOP71M/hqdefault.jpg)](https://youtu.be/YPPJBVOP71M)

## 👾 Background

In the realm of computer programming, the significance of comprehensive project documentation, including detailed explanations for each Python file, cannot be overstated. Such documentation serves as the cornerstone for understanding, maintaining, and enhancing the codebase. It provides essential context and rationale for the code, making it easier for current and future developers to comprehend the purpose, functionality, and structure of the software. It not only facilitates current and future developers in grasping the project's purpose and structure but also ensures that the project remains accessible and modifiable over time, significantly easing the learning curve for new team members.

Traditionally, creating and maintaining software documentation demanded significant human effort and expertise, a challenge for small teams without dedicated personnel. The introduction of Large Language Models (LLMs) like GPT has transformed this, enabling AI to handle much of the documentation process. This shift allows human developers to focus on verification and fine-tuning, greatly reducing the manual burden of documentation.

**🏆 Our goal is to create an intelligent document assistant that helps people read and understand repositories and generate documents, ultimately helping people improve efficiency and save time.**

## ✨ Features

- **🤖 Automatically detects changes in Git repositories, tracking additions, deletions, and modifications of files.**
- **📝 Independently analyzes the code structure through AST, generating documents for individual objects.**
- **🔍 Accurate identification of inter-object bidirectional invocation relationships, enriching the global perspective of document content.**
- **📚 Seamlessly replaces Markdown content based on changes, maintaining consistency in documentation.**
- **🕙 Executes multi-threaded concurrent operations, enhancing the efficiency of document generation.**
- **👭 Offer a sustainable, automated documentation update method for team collaboration.**
- **😍 Display Code Documentation in an amazing way. (with document book per project powered by Gitbook)**


## 🚀 Getting Started

### Installation Method

#### Using GitHub Actions

This repository supports GitHub Actions for automating workflows such as building, testing, and deploying. For detailed instructions on setting up and using GitHub Actions with this repository, please refer to the [actions/run-repoagent](https://github.com/marketplace/actions/run-repoagent).

#### Using pip (Recommended for Users)

Install the `repoagent` package directly using pip:

```bash
pip install repoagent
```

#### Development Setup Using PDM

If you're looking to contribute or set up a development environment:

- **Install PDM**: If you haven't already, [install PDM](https://pdm-project.org/latest/#installation).
- **Use CodeSpace, or Clone the Repository**:

    - **Use CodeSpace**
    The easiest way to get RepoAgent enviornment. Click below to use the GitHub Codespace, then go to the next step.
  
    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/LOGIC-10/RepoAgent?quickstart=1)
  
    - **Clone the Repository**
  
    ```bash
    git clone https://github.com/LOGIC-10/RepoAgent.git
    cd RepoAgent
    ```

- **Setup with PDM**

    - Initialize the Python virtual environment. Make sure to run the below cmd in `/RepoAgent` directory:
    
      ```bash
      pdm venv create --name repoagent
      ```
    
    - [Activate virtual environment](https://pdm-project.org/latest/usage/venv/#activate-a-virtualenv)
    
    - Install dependencies using PDM
    
      ```bash
       pdm install
      ```

### Configuring RepoAgent

Before configuring specific parameters for RepoAgent, please ensure that the OpenAI API is configured as an environment variable in the command line:

```sh
export OPENAI_API_KEY=YOUR_API_KEY # on Linux/Mac
set OPENAI_API_KEY=YOUR_API_KEY # on Windows
$Env:OPENAI_API_KEY = "YOUR_API_KEY" # on Windows (PowerShell)
```

## Run RepoAgent

Enter the root directory of RepoAgent and try the following command in the terminal:
```sh
repoagent run #this command will generate doc, or update docs(pre-commit-hook will automatically call this)
repoagent run --print-hierarchy # Print how repo-agent parse the target repo
```

The run command supports the following optional flags (if set, will override config defaults):

- `-m`, `--model` TEXT: Specifies the model to use for completion. Default: `gpt-3.5-turbo`
- `-t`, `--temperature` FLOAT: Sets the generation temperature for the model. Lower values make the model more deterministic. Default: `0.2`
- `-r`, `--request-timeout` INTEGER: Defines the timeout in seconds for the API request. Default: `60`
- `-b`, `--base-url` TEXT: The base URL for the API calls. Default: `https://api.openai.com/v1`
- `-tp`, `--target-repo-path` PATH: The file system path to the target repository. Used as the root for documentation generation. Default: `path/to/your/target/repository`
- `-hp`, `--hierarchy-path` TEXT: The name or path for the project hierarchy file, used to organize documentation structure. Default: `.project_doc_record`
- `-mdp`, `--markdown-docs-path` TEXT: The folder path where Markdown documentation will be stored or generated. Default: `markdown_docs`
- `-i`, `--ignore-list` TEXT: A list of files or directories to ignore during documentation generation, separated by commas.
- `-l`, `--language` TEXT: The ISO 639 code or language name for the documentation. Default: `Chinese`
- `-ll`, `--log-level` [DEBUG|INFO|WARNING|ERROR|CRITICAL]: Sets the logging level for the application. Default: `INFO`

You can also try the following feature

```sh
repoagent clean # Remove repoagent-related cache
repoagent diff # Check what docs will be updated/generated based on current code change
```

If it's your first time generating documentation for the target repository, RepoAgent will automatically create a JSON file maintaining the global structure information and a folder named Markdown_Docs in the root directory of the target repository for storing documents.

Once you have initially generated the global documentation for the target repository, or if the project you cloned already contains global documentation information, you can then seamlessly and automatically maintain internal project documentation with your team by configuring the **pre-commit hook** in the target repository! 

### Use `pre-commit` 

RepoAgent currently supports generating documentation for projects, which requires some configuration in the target repository.

First, ensure that the target repository is a git repository and has been initialized.

```sh
git init
```
Install pre-commit in the target repository to detect changes in the git repository.

```sh
pip install pre-commit
```
Create a file named `.pre-commit-config.yaml` in the root directory of the target repository. An example is as follows:

```yml
repos:
  - repo: local
    hooks:
    - id: repo-agent
      name: RepoAgent
      entry: repoagent
      language: system
      pass_filenames: false # prevent from passing filenames to the hook
      # You can specify the file types that trigger the hook, but currently only python is supported.
      types: [python]
```

For specific configuration methods of hooks, please refer to [pre-commit](https://pre-commit.com/#plugins).
After configuring the yaml file, execute the following command to install the hook.

```sh
pre-commit install
```

In this way, each git commit will trigger the RepoAgent's hook, automatically detecting changes in the target repository and generating corresponding documents.
Next, you can make some modifications to the target repository, such as adding a new file to the target repository, or modifying an existing file.
You just need to follow the normal git workflow: git add, git commit -m "your commit message", git push
The RepoAgent hook will automatically trigger at git commit, detect the files you added in the previous step, and generate corresponding documents.

After execution, RepoAgent will automatically modify the staged files in the target repository and formally submit the commit. After the execution is completed, the green "Passed" will be displayed, as shown in the figure below:
![Execution Result](https://raw.githubusercontent.com/OpenBMB/RepoAgent/main/assets/images/ExecutionResult.png)

The generated document will be stored in the specified folder in the root directory of the target warehouse. The rendering of the generated document is as shown below:
![Documentation](https://raw.githubusercontent.com/OpenBMB/RepoAgent/main/assets/images/Doc_example.png)
![Documentation](https://raw.githubusercontent.com/OpenBMB/RepoAgent/main/assets/images/8_documents.png)

We utilized the default model **gpt-3.5-turbo** to generate documentation for the [**XAgent**](https://github.com/OpenBMB/XAgent) project, which comprises approximately **270,000 lines** of code. You can view the results of this generation in the Markdown_Docs directory of the XAgent project on GitHub. For enhanced documentation quality, we suggest considering more advanced models like **gpt-4-1106** or **gpt-4-0125-preview**.

**In the end, you can flexibly adjust the output format, template, and other aspects of the document by customizing the prompt. We are excited about your exploration of a more scientific approach to Automated Technical Writing and your contributions to the community.** 

### Exploring chat with repo

We conceptualize **Chat With Repo** as a unified gateway for these downstream applications, acting as a connector that links RepoAgent to human users and other AI agents. Our future research will focus on adapting the interface to various downstream applications and customizing it to meet their unique characteristics and implementation requirements.

Here we demonstrate a preliminary prototype of one of our downstream tasks: Automatic Q&A for Issues and Code Explanation. You can start the server by running the following code.

```sh
pip install repoagent[chat-with-repo]
repoagent chat-with-repo
```

## ✅ Future Work

- [ ] Generate README.md automatically combining with the global documentation
- [ ] **Multi-programming-language support** Support more programming languages like Java, C or C++, etc.
- [x] Local model support like Llama, chatGLM, Qwen, GLM4, etc.

## 🥰 Featured Cases

Here are featured cases that have adopted RepoAgent.

- [MiniCPM](https://github.com/OpenBMB/MiniCPM): An edge-side LLM of 2B size, comparable to 7B model.
- [ChatDev](https://github.com/OpenBMB/ChatDev): Collaborative AI agents for software development.
- [XAgent](https://github.com/OpenBMB/XAgent): An Autonomous LLM Agent for Complex Task Solving.
- [EasyRL4Rec](https://github.com/chongminggao/EasyRL4Rec): A user-friendly RL library for recommender systems.

## 📊 Citation

```bibtex
@misc{luo2024repoagent,
      title={RepoAgent: An LLM-Powered Open-Source Framework for Repository-level Code Documentation Generation}, 
      author={Qinyu Luo and Yining Ye and Shihao Liang and Zhong Zhang and Yujia Qin and Yaxi Lu and Yesai Wu and Xin Cong and Yankai Lin and Yingli Zhang and Xiaoyin Che and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2402.16667},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```



================================================
FILE: LICENSE
================================================
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.



================================================
FILE: pyproject.toml
================================================
[project]
authors = [
    {name = "Qinyu Luo", email = "qinyuluo123@gmail.com"},
]
maintainers = [
    {name = "Edwards Arno", email = "Edwards.Arno@outlook.com"},
]
license = {text = "Apache-2.0"}
requires-python = ">=3.11,<4.0"
dependencies = [
    "loguru>=0.7.2",
    "jedi>=0.19.1",
    "GitPython>=3.1.41",
    "prettytable>=3.9.0",
    "python-iso639>=2024.2.7",
    "pydantic-settings>=2.2.1",
    "click>=8.1.7",
    "python-iso639>=2024.10.22",
    "colorama>=0.4.6",
    "llama-index-llms-openai-like>=0.3.3",
]
name = "repoagent"
version = "0.2.0"
description = "An LLM-Powered Framework for Repository-level Code Documentation Generation."
readme = "README.md"
classifiers = [
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Scientific/Engineering :: Artificial Intelligence"
]

[project.urls]
repository = "https://github.com/LOGIC-10/RepoAgent"

[project.scripts]
repoagent = "repo_agent.main:cli"

[project.optional-dependencies]
chat_with_repo = [
    "markdown>=3.7",
    "llama-index-embeddings-openai>=0.2.5",
    "llama-index-vector-stores-chroma>=0.3.0",
    "gradio>=5.6.0",
]

[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"

[tool.pyright]
reportCallIssue="none"

[tool.ruff]
# General ruff settings can stay here.

[tool.ruff.lint]
select = ["I001"] 

[tool.pdm]
[tool.pdm.dev-dependencies]
test = [
    "pytest<8.0.0,>=7.4.4",
    "pytest-mock<4.0.0,>=3.12.0",
]
lint = [
    "ruff>=0.7.4",
]

[tool.pdm.build]
includes = [
    "repo_agent",
]


================================================
FILE: README_CN.md
================================================
<h1 align="center"><em>RepoAgent：一个用于代码库级别代码文档生成的LLM驱动框架</em></h1>

<p align="center">
  <img src="https://img.shields.io/pypi/dm/repoagent" alt="PyPI - 下载量"/>
  <a href="https://pypi.org/project/repoagent/">
    <img src="https://img.shields.io/pypi/v/repoagent" alt="PyPI - 版本"/>
  </a>
  <a href="Pypi">
    <img src="https://img.shields.io/pypi/pyversions/repoagent" alt="PyPI - Python版本"/>
  </a>
  <img alt="GitHub授权许可" src="https://img.shields.io/github/license/LOGIC-10/RepoAgent">
  <img alt="GitHub仓库星标" src="https://img.shields.io/github/stars/LOGIC-10/RepoAgent?style=social">
  <img alt="GitHub问题" src="https://img.shields.io/github/issues/LOGIC-10/RepoAgent">
  <a href="https://arxiv.org/abs/2402.16667v1">
    <img src="https://img.shields.io/badge/cs.CL-2402.16667-b31b1b?logo=arxiv&logoColor=red" alt="arXiv"/>
  </a>
</p>

<p align="center">
  <img src="https://raw.githubusercontent.com/OpenBMB/RepoAgent/main/assets/images/RepoAgent.png" alt="RepoAgent"/>
</p>

<p align="center">
  <a href="https://github.com/LOGIC-10/RepoAgent/blob/main/README.md">English README</a>
   • 
  <a href="https://github.com/LOGIC-10/RepoAgent/blob/main/README_CN.md">简体中文说明</a>
</p>

## 👾 背景

在计算机编程领域，全面的项目文档的重要性，包括每个Python文件的详细解释，不言而喻。这样的文档是理解、维护和增强代码库的基石。它提供了代码的必要上下文和理由，使当前和未来的开发者更容易理解软件的目的、功能和结构。它不仅便于当前和未来的开发者理解项目的目的和结构，还确保了项目随时间的推移保持可访问和可修改，大大简化了新团队成员的学习曲线。

传统上，创建和维护软件文档需要大量的人力和专业知识，这对没有专门人员的小团队来说是一个挑战。像GPT这样的大型语言模型（LLMs）的引入改变了这一点，使得AI可以处理大部分文档化过程。这种转变允许人类开发者专注于验证和微调，极大地减少了文档化的手动负担。

**🏆 我们的目标是创建一个智能文档助手，帮助人们阅读和理解仓库并生成文档，最终帮助人们提高效率和节省时间。**

## ✨ 特性

- **🤖 自动检测Git仓库中的变化，跟踪文件的增加、删除和修改。**
- **📝 通过AST独立分析代码结构，为各个对象生成文档。**
- **🔍 准确识别对象间的双向调用关系，丰富文档内容的全局视角。**
- **📚 根据变化无缝替换Markdown内容，保持文档一致性。**
- **🕙 执行多线程并发操作，提高文档生成效率。**
- **👭 为团队协作提供可持续的自动化文档更新方法。**
- **😍 以惊人的方式展示代码文档（每个项目都有由Gitbook提供支持的文档书）。**

## 🚀 开始使用

### 安装方法

#### 使用pip（普通用户首选）

直接使用pip安装`repoagent`包：

```bash
pip install repoagent
```

#### 使用PDM进行开发环境设置

如果您想要贡献或者设置一个开发环境：

- **安装PDM**：如果您还没有安装，请[安装PDM](https://pdm-project.org/latest/#installation)。
- **使用CodeSpace或克隆仓库**：

    - **使用CodeSpace**
    获取RepoAgent环境的最简单方式。点击下面链接使用GitHub Codespace，然后进行下一步。
  
    [![在GitHub Codespaces中打开](https://github.com/codespaces/badge.svg)](https://codespaces.new/LOGIC-10/RepoAgent?quickstart=1)
  
    - **克隆仓库**
  
    ```bash
    git clone https://github.com/LOGIC-10/RepoAgent.git
    cd RepoAgent
    ```

- **使用PDM设置**

    - 初始化Python虚拟环境。确保在`/RepoAgent`目录下运行下面的命令：
    
      ```bash
      pdm venv create --name repoagent
      ```
    
    - [激活虚拟环境](https://pdm-project.org/latest/usage/venv/#activate-a-virtualenv)
    
    - 使用PDM安装依赖
    
      ```bash
       pdm install
      ```

### 配置RepoAgent

在配置RepoAgent具体参数之前，请先确保已经在命令行配置 OpenAI API 作为环境变量：

```sh
export OPENAI_API_KEY=YOUR_API_KEY # on Linux/Mac

set OPENAI_API_KEY=YOUR_API_KEY # on Windows
$Env:OPENAI_API_KEY = "YOUR_API_KEY" # on Windows (PowerShell)
```

## 运行RepoAgent

进入RepoAgent根目录并在终端尝试以下命令：
```sh
repoagent run # 这条命令会生成文档或自动更新文档 (pre-commit-hook 会自动调用它)
repoagent --print-hierarchy # 此命令将打印repoagent解析出的目标仓库
```

run 命令支持以下可选标志（如果设置，将覆盖配置默认值）：

- `-m`, `--model` TEXT：指定用于完成的模型。默认值：`gpt-3.5-turbo`
- `-t`, `--temperature` FLOAT：设置模型的生成温度。较低的值使模型更确定性。默认值：`0.2`
- `-r`, `--request-timeout` INTEGER：定义 API 请求的超时时间（秒）。默认值：`60`
- `-b`, `--base-url` TEXT：API 调用的基础 URL。默认值：`https://api.openai.com/v1`
- `-tp`, `--target-repo-path` PATH：目标仓库的文件系统路径。用作文档生成的根路径。默认值：`path/to/your/target/repository`
- `-hp`, `--hierarchy-path` TEXT：项目层级文件的名称或路径，用于组织文档结构。默认值：`.project_doc_record`
- `-mdp`, `--markdown-docs-path` TEXT：Markdown 文档将被存储或生成的文件夹路径。默认值：`markdown_docs`
- `-i`, `--ignore-list` TEXT：在文档生成过程中要忽略的文件或目录列表，用逗号分隔。
- `-l`, `--language` TEXT：文档的 ISO 639 代码或语言名称。默认值：`Chinese`
- `-ll`, `--log-level` [DEBUG|INFO|WARNING|ERROR|CRITICAL]：设置应用程序的日志级别。默认值：`INFO`

你也可以尝试以下功能

```sh
repoagent clean # 此命令将删除与repoagent相关的缓存
repoagent diff # 此命令将检查基于当前代码更改将更新/生成哪些文档
```

如果您是第一次对目标仓库生成文档，此时RepoAgent会自动生成一个维护全局结构信息的json文件，并在目标仓库根目录下创建一个文件夹用于存放文档。
全局结构信息json文件和文档文件夹的路径都可以在`config.yml`中进行配置。

当您首次完成对目标仓库生成全局文档后，或您clone下来的项目已经包含了全局文档信息后，就可以通过**pre-commit**配置目标仓库**hook**和团队一起无缝自动维护一个项目内部文档了！

### 配置目标仓库

RepoAgent目前支持对项目的文档生成和自动维护，因此需要对目标仓库进行一定的配置。

首先，确保目标仓库是一个git仓库，且已经初始化。
```
git init
```
在目标仓库中安装pre-commit，用于检测git仓库中的变更。
```
pip install pre-commit
```
在目标仓库根目录下，创建一个名为`.pre-commit-config.yaml`的文件，示例如下：
```
repos:
  - repo: local
    hooks:
    - id: repo-agent
      name: RepoAgent
      entry: repoagent
      language: system
      pass_filenames: false # 阻止pre commit传入文件名作为参数
      # 可以指定钩子触发的文件类型，但是目前只支持python
      types: [python]
```
具体hooks的配置方法请参考[pre-commit](https://pre-commit.com/#plugins)。
配置好yaml文件后，执行以下命令，安装钩子。
```
pre-commit install
```
这样，每次git commit时，都会触发RepoAgent的钩子，自动检测目标仓库中的变更，并生成对应的文档。
接着，可以对目标仓库进行一些修改，例如在目标仓库中添加一个新的文件，或者修改一个已有的文件。
您只需要正常执行git的工作流程: git add, git commit -m "your commit message", git push
RepoAgent hook会在git commit时自动触发，检测前一步您git add的文件，并生成对应的文档。

执行后，RepoAgent会自动更改目标仓库中的已暂存文件并正式提交commit，执行完毕后会显示绿色的Passed，如下图所示：
![Execution Result](https://raw.githubusercontent.com/OpenBMB/RepoAgent/main/assets/images/ExecutionResult.png)

生成的文档将存放在目标仓库根目录下的指定文件夹中，生成的文档效果如下图所示：
![Documentation](https://raw.githubusercontent.com/OpenBMB/RepoAgent/main/assets/images/Doc_example.png)
![Documentation](https://raw.githubusercontent.com/OpenBMB/RepoAgent/main/assets/images/8_documents.png)


我们使用默认模型**gpt-3.5-turbo**对一个约**27万行**的中大型项目[**XAgent**](https://github.com/OpenBMB/XAgent)生成了文档。您可以前往XAgent项目的Markdown_Docs文件目录下查看生成效果。如果您希望得到更好的文档效果，我们建议您使用更先进的模型，如**gpt-4-1106** 或 **gpt-4-0125-preview**。

**最后，您可以通过自定义Prompt来灵活调整文档的输出格式、模板等方面的效果。 我们很高兴您探索更科学的自动化Technical Writing Prompts并对社区作出贡献。**

### 探索 chat with repo

我们将与仓库对话视为所有下游应用的统一入口，作为连接RepoAgent与人类用户和其他AI智能体之间的接口。我们未来的研究将探索适配各种下游应用的接口，并实现这些下游任务的独特性和现实要求。

在这里，我们展示了我们的下游任务之一的初步原型：自动issue问题解答和代码解释。您可以通过在终端运行以下代码启动服务。

```sh
pip install repoagent[chat-with-repo]
repoagent chat-with-repo
```

# ✅ 未来工作

- [x] 支持通过`pip install repoagent`将项目作为包进行安装配置
- [ ] 通过全局文档信息自动生成仓库README.md文件
- [ ] **多编程语言支持** 支持更多编程语言，如Java、C或C++等
- [ ] 本地模型支持如 Llama、chatGLM、Qianwen 等


# 🥰 精选案例

以下是采用了RepoAgent的开源项目精选案例。

- [MiniCPM](https://github.com/OpenBMB/MiniCPM): 一个端侧大语言模型，大小为2B，效果可与7B模型媲美。
- [ChatDev](https://github.com/OpenBMB/ChatDev): 用于软件开发的协作式AI智能体。
- [XAgent](https://github.com/OpenBMB/XAgent): 一个用于解决复杂任务的自主大型语言模型智能体。
- [EasyRL4Rec](https://github.com/chongminggao/EasyRL4Rec): 一个用户友好的推荐系统强化学习库。

# 📊 引用我们
```bibtex
@misc{luo2024repoagent,
      title={RepoAgent: An LLM-Powered Open-Source Framework for Repository-level Code Documentation Generation}, 
      author={Qinyu Luo and Yining Ye and Shihao Liang and Zhong Zhang and Yujia Qin and Yaxi Lu and Yesai Wu and Xin Cong and Yankai Lin and Yingli Zhang and Xiaoyin Che and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2402.16667},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```



================================================
FILE: display/Makefile
================================================
# get repo path from ../config.yml
REPO_PATH := $(shell grep 'repo_path:' ../config.yml | awk '{ print $$2 }')

Markdown_Docs_folder := $(shell grep 'Markdown_Docs_folder:' ../config.yml | awk '{ print $$2 }')

# get book name from REPO_PATH
BOOK_NAME := $(notdir $(REPO_PATH))
$(info BOOK_NAME is $(BOOK_NAME))
MARKDOWN_DOCS_FOLDER := $(Markdown_Docs_folder)
$(info MARKDOWN_DOCS_FOLDER is $(MARKDOWN_DOCS_FOLDER))
GITBOOK_PORT := 4000
GITBOOK_LRPORT := 30000

# info colors
GREEN  := $(shell tput -Txterm setaf 2)
YELLOW := $(shell tput -Txterm setaf 3)
WHITE  := $(shell tput -Txterm setaf 7)
RESET  := $(shell tput -Txterm sgr0)


.PHONY: help
.PHONY: init_env env_install
.PHONY: init npm_install clear_book copy_book_json install
.PHONY: generate generate_repo_agent_books generate_summary


################################################################################
# We need nodejs 10.x to run gitbook, this target will install nodejs 10.x
################################################################################
env_install:
	chmod +x ./scripts/install_nodejs.sh
	./scripts/install_nodejs.sh

## init nodejs 10.x env
init_env: env_install
	echo "You have prepared nodejs 10.x  environment."



################################################################################
# The following targets are used to init the gitbook environment
################################################################################
### Install gitbook-cli
npm_install:
	npm install gitbook-cli -g

## clear repo generated book
clear_book:
	-rm -rf ./books/$(BOOK_NAME)

### copy book.json
copy_book_json: clear_book
	mkdir -p ./books/$(BOOK_NAME)
	cp ./book_template/book.json ./books/$(BOOK_NAME)/book.json

### gitbook install plugins
install:
	echo "You need to make sure you have installed nodejs 10.x."
	cd ./books/$(BOOK_NAME) && gitbook install

## gitbook init to install plugins
init: npm_install clear_book copy_book_json install
	@echo Compelete init docs book


################################################################################
# The following targets are used to generate the book and book.json for the gitbook
################################################################################
clear_src:
	-rm -rf ./books/$(BOOK_NAME)/src

generate_repo_agent_books:
	@echo "Generating Repo Agent books..."
	@python ./book_tools/generate_repoagent_books.py $(MARKDOWN_DOCS_FOLDER) $(BOOK_NAME) $(REPO_PATH)

generate_summary:
	@echo "Generating summary..."
	@python ./book_tools/generate_summary_from_book.py $(BOOK_NAME)

## generate repo book
generate: clear_src generate_repo_agent_books generate_summary
	@echo complete repo book: $(BOOK_NAME) generate

## serve gitbook
serve:  generate
	gitbook --port $(GITBOOK_PORT) --lrport $(GITBOOK_LRPORT) serve ./books/$(BOOK_NAME)


TASK_MAX_CHAR_NUM=30
## make help info
help:
	@echo ''
	@echo 'Usage:'
	@echo '  ${YELLOW}make${RESET} ${GREEN}<task>${RESET}'
	@echo ''
	@echo 'Tasks:'
	@awk '/^[a-zA-Z\-\_0-9]+:/ { \
		helpInfo = match(lastLine, /^## (.*)/); \
		if (helpInfo) { \
			helpCommand = substr($$1, 0, index($$1, ":")-1); \
			helpInfo = substr(lastLine, RSTART + 3, RLENGTH); \
			printf "  ${YELLOW}%-$(TASK_MAX_CHAR_NUM)s${RESET} ${GREEN}%s${RESET}\n", helpCommand, helpInfo; \
		} \
	} \
	{ lastLine = $$0 }' $(MAKEFILE_LIST)


================================================
FILE: display/README_DISPLAY.md
================================================
[Binary file]


================================================
FILE: display/book_template/book.json
================================================
{
    
    "title": "RepoAgent所生成文GitBook",
    "description": "RepoAgent根据项目repo所生成文档GitBook",
    "generator": "site",
    "author": "RepoAgent <x-agent.net>",
    "language": "zh-hans",
    "gitbook": "3.2.3",
    "root": "./src",

    "links": {
        "sidebar": {
            "XAgent": "https://x-agent.net",
            "RepoAgent": "https://github.com/LOGIC-10/RepoAgent.git"
        }
    },

    "pluginsConfig": {
        "github-buttons": {
            "buttons": [
                {
                "repo": "RepoAgent",
                "user": "LOGIC-10",
                "type": "star",
                "count": true,
                "size": "small"
                }
            ]
        },
		
		"image-captions": {
          "caption": "Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_"
		},
		"autotheme": {
          "white": [9, 10, 11, 12, 13, 14, 15, 16],
          "sepia": [6, 7, 8, 17, 18, 19],
          "night": [20, 21, 22, 23, 0, 1, 2, 3, 4, 5]
        },
        "callouts": {
            "showTypeInHeader": false
        },
        "theme-default": {
            "showLevel": false
        },
        "disqus": {
            "shortName": ""
        },
        "prism": {
            "css": [
                "prism-themes/themes/prism-atom-dark.css"
            ]
        },
        "sharing": {
            "douban": false,
            "facebook": true,
            "google": false,
            "hatenaBookmark": false,
            "instapaper": false,
            "line": false,
            "linkedin": false,
            "messenger": false,
            "pocket": false,
            "qq": true,
            "qzone": false,
            "stumbleupon": false,
            "twitter": true,
            "viber": false,
            "vk": false,
            "weibo": true,
            "whatsapp": false,
            "all": [
                "douban",
                "facebook",
                "google",
                "instapaper",
                "line",
                "linkedin",
                "messenger",
                "pocket",
                "qq",
                "qzone",
                "stumbleupon",
                "twitter",
                "viber",
                "vk",
                "weibo",
                "whatsapp"
            ]
        },
        "tbfed-pagefooter": {
            "copyright": "https://x-agent.net，RepoAgent </a>发布",
            "modify_label": "最后更新：",
            "modify_format": "YYYY-MM-DD HH:mm:ss"
        }
    },
        
    "plugins": [
        "theme-comscore",
        "anchors",
        "-lunr",
        "-search",
        "search-plus",
        "disqus",
        "-highlight",
        "prism",
        "prism-themes",
        "github-buttons",
        "splitter",
        "-sharing",
        "sharing-plus",
        "tbfed-pagefooter",
        "expandable-chapters-small",
        "copy-code-button",
        "callouts",
		"image-captions",
		"autotheme"
    ]
    
}


================================================
FILE: display/book_tools/generate_repoagent_books.py
================================================
import os
import shutil
import sys


def main():
    markdown_docs_folder = sys.argv[1]
    book_name = sys.argv[2]
    repo_path = sys.argv[3]

    # mkdir the book folder
    dst_dir = os.path.join('./books', book_name, 'src')
    docs_dir = os.path.join(repo_path, markdown_docs_folder)

    # check the dst_dir
    if not os.path.exists(dst_dir):
        os.makedirs(dst_dir)
        print("mkdir %s" % dst_dir)

    # cp the Markdown_Docs_folder to dst_dir
    for item in os.listdir(docs_dir):
        src_path = os.path.join(docs_dir, item)
        dst_path = os.path.join(dst_dir, item)

        # check the src_path
        if os.path.isdir(src_path):
            # if the src_path is a folder, use shutil.copytree to copy
            shutil.copytree(src_path, dst_path)
            print("copytree %s to %s" % (src_path, dst_path))
        else:
            # if the src_path is a file, use shutil.copy2 to copy
            shutil.copy2(src_path, dst_path)
            print("copy2 %s to %s" % (src_path, dst_path))

    def create_book_readme_if_not_exist(dire):
        readme_path = os.path.join(dire, 'README.md')

        if not os.path.exists(readme_path):
            with open(readme_path, 'w') as readme_file:
                readme_file.write('# {}\n'.format(book_name))

    # create book README.md if not exist
    create_book_readme_if_not_exist(dst_dir)


if __name__ == '__main__':
    main()



================================================
FILE: display/book_tools/generate_summary_from_book.py
================================================
import os
import re
import sys


def create_readme_if_not_exist(dire):
    readme_path = os.path.join(dire, 'README.md')

    if not os.path.exists(readme_path):
        with open(readme_path, 'w') as readme_file:
            dirname = os.path.basename(dire)
            readme_file.write('# {}\n'.format(dirname))


# def output_markdown(dire, base_dir, output_file, iter_depth=0):
#     for filename in os.listdir(dire):
#         print('add readme ', filename)
#         file_or_path = os.path.join(dire, filename)
#         if os.path.isdir(file_or_path):
#             create_readme_if_not_exist(file_or_path)
#
#     for filename in os.listdir(dire):
#         print('deal with ', filename)
#         file_or_path = os.path.join(dire, filename)
#         if os.path.isdir(file_or_path):
#             # create_readme_if_not_exist(file_or_path)
#
#             if markdown_file_in_dir(file_or_path):
#                 output_file.write('  ' * iter_depth + '- ' + filename + '\n')
#                 output_markdown(file_or_path, base_dir, output_file,
#                                 iter_depth + 1)
#         else:
#             if is_markdown_file(filename):
#                 if (filename not in ['SUMMARY.md',
#                                      'README.md']
#                         or iter_depth != 0):
#                     output_file.write('  ' * iter_depth +
#                                       '- [{}]({})\n'.format(is_markdown_file(filename),
#                                                             os.path.join(os.path.relpath(dire, base_dir),
#                                                                          filename)))

def output_markdown(dire, base_dir, output_file, iter_depth=0):
    for filename in os.listdir(dire):
        print('add readme ', filename)
        file_or_path = os.path.join(dire, filename)
        if os.path.isdir(file_or_path):
            create_readme_if_not_exist(file_or_path)

    for filename in os.listdir(dire):
        print('deal with ', filename)
        file_or_path = os.path.join(dire, filename)
        if os.path.isdir(file_or_path):
            # Check if README.md exists in the directory
            readme_path = os.path.join(file_or_path, 'README.md')
            if os.path.exists(readme_path):
                # If README.md exists, create a markdown link to it
                relative_path = os.path.join(os.path.relpath(file_or_path, base_dir), 'README.md')
                output_file.write('  ' * iter_depth + '- [{}]({})\n'.format(filename, relative_path))
            # Recursively call output_markdown for nested directories
            output_markdown(file_or_path, base_dir, output_file, iter_depth + 1)
        else:
            if is_markdown_file(filename):
                if filename not in ['SUMMARY.md', 'README.md'] or iter_depth != 0 and filename not in ['README.md']:
                    relative_path = os.path.join(os.path.relpath(dire, base_dir), filename)
                    output_file.write('  ' * iter_depth + '- [{}]({})\n'.format(is_markdown_file(filename), relative_path))



def markdown_file_in_dir(dire):
    for root, dirs, files in os.walk(dire):
        for filename in files:
            if re.search('.md$|.markdown$', filename):
                return True
    return False


def is_markdown_file(filename):
    match = re.search('.md$|.markdown$', filename)
    if not match:
        return False
    elif len(match.group()) is len('.md'):
        return filename[:-3]
    elif len(match.group()) is len('.markdown'):
        return filename[:-9]


def main():
    book_name = sys.argv[1]

    # mkdir the book folder
    dir_input = os.path.join('./books', book_name, 'src')

    # check the dst_dir
    if not os.path.exists(dir_input):
        print(dir_input)
        os.makedirs(dir_input)
    # Ensure the directory exists or create it
    if not os.path.exists(dir_input):
        os.makedirs(dir_input)

    # Then proceed to create the file
    output_path = os.path.join(dir_input, 'SUMMARY.md')
    output = open(output_path, 'w')
    # output = open(os.path.join(dir_input, 'SUMMARY.md'), 'w')
    output.write('# Summary\n\n')
    output_markdown(dir_input, dir_input, output)

    print('GitBook auto summary finished:) ')
    return 0


if __name__ == '__main__':
    main()



================================================
FILE: display/books/BOOKS.md
================================================
支持多个 repo book 的创建。

统一 book template，支持完全自定义。

在 config.yml 中，配置生成的是repo doc，不用更改config.yml，直接运行 make 命令即可生成对应的repo book展示。


================================================
FILE: display/scripts/install_nodejs.sh
================================================
#!/bin/bash

export NVM_DIR="$HOME/.nvm"
[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"  # This loads nvm
[ -s "$NVM_DIR/bash_completion" ] && \. "$NVM_DIR/bash_completion"  # This loads nvm bash_completion


# 检查是否已经安装了 nvm
check_nvm_installed() {
    if [ -s "$NVM_DIR/nvm.sh" ]; then
        echo "nvm is already installed"
        return 0
    else
        echo "nvm is not installed"
        return 1
    fi
}

# 安装 nvm
install_nvm_linux() {
    curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash
    source "$NVM_DIR/nvm.sh"
}

install_nvm_mac() {
    curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash
    source "$NVM_DIR/nvm.sh"
}

install_nvm_windows() {
    echo "Downloading nvm for Windows..."
    curl -o nvm-setup.exe -L https://github.com/coreybutler/nvm/releases/download/1.1.12/nvm-setup.exe
    echo "Installing nvm for Windows..."
    ./nvm-setup.exe
    echo "nvm version:"
    nvm -v
    echo "nvm installation for Windows completed."
    rm -f nvm-setup.exe
}

# 安装 Node.js 10
install_nodejs() {
    nvm install 10
    nvm use 10
}

# 检查 Node.js 是否安装成功
check_node() {
    node_version=$(node -v)
    echo "Installed Node.js version: $node_version"
    if [[ "$node_version" == v10* ]]; then
        echo "Node.js 10 is installed successfully."
    else
        echo "Node.js 10 is not installed."
        exit 1
    fi
}

# 检测操作系统并安装 nvm（如果需要）
case "$OSTYPE" in
  linux-gnu*)
    if ! check_nvm_installed; then
        install_nvm_linux
    fi
    ;;
  darwin*)
    if ! check_nvm_installed; then
        install_nvm_mac
    fi
    ;;
  cygwin*|msys*|mingw*|bccwin*|wsl*)
    if ! check_nvm_installed; then
        install_nvm_windows
    fi
    ;;
  *)
    echo "Unsupported OS, You could install nvm manually"
    exit 1
    ;;
esac

# 安装 Node.js 10
install_nodejs

check_node



================================================
FILE: markdown_docs/display/book_tools/generate_repoagent_books.md
================================================
## FunctionDef main
**main**: The function of main is to create a directory structure for a book and copy Markdown documentation files into it.

**parameters**: The parameters of this Function.
· parameter1: markdown_docs_folder - The name of the folder containing Markdown documentation files to be copied.
· parameter2: book_name - The name of the book for which the directory structure is created.
· parameter3: repo_path - The path to the repository where the Markdown documentation folder is located.

**Code Description**: The main function begins by retrieving command-line arguments that specify the folder containing Markdown documentation, the desired book name, and the repository path. It constructs the destination directory path where the book's source files will be stored, specifically under './books/{book_name}/src'. It also constructs the source directory path for the Markdown documentation files based on the provided repository path and the specified folder name.

The function then checks if the destination directory exists. If it does not exist, it creates the directory and prints a confirmation message indicating that the directory has been created. 

Next, the function iterates over each item in the source directory. For each item, it constructs the full source and destination paths. If the item is a directory, it uses `shutil.copytree` to recursively copy the entire directory to the destination. If the item is a file, it uses `shutil.copy2` to copy the file to the destination. For each copy operation, a message is printed to confirm the action taken.

Additionally, the function defines a nested function called `create_book_readme_if_not_exist`, which checks for the existence of a README.md file in the destination directory. If the README.md file does not exist, it creates one and writes the book name as the title in Markdown format.

Finally, the main function calls `create_book_readme_if_not_exist` to ensure that a README.md file is created for the book if it is not already present.

**Note**: It is important to ensure that the specified paths and folder names are valid and accessible. The function relies on the presence of the `shutil` and `os` modules, which must be imported for the code to execute successfully. Additionally, the function assumes that the command-line arguments are provided in the correct order and format.
### FunctionDef create_book_readme_if_not_exist(dire)
**create_book_readme_if_not_exist**: The function of create_book_readme_if_not_exist is to create a README.md file in a specified directory if it does not already exist.

**parameters**: The parameters of this Function.
· dire: A string representing the directory path where the README.md file should be created.

**Code Description**: The create_book_readme_if_not_exist function is designed to check for the existence of a README.md file in a specified directory. It takes one parameter, 'dire', which is the path to the directory where the README.md file is intended to be created. 

The function first constructs the full path to the README.md file by joining the provided directory path with the filename 'README.md' using the os.path.join method. It then checks if the file already exists at that path using os.path.exists. If the file does not exist, the function proceeds to create it. 

Within a context manager (using the 'with' statement), the function opens the README.md file in write mode ('w'). This ensures that if the file is created, it will be properly closed after writing. The function writes a header line to the file, formatted as '# {book_name}', where 'book_name' is expected to be a variable that holds the name of the book. However, it is important to note that 'book_name' must be defined in the scope where this function is called, as it is not passed as a parameter to the function itself.

**Note**: It is essential to ensure that the variable 'book_name' is defined before calling this function, as it is used in the content written to the README.md file. Additionally, the function does not handle exceptions that may arise from file operations, so it is advisable to implement error handling if necessary.
***



================================================
FILE: markdown_docs/display/book_tools/generate_summary_from_book.md
================================================
## FunctionDef create_readme_if_not_exist(dire)
**create_readme_if_not_exist**: The function of create_readme_if_not_exist is to create a README.md file in a specified directory if it does not already exist.

**parameters**: The parameters of this Function.
· dire: The directory path where the README.md file should be created.

**Code Description**: The create_readme_if_not_exist function checks if a README.md file exists in the specified directory (denoted by the parameter 'dire'). If the file does not exist, the function creates it and writes a header containing the name of the directory as the title. The path for the README.md file is constructed using the os.path.join method, which combines the directory path with the file name 'README.md'. The function uses os.path.exists to verify the existence of the README.md file. If the file is absent, it opens the file in write mode and writes a formatted string that includes the base name of the directory, which is obtained using os.path.basename.

This function is called by the output_markdown function, which iterates through the contents of a specified directory. During its execution, output_markdown checks each item in the directory; if an item is a subdirectory, it invokes create_readme_if_not_exist to ensure that a README.md file is present in that subdirectory. This relationship indicates that create_readme_if_not_exist is a utility function designed to support the documentation generation process by ensuring that each directory has a README.md file, which can be useful for providing context or information about the contents of the directory.

**Note**: It is important to ensure that the directory path provided to the create_readme_if_not_exist function is valid and accessible. Additionally, the function will only create the README.md file if it does not already exist, preventing overwriting any existing documentation.
## FunctionDef output_markdown(dire, base_dir, output_file, iter_depth)
**output_markdown**: The function of output_markdown is to generate a Markdown-formatted summary of files and directories, including links to README.md files and relevant Markdown files.

**parameters**: The parameters of this Function.
· dire: A string representing the directory path to be processed for Markdown files and subdirectories.
· base_dir: A string representing the base directory path used to create relative paths for the output.
· output_file: A file object where the generated Markdown summary will be written.
· iter_depth: An integer indicating the current depth of recursion, used for formatting the output.

**Code Description**: The output_markdown function is designed to traverse a specified directory (denoted by the parameter 'dire') and its subdirectories, generating a structured Markdown summary of the contents. The function begins by iterating through the files and directories within 'dire'. For each item, it checks if it is a directory. If it is, the function calls create_readme_if_not_exist to ensure that a README.md file exists in that directory. This utility function is crucial for maintaining documentation consistency across directories.

After ensuring that README.md files are present, the function continues to process each item in the directory. If an item is a directory and contains a README.md file, the function creates a relative Markdown link to that file in the output. The relative path is constructed using os.path.relpath to ensure that the link is correctly formatted based on the base directory.

For files that are not directories, the function utilizes is_markdown_file to determine if the file is a Markdown file. If the file is identified as a Markdown file and is not excluded by specific conditions (such as being named 'SUMMARY.md' or 'README.md' at the top level), the function writes a relative link to that file in the output.

The output_markdown function is called by the main function, which serves as the entry point of the program. In main, the function is invoked after creating the necessary directory structure and opening the output file for writing. This relationship indicates that output_markdown is a critical component of the documentation generation process, responsible for compiling the contents of the specified directory into a cohesive Markdown summary.

**Note**: It is important to ensure that the directory path provided to output_markdown is valid and accessible. The function assumes that the output_file is opened in write mode before being passed to it. Additionally, care should be taken to manage the depth of recursion, as excessive nesting may lead to performance issues or stack overflow errors.
## FunctionDef markdown_file_in_dir(dire)
**markdown_file_in_dir**: The function of markdown_file_in_dir is to check whether any Markdown file (with .md or .markdown extension) exists in a specified directory or its subdirectories.

**parameters**: 
- parameter1: dire (str) - The directory path to be searched for Markdown files.

**Code Description**: 
The function `markdown_file_in_dir` is designed to traverse a specified directory (`dire`) and its subdirectories to check for the existence of files with `.md` or `.markdown` extensions. It utilizes Python's `os.walk` function to walk through the directory tree, where `root` is the current directory path, `dirs` is a list of subdirectories, and `files` is a list of filenames in the current directory.

For each file in the list `files`, the function checks whether the filename matches the regular expression pattern `'.md$|.markdown$'`, which identifies files with the `.md` or `.markdown` extensions. If such a file is found, the function immediately returns `True`, indicating that at least one Markdown file exists within the directory or its subdirectories.

If no Markdown files are found during the entire directory traversal, the function returns `False`.

**Note**: 
- The function stops as soon as a Markdown file is found and returns `True`, which means it does not continue searching further once the condition is met.
- The function uses regular expressions to identify files with `.md` or `.markdown` extensions. Be aware that this check is case-sensitive by default, meaning it will only match lowercase `.md` or `.markdown`. If case-insensitive matching is needed, the regular expression pattern can be modified accordingly.
- This function only returns a Boolean value (True or False). It does not provide any information about the specific files found, just the presence or absence of such files.

**Output Example**:
- If there is at least one `.md` or `.markdown` file in the directory, the return value would be:
  `True`
- If there are no `.md` or `.markdown` files in the directory, the return value would be:
  `False`
## FunctionDef is_markdown_file(filename)
**is_markdown_file**: The function of is_markdown_file is to determine if a given filename corresponds to a Markdown file and return the filename without its extension if it does.

**parameters**: The parameters of this Function.
· filename: A string representing the name of the file to be checked.

**Code Description**: The is_markdown_file function uses a regular expression to check if the provided filename ends with either '.md' or '.markdown'. If the filename does not match either of these patterns, the function returns False, indicating that the file is not a Markdown file. If the filename matches '.md', the function returns the filename without the last three characters (the '.md' extension). If the filename matches '.markdown', it returns the filename without the last nine characters (the '.markdown' extension). 

This function is called within the output_markdown function, which is responsible for generating a Markdown-formatted summary of files and directories. In output_markdown, the is_markdown_file function is used to filter out files that are Markdown files. Specifically, it checks each file in the specified directory and its subdirectories. If a file is identified as a Markdown file (and is not 'SUMMARY.md' or 'README.md' under certain conditions), its relative path is formatted and written to the output file. This relationship highlights the utility of is_markdown_file in ensuring that only relevant Markdown files are included in the generated summary.

**Note**: It is important to ensure that the filename passed to the function is a valid string. The function does not handle exceptions for invalid inputs, so care should be taken to validate the input before calling this function.

**Output Example**: 
- If the input is 'example.md', the output will be 'example'.
- If the input is 'document.markdown', the output will be 'document'.
- If the input is 'image.png', the output will be False.
## FunctionDef main
**main**: The function of main is to generate a Markdown summary file for a specified book by creating the necessary directory structure and invoking the output_markdown function.

**parameters**: The parameters of this Function.
· book_name: A string representing the name of the book, which is passed as a command-line argument.

**Code Description**: The main function serves as the entry point for the script, responsible for orchestrating the creation of a Markdown summary file for a book. It begins by retrieving the book name from the command-line arguments using `sys.argv[1]`. This book name is then used to construct the path for the source directory where the summary will be generated, specifically `./books/{book_name}/src`.

The function checks if the specified directory exists using `os.path.exists(dir_input)`. If the directory does not exist, it creates the directory structure using `os.makedirs(dir_input)`. This ensures that the environment is prepared for the subsequent operations.

Once the directory is confirmed to exist, the function proceeds to create the summary file named 'SUMMARY.md' within the specified directory. It opens this file in write mode using `open(output_path, 'w')` and writes a header '# Summary\n\n' to initialize the content.

The core functionality of generating the summary is delegated to the `output_markdown` function. This function is called with the parameters `dir_input`, `dir_input` (as the base directory), and the opened output file. The `output_markdown` function is responsible for traversing the directory structure, identifying Markdown files, and generating the appropriate links in the summary file.

After the summary generation process is completed, the function prints a confirmation message indicating that the GitBook auto summary has finished. The function concludes by returning 0, signaling successful execution.

The relationship with the `output_markdown` function is crucial, as it handles the detailed processing of the directory contents and the creation of the Markdown links, making it an integral part of the summary generation workflow.

**Note**: It is important to ensure that the book name provided as a command-line argument is valid and corresponds to an existing book directory structure. The function assumes that the necessary permissions are in place for creating directories and files in the specified path.

**Output Example**: 
When executed with a valid book name, the function will create a directory structure like:
```
./books/
    └── example_book/
        └── src/
            └── SUMMARY.md
```
The content of 'SUMMARY.md' might look like:
```
# Summary

- [Chapter 1](./chapter1.md)
- [Chapter 2](./chapter2.md)
- [Subdirectory](./subdirectory/README.md)
```



================================================
FILE: markdown_docs/repo_agent/change_detector.md
================================================
## ClassDef ChangeDetector
**ChangeDetector**: The function of ChangeDetector is to handle file differences and change detection in a Git repository.

**attributes**: The attributes of this Class.
· repo_path: The path to the repository.
· repo: An instance of the Git repository initialized with the provided repo_path.

**Code Description**: The ChangeDetector class is designed to facilitate the detection of changes in files within a Git repository. It utilizes the GitPython library to interact with the Git repository, allowing it to track staged and unstaged changes effectively. 

Upon initialization, the class requires a repository path, which it uses to create a Git repository object. This object serves as the primary interface for executing Git commands and retrieving information about the repository's state.

The class includes several methods:

1. **get_staged_pys**: This method retrieves Python files that have been staged for commit. It checks the differences between the staging area and the last commit (HEAD) to identify files that are either newly added or modified. The method returns a dictionary where the keys are the file paths and the values are booleans indicating whether the file is new.

2. **get_file_diff**: This method fetches the differences for a specific file. If the file is new, it stages the file first and then retrieves the differences from the staging area. For existing files, it retrieves the differences from the last commit. The result is a list of changes made to the file.

3. **parse_diffs**: This method processes the list of differences obtained from get_file_diff. It extracts added and removed lines, returning a structured dictionary that categorizes the changes.

4. **identify_changes_in_structure**: This method analyzes the changed lines to determine which functions or classes have been modified. It checks if the changed lines fall within the start and end lines of known structures and records the changes accordingly.

5. **get_to_be_staged_files**: This method identifies files that are modified but not yet staged, based on specific conditions, such as whether a corresponding Markdown file exists for a staged Python file. It returns a list of paths to these files.

6. **add_unstaged_files**: This method stages the identified unstaged files that meet certain conditions, preparing them for the next commit.

The ChangeDetector class is instantiated in the Runner class of the project, where it is used to monitor changes in the repository. The Runner class initializes the ChangeDetector with the target repository path, allowing it to leverage its methods for detecting and managing file changes. This integration ensures that the project can effectively track modifications and prepare files for version control.

**Note**: When using the ChangeDetector class, ensure that the repository path is correctly specified and that the GitPython library is properly installed and configured. The methods are designed to interact with the Git command line, so the underlying Git environment must be accessible.

**Output Example**: A possible output from the get_staged_pys method could be:
```python
{
    'new_test_file.py': True,
    'existing_file.py': False
}
```
This output indicates that 'new_test_file.py' is a newly added file, while 'existing_file.py' has been modified but was already present in the repository.
### FunctionDef __init__(self, repo_path)
**__init__**: __init__的功能是初始化一个ChangeDetector对象。

**parameters**: 该函数的参数。
· repo_path: 一个字符串，表示仓库的路径。

**Code Description**: 该函数是ChangeDetector类的构造函数，用于初始化一个ChangeDetector对象。在调用该函数时，必须提供一个参数repo_path，该参数是一个字符串，表示要监测的Git仓库的路径。函数内部将传入的repo_path赋值给实例变量self.repo_path，以便在对象的其他方法中使用。此外，该函数还使用git库中的Repo类来创建一个新的Repo对象，并将其赋值给self.repo，这样可以通过该对象与指定的Git仓库进行交互。

**Note**: 使用该代码时，请确保提供的repo_path是一个有效的Git仓库路径，否则将会引发错误。确保在调用该构造函数之前，已安装并正确配置了git库。
***
### FunctionDef get_staged_pys(self)
**get_staged_pys**: The function of get_staged_pys is to retrieve a dictionary of Python files that have been staged in the Git repository.

**parameters**: The parameters of this Function.
· None

**Code Description**: The get_staged_pys function is designed to identify and return a collection of Python files that have been staged in the Git repository. It utilizes the GitPython library to access the repository's index and compare the current state of staged files against the last commit (HEAD). The function specifically looks for files that have been added or modified, indicated by the change types "A" (added) and "M" (modified). 

The function begins by initializing an empty dictionary called staged_files, which will store the paths of the staged Python files as keys and a boolean value indicating whether each file is newly created as the corresponding value. The core logic of the function involves calling the repo.index.diff("HEAD", R=True) method, which retrieves the differences between the current staging area and the last commit. The R=True parameter is crucial as it reverses the comparison logic, allowing the function to correctly identify newly added files that do not exist in the HEAD commit.

The function then iterates over the differences obtained from the diff call. For each difference, it checks if the change type is either "A" or "M" and if the file path ends with the ".py" extension, ensuring that only Python files are considered. If a file is determined to be newly created (change type "A"), the function marks it as such in the staged_files dictionary.

This function is called within the test_get_staged_pys method of the TestChangeDetector class, which is part of the testing suite for the ChangeDetector functionality. In the test, a new Python file is created and staged using the Git command. The get_staged_pys function is then invoked to verify that the newly created file is correctly identified as staged. The test asserts that the new file appears in the list of staged files, demonstrating the function's effectiveness in tracking changes to Python files in the repository.

**Note**: It is important to ensure that the GitPython library is properly configured and that the repository is in a valid state for the function to operate correctly.

**Output Example**: An example of the return value from get_staged_pys might look like this:
{
    'new_test_file.py': True,
    'existing_file.py': False
}
In this example, 'new_test_file.py' is a newly created file, while 'existing_file.py' has been modified but was already present in the repository.
***
### FunctionDef get_file_diff(self, file_path, is_new_file)
**get_file_diff**: The function of get_file_diff is to retrieve the changes made to a specific file.

**parameters**: The parameters of this Function.
· file_path: The relative path of the file.
· is_new_file: Indicates whether the file is a new file.

**Code Description**: The get_file_diff function is designed to obtain the differences in a specified file within a Git repository. It takes two parameters: file_path, which is a string representing the relative path of the file in the repository, and is_new_file, a boolean that indicates whether the file is newly created or an existing one.

When is_new_file is set to True, the function first stages the new file by executing a Git command to add it to the staging area. This is done using the subprocess module to run the command `git -C {repo.working_dir} add {file_path}`. After staging the file, it retrieves the differences using `repo.git.diff("--staged", file_path)`, which provides the changes that have been staged for the new file.

If is_new_file is False, the function retrieves the differences from the last committed state (HEAD) using `repo.git.diff("HEAD", file_path)`. The differences are then split into lines and returned as a list.

This function is called by the process_file_changes method in the Runner class. The process_file_changes method is responsible for processing changes in files detected in a repository. It utilizes get_file_diff to obtain the changes in the specified file, which are then parsed and analyzed to identify structural changes in the code. The results are logged and may lead to updates in a JSON file that tracks project hierarchy or the generation of Markdown documentation for the changed file.

**Note**: It is important to ensure that the file path provided is correct and that the Git repository is properly initialized and accessible. Additionally, the subprocess module requires appropriate permissions to execute Git commands.

**Output Example**: An example of the output from get_file_diff might look like the following:
```
[
    "- def old_function():",
    "+ def new_function():",
    "    print('This is a new function')"
]
```
***
### FunctionDef parse_diffs(self, diffs)
**parse_diffs**: The function of parse_diffs is to parse the difference content and extract the added and deleted object information from a list of diffs.

**parameters**: The parameters of this Function.
· diffs: A list containing difference content. Obtained by the get_file_diff() function inside the class.

**Code Description**: The parse_diffs function processes a list of differences (diffs) typically generated by a version control system like Git. It identifies lines that have been added or removed in the context of a file's changes. The function initializes a dictionary called changed_lines to store the results, which includes two keys: "added" and "removed". Each key holds a list of tuples, where each tuple contains the line number and the corresponding line content.

The function iterates through each line in the diffs list. It first checks for line number information using a regular expression that matches the format of diff headers (e.g., "@@ -43,33 +43,40 @@"). If a match is found, it updates the current line numbers for both the original and changed content. 

For lines that start with a "+", indicating an addition, the function appends the line number and content (excluding the "+") to the "added" list. Conversely, lines that start with a "-", indicating a removal, are appended to the "removed" list. If a line does not indicate a change, the function increments both line numbers to account for unchanged lines.

The output of this function is a dictionary that provides a structured representation of the changes, allowing other parts of the code to easily access information about what has been added or removed.

The parse_diffs function is called within the process_file_changes method of the Runner class. This method is responsible for processing changes in files detected in a repository. It retrieves the diffs for a specific file using the get_file_diff function and then passes this list to parse_diffs to obtain structured information about the changes. The results are subsequently used to identify changes in the file's structure and update relevant documentation accordingly.

**Note**: It is important to understand that the additions identified by this function do not necessarily indicate newly created objects; modifications in the code are represented as both deletions and additions in the diff output. To determine if an object is newly added, the get_added_objs() function should be used.

**Output Example**: A possible appearance of the code's return value could be:
{
    'added': [
        (86, '    '),
        (87, '    def to_json_new(self, comments = True):'),
        (88, '        data = {'),
        (89, '            "name": self.node_name,'),
        (95, '')
    ],
    'removed': []
}
***
### FunctionDef identify_changes_in_structure(self, changed_lines, structures)
**identify_changes_in_structure**: The function of identify_changes_in_structure is to identify the structures (functions or classes) that have changed in a given set of modified lines of code.

**parameters**: The parameters of this Function.
· changed_lines: A dictionary containing the line numbers where changes have occurred, structured as {'added': [(line number, change content)], 'removed': [(line number, change content)]}.
· structures: A list of structures (functions or classes) obtained from get_functions_and_classes, where each structure is represented by its type, name, start line number, end line number, and parent structure name.

**Code Description**: The identify_changes_in_structure function processes a dictionary of changed lines and a list of structures to determine which functions or classes have been modified. It initializes a result dictionary, changes_in_structures, with keys 'added' and 'removed', both containing empty sets. The function then iterates through each change type (either 'added' or 'removed') and the corresponding lines. For each line number that has changed, it checks against the list of structures to see if the line number falls within the start and end line numbers of any structure. If a match is found, the structure's name and its parent structure's name are added to the appropriate set in the changes_in_structures dictionary.

This function is called by the process_file_changes method in the Runner class. In that context, it is used to analyze changes detected in a Python file, where it receives the changed lines and the structures of the file. The output of identify_changes_in_structure is then logged and can be used to update project documentation or JSON structure information. This integration ensures that any modifications in the codebase are accurately reflected in the project's metadata and documentation.

**Note**: It is important to ensure that the structures provided to this function are accurate and up-to-date, as any discrepancies may lead to incorrect identification of changes.

**Output Example**: An example of the function's return value could be: {'added': {('NewFunction', 'ParentClass'), ('AnotherFunction', None)}, 'removed': set()}. This indicates that 'NewFunction' was added under 'ParentClass', while no functions were removed.
***
### FunctionDef get_to_be_staged_files(self)
**get_to_be_staged_files**: The function of get_to_be_staged_files is to retrieve all unstaged files in the repository that meet specific conditions for staging.

**parameters**: The parameters of this Function.
· No parameters are required for this function.

**Code Description**: The get_to_be_staged_files method is designed to identify and return a list of file paths that are either modified but not staged or untracked, based on certain criteria. The method performs the following operations:

1. It initializes an empty list called to_be_staged_files to store the paths of files that need to be staged.
2. It retrieves a list of already staged files by comparing the current index with the HEAD commit using the Git repository's diff method.
3. The method then fetches the current project settings using the SettingsManager's get_setting method, which provides access to configuration details such as project hierarchy and markdown documentation folder.
4. It gathers a list of all unstaged changes (diffs) in the repository and identifies untracked files that exist in the working directory but have not been added to the staging area.
5. The method iterates through the untracked files and checks if they meet the following conditions:
   - If the untracked file's path starts with the markdown documentation folder name, it is added to the to_be_staged_files list.
   - If the untracked file is a markdown file (.md) and has a corresponding Python file (.py) that is already staged, the markdown file is also added to the list.
   - If the untracked file's path matches the project hierarchy, it is added to the list as well.
6. The method then processes the unstaged files, similarly checking if they are markdown files or match the project hierarchy, and adds them to the to_be_staged_files list if they meet the criteria.
7. Finally, the method returns the list of paths that need to be staged.

This method is called by the add_unstaged_files method within the ChangeDetector class, which utilizes the output of get_to_be_staged_files to determine which files should be added to the staging area. Additionally, it is tested in the TestChangeDetector class through unit tests that verify its functionality by checking if modified markdown files are correctly identified as unstaged.

**Note**: It is important to ensure that the repository is in a clean state and that the project settings are correctly configured before invoking this method, as any discrepancies may lead to inaccurate results.

**Output Example**: A possible appearance of the code's return value when calling get_to_be_staged_files could be:
```
[
    'path/to/repo/markdown_docs/test_file.md',
    'path/to/repo/markdown_docs/another_file.md',
    'path/to/repo/documentation'
]
```
***
### FunctionDef add_unstaged_files(self)
**add_unstaged_files**: The function of add_unstaged_files is to add unstaged files that meet specific conditions to the staging area of a Git repository.

**parameters**: The parameters of this Function.
· No parameters are required for this function.

**Code Description**: The add_unstaged_files method is designed to identify and stage files in a Git repository that are currently unstaged but meet certain criteria for staging. This function operates as follows:

1. It first calls the get_to_be_staged_files method, which retrieves a list of file paths for all unstaged files that meet specific conditions. These conditions typically include files that are modified but not staged or untracked files that should be staged based on project settings.

2. The method then iterates over the list of unstaged files obtained from get_to_be_staged_files. For each file path, it constructs a Git command to add the file to the staging area. The command is formatted as `git -C {self.repo.working_dir} add {file_path}`, where `self.repo.working_dir` is the path to the working directory of the repository.

3. The subprocess.run function is used to execute the constructed Git command. The `shell=True` argument allows the command to be run in the shell, and `check=True` ensures that an exception is raised if the command fails.

4. After processing all unstaged files, the method returns the list of file paths that were identified as needing to be staged.

This method is called by the run method in the Runner class, which is responsible for managing the document update process. The run method detects changes in the repository, processes them, and ultimately invokes add_unstaged_files to ensure that any newly generated or modified Markdown files are added to the staging area. Additionally, it is also called in the process_file_changes method, which handles changes to individual files and ensures that any corresponding documentation is updated and staged.

The add_unstaged_files method is crucial for maintaining an accurate staging area in the Git repository, particularly in workflows that involve automatic documentation generation based on changes in Python files.

**Note**: It is important to ensure that the repository is in a clean state and that the project settings are correctly configured before invoking this method, as any discrepancies may lead to inaccurate results.

**Output Example**: A possible appearance of the code's return value when calling add_unstaged_files could be:
```
[
    'path/to/repo/markdown_docs/test_file.md',
    'path/to/repo/markdown_docs/another_file.md',
    'path/to/repo/documentation'
]
```
***



================================================
FILE: markdown_docs/repo_agent/chat_engine.md
================================================
## ClassDef ChatEngine
Doc is waiting to be generated...
### FunctionDef __init__(self, project_manager)
**__init__**: The function of __init__ is to initialize an instance of the ChatEngine class with the necessary configuration settings for the OpenAI API.

**parameters**: The parameters of this Function.
· project_manager: An instance of the ProjectManager class that is responsible for managing the overall project workflow and interactions.

**Code Description**: The __init__ method of the ChatEngine class is designed to set up the initial state of the ChatEngine instance by configuring it with the appropriate settings for the OpenAI API. Upon instantiation, the method first retrieves the current configuration settings by calling the `get_setting` method from the SettingsManager class. This method ensures that the settings are accessed in a consistent manner throughout the application, adhering to the Singleton design pattern.

The retrieved settings include critical parameters such as the OpenAI API key, the base URL for API requests, the timeout duration for requests, the model to be used for chat completions, and the temperature setting that influences the randomness of the generated responses. These parameters are essential for the ChatEngine to function correctly and interact with the OpenAI API effectively.

The OpenAI instance is then created using these settings, allowing the ChatEngine to perform chat-related functionalities, such as generating responses based on user input. The integration of the SettingsManager ensures that the ChatEngine is always configured with the latest settings, promoting maintainability and reducing the risk of errors due to misconfiguration.

From a functional perspective, the ChatEngine class relies on the SettingsManager to provide the necessary configuration settings, which are crucial for its operation. This relationship exemplifies the design principle of separation of concerns, where the SettingsManager handles the management of configuration settings, while the ChatEngine focuses on its primary functionality of facilitating chat interactions.

**Note**: It is important to ensure that the SettingsManager is properly configured and that the Setting class contains valid attributes before instantiating the ChatEngine. Any misconfiguration may lead to runtime errors or unexpected behavior when the ChatEngine attempts to utilize the OpenAI API settings.
***
### FunctionDef build_prompt(self, doc_item)
Doc is waiting to be generated...
#### FunctionDef get_referenced_prompt(doc_item)
**get_referenced_prompt**: The function of get_referenced_prompt is to generate a formatted string that summarizes the references made by a given DocItem, including details about the referenced objects and their documentation.

**parameters**: The parameters of this Function.
· doc_item: An instance of the DocItem class, which contains information about the documentation item and its references.

**Code Description**: The get_referenced_prompt function is designed to create a prompt that outlines the references associated with a specific DocItem. It first checks if the provided doc_item has any references by evaluating the length of the reference_who attribute, which is a list of DocItem instances that reference the current item. If there are no references, the function returns an empty string.

If references are present, the function initializes a list called prompt with a predefined introductory string. It then iterates over each reference_item in the doc_item.reference_who list. For each reference_item, the function constructs a detailed string (instance_prompt) that includes the full name of the referenced object, its corresponding documentation content, and the raw code associated with it. The get_full_name method of the reference_item is called to retrieve its full hierarchical name, ensuring clarity in the context of the documentation.

The instance_prompt is formatted to include the object's name, its documentation (if available), and the raw code, all separated by a visual divider. Each instance_prompt is appended to the prompt list. Finally, the function joins all elements of the prompt list into a single string, separated by newline characters, and returns this string.

This function is particularly useful in the context of generating documentation, as it provides a clear overview of how different documentation items are interconnected through references. It aids in understanding the relationships between various code elements, which is essential for maintaining comprehensive and accurate documentation.

**Note**: When using the get_referenced_prompt function, ensure that the doc_item passed to it has been properly initialized and contains valid references. This will guarantee that the generated prompt accurately reflects the relationships and documentation of the referenced items.

**Output Example**: An example output of the get_referenced_prompt function for a DocItem with references might look like this:
```
As you can see, the code calls the following objects, their code and docs are as following:
obj: repo_agent/doc_meta_info.py/DocItem
Document: 
**DocItem**: The function of DocItem is to represent individual documentation items within a project, encapsulating their metadata and relationships.
Raw code:```
class DocItem:
    ...
```
obj: repo_agent/another_file.py/AnotherClass
Document: 
**AnotherClass**: This class serves a different purpose within the project.
Raw code:```
class AnotherClass:
    ...
```
```
***
#### FunctionDef get_referencer_prompt(doc_item)
**get_referencer_prompt**: The function of get_referencer_prompt is to generate a prompt string that lists all the objects that reference a given documentation item, along with their associated documentation and code.

**parameters**: The parameters of this Function.
· doc_item: An instance of the DocItem class, which represents the documentation item for which the referencing objects are being retrieved.

**Code Description**: The get_referencer_prompt function is designed to create a formatted string that provides information about the objects that reference a specific documentation item. It begins by checking if the provided doc_item has any references in its who_reference_me attribute, which is a list of DocItem instances that reference the current item. If this list is empty, the function returns an empty string, indicating that there are no references to display.

If there are references, the function initializes a prompt list with a header string that introduces the subsequent information. It then iterates over each DocItem in the who_reference_me list. For each referencing item, it constructs a detailed string that includes the full name of the referencing object (obtained by calling the get_full_name method on the referencer_item), the last version of its markdown content (if available), and its raw code content (if present). Each of these details is formatted in a readable manner, separated by line breaks and a visual divider.

Finally, the function joins all the strings in the prompt list into a single string, separated by newline characters, and returns this formatted string. This output serves as a comprehensive reference for developers, allowing them to quickly understand which objects are related to the given documentation item and to access their associated documentation and code.

The get_referencer_prompt function is particularly useful in the context of documentation generation and management, as it helps to clarify the relationships between different code elements. By providing a clear overview of the references, it aids developers in navigating the documentation and understanding the dependencies within the codebase.

**Note**: When using this function, ensure that the doc_item parameter is a properly initialized instance of the DocItem class with an established hierarchy and references. This will ensure accurate and meaningful output.

**Output Example**: An example output of the get_referencer_prompt function might look like this:
```
Also, the code has been called by the following objects, their code and docs are as following:
obj: repo_agent/doc_meta_info.py/DocItem
Document: 
This is a documentation item that describes a specific code element.
Raw code:```
class DocItem:
    ...
```
==========
obj: repo_agent/another_file.py/AnotherClass
Document: 
This class interacts with the DocItem and provides additional functionality.
Raw code:```
class AnotherClass:
    ...
```
```
***
#### FunctionDef get_relationship_description(referencer_content, reference_letter)
**get_relationship_description**: The function of get_relationship_description is to generate a descriptive string regarding the relationship of a referencer with its callers and callees based on the provided inputs.

**parameters**: The parameters of this Function.
· referencer_content: A boolean indicating whether there is content related to the referencer.
· reference_letter: A boolean indicating whether there is a reference letter available.

**Code Description**: The get_relationship_description function evaluates the presence of two boolean parameters: referencer_content and reference_letter. It constructs and returns a specific string based on the combination of these parameters. 

- If both referencer_content and reference_letter are true, the function returns a string that requests the inclusion of the reference relationship with both callers and callees from a functional perspective.
- If only referencer_content is true, it returns a string that requests the inclusion of the relationship with callers from a functional perspective.
- If only reference_letter is true, it returns a string that requests the inclusion of the relationship with callees from a functional perspective.
- If neither parameter is true, the function returns an empty string.

This design allows for flexible output based on the available information regarding the referencer, ensuring that the user receives relevant instructions based on the context provided.

**Note**: It is important to ensure that the parameters are boolean values, as the function logic relies on their truthiness to determine the appropriate output. Providing non-boolean values may lead to unexpected results.

**Output Example**: 
- If both parameters are true: "And please include the reference relationship with its callers and callees in the project from a functional perspective."
- If only referencer_content is true: "And please include the relationship with its callers in the project from a functional perspective."
- If only reference_letter is true: "And please include the relationship with its callees in the project from a functional perspective."
- If neither parameter is true: "" (an empty string).
***
***
### FunctionDef generate_doc(self, doc_item)
Doc is waiting to be generated...
***



================================================
FILE: markdown_docs/repo_agent/file_handler.md
================================================
## ClassDef FileHandler
# Class `FileHandler`

The `FileHandler` class provides a set of methods to interact with files within a Git repository, specifically for handling changes, reading file contents, extracting code information, and writing back changes to the repository. This class allows for tasks such as retrieving modified file versions, extracting function and class structures from code, and generating project file structures using Abstract Syntax Tree (AST) parsing.

## Methods Overview

### `__init__(self, repo_path, file_path)`
Initializes a `FileHandler` instance with the given repository and file path.

#### Parameters:
- `repo_path` (str): The absolute path to the Git repository.
- `file_path` (str): The relative path of the file within the repository.

### `read_file(self)`
Reads the contents of the file specified by `file_path`.

#### Returns:
- `str`: The content of the current file.

### `get_obj_code_info(self, code_type, code_name, start_line, end_line, params, file_path=None)`
Retrieves detailed information about a given code object (e.g., function or class) in the file.

#### Parameters:
- `code_type` (str): The type of the code object (e.g., 'FunctionDef', 'ClassDef').
- `code_name` (str): The name of the code object.
- `start_line` (int): The starting line number of the code object.
- `end_line` (int): The ending line number of the code object.
- `params` (list): A list of parameters associated with the code object.
- `file_path` (str, optional): The path to the file containing the code object. Defaults to `None`, in which case the `file_path` provided during initialization is used.

#### Returns:
- `dict`: A dictionary containing information about the code object, including its content, line numbers, and parameters.

### `write_file(self, file_path, content)`
Writes the provided content to a file at the specified path.

#### Parameters:
- `file_path` (str): The relative path of the file to write to.
- `content` (str): The content to write into the file.

### `get_modified_file_versions(self)`
Retrieves the current and previous versions of a modified file.

#### Returns:
- `tuple`: A tuple containing:
  - `current_version` (str): The content of the current version of the file.
  - `previous_version` (str): The content of the previous version of the file (from the last Git commit).

### `get_end_lineno(self, node)`
Gets the end line number of a given AST node.

#### Parameters:
- `node`: The AST node for which to determine the end line number.

#### Returns:
- `int`: The end line number of the node, or `-1` if no line number is available.

### `add_parent_references(self, node, parent=None)`
Recursively adds a reference to the parent node for all child nodes in an Abstract Syntax Tree (AST).

#### Parameters:
- `node`: The AST node to start from.
- `parent` (optional): The parent node, which defaults to `None`.

#### Returns:
- `None`

### `get_functions_and_classes(self, code_content)`
Extracts all functions, classes, and their parameters from a given code content, including hierarchical relationships.

#### Parameters:
- `code_content` (str): The code content to parse.

#### Returns:
- `list`: A list of tuples, each containing:
  - The type of the node (e.g., `FunctionDef`, `ClassDef`),
  - The name of the node,
  - The starting line number,
  - The ending line number,
  - The list of parameters (if any).

### `generate_file_structure(self, file_path)`
Generates the file structure of a given file, including all functions, classes, and their parameters.

#### Parameters:
- `file_path` (str): The relative path of the file to process.

#### Returns:
- `list`: A list of dictionaries, each containing code information for a function or class in the file.

### `generate_overall_structure(self, file_path_reflections, jump_files)`
Generates the overall file structure for a repository, parsing all relevant files and skipping files that are either ignored or not staged.

#### Parameters:
- `file_path_reflections` (dict): A dictionary mapping file paths to their corresponding reflections (for handling fake files or renamed files).
- `jump_files` (list): A list of files to skip during processing.

#### Returns:
- `dict`: A dictionary representing the overall structure of the repository, with file paths as keys and lists of code object information as values.

### `convert_to_markdown_file(self, file_path=None)`
Converts the content of a file to markdown format.

#### Parameters:
- `file_path` (str, optional): The relative path of the file to convert. If not provided, the default `file_path` will be used.

#### Returns:
- `str`: The content of the file in markdown format.

#### Raises:
- `ValueError`: If no file object is found for the specified file path.

---

## Usage Example

```python
# Initialize the FileHandler with the repository path and file path
file_handler = FileHandler(repo_path="/path/to/repo", file_path="src/example.py")

# Read the content of the file
file_content = file_handler.read_file()

# Get code information for a function named 'example_function'
code_info = file_handler.get_obj_code_info(
    code_type="FunctionDef",
    code_name="example_function",
    start_line=10,
    end_line=20,
    params=["param1", "param2"]
)

# Write new content to the file
file_handler.write_file(file_path="src/example.py", content="new content")

# Get the current and previous versions of the modified file
current_version, previous_version = file_handler.get_modified_file_versions()

# Generate the file structure for a given file
file_structure = file_handler.generate_file_structure(file_path="src/example.py")

# Generate the overall file structure for the repository, skipping specified files
repo_structure = file_handler.generate_overall_structure(file_path_reflections={}, jump_files=["skip_file.py"])

# Convert the file content to markdown
markdown_content = file_handler.convert_to_markdown_file(file_path="src/example.py")
```

## Dependencies
- `os`: For file path manipulation and file operations.
- `gitpython`: For interacting with the Git repository.
- `ast`: For parsing Python code into an Abstract Syntax Tree.
- `tqdm`: For progress bar display during repository processing.
- `logging`: For logging error messages.

The `FileHandler` class provides an effective set of utilities for managing and analyzing code files in a Git repository, making it ideal for scenarios involving file change tracking, code analysis, and file versioning.
### FunctionDef __init__(self, repo_path, file_path)
**__init__**: The function of __init__ is to initialize an instance of the FileHandler class with the specified repository and file paths.

**parameters**: The parameters of this Function.
· repo_path: This parameter represents the path to the repository where the project files are located. It is expected to be an absolute or relative path that points to the root of the repository.
· file_path: This parameter is the path to a specific file within the repository. It should be a path relative to the root directory of the repository.

**Code Description**: The __init__ method serves as the constructor for the FileHandler class. It initializes the instance by setting two attributes: `file_path` and `repo_path`. The `file_path` attribute is assigned the value of the `file_path` parameter, which is intended to be relative to the root directory of the repository. The `repo_path` attribute is similarly assigned the value of the `repo_path` parameter, establishing a reference to the repository's location.

Additionally, the method retrieves the current project settings by invoking the `get_setting` method from the SettingsManager class. This call ensures that the FileHandler instance has access to the latest configuration settings defined for the project. The retrieved settings are then used to construct the `project_hierarchy` attribute, which combines the target repository path with the hierarchy name specified in the project settings. This hierarchical structure is essential for managing files and directories within the project context.

The relationship with the SettingsManager is critical, as it centralizes the configuration management for the project. By utilizing the `get_setting` method, the FileHandler class ensures that it operates with the most up-to-date settings, which may include paths, logging configurations, and other project-specific parameters. This design promotes consistency and reduces the risk of errors that could arise from hardcoded values or outdated configurations.

**Note**: It is important to ensure that the SettingsManager is properly configured before instantiating the FileHandler class. Any misconfiguration in the settings may lead to runtime errors or unexpected behavior when accessing the project hierarchy or file paths.
***
### FunctionDef read_file(self)
**read_file**: read_file的功能是读取当前更改文件的内容。

**parameters**: 该函数没有参数。

**Code Description**: 
read_file函数用于读取指定路径的文件内容。它首先通过os.path.join方法将存储库路径（repo_path）和文件路径（file_path）组合成一个绝对文件路径（abs_file_path）。接着，函数以只读模式打开该文件，并使用UTF-8编码读取文件的全部内容。读取完成后，函数将文件内容作为字符串返回。

在项目中，read_file函数被多个对象调用。具体来说，在repo_agent/runner.py中的add_new_item和process_file_changes方法中都有调用。add_new_item方法使用read_file函数来获取文件的源代码，以便提取文件中的函数和类信息，并生成相应的文档。process_file_changes方法则在处理文件变更时调用read_file，获取整个Python文件的代码，以便分析文件的变更情况。这表明read_file函数在文件处理和文档生成的过程中起到了关键作用。

**Note**: 使用该函数时，请确保提供的repo_path和file_path是有效的路径，以避免文件读取错误。

**Output Example**: 假设文件内容为“Hello, World!”，则该函数的返回值将是字符串“Hello, World!”。
***
### FunctionDef get_obj_code_info(self, code_type, code_name, start_line, end_line, params, file_path)
**get_obj_code_info**: The function of get_obj_code_info is to retrieve detailed information about a specific code segment within a file.

**parameters**: The parameters of this Function.
· code_type: A string representing the type of the code being analyzed.
· code_name: A string indicating the name of the code object.
· start_line: An integer specifying the starting line number of the code segment.
· end_line: An integer specifying the ending line number of the code segment.
· params: A collection of parameters associated with the code.
· file_path: An optional string that provides the path to the file. If not specified, it defaults to None.

**Code Description**: The get_obj_code_info function is designed to extract and return information about a specific segment of code from a file. It takes in several parameters that define the characteristics of the code segment, including its type, name, and the range of lines it occupies. The function initializes a dictionary, code_info, to store various attributes related to the code segment.

The function opens the specified file in read mode and reads all lines into a list. It then concatenates the lines from start_line to end_line to form the complete code content. Additionally, it checks for the presence of the code_name in the first line of the specified range to determine its column position. The function also checks if the code segment contains a return statement, which is a common indicator of a function's output.

Finally, the function populates the code_info dictionary with the gathered information, including the type, name, start and end lines, parameters, the presence of a return statement, the code content, and the column position of the code name. The populated dictionary is then returned as the output of the function.

**Note**: It is important to ensure that the specified start_line and end_line are valid and within the bounds of the file's total line count to avoid potential errors when reading the file. The file_path parameter should be correctly set to point to the desired file location.

**Output Example**: A possible return value of the function could look like this:
{
    "type": "function",
    "name": "calculate_sum",
    "md_content": [],
    "code_start_line": 10,
    "code_end_line": 15,
    "params": ["a", "b"],
    "have_return": true,
    "code_content": "def calculate_sum(a, b):\n    return a + b\n",
    "name_column": 4
}
***
### FunctionDef write_file(self, file_path, content)
**write_file**: write_file的功能是将内容写入指定路径的文件中。

**parameters**: 该函数的参数如下：
· parameter1: file_path (str) - 文件的相对路径。
· parameter2: content (str) - 要写入文件的内容。

**Code Description**: write_file函数用于将指定内容写入到给定的文件路径。首先，该函数会检查file_path是否为绝对路径，如果是，则去掉路径开头的斜杠，以确保file_path是相对路径。接着，函数通过os.path.join将repo_path与file_path组合成绝对路径abs_file_path，并使用os.makedirs确保该路径的目录存在，如果不存在则创建它。然后，函数以写入模式打开文件，并将内容写入该文件，使用utf-8编码格式。

在项目中，write_file函数被Runner类中的add_new_item和process_file_changes两个方法调用。在add_new_item方法中，write_file用于将生成的Markdown文档写入到指定的.md文件中，确保新添加的项目的文档能够被正确保存。而在process_file_changes方法中，write_file同样用于更新Markdown文档，确保在文件变更后，文档内容能够及时反映最新的代码结构信息。这两个调用场景表明，write_file函数在文件处理和文档生成中起到了重要的作用。

**Note**: 使用该函数时，请确保提供的file_path是相对路径，并且确保repo_path已正确设置，以避免文件写入错误。
***
### FunctionDef get_modified_file_versions(self)
**get_modified_file_versions**: get_modified_file_versions的功能是获取被修改文件的当前版本和之前版本。

**parameters**: 该函数没有参数。

**Code Description**: get_modified_file_versions函数用于获取指定文件的当前版本和上一个版本。首先，它通过git库获取当前工作目录中指定文件的内容，作为当前版本。然后，它通过访问git提交历史记录，获取该文件在最近一次提交中的内容，作为之前版本。如果文件在之前的提交中不存在（例如，文件是新添加的），则之前版本将被设置为None。最终，该函数返回一个包含当前版本和之前版本的元组。

该函数在项目中的调用场景主要出现在Runner类的get_new_objects方法中。在该方法中，get_modified_file_versions被用来获取当前和之前版本的文件内容，以便比较这两个版本之间的差异。具体来说，get_new_objects方法利用当前版本和之前版本的信息，解析出新增和删除的对象，从而实现对文件内容变化的检测。

**Note**: 使用该函数时，请确保指定的文件路径正确，并且该文件在git仓库中存在，以避免KeyError异常。

**Output Example**: 可能的返回值示例为：
```
(
    "def new_function():\n    pass\n", 
    "def old_function():\n    pass\n"
)
```
***
### FunctionDef get_end_lineno(self, node)
**get_end_lineno**: get_end_lineno的功能是获取给定节点的结束行号。

**parameters**: 此函数的参数。
· parameter1: node - 要查找结束行号的节点。

**Code Description**: get_end_lineno函数用于获取AST（抽象语法树）节点的结束行号。首先，该函数检查传入的节点是否具有行号属性。如果节点没有行号，则返回-1，表示该节点没有有效的行号。接下来，函数初始化一个变量end_lineno为节点的行号，并遍历该节点的所有子节点。对于每个子节点，函数尝试获取其结束行号，如果子节点没有结束行号，则递归调用get_end_lineno函数来获取其结束行号。只有当子节点的结束行号有效时，end_lineno才会被更新为子节点的结束行号和当前节点的结束行号中的较大值。最终，函数返回计算得到的结束行号。

该函数在get_functions_and_classes函数中被调用，用于获取每个函数或类节点的结束行号。get_functions_and_classes函数解析整个代码内容，遍历AST树中的所有节点，并将每个函数和类的相关信息（包括开始行号和结束行号）收集到一个列表中。通过调用get_end_lineno，get_functions_and_classes能够准确地获取每个节点的结束行号，从而提供更完整的节点信息。

**Note**: 使用此代码时，请确保传入的节点是有效的AST节点，并且具有相应的行号属性，以避免返回-1的情况。

**Output Example**: 假设传入的节点的行号为10，且其子节点的结束行号为15，则该函数的返回值将为15。
***
### FunctionDef add_parent_references(self, node, parent)
**add_parent_references**: add_parent_references的功能是为抽象语法树（AST）中的每个节点添加父引用。

**parameters**: 该函数的参数如下：
· parameter1: node - 当前在AST中的节点。
· parameter2: parent - 当前节点的父节点，默认为None。

**Code Description**: add_parent_references函数用于遍历给定的抽象语法树（AST）节点，并为每个节点添加一个指向其父节点的引用。函数首先通过ast.iter_child_nodes(node)获取当前节点的所有子节点，然后将当前节点（node）赋值给每个子节点的parent属性。接着，函数递归调用自身以处理每个子节点，确保所有节点都能正确地引用其父节点。

该函数在get_functions_and_classes方法中被调用。get_functions_and_classes的主要功能是解析给定的代码内容，提取出所有函数和类及其参数，并建立它们之间的层级关系。在解析AST树时，首先调用add_parent_references函数，以确保每个节点都能访问到其父节点的信息，这对于后续的层级关系分析至关重要。通过这种方式，get_functions_and_classes能够准确地构建出函数和类的层级结构，提供更清晰的代码解析结果。

**Note**: 使用该函数时，请确保传入的节点是有效的AST节点，并注意在递归调用时可能导致的栈溢出问题，尤其是在处理深层嵌套的AST时。
***
### FunctionDef get_functions_and_classes(self, code_content)
**get_functions_and_classes**: get_functions_and_classes的功能是检索所有函数、类及其参数（如果有的话）以及它们的层级关系。

**parameters**: 此函数的参数如下：
· parameter1: code_content - 要解析的整个文件的代码内容。

**Code Description**: get_functions_and_classes函数用于解析给定的代码内容，提取出所有函数和类的相关信息，包括它们的名称、起始行号、结束行号、父节点名称以及参数列表。该函数首先使用ast.parse将代码内容转换为抽象语法树（AST），然后调用add_parent_references函数为每个节点添加父引用，以便后续分析时能够访问到父节点的信息。

接下来，函数遍历AST树中的所有节点，检查每个节点是否为函数定义（FunctionDef）、类定义（ClassDef）或异步函数定义（AsyncFunctionDef）。对于每个符合条件的节点，函数获取其起始行号和结束行号，并提取参数列表。最终，所有收集到的信息以元组的形式存储在一个列表中并返回。

该函数在多个地方被调用，例如在generate_file_structure函数中用于生成文件结构时，和在add_new_item函数中用于处理新增项目时。通过调用get_functions_and_classes，其他函数能够获取到代码中的结构信息，从而进行进一步的处理和文档生成。

**Note**: 使用此函数时，请确保传入的代码内容是有效的Python代码，以便能够正确解析AST并提取信息。

**Output Example**: 假设传入的代码内容包含以下函数和类定义，函数的返回值可能如下所示：
[
    ('FunctionDef', 'AI_give_params', 86, 95, None, ['param1', 'param2']),
    ('ClassDef', 'PipelineEngine', 97, 104, None, []),
    ('FunctionDef', 'get_all_pys', 99, 104, 'PipelineEngine', ['param1'])
]
***
### FunctionDef generate_file_structure(self, file_path)
**generate_file_structure**: generate_file_structure的功能是生成给定文件路径的文件结构。

**parameters**: 此函数的参数如下：
· parameter1: file_path (str): 文件的相对路径。

**Code Description**: generate_file_structure函数用于生成指定文件路径的文件结构信息。该函数首先打开指定路径的文件，并读取其内容。接着，它调用get_functions_and_classes方法来解析文件内容，提取出所有函数和类的相关信息，包括它们的名称、起始行号、结束行号及参数列表。解析得到的结构信息以元组的形式存储在一个列表中。

在获取到所有结构信息后，函数会遍历这些信息，并调用get_obj_code_info方法来获取每个对象的详细代码信息，包括对象的类型、名称、起始和结束行号、参数等。最终，所有收集到的对象信息以列表的形式返回。

该函数被generate_overall_structure函数调用，用于生成目标仓库中所有文件的结构信息。generate_overall_structure函数会遍历所有未被忽略的文件，并对每个文件调用generate_file_structure，以获取其结构信息并存储在repo_structure字典中。

**Note**: 使用此函数时，请确保传入的文件路径是有效的，并且文件内容是有效的Python代码，以便能够正确解析并提取信息。

**Output Example**: 假设传入的文件路径对应的文件内容包含以下函数和类定义，函数的返回值可能如下所示：
[
    {
        "function_name": {
            "type": "function",
            "start_line": 10,
            "end_line": 20,
            "parent": "class_name"
        },
        "class_name": {
            "type": "class",
            "start_line": 5,
            "end_line": 25,
            "parent": None
        }
    }
]
***
### FunctionDef generate_overall_structure(self, file_path_reflections, jump_files)
**generate_overall_structure**: The function of generate_overall_structure is to retrieve the file structure of a target repository by analyzing its contents while excluding certain files based on specified criteria.

**parameters**: The parameters of this Function.
· parameter1: file_path_reflections (dict) - A dictionary mapping original file paths to their reflections, used to identify files that may have been renamed or moved.
· parameter2: jump_files (list) - A list of file names that should be ignored during the processing, as they are not to be parsed.

**Code Description**: The generate_overall_structure method is designed to construct a comprehensive representation of the file structure within a specified repository. It begins by initializing an empty dictionary called repo_structure, which will ultimately hold the file paths and their corresponding structures.

The method instantiates a GitignoreChecker object, which is responsible for checking the repository directory against patterns defined in a .gitignore file. This checker is crucial for filtering out files and folders that should be ignored based on the project's version control settings.

The method then utilizes the tqdm library to create a progress bar that reflects the ongoing process of checking files and folders. It iterates over the list of non-ignored files provided by the GitignoreChecker's check_files_and_folders method. For each file, the following checks are performed:

1. If the file is present in the jump_files list, it is skipped, and a message is printed to indicate that the file will not be processed.
2. If the file name ends with a specific substring indicating a "latest version," it is also skipped, with a corresponding message printed to the console.

If the file passes these checks, the method attempts to generate its structure by calling the generate_file_structure method, passing the file name as an argument. If an error occurs during this process, it is logged, and the method continues to the next file.

The progress bar is updated to reflect the current file being processed, and once all files have been evaluated, the method returns the repo_structure dictionary, which contains the paths of the files and their respective structures.

This method is integral to the FileHandler class, as it consolidates the information about the repository's file structure while adhering to the rules defined in the .gitignore file and respecting the files specified in the jump_files list.

**Note**: It is essential to ensure that the .gitignore file is correctly formatted and accessible to avoid unintended exclusions of files. Additionally, the jump_files list should be accurately populated to ensure that the intended files are ignored during processing.

**Output Example**: An example output of the generate_overall_structure method might look like this:
```
{
    "src/module1.py": { ... },  # Structure of module1.py
    "src/module2.py": { ... },  # Structure of module2.py
    "tests/test_module1.py": { ... }  # Structure of test_module1.py
}
```
This output indicates that the method has successfully generated the structures for the specified files, with each file path mapped to its corresponding structure representation.
***
### FunctionDef convert_to_markdown_file(self, file_path)
**convert_to_markdown_file**: The function of convert_to_markdown_file is to convert the content of a specified file into markdown format.

**parameters**: The parameters of this Function.
· file_path: (str, optional) The relative path of the file to be converted. If not provided, the default file path will be used.

**Code Description**: The convert_to_markdown_file function is designed to read a file's metadata from a JSON structure and convert it into a markdown representation. The function begins by opening a JSON file that contains the project hierarchy, which is expected to be structured in a way that associates file paths with their corresponding metadata. If the file_path parameter is not provided, the function defaults to using an internal file path attribute.

The function retrieves the relevant file object from the loaded JSON data using the specified or default file path. If no matching file object is found, it raises a ValueError, indicating that the specified file path does not exist in the project hierarchy.

Once the file object is successfully located, the function initializes an empty string to accumulate the markdown content. It sorts the objects associated with the file based on their starting line numbers in the code. The function then constructs a parent-child relationship mapping for the objects, which is crucial for determining the hierarchy levels in the markdown output.

For each object, the function calculates its level in the hierarchy by traversing the parent dictionary. It constructs the markdown string by appending the object's type, name, and parameters, formatted according to its level. The markdown content includes the last piece of markdown content associated with the object, if available. Finally, the function appends a closing separator to the markdown string and returns the complete markdown representation.

**Note**: It is important to ensure that the project_hierarchy.json file is correctly formatted and accessible, as the function relies on this data to perform its operations. Additionally, the function expects the objects within the JSON to have specific attributes such as "type", "name", "params", and "md_content" for proper markdown generation.

**Output Example**: 
A possible appearance of the code's return value could be:
```
# FunctionDef my_function(param1, param2):
This function does something important.

# AsyncFunctionDef my_async_function():
This async function handles asynchronous operations.

***
```
***



================================================
FILE: markdown_docs/repo_agent/log.md
================================================
## ClassDef InterceptHandler
**InterceptHandler**: The function of InterceptHandler is to redirect standard logging output to the Loguru logging system.

**attributes**: The attributes of this Class.
· record: logging.LogRecord - This parameter represents the log record containing all the information pertinent to the event being logged.

**Code Description**: The InterceptHandler class extends the logging.Handler class to facilitate the integration of Python's standard logging module with the Loguru logging system. The primary method of this class is `emit`, which is responsible for processing log records. 

When a log record is received, the `emit` method first attempts to map the standard logging level (e.g., DEBUG, INFO, WARNING) to the corresponding Loguru level. If the mapping fails, it defaults to using the numeric level of the log record. This ensures that all log messages are appropriately categorized according to their severity.

Next, the method identifies the caller of the log message by traversing the call stack. It uses the `inspect.currentframe()` function to obtain the current stack frame and iterates through the frames to find the origin of the log message. This is particularly useful for debugging, as it provides context about where the log message was generated.

Finally, the method logs the message using Loguru's logging capabilities, including any exception information if present. The `logger.opt()` method is utilized to set the depth of the stack trace and to include exception details, ensuring that the log output is informative and relevant.

The InterceptHandler is specifically invoked within the `set_logger_level_from_config` function. This function configures the Loguru logger with a specified log level and integrates it with the standard logging module. By calling `logging.basicConfig()` with an instance of InterceptHandler, it effectively redirects all standard logging output to Loguru, allowing for a unified logging approach across the application. This integration is crucial for maintaining consistent logging behavior, especially in applications that utilize both standard logging and Loguru.

**Note**: When using the InterceptHandler, it is important to ensure that the logging configuration is set up correctly to avoid conflicts between standard logging and Loguru. Additionally, developers should be aware of the performance implications of logging, particularly in multi-threaded environments, where the `enqueue=True` option in Loguru can help manage log messages safely.
### FunctionDef emit(self, record)
**emit**: emit函数的功能是将日志记录发送到Loguru日志系统。

**parameters**: 该函数的参数。
· record: logging.LogRecord - 包含日志记录信息的对象。

**Code Description**: emit函数首先尝试获取与传入的日志记录的级别相对应的Loguru级别。如果成功，则使用该级别；如果失败，则使用记录的级别号。接着，函数通过inspect模块获取当前调用栈的帧信息，以确定日志消息的来源。它会遍历调用栈，直到找到一个非logging模块的帧，从而确定日志消息的深度。最后，使用Loguru的logger对象，结合深度和异常信息，记录日志消息。

具体步骤如下：
1. 使用logger.level方法获取与record.levelname对应的Loguru级别名称。如果该级别不存在，则使用record.levelno作为级别。
2. 通过inspect.currentframe()获取当前帧，并初始化深度为0。然后，使用while循环遍历调用栈，直到找到一个非logging模块的帧。
3. 使用logger.opt方法记录日志，传入深度和异常信息，并调用record.getMessage()获取日志消息的内容。

**Note**: 使用该函数时，请确保传入的record对象是有效的logging.LogRecord实例，以避免潜在的错误。同时，确保Loguru库已正确配置，以便能够处理日志记录。
***
## FunctionDef set_logger_level_from_config(log_level)
**set_logger_level_from_config**: The function of set_logger_level_from_config is to configure the loguru logger with a specified log level and integrate it with the standard logging module.

**parameters**: The parameters of this Function.
· log_level: str - The log level to set for loguru (e.g., "DEBUG", "INFO", "WARNING").

**Code Description**: The set_logger_level_from_config function is designed to set the logging level for the loguru logger based on the provided log_level argument. It begins by removing any existing loguru handlers to ensure that there are no conflicts or duplications in logging output. Following this, it adds a new handler to the loguru logger that directs output to stderr at the specified log level. The parameters `enqueue=True`, `backtrace=False`, and `diagnose=False` are used to ensure that logging is thread-safe, minimizes detailed traceback information, and suppresses additional diagnostic information, respectively.

Additionally, the function redirects the standard logging output to the loguru logger by utilizing the InterceptHandler class. This integration allows loguru to handle all logging consistently across the application, which is particularly useful in scenarios where both standard logging and loguru are used. The function concludes by logging a success message indicating that the log level has been set.

The set_logger_level_from_config function is called within the run function located in the repo_agent/main.py file. In this context, it retrieves the logging configuration from the SettingsManager and applies it by calling set_logger_level_from_config with the appropriate log level. This ensures that the logging configuration is established before any tasks are executed, allowing for consistent logging behavior throughout the application.

**Note**: When using the set_logger_level_from_config function, it is essential to ensure that the logging configuration is correctly set up to avoid conflicts between standard logging and loguru. Developers should also consider the implications of logging performance, especially in multi-threaded environments, where the `enqueue=True` option can help manage log messages safely.



================================================
FILE: markdown_docs/repo_agent/main.md
================================================
## FunctionDef cli
**cli**: cli函数的功能是为基于LLM的框架提供仓库级代码文档生成。

**parameters**: 该函数没有参数。

**Code Description**: cli函数是一个空函数，当前没有实现任何具体的功能。根据其文档字符串，cli的目的是为一个基于大型语言模型（LLM）的框架提供仓库级别的代码文档生成。这表明该函数可能是未来扩展的基础，旨在处理与代码文档生成相关的任务。

在项目结构中，cli函数被调用于repo_agent/__main__.py文件中。虽然在__main__.py中没有提供具体的调用代码，但通常情况下，__main__.py文件是Python程序的入口点，cli函数可能会在程序启动时被调用，以初始化或配置文档生成的相关功能。

**Note**: 由于cli函数目前未实现任何功能，开发者在使用时应注意该函数尚未完成，可能需要进一步的开发和实现才能达到预期的文档生成效果。
## FunctionDef handle_setting_error(e)
**handle_setting_error**: handle_setting_error的功能是处理设置中的配置错误。

**parameters**: 该函数的参数。
· e: ValidationError - 表示验证错误的异常对象，包含有关配置错误的详细信息。

**Code Description**: handle_setting_error函数用于处理在程序运行过程中遇到的配置错误。当程序尝试获取设置时，如果出现ValidationError异常，该函数将被调用。函数首先通过click库打印一条通用的错误消息，提示用户检查其设置。接着，函数遍历ValidationError对象中的错误信息，针对每个错误输出更详细的字段缺失信息，并使用不同的颜色进行区分。

如果错误类型为“missing”，函数会提示用户缺少必需的字段，并建议设置相应的环境变量；如果是其他类型的错误，则直接输出错误消息。最后，函数通过抛出click.ClickException优雅地终止程序，并显示一条终止程序的错误消息。

在项目中，handle_setting_error函数被多个函数调用，包括run、print_hierarchy和diff。这些函数在尝试获取设置时，如果遇到ValidationError异常，都会调用handle_setting_error来处理错误并输出相关信息，从而确保用户能够及时了解配置问题并进行修正。

**Note**: 使用该函数时，请确保传入的参数是ValidationError类型的异常对象，以便正确处理和输出错误信息。
## FunctionDef run
Doc is waiting to be generated...
## FunctionDef clean
**clean**: The function of clean is to remove the fake files generated by the documentation process.

**parameters**: The parameters of this Function.
· No parameters are required for this function.

**Code Description**: The clean function is designed to facilitate the cleanup of temporary files, referred to as "fake files," that are created during the documentation generation process. This function achieves its purpose by invoking the delete_fake_files function, which is responsible for identifying and removing these temporary files.

When the clean function is called, it executes the delete_fake_files function, which performs a thorough search through the project's directory structure to locate and delete any files that match specific criteria indicative of temporary files. Upon successful completion of the deletion process, the clean function logs a success message indicating that the fake files have been cleaned up.

The delete_fake_files function operates by first retrieving the project settings through the SettingsManager's get_setting method. It then utilizes a nested helper function, gci, to recursively traverse the specified directory. The gci function checks each file and directory, identifying those that are temporary based on their naming conventions. If a temporary file is found, it either deletes it if it is empty or renames it back to its original name if it contains content.

The clean function is crucial in ensuring that the workspace remains free of unnecessary files after documentation tasks are completed. It is typically called at the end of the documentation process to maintain an organized project structure.

**Note**: It is important to ensure that the project settings are correctly configured and that the target repository is accessible before invoking the clean function. Any issues related to file permissions or incorrect paths may lead to errors during the cleanup process.
## FunctionDef print_hierarchy
Doc is waiting to be generated...
## FunctionDef diff
Doc is waiting to be generated...



================================================
FILE: markdown_docs/repo_agent/multi_task_dispatch.md
================================================
## ClassDef Task
**Task**: The function of Task is to represent a unit of work with its dependencies and status.

**attributes**: The attributes of this Class.
· task_id: An integer that uniquely identifies the task.
· dependencies: A list of Task objects that this task depends on.
· extra_info: Any additional information associated with the task, which can be of any type.
· status: An integer representing the current status of the task (0 for not started, 1 for in progress, 2 for completed, 3 for error).

**Code Description**: The Task class is designed to encapsulate the concept of a task within a multi-tasking framework. Each Task object is initialized with a unique identifier (task_id), a list of dependencies that must be completed before this task can start, and optional extra information that can provide context or metadata about the task. The status attribute tracks the current state of the task, allowing for management and monitoring of its progress.

The Task class is utilized within the MultiTaskDispatch system, where it plays a crucial role in task management. Specifically, the TaskManager class, which is responsible for managing multiple tasks, creates instances of the Task class when new tasks are added. The add_task method in TaskManager takes a list of dependency task IDs and creates a new Task object, linking it to its dependencies. This relationship ensures that tasks are executed in the correct order based on their dependencies.

Furthermore, the Task class is referenced in the print_task_list method of the MetaInfo class, which formats and displays a list of tasks along with their statuses and dependencies. This integration highlights the importance of the Task class in providing a structured way to manage and visualize tasks within the system.

**Note**: When using the Task class, it is important to ensure that the dependencies are properly managed to avoid circular dependencies, which could lead to errors in task execution. Additionally, the status attribute should be updated appropriately to reflect the current state of the task throughout its lifecycle.
### FunctionDef __init__(self, task_id, dependencies, extra_info)
**__init__**: The function of __init__ is 初始化任务对象。

**parameters**: The parameters of this Function.
· parameter1: task_id (int) - 任务的唯一标识符。
· parameter2: dependencies (List[Task]) - 该任务所依赖的其他任务列表。
· parameter3: extra_info (Any, 可选) - 额外的信息，可以是任何类型的数据，默认为None。

**Code Description**: 该__init__函数是任务类的构造函数，用于初始化任务对象的基本属性。首先，它接收一个整数类型的参数task_id，用于唯一标识该任务。接着，dependencies参数是一个任务对象的列表，表示当前任务所依赖的其他任务，这对于任务调度和执行顺序非常重要。extra_info参数是一个可选参数，可以存储与任务相关的额外信息，默认为None。最后，status属性被初始化为0，表示任务的初始状态为“未开始”。状态值的定义如下：0表示未开始，1表示正在进行，2表示已经完成，3表示出错了。

**Note**: 在使用该构造函数时，确保传入的dependencies参数是一个有效的任务列表，以避免在后续任务调度中出现错误。同时，task_id应保持唯一性，以确保任务的正确识别和管理。
***
## ClassDef TaskManager
**TaskManager**: The function of TaskManager is to manage and dispatch multiple tasks based on their dependencies.

**attributes**: The attributes of this Class.
· task_dict: A dictionary that maps task IDs to Task objects.  
· task_lock: A threading.Lock used for thread synchronization when accessing the task_dict.  
· now_id: An integer representing the current task ID.  
· query_id: An integer representing the current query ID.  
· sync_func: A placeholder for a synchronization function, initially set to None.  

**Code Description**: The TaskManager class is designed to facilitate the management of multiple tasks in a concurrent environment. It initializes with an empty task dictionary (task_dict) that will hold Task objects indexed by their unique IDs. The class employs a threading lock (task_lock) to ensure that access to the task dictionary is thread-safe, preventing race conditions when multiple threads attempt to modify the task list simultaneously.

The now_id attribute keeps track of the next available task ID, while query_id is used to track the number of queries made for tasks. The sync_func attribute is intended to hold a function that can be called for synchronization purposes, though it is not defined upon initialization.

The class provides several key methods:
- **all_success**: A property that checks if all tasks have been completed by verifying if the task dictionary is empty.
- **add_task**: This method allows the addition of a new task to the task dictionary. It takes a list of dependency task IDs and optional extra information. The method locks the task dictionary during the addition process to ensure thread safety, creates a new Task object, and increments the now_id for the next task.
- **get_next_task**: This method retrieves the next available task for a specified process ID. It checks the task dictionary for tasks that have no dependencies and are not currently in progress. If a task is found, it updates its status to indicate that it is now being processed and may call the sync_func every ten queries.
- **mark_completed**: This method marks a specified task as completed and removes it from the task dictionary. It also updates the dependencies of other tasks that may rely on the completed task.

The TaskManager class is utilized within the MetaInfo class in the repo_agent/doc_meta_info.py file. Specifically, it is called in the methods get_task_manager and get_topology. The get_task_manager method constructs a TaskManager instance and populates it with tasks based on the dependencies of document items in a hierarchical structure. The get_topology method orchestrates the overall process of calculating the topological order of all objects in a repository, leveraging the TaskManager to manage the tasks that arise from this calculation.

**Note**: When using the TaskManager, ensure that the sync_func is properly defined if synchronization is required during task processing. Additionally, be aware of potential circular dependencies in task management, which may complicate the task retrieval process.

**Output Example**: A possible return value from the get_next_task method could be a tuple containing a Task object and its ID, such as (Task(task_id=0, dependencies=[], extra_info=None), 0), indicating that the task with ID 0 is ready for processing.
### FunctionDef __init__(self)
**__init__**: The function of __init__ is to initialize a MultiTaskDispatch object.

**parameters**: The __init__ function does not take any parameters.

**Code Description**: The __init__ method is responsible for setting up a new instance of the MultiTaskDispatch class. It initializes several key attributes that are essential for managing multiple tasks within a multi-tasking framework. 

- `task_dict`: This attribute is a dictionary that maps integer task IDs to Task objects. It serves as a central repository for all tasks being managed, allowing for efficient retrieval and management of tasks based on their unique identifiers.

- `task_lock`: This attribute is an instance of `threading.Lock`, which is utilized for thread synchronization. It ensures that access to the `task_dict` is thread-safe, preventing race conditions that could occur when multiple threads attempt to modify or access the task dictionary simultaneously.

- `now_id`: This integer attribute keeps track of the current task ID being processed. It is initialized to zero, indicating that no tasks have been processed yet.

- `query_id`: Similar to `now_id`, this integer attribute is used to track the current query ID. It is also initialized to zero.

- `sync_func`: This attribute is initialized to None and serves as a placeholder for a synchronization function that may be defined later. This allows for flexibility in managing task synchronization as needed.

The initialization of these attributes is crucial for the proper functioning of the MultiTaskDispatch system, as they lay the groundwork for task management and synchronization. The MultiTaskDispatch class relies on the Task class to represent individual tasks, which are stored in `task_dict`. The relationship between MultiTaskDispatch and Task is fundamental, as MultiTaskDispatch orchestrates the execution and management of these Task objects, ensuring that tasks are executed in accordance with their dependencies and statuses.

**Note**: When using the MultiTaskDispatch class, it is important to ensure that the task management system is properly configured, particularly with respect to thread safety and the handling of task dependencies. Proper initialization of the attributes is essential for the smooth operation of the task management framework.
***
### FunctionDef all_success(self)
**all_success**: all_success的功能是检查任务管理器中的任务字典是否为空。

**parameters**: 此函数没有参数。

**Code Description**: all_success函数用于判断任务管理器中的任务字典（task_dict）是否为空。具体来说，它通过计算任务字典的长度来实现这一点。如果任务字典的长度为零，表示没有待处理的任务，函数将返回True；否则，返回False。

在项目中，all_success函数被调用于repo_agent/runner.py中的Runner类的run方法。在run方法中，任务管理器的状态被检查，以确定是否所有文档生成任务都已完成。如果all_success返回True，表示任务队列中没有任务，所有文档都已完成且是最新的，这时会记录一条日志，表明没有任务在队列中。

**Note**: 使用此函数时，请确保任务字典的状态已正确更新，以避免误判任务是否完成。

**Output Example**: 假设任务字典为空，调用all_success将返回True。
***
### FunctionDef add_task(self, dependency_task_id, extra)
**add_task**: The function of add_task is to add a new task to the task dictionary while managing its dependencies.

**parameters**: The parameters of this Function.
· dependency_task_id: List[int] - A list of task IDs that the new task depends on.
· extra: Any, optional - Extra information associated with the task. Defaults to None.

**Code Description**: The add_task method is responsible for creating and adding a new task to the task manager's internal dictionary of tasks. It takes a list of dependency task IDs, which represent other tasks that must be completed before the new task can start. The method also accepts an optional parameter, extra, which can hold any additional information related to the task.

When the add_task method is invoked, it first acquires a lock (self.task_lock) to ensure thread safety while modifying the task dictionary. It then retrieves the Task objects corresponding to the provided dependency_task_id list. These Task objects are stored in the depend_tasks list. 

Next, a new Task object is instantiated using the current task ID (self.now_id), the list of dependencies (depend_tasks), and the optional extra information. This new Task object is then added to the task dictionary with the current task ID as the key. After successfully adding the task, the method increments the now_id counter to ensure that the next task added will have a unique identifier. Finally, the method returns the ID of the newly added task.

The add_task method is called within the get_task_manager method of the MetaInfo class. In this context, get_task_manager is responsible for constructing a TaskManager instance and populating it with tasks based on the relationships between various document items. As it processes each document item, it determines the dependencies for the task to be created and invokes add_task to register the new task in the TaskManager. This integration highlights the role of add_task in establishing the task management framework, ensuring that tasks are created with the correct dependencies and are properly tracked within the system.

**Note**: When using the add_task method, it is essential to ensure that the dependency_task_id list does not contain circular references, as this could lead to issues in task execution. Additionally, the extra parameter should be used judiciously to provide relevant context for the task without introducing unnecessary complexity.

**Output Example**: A possible return value of the add_task method could be an integer representing the ID of the newly added task, such as 5, indicating that the task has been successfully added to the task manager with that identifier.
***
### FunctionDef get_next_task(self, process_id)
**get_next_task**: get_next_task的功能是为给定的进程ID获取下一个任务。

**parameters**: 该函数的参数。
· parameter1: process_id (int) - 进程的ID。

**Code Description**: 
get_next_task函数用于根据提供的进程ID获取下一个可用的任务。函数首先通过self.task_lock锁定任务，以确保在多线程环境中对任务的安全访问。接着，query_id自增1，用于跟踪查询次数。函数遍历task_dict字典中的所有任务ID，检查每个任务的依赖关系和状态。只有当任务的依赖关系为空且状态为0（表示任务可用）时，才将其标记为已获取（状态设置为1）。在获取任务时，函数会打印出当前进程ID、获取的任务ID以及剩余任务的数量。如果query_id是10的倍数，则调用sync_func函数进行同步。最后，函数返回获取的任务对象及其ID。如果没有可用的任务，函数将返回(None, -1)。

**Note**: 使用该函数时，请确保在调用前已正确初始化task_dict，并且在多线程环境中使用task_lock来避免竞争条件。

**Output Example**: 
假设有一个可用的任务，其ID为5，返回值可能为：
(task_object, 5)  # 其中task_object是获取的任务对象。 

如果没有可用的任务，返回值将为：
(None, -1)
***
### FunctionDef mark_completed(self, task_id)
**mark_completed**: mark_completed的功能是将指定任务标记为已完成，并从任务字典中移除该任务。

**parameters**: 该函数的参数。
· parameter1: task_id (int) - 要标记为已完成的任务的ID。

**Code Description**: mark_completed函数用于将指定的任务标记为已完成，并从任务管理器的任务字典中删除该任务。函数接收一个整数类型的参数task_id，表示要处理的任务的唯一标识符。函数内部首先通过自我锁定（self.task_lock）来确保在多线程环境下对任务字典的安全访问。接着，函数通过task_id从任务字典中获取目标任务（target_task）。然后，函数遍历任务字典中的所有任务，检查目标任务是否在其他任务的依赖列表中。如果目标任务存在于其他任务的依赖中，则将其从依赖列表中移除。最后，函数调用pop方法从任务字典中删除该任务，确保任务不再被管理。

**Note**: 使用该函数时，请确保传入的task_id是有效的，并且对应的任务在任务字典中存在。调用此函数后，相关依赖关系也会被更新，因此在调用之前应考虑任务之间的依赖关系。
***
## FunctionDef worker(task_manager, process_id, handler)
**worker**: worker函数用于执行由任务管理器分配的任务。

**parameters**: 该函数的参数如下：
· parameter1: task_manager - 任务管理器对象，用于分配任务给工作线程。
· parameter2: process_id (int) - 当前工作进程的ID。
· parameter3: handler (Callable) - 处理任务的函数。

**Code Description**: worker函数是一个无限循环的工作线程，它从任务管理器中获取任务并执行。首先，它会检查任务管理器的状态，如果所有任务都已成功完成，则函数返回，结束执行。接着，worker调用task_manager的get_next_task方法，获取当前进程ID对应的下一个任务及其ID。如果没有可用的任务，worker会暂停0.5秒后继续循环。

一旦获取到任务，worker会调用传入的handler函数，处理任务的额外信息。处理完成后，worker会调用task_manager的mark_completed方法，标记该任务为已完成。此函数的设计允许多个worker并行处理任务，提升了任务执行的效率。

在项目中，worker函数被repo_agent/runner.py中的first_generate和run方法调用。具体来说，这两个方法在生成文档的过程中会创建多个线程，每个线程都运行worker函数，以并行处理任务。first_generate方法负责初始化任务列表并启动worker线程，而run方法则在检测到文件变更时重新生成文档，并同样启动worker线程来处理任务。

**Note**: 使用该函数时，需要确保任务管理器的状态正确，以避免在没有任务可执行时造成不必要的等待。

**Output Example**: 假设任务管理器分配了一个任务，worker函数在处理后可能会返回如下信息：
```
任务ID: 12345 已成功完成。
```
## FunctionDef some_function
**some_function**: some_function的功能是随机暂停一段时间。

**parameters**: 该函数没有参数。

**Code Description**: some_function是一个简单的函数，其主要功能是使程序随机暂停一段时间。具体实现上，函数内部调用了time.sleep()方法，传入的参数是一个随机生成的浮点数，该浮点数的范围是0到3秒之间。这个随机数是通过random.random()生成的，random.random()返回一个在[0.0, 1.0)范围内的随机浮点数，因此乘以3后，最终的暂停时间会在0到3秒之间变化。这种随机暂停的功能可以用于需要模拟延迟或等待的场景，例如在多线程或异步编程中，可能需要随机延迟以避免资源竞争或模拟真实用户的操作行为。

**Note**: 使用该函数时，请注意它会导致程序暂停，因此在需要高性能或实时响应的场景中应谨慎使用。此外，由于暂停时间是随机的，可能会影响程序的可预测性。



================================================
FILE: markdown_docs/repo_agent/project_manager.md
================================================
## ClassDef ProjectManager
**ProjectManager**: The function of ProjectManager is to manage and retrieve the structure of a project repository.

**attributes**: The attributes of this Class.
· repo_path: The file path to the project repository.
· project: An instance of the Jedi Project class, initialized with the repo_path.
· project_hierarchy: The file path to the project hierarchy JSON file, constructed from the repo_path and project_hierarchy parameter.

**Code Description**: The ProjectManager class is designed to facilitate the management of a project repository by providing methods to retrieve the project's directory structure and build a reference path tree. Upon initialization, the class requires two parameters: `repo_path`, which specifies the location of the project repository, and `project_hierarchy`, which indicates the name of the hierarchy to be used. The class constructs the path to the project hierarchy JSON file by combining the repo_path with the project_hierarchy name.

The `get_project_structure` method is responsible for returning the structure of the project by recursively traversing the directory tree starting from the repo_path. It constructs a string representation of the project structure, including all directories and Python files, while ignoring hidden files and directories. This method utilizes a nested function `walk_dir` to perform the recursive traversal.

The `build_path_tree` method creates a hierarchical tree structure based on two lists of paths: `who_reference_me` and `reference_who`, as well as a specific `doc_item_path`. It constructs a nested dictionary using `defaultdict` to represent the tree structure. The method modifies the last part of the `doc_item_path` to indicate a specific item with a star symbol. Finally, it converts the tree structure into a string format for easier visualization.

The ProjectManager class is instantiated within the Runner class, where it is initialized with the target repository and hierarchy name obtained from the SettingsManager. This integration allows the Runner to leverage the ProjectManager's capabilities to manage and retrieve project structure information, which is essential for the overall functionality of the application.

**Note**: When using the ProjectManager class, ensure that the provided repo_path is valid and accessible. The project_hierarchy should correspond to an existing hierarchy name to avoid file path errors.

**Output Example**: A possible output of the `get_project_structure` method might look like this:
```
project_root
  src
    main.py
    utils.py
  tests
    test_main.py
```
### FunctionDef __init__(self, repo_path, project_hierarchy)
**__init__**: __init__的功能是初始化ProjectManager类的实例。

**parameters**: 该函数的参数如下：
· parameter1: repo_path - 指定项目的存储库路径。
· parameter2: project_hierarchy - 指定项目层次结构的路径。

**Code Description**: 该__init__函数用于初始化ProjectManager类的实例。在函数内部，首先将传入的repo_path参数赋值给实例变量self.repo_path，以便在类的其他方法中使用。接着，使用jedi库创建一个新的Project对象，并将其赋值给self.project，传入的repo_path作为参数。这使得ProjectManager能够利用jedi库提供的功能来处理代码分析和自动补全等任务。最后，函数通过os.path.join方法构建项目层次结构的完整路径，将其赋值给self.project_hierarchy。该路径由repo_path、project_hierarchy参数和一个名为"project_hierarchy.json"的文件名组成，这样可以方便地访问项目的层次结构数据。

**Note**: 使用该代码时，请确保传入的repo_path是有效的文件路径，并且project_hierarchy参数指向的目录中存在"project_hierarchy.json"文件，以避免在实例化过程中出现错误。
***
### FunctionDef get_project_structure(self)
**get_project_structure**: The function of get_project_structure is to return the structure of the project by recursively walking through the directory tree.

**parameters**: The parameters of this Function.
· There are no parameters for this function.

**Code Description**: The get_project_structure function is designed to generate a string representation of the project's directory structure. It does this by defining an inner function called walk_dir, which takes two arguments: root (the current directory being processed) and prefix (a string used to format the output). The function initializes an empty list called structure to hold the formatted directory and file names.

The walk_dir function begins by appending the base name of the current directory (root) to the structure list, prefixed by the provided prefix. It then creates a new prefix by adding two spaces to the existing prefix to indicate a deeper level in the directory hierarchy. The function proceeds to iterate over the sorted list of items in the current directory, skipping any hidden files or directories (those starting with a dot).

For each item, it constructs the full path and checks if it is a directory or a Python file (ending with ".py"). If it is a directory, the function calls itself recursively with the new prefix. If it is a Python file, it appends the file name to the structure list with the new prefix.

Finally, after the walk_dir function has processed all directories and files, the get_project_structure function joins the elements of the structure list into a single string, separated by newline characters, and returns this string.

**Note**: It is important to ensure that the repo_path attribute of the class instance is correctly set to the root directory of the project before calling this function. The function will only include Python files in the output, ignoring other file types.

**Output Example**: 
```
project_name
  module1
    file1.py
    file2.py
  module2
    file3.py
  README.md
```
#### FunctionDef walk_dir(root, prefix)
**walk_dir**: walk_dir的功能是遍历指定目录及其子目录，并收集所有Python文件的结构信息。

**parameters**: 此函数的参数如下：
· parameter1: root - 要遍历的根目录的路径。
· parameter2: prefix - 用于格式化输出的前缀字符串，默认为空字符串。

**Code Description**: 
walk_dir函数用于递归遍历给定的目录（root）及其所有子目录。它首先将当前目录的名称（通过os.path.basename(root)获取）添加到结构列表中（structure），并在前缀字符串（prefix）后添加空格以便于格式化。接着，函数使用os.listdir(root)列出当前目录中的所有文件和子目录，并对这些名称进行排序。

在遍历每个名称时，函数会检查名称是否以点（.）开头，以此来忽略隐藏文件和目录。如果名称不是隐藏的，函数会构造该名称的完整路径（path）。如果该路径是一个目录，函数会递归调用walk_dir，传入新的前缀（new_prefix）。如果该路径是一个文件且文件名以“.py”结尾，函数则将该文件的名称添加到结构列表中，前面加上新的前缀。

该函数的设计使得它能够有效地收集指定目录下所有Python文件的结构信息，并以层级方式展示。

**Note**: 使用此代码时，请确保传入的根目录路径是有效的，并且具有读取权限。此外，函数会忽略所有以点开头的文件和目录，因此如果需要处理这些文件，需调整相关逻辑。
***
***
### FunctionDef build_path_tree(self, who_reference_me, reference_who, doc_item_path)
**build_path_tree**: The function of build_path_tree is to construct a hierarchical representation of file paths based on two reference lists and a specific document item path.

**parameters**: The parameters of this Function.
· who_reference_me: A list of file paths that reference the current object.
· reference_who: A list of file paths that are referenced by the current object.
· doc_item_path: A specific file path that needs to be highlighted in the tree structure.

**Code Description**: The build_path_tree function creates a nested dictionary structure representing a tree of file paths. It utilizes the `defaultdict` from the `collections` module to facilitate the creation of this tree. The function begins by defining an inner function, `tree`, which initializes a new `defaultdict` that can recursively create nested dictionaries.

The function then processes the two input lists, `who_reference_me` and `reference_who`. For each path in these lists, it splits the path into its components using the operating system's path separator (`os.sep`). It traverses the tree structure, creating a new node for each part of the path.

Next, the function processes the `doc_item_path`. It splits this path into components as well, but modifies the last component by prefixing it with a star symbol (✳️) to indicate that it is the item of interest. This modified path is then added to the tree in the same manner as the previous paths.

Finally, the function defines another inner function, `tree_to_string`, which converts the nested dictionary structure into a formatted string representation. This function recursively traverses the tree, indenting each level of the hierarchy for clarity. The resulting string is returned as the output of the build_path_tree function.

**Note**: It is important to ensure that the paths provided in `who_reference_me` and `reference_who` are valid and correctly formatted. The function assumes that the paths are well-structured and uses the operating system's path separator for splitting.

**Output Example**: 
Given the following inputs:
- who_reference_me: ["folder1/fileA.txt", "folder1/folder2/fileB.txt"]
- reference_who: ["folder3/fileC.txt"]
- doc_item_path: "folder1/folder2/fileB.txt"

The output of the function might look like this:
```
folder1
    fileA.txt
    folder2
        ✳️fileB.txt
folder3
    fileC.txt
```
#### FunctionDef tree
**tree**: tree函数的功能是返回一个默认字典，该字典的默认值是一个新的tree函数。

**parameters**: 该函数没有参数。

**Code Description**: tree函数使用了Python的defaultdict类。defaultdict是collections模块中的一个字典子类，它提供了一个默认值，当访问的键不存在时，会自动调用一个指定的工厂函数来生成这个默认值。在这个函数中，tree函数本身被用作工厂函数，这意味着每当访问一个不存在的键时，defaultdict将会创建一个新的tree对象。这种递归的结构使得返回的字典可以用于构建树形结构，其中每个节点都可以有多个子节点，且子节点的数量和内容是动态生成的。

**Note**: 使用该函数时，请注意避免无限递归的情况。由于tree函数返回的是一个defaultdict，其默认值也是tree函数本身，因此在访问未定义的键时会不断创建新的defaultdict，可能导致内存消耗过大。

**Output Example**: 调用tree函数后，可能的返回值如下：
```
defaultdict(<function tree at 0x...>, {})
```
此返回值表示一个空的defaultdict，且其默认值是tree函数本身。若访问一个不存在的键，例如`my_tree['a']`，则会创建一个新的defaultdict，作为'a'的值。
***
#### FunctionDef tree_to_string(tree, indent)
**tree_to_string**: tree_to_string的功能是将树形结构转换为字符串格式，便于可视化展示。

**parameters**: 此函数的参数如下：
· parameter1: tree - 一个字典类型的树形结构，其中键表示节点，值可以是子节点的字典或其他类型。
· parameter2: indent - 一个整数，表示当前节点的缩进级别，默认为0。

**Code Description**: tree_to_string函数通过递归的方式将树形结构转换为字符串。首先，函数初始化一个空字符串s。然后，它对传入的tree字典进行排序，并遍历每一个键值对。在遍历过程中，函数将当前键（节点）添加到字符串s中，并根据indent参数添加相应数量的空格以实现缩进。如果当前值是一个字典，表示该节点有子节点，函数会递归调用tree_to_string，将子节点转换为字符串并添加到s中。最终，函数返回构建好的字符串s。

**Note**: 使用此函数时，请确保传入的tree参数是一个有效的字典结构，并且可以包含嵌套的字典。indent参数用于控制输出的格式，通常不需要手动设置，除非在特定情况下需要调整缩进。

**Output Example**: 假设输入的tree为如下结构：
{
    "根节点": {
        "子节点1": {},
        "子节点2": {
            "孙节点1": {}
        }
    }
}
调用tree_to_string(tree)将返回：
根节点
    子节点1
    子节点2
        孙节点1
***
***



================================================
FILE: markdown_docs/repo_agent/runner.md
================================================
[Binary file]


================================================
FILE: markdown_docs/repo_agent/settings.md
================================================
## ClassDef LogLevel
**LogLevel**: LogLevel 的功能是定义日志级别的枚举类型。

**attributes**: 该类的属性包括：
· DEBUG: 表示调试信息的日志级别。
· INFO: 表示一般信息的日志级别。
· WARNING: 表示警告信息的日志级别。
· ERROR: 表示错误信息的日志级别。
· CRITICAL: 表示严重错误信息的日志级别。

**Code Description**: LogLevel 类继承自 StrEnum，定义了一组常量，用于表示不同的日志级别。这些日志级别包括 DEBUG、INFO、WARNING、ERROR 和 CRITICAL，分别对应不同的日志记录重要性。使用枚举类型的好处在于，它提供了一种清晰且类型安全的方式来处理日志级别，避免了使用字符串常量可能带来的错误。

在项目中，LogLevel 类被 ProjectSettings 类引用，作为 log_level 属性的类型。ProjectSettings 类是一个配置类，负责管理项目的设置，其中 log_level 属性默认设置为 LogLevel.INFO。这意味着在没有特别指定的情况下，项目的日志级别将为信息级别。

此外，ProjectSettings 类中的 set_log_level 方法用于验证和设置日志级别。该方法会将输入的字符串转换为大写，并检查其是否为有效的日志级别。如果输入的值不在 LogLevel 的定义范围内，将会抛出一个 ValueError 异常。这确保了在项目中使用的日志级别始终是有效且一致的。

**Note**: 使用 LogLevel 时，请确保所使用的日志级别是预定义的常量之一，以避免运行时错误。在设置日志级别时，建议使用大写字母输入，以符合枚举的定义。
## ClassDef ProjectSettings
**ProjectSettings**: The function of ProjectSettings is to manage the configuration settings for the project.

**attributes**: The attributes of this Class.
· target_repo: DirectoryPath - Specifies the target repository directory path.
· hierarchy_name: str - Defines the name of the hierarchy for project documentation.
· markdown_docs_name: str - Indicates the name of the directory where markdown documentation is stored.
· ignore_list: list[str] - A list of items to be ignored in the project settings.
· language: str - Specifies the language used in the project, defaulting to "Chinese".
· max_thread_count: PositiveInt - Sets the maximum number of threads allowed, defaulting to 4.
· log_level: LogLevel - Defines the logging level for the project, defaulting to LogLevel.INFO.

**Code Description**: The ProjectSettings class inherits from BaseSettings and serves as a configuration class that encapsulates various settings required for the project. It includes attributes that define the target repository, documentation hierarchy, language preferences, and logging configurations. 

The class utilizes field validators to ensure that the values assigned to certain attributes are valid. For instance, the `validate_language_code` method checks if the provided language code corresponds to a valid ISO 639 code or language name, raising a ValueError if the input is invalid. This ensures that only recognized language codes are accepted, enhancing the robustness of the configuration.

Similarly, the `set_log_level` method validates the log level input, converting it to uppercase and checking its validity against the predefined LogLevel enumeration. If the input does not match any of the defined log levels, a ValueError is raised, ensuring that the logging configuration remains consistent and valid throughout the project.

The ProjectSettings class is referenced by the Setting class, which aggregates various settings for the project, including ProjectSettings and ChatCompletionSettings. This hierarchical structure allows for organized management of project configurations, where ProjectSettings plays a crucial role in defining the core settings that govern the behavior of the application.

**Note**: When using the ProjectSettings class, ensure that the values assigned to attributes like language and log_level are valid to avoid runtime errors. It is recommended to use the predefined constants for log levels and valid ISO codes for languages to maintain consistency and reliability in the project's configuration.

**Output Example**: An instance of ProjectSettings might look like this:
```
ProjectSettings(
    target_repo="/path/to/repo",
    hierarchy_name=".project_doc_record",
    markdown_docs_name="markdown_docs",
    ignore_list=["temp", "cache"],
    language="English",
    max_thread_count=4,
    log_level=LogLevel.INFO
)
```
### FunctionDef validate_language_code(cls, v)
**validate_language_code**: validate_language_code的功能是验证并返回有效的语言名称。

**parameters**: 该函数的参数。
· v: 字符串类型，表示待验证的语言代码或语言名称。

**Code Description**: validate_language_code是一个类方法，用于验证输入的语言代码或语言名称是否有效。该方法接受一个字符串参数v，表示用户输入的语言代码或名称。函数内部使用Language.match(v)来尝试匹配输入的语言。如果匹配成功，将返回对应的语言名称。如果输入的语言代码或名称无效，则会引发LanguageNotFoundError异常，进而抛出一个ValueError，提示用户输入有效的ISO 639代码或语言名称。

该函数的主要目的是确保用户输入的语言信息是有效的，并提供相应的反馈，以便用户能够纠正输入错误。

**Note**: 使用该函数时，请确保传入的参数是字符串类型，并且符合ISO 639标准或已知的语言名称。若输入无效，函数将抛出异常，需在调用时做好异常处理。

**Output Example**: 假设输入参数为"en"，函数将返回"English"。如果输入参数为"invalid_code"，则将抛出ValueError，提示"Invalid language input. Please enter a valid ISO 639 code or language name."
***
### FunctionDef set_log_level(cls, v)
**set_log_level**: The function of set_log_level is to validate and set the logging level for the application.

**parameters**: The parameters of this Function.
· cls: This parameter refers to the class itself, allowing the method to be called on the class rather than an instance.
· v: A string that represents the desired logging level to be set.

**Code Description**: The set_log_level function is a class method designed to validate and convert a provided string input into a corresponding LogLevel enumeration value. The function first checks if the input value v is of type string. If it is, the function converts the string to uppercase to ensure consistency with the predefined log level constants. 

Next, the function checks if the uppercase version of v exists within the members of the LogLevel enumeration, specifically by referencing LogLevel._value2member_map_. This mapping allows the function to verify if the provided value corresponds to one of the valid log levels defined in the LogLevel class, which includes DEBUG, INFO, WARNING, ERROR, and CRITICAL.

If the value is valid, the function returns the corresponding LogLevel enumeration member. However, if the value does not match any of the predefined log levels, the function raises a ValueError, indicating that the provided log level is invalid. This mechanism ensures that only valid log levels are accepted, maintaining the integrity of the logging configuration within the application.

The set_log_level function is closely related to the LogLevel class, which defines the valid logging levels as an enumeration. This relationship is crucial as it ensures that the logging level set by the ProjectSettings class is always one of the predefined constants, thus preventing runtime errors associated with invalid log levels.

**Note**: When using the set_log_level function, it is important to provide the log level as a string in uppercase to match the enumeration definitions. This practice helps avoid errors and ensures that the logging configuration is set correctly.

**Output Example**: If the input value is "info", the function will convert it to "INFO" and return LogLevel.INFO. If the input value is "verbose", the function will raise a ValueError with the message "Invalid log level: VERBOSE".
***
## ClassDef MaxInputTokens
**MaxInputTokens**: The function of MaxInputTokens is to define and manage the token limits for various AI models.

**attributes**: The attributes of this Class.
· gpt_4o_mini: int - Represents the token limit for the "gpt-4o-mini" model, defaulting to 128,000 tokens.  
· gpt_4o: int - Represents the token limit for the "gpt-4o" model, defaulting to 128,000 tokens.  
· o1_preview: int - Represents the token limit for the "o1-preview" model, defaulting to 128,000 tokens.  
· o1_mini: int - Represents the token limit for the "o1-mini" model, defaulting to 128,000 tokens.  

**Code Description**: The MaxInputTokens class is a subclass of BaseModel, which is likely part of a data validation library such as Pydantic. This class is designed to encapsulate the configuration of token limits for different AI models. Each model has a predefined token limit set to 128,000 tokens. The class utilizes the `Field` function to define these attributes, allowing for the specification of aliases that can be used to refer to these fields in a more user-friendly manner.

The class includes two class methods: `get_valid_models` and `get_token_limit`. The `get_valid_models` method returns a list of valid model names by iterating over the model fields and extracting their aliases. This is useful for validating model names against a known set of options. The `get_token_limit` method takes a model name as an argument, creates an instance of the MaxInputTokens class, and retrieves the corresponding token limit by accessing the attribute that matches the model name (with hyphens replaced by underscores).

The MaxInputTokens class is utilized by other components in the project, specifically in the ChatCompletionSettings class. The `validate_model` method in ChatCompletionSettings calls `MaxInputTokens.get_valid_models()` to ensure that the provided model name is valid. If the model name is not found in the list of valid models, a ValueError is raised, ensuring that only acceptable model names are processed.

Additionally, the `get_token_limit` method in ChatCompletionSettings leverages `MaxInputTokens.get_token_limit(self.model)` to retrieve the token limit for the model specified in the settings. This integration ensures that the token limits are consistently applied and validated across the application.

**Note**: It is important to ensure that the model names used in the application match the aliases defined in the MaxInputTokens class to avoid validation errors. 

**Output Example**: For a valid model name "gpt-4o", calling `MaxInputTokens.get_token_limit("gpt-4o")` would return 128000, indicating the token limit for that model.
### FunctionDef get_valid_models(cls)
**get_valid_models**: get_valid_models的功能是返回所有有效模型的名称或别名列表。

**parameters**: 此函数没有参数。

**Code Description**: get_valid_models是一个类方法，主要用于获取与模型相关的所有字段的别名或名称。它通过访问类的model_fields属性，遍历其中的每一个字段，提取出字段的别名（如果存在）或字段的名称。返回的结果是一个字符串列表，包含了所有有效模型的名称或别名。

在项目中，get_valid_models函数被ChatCompletionSettings类的validate_model方法调用。validate_model方法的作用是验证传入的模型名称是否在有效模型列表中。如果传入的模型名称不在由get_valid_models返回的有效模型列表中，validate_model将抛出一个ValueError异常，提示用户输入的模型无效，并列出所有有效模型。这种设计确保了只有有效的模型名称才能被使用，从而提高了代码的健壮性和可维护性。

**Note**: 使用此代码时，请确保model_fields属性已正确定义并包含所需的字段信息，以避免运行时错误。

**Output Example**: 假设model_fields包含以下字段：
- name: "gpt-3.5-turbo", alias: "gpt-3.5"
- name: "gpt-4", alias: None

那么get_valid_models的返回值将是：
["gpt-3.5", "gpt-4"]
***
### FunctionDef get_token_limit(cls, model_name)
**get_token_limit**: get_token_limit的功能是根据给定的模型名称返回相应的令牌限制值。

**parameters**: 该函数的参数。
· model_name: 字符串类型，表示模型的名称。

**Code Description**: get_token_limit是一个类方法，接受一个字符串参数model_name。该方法首先创建当前类的一个实例，然后通过将model_name中的短横线（-）替换为下划线（_）来获取相应的属性值。最终，它返回该属性的值，该值通常代表与指定模型相关的令牌限制。此方法的设计使得可以灵活地根据不同的模型名称动态获取其对应的令牌限制。

**Note**: 使用该代码时，请确保model_name参数对应的属性在类中是存在的，否则将引发AttributeError。确保传入的模型名称格式正确，以避免不必要的错误。

**Output Example**: 假设调用get_token_limit("gpt-3")，如果gpt-3对应的属性值为4096，则返回值将是4096。
***
## ClassDef ChatCompletionSettings
**ChatCompletionSettings**: The function of ChatCompletionSettings is to manage and validate settings related to chat completion models used in the application.

**attributes**: The attributes of this Class.
· model: str - The model to be used for chat completion, defaulting to "gpt-4o-mini".  
· temperature: PositiveFloat - A float value that influences the randomness of the model's output, defaulting to 0.2.  
· request_timeout: PositiveFloat - The timeout duration for requests, defaulting to 5 seconds.  
· openai_base_url: str - The base URL for the OpenAI API, defaulting to "https://api.openai.com/v1".  
· openai_api_key: SecretStr - The API key required for authentication with the OpenAI service, marked to be excluded from certain outputs.

**Code Description**: The ChatCompletionSettings class inherits from BaseSettings and is designed to encapsulate the configuration settings necessary for interacting with OpenAI's chat completion models. It includes attributes for specifying the model type, temperature, request timeout, base URL, and API key. The class employs field validators to ensure that the provided values for the model and base URL conform to expected formats and constraints.

The `convert_base_url_to_str` method is a class method that converts the base URL into a string format before validation, ensuring that the URL is correctly formatted. The `validate_model` method checks if the specified model is valid by comparing it against a list of acceptable models obtained from the MaxInputTokens class. If the model is invalid, it raises a ValueError with a descriptive message.

Additionally, the class includes a method `get_token_limit`, which retrieves the token limit based on the specified model. This method interacts with the MaxInputTokens class to determine the appropriate limit for the current model setting.

In the context of the project, the ChatCompletionSettings class is instantiated within the Setting class, where it is used to define the chat completion settings for the application. This relationship indicates that any instance of Setting will have a corresponding ChatCompletionSettings object, allowing for structured management of chat-related configurations.

**Note**: It is important to ensure that the model specified is valid and that the API key is securely managed, as it is critical for authenticating requests to the OpenAI service.

**Output Example**: An example of the output when retrieving the token limit for a valid model might look like this:
```
{
  "model": "gpt-4o-mini",
  "token_limit": 4096
}
```
### FunctionDef convert_base_url_to_str(cls, openai_base_url)
**convert_base_url_to_str**: convert_base_url_to_str 的功能是将给定的 openai_base_url 转换为字符串格式。

**parameters**: 此函数的参数。
· openai_base_url: 类型为 HttpUrl 的参数，表示 OpenAI 的基础 URL。

**Code Description**: convert_base_url_to_str 是一个类方法，接受一个 HttpUrl 类型的参数 openai_base_url，并将其转换为字符串。该方法使用 Python 的内置 str() 函数来实现转换。HttpUrl 是一个类型提示，通常用于确保传入的 URL 是有效的格式。此方法的主要用途是在需要将 URL 作为字符串处理时，确保类型的一致性和正确性。

**Note**: 使用此代码时，请确保传入的 openai_base_url 是有效的 HttpUrl 类型，以避免类型错误或异常。

**Output Example**: 假设传入的 openai_base_url 为 "https://api.openai.com/v1/", 则该函数的返回值将是 "https://api.openai.com/v1/"。
***
### FunctionDef validate_model(cls, value)
**validate_model**: The function of validate_model is to ensure that a given model name is valid by checking it against a list of predefined valid models.

**parameters**:
· value: str - A string representing the model name to be validated.

**Code Description**:  
The `validate_model` method is a class method that verifies if a given model name is part of the set of valid model names. This function accepts a single parameter, `value`, which is expected to be a string representing the model name.

1. **Validation Process**:  
   The function calls the `get_valid_models` method from the `MaxInputTokens` class. This method returns a list of valid model names, which includes the aliases of the models defined in the `MaxInputTokens` class. 

2. **Comparison**:  
   The provided `value` (the model name to be validated) is then checked to see if it exists within the list of valid models. If the model name is not found, the function raises a `ValueError`, indicating that the provided model is invalid and listing the valid options.

3. **Return**:  
   If the model name is valid (i.e., it exists in the list of valid models), the function returns the same model name (`value`).

The `validate_model` function is used primarily to ensure that only models which are defined as valid in the system are accepted for further processing. By calling the `MaxInputTokens.get_valid_models()` method, the function directly leverages the list of predefined models to perform this check.

**Note**:  
- It is important to ensure that the `MaxInputTokens.get_valid_models()` method correctly returns the list of valid model names, including any aliases or variations. If the model name provided to `validate_model` does not match a valid entry, a `ValueError` will be raised, which could interrupt the workflow.
- This function expects the model names to be exactly as defined in the valid models list, and does not perform any automatic corrections or formatting on the input value.

**Output Example**:  
For a valid input model name "gpt-4o", assuming this model is present in the valid models list returned by `MaxInputTokens.get_valid_models()`, the function would simply return "gpt-4o".

In the case of an invalid model name like "gpt-5", the function would raise an exception:
```
ValueError: Invalid model 'gpt-5'. Must be one of ['gpt-4o', 'gpt-4o-mini', 'o1-preview', 'o1-mini'].
```
***
### FunctionDef get_token_limit(self)
**get_token_limit**: The function of get_token_limit is to retrieve the token limit associated with a specified AI model.

**parameters**: 
· None.

**Code Description**:  
The `get_token_limit` function is a method defined within the `ChatCompletionSettings` class. It is responsible for retrieving the token limit corresponding to the model specified in the instance's `model` attribute. 

The function works by calling the `get_token_limit` method of the `MaxInputTokens` class, which is designed to return the token limit for a given AI model. The method passes the value of `self.model` (which represents the model name) to `MaxInputTokens.get_token_limit()`. The `get_token_limit` method in `MaxInputTokens` is a class method that accepts a model name as a string and returns the token limit for that model. It does this by accessing the appropriate attribute in the `MaxInputTokens` class, which corresponds to the given model name (with hyphens replaced by underscores).

The relationship with other components in the project is as follows:  
1. The `ChatCompletionSettings` class utilizes the `get_token_limit` method to dynamically fetch the token limit for the model specified in its settings. 
2. The method relies on the `MaxInputTokens` class, which encapsulates predefined token limits for different models. This connection ensures that the `get_token_limit` function in `ChatCompletionSettings` accurately reflects the correct token limit based on the specified model.
3. In the `MaxInputTokens` class, the `get_token_limit` method is a class method that matches model names with their corresponding attributes and retrieves the token limit (defaulting to 128,000 tokens for each model).

**Note**:  
It is important to ensure that the model name specified in `self.model` matches one of the valid model names defined in the `MaxInputTokens` class, such as "gpt-4o" or "o1-mini", to avoid errors. If an invalid model name is provided, the method will raise an exception when attempting to fetch the token limit.

**Output Example**:  
If the `model` attribute of the `ChatCompletionSettings` instance is set to `"gpt-4o"`, calling `get_token_limit()` will return `128000`, which is the token limit for the "gpt-4o" model as defined in the `MaxInputTokens` class.
***
## ClassDef Setting
**Setting**: The function of Setting is to aggregate and manage configuration settings for the project, including project-specific and chat completion settings.

**attributes**: The attributes of this Class.
· project: ProjectSettings - An instance that holds the configuration settings related to the project, including repository paths, documentation hierarchy, language preferences, and logging configurations.  
· chat_completion: ChatCompletionSettings - An instance that manages settings related to chat completion models, including model type, temperature, request timeout, and API key.

**Code Description**: The Setting class inherits from BaseSettings and serves as a central configuration class that encapsulates various settings required for the project. It contains two primary attributes: `project`, which is an instance of the ProjectSettings class, and `chat_completion`, which is an instance of the ChatCompletionSettings class. 

The ProjectSettings class is responsible for managing the configuration settings specific to the project, such as the target repository directory path, hierarchy name for documentation, language preferences, maximum thread count, and logging level. It ensures that the values assigned to these attributes are valid through field validators, enhancing the robustness of the configuration.

The ChatCompletionSettings class, on the other hand, manages settings related to chat completion models used in the application. It includes attributes for specifying the model type, temperature, request timeout, base URL for the OpenAI API, and the API key required for authentication. This class also employs field validators to ensure that the provided values conform to expected formats and constraints.

The Setting class is referenced by the SettingsManager class, which is responsible for managing the instantiation of the Setting object. The SettingsManager maintains a private class attribute `_setting_instance` that holds the instance of the Setting class. The `get_setting` class method checks if the `_setting_instance` has been initialized; if not, it creates a new instance of Setting. This design pattern ensures that there is a single instance of the Setting class throughout the application, promoting consistent access to configuration settings.

**Note**: When using the Setting class, it is important to ensure that the values assigned to the attributes of ProjectSettings and ChatCompletionSettings are valid to avoid runtime errors. Proper management of the API key in ChatCompletionSettings is crucial for secure authentication with the OpenAI service.
## ClassDef SettingsManager
**SettingsManager**: The function of SettingsManager is to manage the instantiation and access to the configuration settings for the project.

**attributes**: The attributes of this Class.
· _setting_instance: Optional[Setting] - A private class attribute that holds the singleton instance of the Setting class, initially set to None.

**Code Description**: The SettingsManager class is designed to provide a centralized access point for the configuration settings of the project. It utilizes a class method, `get_setting`, to ensure that there is only one instance of the Setting class throughout the application, implementing the Singleton design pattern.

The class maintains a private class attribute, `_setting_instance`, which is initially set to None. When the `get_setting` method is called, it first checks if `_setting_instance` is None, indicating that the Setting object has not yet been instantiated. If this is the case, it creates a new instance of the Setting class and assigns it to `_setting_instance`. This ensures that subsequent calls to `get_setting` return the same instance of the Setting class, thereby promoting consistent access to configuration settings across the application.

The SettingsManager class is called by various components within the project, including the ChangeDetector, ChatEngine, and MetaInfo classes. For instance, in the `get_to_be_staged_files` method of the ChangeDetector class, the SettingsManager is invoked to retrieve the current settings, which are then used to determine the project hierarchy and manage file staging. Similarly, in the ChatEngine's `__init__` method, the SettingsManager is used to access the OpenAI API settings, ensuring that the chat engine is configured correctly with the necessary parameters.

This design allows for a clear separation of concerns, where the SettingsManager handles the instantiation and retrieval of settings, while other components focus on their specific functionalities. By centralizing the configuration management, the SettingsManager enhances the maintainability and scalability of the project.

**Note**: It is important to ensure that the Setting class is properly configured before accessing its attributes through the SettingsManager. Any misconfiguration may lead to runtime errors when the application attempts to utilize the settings.

**Output Example**: A possible appearance of the code's return value when calling `SettingsManager.get_setting()` could be an instance of the Setting class containing project-specific configurations such as project paths, logging levels, and chat completion settings.
### FunctionDef get_setting(cls)
**get_setting**: The function of get_setting is to provide a singleton instance of the Setting class, ensuring that configuration settings are consistently accessed throughout the application.

**parameters**: The parameters of this Function.
· No parameters are required for this function.

**Code Description**: The get_setting class method is a crucial component of the SettingsManager class, designed to manage the instantiation of the Setting object. This method first checks if the class attribute `_setting_instance` is None, indicating that the Setting instance has not yet been created. If it is None, the method initializes `_setting_instance` by creating a new instance of the Setting class. This ensures that only one instance of the Setting class exists, adhering to the singleton design pattern. The method then returns the `_setting_instance`, allowing other parts of the application to access the configuration settings encapsulated within the Setting instance.

The Setting class itself is responsible for managing various configuration settings for the project, including project-specific settings and chat completion settings. It contains attributes that hold instances of ProjectSettings and ChatCompletionSettings, which further manage specific configurations related to the project and chat functionalities, respectively.

The get_setting method is called by various components within the project, such as the ChangeDetector, ChatEngine, and MetaInfo classes. For instance, in the ChangeDetector's get_to_be_staged_files method, get_setting is invoked to retrieve the current project settings, which are then used to determine which files need to be staged based on the project's hierarchy and markdown documentation requirements. Similarly, in the ChatEngine's __init__ method, get_setting is called to configure the OpenAI API settings, ensuring that the chat functionalities are properly initialized with the correct parameters.

This method plays a vital role in maintaining a centralized access point for configuration settings, promoting consistency and reducing the risk of errors that may arise from multiple instances of the Setting class.

**Note**: It is important to ensure that the Setting class is properly configured before accessing its attributes through get_setting. Any misconfiguration may lead to runtime errors or unexpected behavior in the application.

**Output Example**: A possible appearance of the code's return value could be an instance of the Setting class containing initialized attributes for project settings and chat completion settings, such as:
```
Setting(
    project=ProjectSettings(
        target_repo='path/to/repo',
        hierarchy_name='documentation',
        log_level='INFO',
        ignore_list=['*.pyc', '__pycache__']
    ),
    chat_completion=ChatCompletionSettings(
        openai_api_key='your_api_key',
        openai_base_url='https://api.openai.com',
        request_timeout=30,
        model='gpt-3.5-turbo',
        temperature=0.7
    )
)
```
***



================================================
FILE: markdown_docs/repo_agent/utils/gitignore_checker.md
================================================
## ClassDef GitignoreChecker
**GitignoreChecker**: The function of GitignoreChecker is to check files and folders in a specified directory against patterns defined in a .gitignore file, identifying which files are not ignored and have a specific extension.

**attributes**: The attributes of this Class.
· directory: The directory to be checked for files and folders.
· gitignore_path: The path to the .gitignore file.
· folder_patterns: A list of folder patterns extracted from the .gitignore file.
· file_patterns: A list of file patterns extracted from the .gitignore file.

**Code Description**: The GitignoreChecker class is designed to facilitate the checking of files and folders in a specified directory against the rules defined in a .gitignore file. Upon initialization, it requires two parameters: the directory to be checked and the path to the .gitignore file. The constructor reads the .gitignore file, parsing its contents to separate folder patterns from file patterns.

The class contains several methods:
- `_load_gitignore_patterns`: This method attempts to load the .gitignore file from the specified path. If the file is not found, it falls back to a default .gitignore file located two directories up from the current file. It returns a tuple containing lists of folder and file patterns.
- `_parse_gitignore`: This static method processes the content of the .gitignore file, extracting valid patterns while ignoring comments and empty lines.
- `_split_gitignore_patterns`: This static method takes a list of patterns and categorizes them into folder patterns (ending with a '/') and file patterns.
- `_is_ignored`: This static method checks if a given path matches any of the provided patterns, determining if the path should be ignored based on whether it is a directory or a file.
- `check_files_and_folders`: This method walks through the specified directory, checking each file and folder against the extracted patterns. It returns a list of file paths that are not ignored and have a '.py' extension, with paths being relative to the specified directory.

The GitignoreChecker is utilized in the `generate_overall_structure` method of the FileHandler class. In this context, it is instantiated to check the repository's directory for files that are not ignored by the .gitignore rules. The method iterates over the list of non-ignored files, performing additional checks and processing for each file, ultimately contributing to the generation of the repository's overall structure.

**Note**: When using the GitignoreChecker, ensure that the specified .gitignore file is accessible and correctly formatted to avoid falling back to the default path unintentionally.

**Output Example**: An example output of the `check_files_and_folders` method might look like this:
```
[
    "src/module1.py",
    "src/module2.py",
    "tests/test_module1.py"
]
``` 
This output indicates that the listed Python files are not ignored according to the rules defined in the .gitignore file.
### FunctionDef __init__(self, directory, gitignore_path)
**__init__**: The function of __init__ is to initialize the GitignoreChecker with a specific directory and the path to a .gitignore file.

**parameters**: The parameters of this Function.
· directory: The directory to be checked.
· gitignore_path: The path to the .gitignore file.

**Code Description**: The __init__ method is the constructor for the GitignoreChecker class. It takes two parameters: `directory`, which specifies the directory that will be checked for files and folders to ignore, and `gitignore_path`, which indicates the location of the .gitignore file that contains the ignore patterns. Upon initialization, these parameters are assigned to instance variables `self.directory` and `self.gitignore_path`, respectively.

Additionally, the constructor calls the private method `_load_gitignore_patterns`, which is responsible for loading and parsing the .gitignore file. This method returns a tuple containing two lists: one for folder patterns and another for file patterns. These lists are then assigned to the instance variables `self.folder_patterns` and `self.file_patterns`. This structured approach ensures that the GitignoreChecker has immediate access to the relevant patterns for processing files and directories according to the rules defined in the .gitignore file.

The `_load_gitignore_patterns` method is crucial for the initialization process, as it ensures that the patterns are correctly loaded and categorized. If the specified .gitignore file is not found, the method will attempt to load a default .gitignore file from a predetermined location, ensuring that the GitignoreChecker can still function even in the absence of a user-defined file.

**Note**: It is important to ensure that the provided .gitignore file is correctly formatted and accessible at the specified path to avoid falling back to the default file unintentionally. Proper handling of file paths and existence checks is essential for the reliable operation of the GitignoreChecker.
***
### FunctionDef _load_gitignore_patterns(self)
**_load_gitignore_patterns**: The function of _load_gitignore_patterns is to load and parse the .gitignore file, then split the patterns into folder and file patterns.

**parameters**: The parameters of this Function.
· self: An instance of the GitignoreChecker class, which contains the attributes necessary for loading the .gitignore file.

**Code Description**: The _load_gitignore_patterns method is responsible for reading the content of a .gitignore file from a specified path. If the specified file is not found, it falls back to a default .gitignore file located two directories up from the current file's directory. The method attempts to open the .gitignore file in read mode with UTF-8 encoding. If successful, it reads the entire content of the file into a string variable named gitignore_content. In the event of a FileNotFoundError, the method constructs a default path and attempts to read from that file instead.

Once the content of the .gitignore file is obtained, the method calls the _parse_gitignore function, passing the gitignore_content as an argument. This function processes the content and returns a list of patterns that are relevant for ignoring files and directories. Subsequently, the _load_gitignore_patterns method calls the _split_gitignore_patterns function, providing it with the list of patterns. This function categorizes the patterns into two separate lists: one for folder patterns and another for file patterns. Finally, _load_gitignore_patterns returns a tuple containing these two lists.

This method is invoked during the initialization of the GitignoreChecker class, where it is used to populate the folder_patterns and file_patterns attributes with the relevant patterns extracted from the .gitignore file. This structured approach ensures that the patterns are readily available for further processing or application within the project.

**Note**: It is essential to ensure that the .gitignore file is properly formatted and accessible at the specified path to avoid falling back to the default file unintentionally.

**Output Example**: An example of the return value from _load_gitignore_patterns could be:
```python
(['src', 'docs'], ['README.md', 'LICENSE'])
```
In this example, the method would return a tuple where the first list contains folder patterns 'src' and 'docs', while the second list contains file patterns 'README.md' and 'LICENSE'.
***
### FunctionDef _parse_gitignore(gitignore_content)
**_parse_gitignore**: The function of _parse_gitignore is to parse the content of a .gitignore file and return a list of patterns.

**parameters**: The parameters of this Function.
· gitignore_content: A string representing the content of the .gitignore file.

**Code Description**: The _parse_gitignore function is designed to process the content of a .gitignore file, which typically contains rules for ignoring files and directories in a Git repository. The function takes a single argument, gitignore_content, which is expected to be a string containing the raw text of the .gitignore file.

The function begins by initializing an empty list called patterns. It then splits the gitignore_content into individual lines using the splitlines() method. For each line, it performs the following operations:
1. It trims any leading or trailing whitespace using the strip() method.
2. It checks if the line is not empty and does not start with a "#" character, which denotes a comment in .gitignore files.
3. If the line meets these criteria, it appends the line to the patterns list.

Once all lines have been processed, the function returns the patterns list, which contains only the relevant patterns extracted from the .gitignore content.

The _parse_gitignore function is called by the _load_gitignore_patterns method within the GitignoreChecker class. The _load_gitignore_patterns method is responsible for loading the content of a .gitignore file from a specified path. After reading the file content, it invokes _parse_gitignore to extract the patterns before further processing them. This relationship highlights the utility of _parse_gitignore as a helper function that simplifies the task of filtering out valid patterns from the potentially noisy content of a .gitignore file.

**Note**: It is important to ensure that the input to _parse_gitignore is a properly formatted string representing the content of a .gitignore file. Lines that are empty or comments will be ignored in the output.

**Output Example**: An example of the return value from _parse_gitignore could be:
```python
["*.log", "temp/", "build/", "# Ignore all .env files"]
```
In this example, the function would return a list containing the patterns that are relevant for ignoring files and directories, excluding any comments or empty lines.
***
### FunctionDef _split_gitignore_patterns(gitignore_patterns)
**_split_gitignore_patterns**: The function of _split_gitignore_patterns is to separate .gitignore patterns into distinct lists for folder patterns and file patterns.

**parameters**: The parameters of this Function.
· gitignore_patterns: A list of patterns extracted from the .gitignore file.

**Code Description**: The _split_gitignore_patterns function takes a list of patterns from a .gitignore file as input. It iterates through each pattern and checks whether it ends with a forward slash ("/"). If a pattern ends with "/", it is identified as a folder pattern, and the trailing slash is removed before appending it to the folder_patterns list. If a pattern does not end with "/", it is treated as a file pattern and is added to the file_patterns list. The function ultimately returns a tuple containing two lists: the first list includes all folder patterns, while the second list contains all file patterns.

This function is called by the _load_gitignore_patterns method within the GitignoreChecker class. The _load_gitignore_patterns method is responsible for loading and parsing the contents of a .gitignore file. After reading the file, it utilizes the _parse_gitignore method to extract the patterns from the content. Once the patterns are obtained, _load_gitignore_patterns calls _split_gitignore_patterns to categorize these patterns into folder and file patterns before returning them as a tuple. This structured approach ensures that the patterns are organized for further processing or application within the project.

**Note**: It is important to ensure that the input list of gitignore_patterns is properly formatted according to .gitignore syntax to achieve accurate results when splitting the patterns.

**Output Example**: An example of the function's return value could be:
```python
(['src', 'docs'], ['README.md', 'LICENSE'])
```
In this example, the first list contains folder patterns 'src' and 'docs', while the second list contains file patterns 'README.md' and 'LICENSE'.
***
### FunctionDef _is_ignored(path, patterns, is_dir)
**_is_ignored**: The function of _is_ignored is to determine if a given path matches any specified patterns, indicating whether the path should be ignored.

**parameters**: The parameters of this Function.
· parameter1: path (str) - The path to check against the patterns.
· parameter2: patterns (list) - A list of patterns that the path will be checked against.
· parameter3: is_dir (bool) - A boolean indicating if the path is a directory; defaults to False.

**Code Description**: The _is_ignored function checks if the provided path matches any of the patterns in the given list. It utilizes the fnmatch module to perform pattern matching. The function iterates through each pattern in the patterns list and checks if the path matches the pattern directly. If the path is a directory (indicated by the is_dir parameter being True), it also checks if the pattern ends with a slash ("/") and if the path matches the pattern without the trailing slash. If any match is found, the function returns True, indicating that the path should be ignored. If no matches are found after checking all patterns, it returns False.

This function is called by the check_files_and_folders method within the GitignoreChecker class. The check_files_and_folders method is responsible for traversing a specified directory and checking each file and folder against the patterns defined for files and folders. It uses _is_ignored to filter out any directories and files that should be ignored based on the patterns provided. The result of this method is a list of files that are not ignored and have a '.py' extension, thus ensuring that only relevant files are returned for further processing.

**Note**: It is important to ensure that the patterns provided are correctly formatted for fnmatch to work as expected. Additionally, the is_dir parameter should be set appropriately when checking directory paths to ensure accurate matching.

**Output Example**: If the function is called with the path "src/main.py" and the patterns ["*.py", "test/"], the expected return value would be True if "src/main.py" matches any of the patterns, indicating that it is not ignored. If the path were "src/test.py" and the patterns were ["test/"], the function would return True, indicating that it should be ignored.
***
### FunctionDef check_files_and_folders(self)
**check_files_and_folders**: The function of check_files_and_folders is to check all files and folders in the specified directory against the defined gitignore patterns and return a list of files that are not ignored and have the '.py' extension.

**parameters**: The parameters of this Function.
· parameter1: None - This function does not take any parameters directly as it operates on the instance's attributes.

**Code Description**: The check_files_and_folders method is responsible for traversing the directory specified by the instance variable self.directory. It utilizes the os.walk function to iterate through all directories and files within the specified path. For each directory, it filters out those that should be ignored based on the patterns defined in self.folder_patterns by calling the _is_ignored method with the is_dir parameter set to True.

For each file encountered, the method constructs the full file path and its relative path to the base directory. It then checks if the file should be ignored by calling the _is_ignored method again, this time with the file name and the patterns defined in self.file_patterns. Additionally, it checks if the file has a '.py' extension. If both conditions are satisfied (the file is not ignored and has a '.py' extension), the relative path of the file is added to the not_ignored_files list.

The method ultimately returns a list of paths to Python files that are not ignored, allowing further processing of relevant files in the project.

This method is called by the generate_overall_structure method in the FileHandler class. In this context, it is used to gather a list of files that should be processed from a repository, excluding any files that are ignored according to the gitignore patterns. The results from check_files_and_folders are then iterated over, and each file is further processed to generate the overall structure of the repository.

**Note**: It is essential to ensure that the gitignore patterns are correctly defined and formatted for accurate matching. The method relies on the _is_ignored function to determine which files and directories should be excluded based on these patterns.

**Output Example**: If the method is executed in a directory containing files such as "script.py", "test_script.py", and "README.md", and the gitignore patterns include "*.py", the expected return value would be a list like ["script.py", "test_script.py"] if those files are not ignored.
***



================================================
FILE: markdown_docs/repo_agent/utils/meta_info_utils.md
================================================
## FunctionDef make_fake_files
**make_fake_files**: The function of make_fake_files is to analyze the git status of a repository and create temporary files that reflect the current state of the working directory, specifically for untracked and unstaged changes.

**parameters**: The parameters of this Function.
· No parameters are required for this function.

**Code Description**: The make_fake_files function is designed to interact with a Git repository to detect changes in the working directory that have not been staged for commit. It performs the following key operations:

1. **Delete Existing Fake Files**: The function begins by calling delete_fake_files to ensure that any previously created temporary files are removed before generating new ones.

2. **Retrieve Project Settings**: It retrieves the current project settings using the SettingsManager's get_setting method, which ensures consistent access to configuration settings throughout the application.

3. **Initialize Git Repository**: The function initializes a Git repository object using the target repository path specified in the project settings.

4. **Detect Unstaged Changes**: It identifies unstaged changes in the repository using the index.diff method, which returns a list of modified files that have not been added to the staging area. Additionally, it collects untracked files that exist in the file system but are not tracked by Git.

5. **Skip Untracked Python Files**: The function iterates through the list of untracked files and skips any that have a ".py" extension, logging a message for each skipped file.

6. **Handle New and Modified Files**: For files that have been modified (but not staged), the function checks if they end with a specific substring (latest_verison_substring). If they do, an error is logged, and the function exits. Otherwise, it renames the original file to include the latest version substring and creates a new file with the original name, writing the original content back into it.

7. **Return Values**: Finally, the function returns a dictionary mapping the original file paths to their corresponding fake file paths, along with a list of files that were skipped during processing.

The make_fake_files function is called within the diff function in the main.py file. This function is responsible for checking for changes in the repository and determining which documents need to be updated or generated. By calling make_fake_files, the diff function ensures that the current state of the repository is accurately reflected in the documentation process.

**Note**: It is crucial to ensure that the target repository is properly configured and that the latest_verison_substring does not conflict with existing file names. Any misconfiguration may lead to runtime errors or unexpected behavior during the execution of this function.

**Output Example**: A possible appearance of the code's return value when calling make_fake_files could be:
```
({
    'original_file_path.py': 'original_file_path.latest_version',
    'another_file.py': 'another_file.latest_version'
}, ['skipped_file.py'])
```
## FunctionDef delete_fake_files
**delete_fake_files**: The function of delete_fake_files is to remove temporary files generated during the documentation process after the task execution is completed.

**parameters**: The parameters of this Function.
· No parameters are required for this function.

**Code Description**: The delete_fake_files function is responsible for cleaning up temporary files, referred to as "fake files," that are created during the documentation generation process. This function utilizes a nested helper function, gci, which performs a recursive traversal of the directory specified by the project settings to identify and delete or rename files based on specific criteria.

The function begins by retrieving the project settings through the SettingsManager's get_setting method, which ensures that the configuration settings are consistently accessed throughout the application. The gci function is then called with the target repository path, which is obtained from the settings.

Within the gci function, the following operations are performed:
1. The function lists all files and directories within the specified filepath.
2. For each file, it checks if it is a directory and recursively calls itself if it is.
3. If the file ends with a specific substring (latest_verison_substring), it indicates that it is a temporary file. The function then constructs the original file name by replacing the substring with ".py".
4. If the original file exists and is successfully deleted, the function prints a message indicating that the temporary file has been deleted. If the temporary file is empty, it is also deleted.
5. If the original file exists but is not empty, the temporary file is renamed back to the original file name, and a message is printed indicating that the latest version has been recovered.

The delete_fake_files function is called in various parts of the project, including the clean function in main.py, which explicitly invokes delete_fake_files to ensure that all temporary files are removed after the documentation process. Additionally, it is called within the diff function to clean up any fake files before checking for changes in the repository. The run method of the Runner class also calls delete_fake_files after completing the document update process, ensuring that any temporary files created during the run are cleaned up.

**Note**: It is important to ensure that the target repository is correctly configured and accessible before invoking delete_fake_files. Any issues with file permissions or incorrect paths may lead to runtime errors during the deletion or renaming processes.
### FunctionDef gci(filepath)
**gci**: The function of gci is to traverse a specified directory and its subdirectories to delete or rename files based on specific criteria.

**parameters**: The parameters of this Function.
· filepath: A string representing the path of the directory to be traversed.

**Code Description**: The gci function begins by listing all files and directories within the specified filepath. It iterates through each item found in the directory. If an item is a directory, the function calls itself recursively to traverse that subdirectory. For files, it checks if the filename ends with a specific substring defined as `latest_verison_substring`. If this condition is met, the function constructs an original filename by replacing the substring with ".py". 

The function then checks the size of the file. If the file size is zero, it indicates that the file is empty, and the function proceeds to delete both the empty file and its corresponding original file. A message is printed to the console indicating the deletion of the temporary file. Conversely, if the file is not empty, the function renames the temporary file back to its original name and prints a message indicating that the latest version is being recovered.

This function effectively manages temporary files by either deleting them if they are empty or restoring the original file if they contain data, ensuring that the directory remains clean and organized.

**Note**: It is important to ensure that the `latest_verison_substring` variable is defined in the scope where this function is used, as it is crucial for determining which files to process. Additionally, the function relies on the presence of the `setting.project.target_repo` variable to format the output messages correctly.
***



================================================
FILE: markdown_docs/tests/test_change_detector.md
================================================
## ClassDef TestChangeDetector
**TestChangeDetector**: The function of TestChangeDetector is to perform unit tests on the ChangeDetector class, specifically focusing on the detection and management of staged and unstaged files in a Git repository.

**attributes**: The attributes of this Class.
· test_repo_path: The file path to the test repository created for the unit tests.
· repo: The initialized Git repository object used for testing.

**Code Description**: The TestChangeDetector class is a unit test case that inherits from unittest.TestCase, providing a framework for testing the functionality of the ChangeDetector class. The class includes setup and teardown methods to prepare and clean up the test environment, specifically a Git repository used for testing file changes.

The setUpClass method is a class method that initializes the test environment before any tests are run. It defines the path for the test repository, creates the directory if it does not exist, initializes a new Git repository, and configures user information for Git operations. It also creates two test files: a Python file and a Markdown file, and performs an initial commit to the repository.

The class contains three test methods:
1. test_get_staged_pys: This method tests the ChangeDetector's ability to identify staged Python files. It creates a new Python file, stages it, and asserts that the file is included in the list of staged files returned by the ChangeDetector.
   
2. test_get_unstaged_mds: This method tests the ChangeDetector's ability to identify unstaged Markdown files. It modifies an existing Markdown file without staging it and asserts that the modified file is included in the list of unstaged files returned by the ChangeDetector.

3. test_add_unstaged_mds: This method ensures that there are unstaged Markdown files and then uses the ChangeDetector to stage them. It checks that after the staging operation, there are no remaining unstaged Markdown files, asserting that the operation was successful.

The tearDownClass method is a class method that cleans up the test environment after all tests have been executed. It closes the Git repository and removes the test repository directory to ensure no residual files remain.

**Note**: It is important to ensure that the ChangeDetector class is properly implemented and available in the testing environment for these tests to execute successfully. Additionally, the tests rely on the presence of the Git command-line tools and the appropriate permissions to create and manipulate files and directories.
### FunctionDef setUpClass(cls)
**setUpClass**: setUpClass的功能是为测试准备一个Git仓库及相关文件。

**parameters**: 此函数没有参数。

**Code Description**: 
setUpClass是一个类方法，用于在测试类执行之前设置测试环境。该方法首先定义了测试仓库的路径，将其设置为当前文件所在目录下的'test_repo'文件夹。如果该文件夹不存在，方法会创建它。接着，使用GitPython库初始化一个新的Git仓库，并将其与指定的路径关联。

在初始化Git仓库后，方法配置了Git用户信息，包括用户的电子邮件和姓名，以便在后续的Git操作中使用。接下来，方法创建了两个测试文件：一个Python文件'test_file.py'，其中包含一行打印语句；另一个Markdown文件'test_file.md'，其中包含一个Markdown标题。

最后，方法模拟了Git操作，通过将所有文件添加到暂存区并提交一个初始提交，完成了测试环境的设置。这些操作确保了在测试执行时，测试环境是干净且可控的。

**Note**: 使用此方法时，请确保在测试类中调用setUpClass，以便在所有测试用例执行之前正确设置测试环境。同时，确保已安装GitPython库，以支持Git操作。
***
### FunctionDef test_get_staged_pys(self)
**test_get_staged_pys**: The function of test_get_staged_pys is to verify that a newly created Python file is correctly identified as staged in the Git repository.

**parameters**: The parameters of this Function.
· None

**Code Description**: The test_get_staged_pys function is a unit test designed to validate the functionality of the ChangeDetector class, specifically its ability to detect staged Python files within a Git repository. The function begins by creating a new Python file named 'new_test_file.py' in a specified test repository path. This file contains a simple print statement. Once the file is created, it is added to the staging area of the Git repository using the Git command `git add`.

Following the staging of the new file, an instance of the ChangeDetector class is instantiated with the test repository path. The method get_staged_pys of the ChangeDetector instance is then called to retrieve a list of Python files that are currently staged for commit. This method is responsible for checking the differences between the staging area and the last commit (HEAD) to identify which files have been added or modified.

The test then asserts that 'new_test_file.py' is included in the list of staged files returned by get_staged_pys. This assertion confirms that the ChangeDetector class is functioning as expected, accurately tracking the newly staged Python file. Additionally, the function prints the list of staged Python files for verification purposes.

This test is crucial for ensuring that the ChangeDetector class operates correctly in identifying changes within a Git repository, particularly for Python files. It serves as a safeguard against potential regressions in the functionality of the change detection mechanism.

**Note**: It is important to ensure that the test environment is properly set up, including the availability of a valid Git repository and the necessary permissions to create and stage files. The GitPython library must also be correctly configured to facilitate interaction with the Git repository.
***
### FunctionDef test_get_unstaged_mds(self)
**test_get_unstaged_mds**: The function of test_get_unstaged_mds is to verify that a modified Markdown file, which has not been staged, is correctly identified as an unstaged file by the ChangeDetector class.

**parameters**: The parameters of this Function.
· No parameters are required for this function.

**Code Description**: The test_get_unstaged_mds function is a unit test designed to validate the functionality of the ChangeDetector class, specifically its ability to identify unstaged Markdown files in a Git repository. The function performs the following operations:

1. It begins by defining the path to a Markdown file named 'test_file.md' within a test repository directory specified by `self.test_repo_path`.
2. The function opens this Markdown file in append mode and writes additional content to it, simulating a modification that has not yet been staged.
3. An instance of the ChangeDetector class is then created, initialized with the path to the test repository.
4. The method `get_to_be_staged_files` of the ChangeDetector instance is called to retrieve a list of files that have been modified but not staged.
5. The function asserts that 'test_file.md' is included in the list of unstaged files by checking if its basename is present in the returned list.
6. Finally, it prints the list of unstaged Markdown files for verification.

This function is called within the test_add_unstaged_mds function, which ensures that there is at least one unstaged Markdown file before attempting to add unstaged files to the staging area. The test_add_unstaged_mds function relies on the successful execution of test_get_unstaged_mds to confirm that the ChangeDetector can accurately identify unstaged files, thereby establishing a dependency between these two test functions.

**Note**: It is essential to ensure that the test repository is correctly set up and that the necessary files exist before running this test. The test environment should be clean to avoid false positives or negatives in the test results.
***
### FunctionDef test_add_unstaged_mds(self)
**test_add_unstaged_mds**: The function of test_add_unstaged_mds is to verify that the ChangeDetector class correctly stages unstaged Markdown files in a Git repository.

**parameters**: The parameters of this Function.
· No parameters are required for this function.

**Code Description**: The test_add_unstaged_mds function is a unit test designed to validate the functionality of the ChangeDetector class, specifically its ability to add unstaged Markdown files to the staging area of a Git repository. The function performs the following operations:

1. It first ensures that there is at least one unstaged Markdown file by invoking the test_get_unstaged_mds function. This function modifies a Markdown file in the test repository, ensuring that it is recognized as unstaged.

2. An instance of the ChangeDetector class is created, initialized with the path to the test repository specified by `self.test_repo_path`. This instance will be used to manage the staging of files.

3. The add_unstaged_files method of the ChangeDetector instance is called. This method identifies all unstaged files that meet specific criteria and stages them in the Git repository.

4. After attempting to stage the files, the function retrieves the list of files that are still unstaged by calling the get_to_be_staged_files method. This method checks for any files that remain unstaged after the add operation.

5. The function asserts that the length of the list of unstaged files after the add operation is zero, indicating that all unstaged Markdown files have been successfully staged.

6. Finally, it prints the number of remaining unstaged Markdown files, which should be zero if the test passes.

This function is dependent on the successful execution of the test_get_unstaged_mds function, which ensures that there is at least one unstaged Markdown file before the add operation is attempted. The relationship between these two functions is crucial, as test_add_unstaged_mds relies on the outcome of test_get_unstaged_mds to validate the staging functionality of the ChangeDetector class.

**Note**: It is essential to ensure that the test repository is correctly set up and that the necessary files exist before running this test. The test environment should be clean to avoid false positives or negatives in the test results.
***
### FunctionDef tearDownClass(cls)
**tearDownClass**: tearDownClass的功能是清理测试仓库。

**parameters**: 该函数没有参数。

**Code Description**: 
tearDownClass是一个类方法，用于在测试类的所有测试用例执行完毕后进行清理工作。该方法首先调用cls.repo.close()，用于关闭与测试仓库相关的资源，确保没有未关闭的连接或文件句柄。接着，使用os.system('rm -rf ' + cls.test_repo_path)命令删除测试仓库的文件夹及其内容。这里的cls.test_repo_path是一个类属性，指向测试仓库的路径。通过这种方式，tearDownClass确保了测试环境的整洁，避免了后续测试受到之前测试的影响。

**Note**: 使用该函数时，请确保在测试用例执行后调用，以避免资源泄漏或文件冲突。同时，注意使用os.system删除文件时要小心，以免误删其他重要文件。
***



================================================
FILE: markdown_docs/tests/test_json_handler.md
================================================
## ClassDef TestJsonFileProcessor
**TestJsonFileProcessor**: The function of TestJsonFileProcessor is to test the functionalities of the JsonFileProcessor class, specifically its methods for reading and extracting data from JSON files.

**attributes**: The attributes of this Class.
· processor: An instance of the JsonFileProcessor class initialized with the filename "test.json".

**Code Description**: The TestJsonFileProcessor class is a unit test case that inherits from unittest.TestCase. It is designed to validate the behavior of the JsonFileProcessor class, which is responsible for handling JSON file operations. The class contains several test methods that utilize the unittest framework's features, such as setup methods and mocking.

The setUp method initializes an instance of JsonFileProcessor with a test JSON file named "test.json". This setup is executed before each test method runs, ensuring that each test has a fresh instance of the processor.

The test_read_json_file method tests the read_json_file method of the JsonFileProcessor class. It uses the @patch decorator to mock the built-in open function, simulating the reading of a JSON file containing a specific structure. The test asserts that the data returned by read_json_file matches the expected dictionary structure and verifies that the open function was called with the correct parameters.

The test_extract_md_contents method tests the extract_md_contents method of the JsonFileProcessor class. It mocks the read_json_file method to return a predefined JSON structure. The test checks that the extracted markdown content includes the expected value "content1".

The test_search_in_json_nested method tests the search_in_json_nested method of the JsonFileProcessor class. Similar to the previous tests, it mocks the open function to provide a different JSON structure. The test asserts that the result of the search matches the expected dictionary for the specified file name and verifies the correct invocation of the open function.

**Note**: It is important to ensure that the JsonFileProcessor class is implemented correctly for these tests to pass. The tests rely on the structure of the JSON data being consistent with the expectations set in the test cases.

**Output Example**: 
For the test_read_json_file method, the expected output when read_json_file is called would be:
{"files": [{"objects": [{"md_content": "content1"}]}]} 

For the test_extract_md_contents method, the expected output for md_contents would include:
["content1"]

For the test_search_in_json_nested method, the expected output when searching for "file1" would be:
{"name": "file1"}
### FunctionDef setUp(self)
**setUp**: setUp的功能是初始化测试环境。

**parameters**: 该函数没有参数。

**Code Description**: setUp函数是一个测试准备函数，通常在单元测试框架中使用。在这个函数中，创建了一个名为processor的实例，类型为JsonFileProcessor，并传入了一个字符串参数"test.json"。这个实例化的过程意味着在每个测试用例执行之前，都会创建一个新的JsonFileProcessor对象，确保每个测试用例都在一个干净的状态下运行。JsonFileProcessor类的具体功能和实现细节并未在此代码片段中提供，但可以推测它与处理JSON文件相关。

**Note**: 使用setUp函数时，确保JsonFileProcessor类已正确实现，并且"test.json"文件存在于预期的路径中，以避免在测试执行时出现文件未找到的错误。
***
### FunctionDef test_read_json_file(self, mock_file)
**test_read_json_file**: The function of test_read_json_file is 测试 read_json_file 方法的功能。

**parameters**: 此函数的参数。
· mock_file: 一个模拟文件对象，用于测试文件读取操作。

**Code Description**: 该函数用于测试 `read_json_file` 方法的正确性。首先，它调用 `self.processor.read_json_file()` 方法以读取 JSON 文件的数据。接着，使用 `self.assertEqual` 方法验证读取的数据是否与预期的字典结构相符，即 `{"files": [{"objects": [{"md_content": "content1"}]}]}`。最后，`mock_file.assert_called_with("test.json", "r", encoding="utf-8")` 用于确认在读取文件时，是否以正确的参数调用了模拟的文件对象，确保文件名为 "test.json"，模式为只读（"r"），并且使用 UTF-8 编码。

**Note**: 使用此代码时，请确保已正确设置模拟文件对象，以便能够准确测试文件读取功能。同时，确保 `read_json_file` 方法能够处理预期的文件格式和内容。
***
### FunctionDef test_extract_md_contents(self, mock_read_json)
**test_extract_md_contents**: The function of test_extract_md_contents is 测试 extract_md_contents 方法的功能。

**parameters**: 此函数的参数。
· mock_read_json: 一个模拟的函数，用于替代实际的 JSON 读取操作。

**Code Description**: 
该函数主要用于测试 `extract_md_contents` 方法的正确性。首先，使用 `mock_read_json` 模拟读取 JSON 文件的操作，返回一个包含文件信息的字典，其中包含一个对象列表，列表中的每个对象都有一个 `md_content` 字段。具体来说，模拟返回的 JSON 数据结构为：
```json
{
  "files": [
    {
      "objects": [
        {
          "md_content": "content1"
        }
      ]
    }
  ]
}
```
接下来，调用 `self.processor.extract_md_contents()` 方法，该方法的目的是提取所有的 `md_content` 内容。最后，使用 `self.assertIn("content1", md_contents)` 断言来验证提取的内容中是否包含 "content1"。如果包含，则测试通过，表明 `extract_md_contents` 方法能够正确提取出 JSON 数据中的 Markdown 内容。

**Note**: 使用此代码时，请确保 `extract_md_contents` 方法能够处理模拟的 JSON 数据结构，并且在测试环境中正确配置了 `mock_read_json`。

**Output Example**: 该函数的返回值可能类似于以下结构：
```python
["content1"]
```
***
### FunctionDef test_search_in_json_nested(self, mock_file)
**test_search_in_json_nested**: The function of test_search_in_json_nested is 测试 search_in_json_nested 方法的功能。

**parameters**: 该函数的参数。
· parameter1: mock_file - 一个模拟文件对象，用于测试文件操作。

**Code Description**: 该函数用于测试 `search_in_json_nested` 方法的功能。首先，它调用 `self.processor.search_in_json_nested` 方法，传入两个参数：文件名 `"test.json"` 和要搜索的关键字 `"file1"`。该方法的预期结果是返回一个字典 `{"name": "file1"}`，表示在 JSON 文件中成功找到与关键字匹配的条目。接着，使用 `self.assertEqual` 方法验证返回结果是否与预期结果相符。如果结果匹配，则测试通过。最后，`mock_file.assert_called_with` 用于验证在测试过程中是否以正确的参数调用了文件打开方法，确保文件 `"test.json"` 以只读模式（"r"）和 UTF-8 编码打开。

**Note**: 使用该代码时，请确保 `mock_file` 已正确配置为模拟文件操作，以避免实际文件的读写操作影响测试结果。同时，确保 `search_in_json_nested` 方法的实现能够正确处理嵌套 JSON 数据，以便返回预期的结果。
***



================================================
FILE: markdown_docs/tests/test_structure_tree.md
================================================
## FunctionDef build_path_tree(who_reference_me, reference_who, doc_item_path)
**build_path_tree**: The function of build_path_tree is to create a hierarchical representation of file paths based on provided references and a specific document item path.

**parameters**: The parameters of this Function.
· parameter1: who_reference_me - A list of file paths that reference the current entity.
· parameter2: reference_who - A list of file paths that reference another entity.
· parameter3: doc_item_path - A specific file path that needs to be highlighted in the output.

**Code Description**: The build_path_tree function constructs a nested dictionary structure representing a tree of file paths. It begins by defining an inner function, tree, which initializes a defaultdict that allows for the creation of nested dictionaries automatically. The variable path_tree is then assigned the result of calling this inner function.

The function processes two lists of paths: who_reference_me and reference_who. For each path in these lists, it splits the path into its components using the operating system's path separator (os.sep). It then traverses the path_tree structure, creating nested dictionaries for each part of the path.

After processing the reference paths, the function handles the doc_item_path. It splits this path into components as well, but modifies the last component by prefixing it with a star symbol (✳️) to indicate it as a special item. The function again traverses the path_tree to include this modified path.

Finally, the function defines another inner function, tree_to_string, which recursively converts the tree structure into a string representation. This function sorts the keys at each level and adds indentation based on the depth of the tree. The resulting string representation of the path_tree is returned as the output of the build_path_tree function.

**Note**: It is important to ensure that the input paths are formatted correctly and that the os module is imported for the path separator to function properly. The output string will visually represent the hierarchy of paths, with the doc_item_path clearly marked.

**Output Example**: 
Assuming the following inputs:
who_reference_me = ["folder1/fileA.txt", "folder1/folder2/fileB.txt"]
reference_who = ["folder3/fileC.txt"]
doc_item_path = "folder1/folder2/fileB.txt"

The output of the function could look like this:
folder1
    fileA.txt
    folder2
        ✳️fileB.txt
folder3
    fileC.txt
### FunctionDef tree
**tree**: tree函数的功能是返回一个默认字典，该字典的默认值是一个新的tree函数。

**parameters**: 该函数没有参数。

**Code Description**: tree函数使用了Python的collections模块中的defaultdict。defaultdict是一个字典子类，它提供了一个默认值，当访问一个不存在的键时，会自动创建一个新的值。在这个实现中，tree函数返回一个defaultdict，其中的默认值是调用tree函数本身。这意味着每当访问一个不存在的键时，defaultdict会自动创建一个新的defaultdict。这种递归的结构可以用于构建树形数据结构，其中每个节点可以有多个子节点，且子节点的数量和名称是动态生成的。

**Note**: 使用此代码时，请注意避免无限递归的情况。由于tree函数返回的defaultdict的默认值是tree函数本身，因此在访问不存在的键时，会不断创建新的defaultdict，直到达到某种条件或限制。

**Output Example**: 调用tree函数后，可能会得到如下结构：
```
defaultdict(<function tree at 0x...>, {
    'key1': defaultdict(<function tree at 0x...>, {
        'subkey1': defaultdict(<function tree at 0x...>, {}),
        'subkey2': defaultdict(<function tree at 0x...>, {})
    }),
    'key2': defaultdict(<function tree at 0x...>, {})
})
``` 
在这个例子中，'key1'和'key2'是顶层键，而'subkey1'和'subkey2'是'key1'下的子键。
***
### FunctionDef tree_to_string(tree, indent)
**tree_to_string**: tree_to_string 函数的功能是将树形结构转换为字符串格式，便于可视化展示。

**parameters**: 此函数的参数如下：
· parameter1: tree - 一个字典类型的树形结构，其中包含键值对，键为节点名称，值为子节点（可以是字典或其他类型）。
· parameter2: indent - 一个整数，表示当前节点的缩进级别，默认为0。

**Code Description**: tree_to_string 函数通过递归的方式遍历给定的树形结构，并将其格式化为字符串。函数首先初始化一个空字符串 s，用于存储最终的结果。接着，函数对树中的每个键值对进行排序，并逐个处理每个键。对于每个键，函数会在字符串中添加相应数量的空格（由 indent 参数控制），然后添加键的名称，并换行。如果该键对应的值是一个字典，函数会递归调用自身，增加缩进级别（indent + 1），以处理子树。最终，函数返回构建好的字符串，展示了树形结构的层次关系。

**Note**: 使用此函数时，请确保传入的 tree 参数为字典类型，并且其值可以是字典或其他类型。缩进参数 indent 应为非负整数，以确保输出格式正确。

**Output Example**: 假设输入的树形结构为：
{
    "根节点": {
        "子节点1": {},
        "子节点2": {
            "孙节点1": {}
        }
    },
    "另一个根节点": {}
}
调用 tree_to_string 函数后，返回的字符串可能如下所示：
根节点
    子节点1
    子节点2
        孙节点1
另一个根节点
***



================================================
FILE: repo_agent/__init__.py
================================================
[Empty file]


================================================
FILE: repo_agent/__main__.py
================================================
from .main import cli

if __name__ == "__main__":
    cli()



================================================
FILE: repo_agent/change_detector.py
================================================
import os
import re
import subprocess

import git
from colorama import Fore, Style

from repo_agent.file_handler import FileHandler
from repo_agent.settings import SettingsManager


class ChangeDetector:
    """
    这个类需要处理文件的差异和变更检测，它可能会用到 FileHandler 类来访问文件系统。
    ChangeDetector 类的核心在于能够识别自上次提交以来文件的变更。
    """

    def __init__(self, repo_path):
        """
        Initializes a ChangeDetector object.

        Parameters:
        repo_path (str): The path to the repository.

        Returns:
        None
        """
        self.repo_path = repo_path
        self.repo = git.Repo(repo_path)

    def get_staged_pys(self):
        """
        Get added python files in the repository that have been staged.

        This function only tracks the changes of Python files in Git that have been staged,
        i.e., the files that have been added using `git add`.

        Returns:
            dict: A dictionary of changed Python files, where the keys are the file paths and the values are booleans indicating whether the file is newly created or not.

        """
        repo = self.repo
        staged_files = {}
        # Detect Staged Changes
        # Please note! The logic of the GitPython library is different from git. Here, the R=True parameter is used to reverse the version comparison logic.
        # In the GitPython library, repo.index.diff('HEAD') compares the staging area (index) as the new state with the original HEAD commit (old state). This means that if there is a new file in the current staging area, it will be shown as non-existent in HEAD, i.e., "deleted".
        # R=True reverses this logic, correctly treating the last commit (HEAD) as the old state and comparing it with the current staging area (new state) (Index). In this case, a new file in the staging area will correctly show as added because it does not exist in HEAD.
        diffs = repo.index.diff("HEAD", R=True)

        for diff in diffs:
            if diff.change_type in ["A", "M"] and diff.a_path.endswith(".py"):
                is_new_file = diff.change_type == "A"
                staged_files[diff.a_path] = is_new_file

        return staged_files

    def get_file_diff(self, file_path, is_new_file):
        """
        The function's purpose is to retrieve the changes made to a specific file. For new files, it uses git diff --staged to get the differences.
        Args:
            file_path (str): The relative path of the file
            is_new_file (bool): Indicates whether the file is a new file
        Returns:
            list: List of changes made to the file
        """
        repo = self.repo

        if is_new_file:
            # For new files, first add them to the staging area.
            add_command = f"git -C {repo.working_dir} add {file_path}"
            subprocess.run(add_command, shell=True, check=True)

            # Get the diff from the staging area.
            diffs = repo.git.diff("--staged", file_path).splitlines()
        else:
            # For non-new files, get the diff from HEAD.
            diffs = repo.git.diff("HEAD", file_path).splitlines()

        return diffs

    def parse_diffs(self, diffs):
        """
        Parse the difference content, extract the added and deleted object information, the object can be a class or a function.
        Output example: {'added': [(86, '    '), (87, '    def to_json_new(self, comments = True):'), (88, '        data = {'), (89, '            "name": self.node_name,')...(95, '')], 'removed': []}
        In the above example, PipelineEngine and AI_give_params are added objects, and there are no deleted objects.
        But the addition here does not mean that it is a newly added object, because in git diff, the modification of a line is represented as deletion and addition in diff.
        So for the modified content, it will also be represented as this object has undergone an added operation.

        If you need to know clearly that an object is newly added, you need to use the get_added_objs() function.
        Args:
            diffs (list): A list containing difference content. Obtained by the get_file_diff() function inside the class.

        Returns:
            dict: A dictionary containing added and deleted line information, the format is {'added': set(), 'removed': set()}
        """
        changed_lines = {"added": [], "removed": []}
        line_number_current = 0
        line_number_change = 0

        for line in diffs:
            # 检测行号信息，例如 "@@ -43,33 +43,40 @@"
            line_number_info = re.match(r"@@ \-(\d+),\d+ \+(\d+),\d+ @@", line)
            if line_number_info:
                line_number_current = int(line_number_info.group(1))
                line_number_change = int(line_number_info.group(2))
                continue

            if line.startswith("+") and not line.startswith("+++"):
                changed_lines["added"].append((line_number_change, line[1:]))
                line_number_change += 1
            elif line.startswith("-") and not line.startswith("---"):
                changed_lines["removed"].append((line_number_current, line[1:]))
                line_number_current += 1
            else:
                # 对于没有变化的行，两者的行号都需要增加
                line_number_current += 1
                line_number_change += 1

        return changed_lines

    # TODO: The key issue is that the changed line numbers correspond to the old function names (i.e., those removed) and the new function names (i.e., those added), and the current implementation does not handle this correctly.
    # We need a way to associate the changed line numbers with their function or class names before and after the change. One method is to build a mapping before processing changed_lines, which can map the names after the change back to the names before the change based on the line number.
    # Then, in the identify_changes_in_structure function, this mapping can be used to correctly identify the changed structure.
    def identify_changes_in_structure(self, changed_lines, structures):
        """
        Identify the structure of the function or class where changes have occurred: Traverse all changed lines, for each line, it checks whether this line is between the start line and the end line of a structure (function or class).
        If so, then this structure is considered to have changed, and its name and the name of the parent structure are added to the corresponding set in the result dictionary changes_in_structures (depending on whether this line is added or deleted).

        Output example: {'added': {('PipelineAutoMatNode', None), ('to_json_new', 'PipelineAutoMatNode')}, 'removed': set()}

        Args:
            changed_lines (dict): A dictionary containing the line numbers where changes have occurred, {'added': [(line number, change content)], 'removed': [(line number, change content)]}
            structures (list): The received is a list of function or class structures from get_functions_and_classes, each structure is composed of structure type, name, start line number, end line number, and parent structure name.

        Returns:
            dict: A dictionary containing the structures where changes have occurred, the key is the change type, and the value is a set of structure names and parent structure names.
                Possible change types are 'added' (new) and 'removed' (removed).
        """
        changes_in_structures = {"added": set(), "removed": set()}
        for change_type, lines in changed_lines.items():
            for line_number, _ in lines:
                for (
                    structure_type,
                    name,
                    start_line,
                    end_line,
                    parent_structure,
                ) in structures:
                    if start_line <= line_number <= end_line:
                        changes_in_structures[change_type].add((name, parent_structure))
        return changes_in_structures

    # TODO:可能有错，需要单元测试覆盖； 可能有更好的实现方式
    def get_to_be_staged_files(self):
        """
        This method retrieves all unstaged files in the repository that meet one of the following conditions:
        1. The file, when its extension is changed to .md, corresponds to a file that is already staged.
        2. The file's path is the same as the 'project_hierarchy' field in the CONFIG.

        It returns a list of the paths of these files.

        :return: A list of relative file paths to the repo that are either modified but not staged, or untracked, and meet one of the conditions above.
        """
        # 已经更改但是暂未暂存的文件，这里只能是.md文件，因为作者不提交的.py文件（即使发生变更）我们不做处理。
        to_be_staged_files = []
        # staged_files是已经暂存的文件，通常这里是作者做了更改后git add 的.py文件 或其他文件
        staged_files = [item.a_path for item in self.repo.index.diff("HEAD")]
        print(
            f"{Fore.LIGHTYELLOW_EX}target_repo_path{Style.RESET_ALL}: {self.repo_path}"
        )
        print(
            f"{Fore.LIGHTMAGENTA_EX}already_staged_files{Style.RESET_ALL}:{staged_files}"
        )

        setting = SettingsManager.get_setting()

        project_hierarchy = setting.project.hierarchy_name
        # diffs是所有未暂存更改文件的列表。这些更改文件是相对于工作区（working directory）的，也就是说，它们是自上次提交（commit）以来在工作区发生的更改，但还没有被添加到暂存区（staging area）
        # 比如原本存在的md文件现在由于代码的变更发生了更新，就会标记为未暂存diff
        diffs = self.repo.index.diff(None)
        # untracked_files是一个包含了所有未跟踪文件的列表。比如说用户添加了新的.py文件后项目自己生成的对应.md文档。它们是在工作区中存在但还没有被添加到暂存区（staging area）的文件。
        # untracked_files中的文件路径是绝对路径
        untracked_files = self.repo.untracked_files
        print(f"{Fore.LIGHTCYAN_EX}untracked_files{Style.RESET_ALL}: {untracked_files}")

        # 处理untrack_files中的内容
        for untracked_file in untracked_files:
            # 连接repo_path和untracked_file以获取完整的绝对路径
            if untracked_file.startswith(setting.project.markdown_docs_name):
                to_be_staged_files.append(untracked_file)
            continue
            print(f"rel_untracked_file:{rel_untracked_file}")
            # import pdb; pdb.set_trace()
            # 判断这个文件的类型：
            if rel_untracked_file.endswith(".md"):
                # 把rel_untracked_file从CONFIG['Markdown_Docs_folder']中拆离出来。判断是否能跟暂存区中的某一个.py文件对应上
                rel_untracked_file = os.path.relpath(
                    rel_untracked_file, setting.project.markdown_docs_name
                )
                corresponding_py_file = os.path.splitext(rel_untracked_file)[0] + ".py"
                print(
                    f"corresponding_py_file in untracked_files:{corresponding_py_file}"
                )
                if corresponding_py_file in staged_files:
                    # 如果是，那么就把这个md文件也加入到unstaged_files中
                    to_be_staged_files.append(
                        os.path.join(
                            self.repo_path.lstrip("/"),
                            setting.project.markdown_docs_name,
                            rel_untracked_file,
                        )
                    )
            elif rel_untracked_file == project_hierarchy:
                to_be_staged_files.append(rel_untracked_file)

        # 处理已追踪但是未暂存的内容
        unstaged_files = [diff.b_path for diff in diffs]
        print(f"{Fore.LIGHTCYAN_EX}unstaged_files{Style.RESET_ALL}: {unstaged_files}")

        for unstaged_file in unstaged_files:
            # 连接repo_path和unstaged_file以获取完整的绝对路径
            if unstaged_file.startswith(
                setting.project.markdown_docs_name
            ) or unstaged_file.startswith(setting.project.hierarchy_name):
                # abs_unstaged_file = os.path.join(self.repo_path, unstaged_file)
                # # # 获取相对于仓库根目录的相对路径
                # # rel_unstaged_file = os.path.relpath(abs_unstaged_file, self.repo_path)
                to_be_staged_files.append(unstaged_file)
            elif unstaged_file == project_hierarchy:  # project_hierarchy永远add
                to_be_staged_files.append(unstaged_file)
            continue
            abs_unstaged_file = os.path.join(self.repo_path, unstaged_file)
            # 获取相对于仓库根目录的相对路径
            rel_unstaged_file = os.path.relpath(abs_unstaged_file, self.repo_path)
            print(f"rel_unstaged_file:{rel_unstaged_file}")
            # 如果它是md文件
            if unstaged_file.endswith(".md"):
                # 把rel_unstaged_file从CONFIG['Markdown_Docs_folder']中拆离出来。判断是否能跟暂存区中的某一个.py文件对应上
                rel_unstaged_file = os.path.relpath(
                    rel_unstaged_file, setting.project.markdown_docs_name
                )
                corresponding_py_file = os.path.splitext(rel_unstaged_file)[0] + ".py"
                print(f"corresponding_py_file:{corresponding_py_file}")
                if corresponding_py_file in staged_files:
                    # 如果是，那么就把这个md文件也加入到unstaged_files中
                    to_be_staged_files.append(
                        os.path.join(
                            self.repo_path.lstrip("/"),
                            setting.project.markdown_docs_name,
                            rel_unstaged_file,
                        )
                    )
            elif unstaged_file == project_hierarchy:  # project_hierarchy永远add
                to_be_staged_files.append(unstaged_file)
        print(
            f"{Fore.LIGHTRED_EX}newly_staged_files{Style.RESET_ALL}: {to_be_staged_files}"
        )
        return to_be_staged_files

    def add_unstaged_files(self):
        """
        Add unstaged files which meet the condition to the staging area.
        """
        unstaged_files_meeting_conditions = self.get_to_be_staged_files()
        for file_path in unstaged_files_meeting_conditions:
            add_command = f"git -C {self.repo.working_dir} add {file_path}"
            subprocess.run(add_command, shell=True, check=True)
        return unstaged_files_meeting_conditions


if __name__ == "__main__":
    repo_path = "/path/to/your/repo/"
    change_detector = ChangeDetector(repo_path)
    changed_files = change_detector.get_staged_pys()
    print(f"\nchanged_files:{changed_files}\n\n")
    for file_path, is_new_file in changed_files.items():
        changed_lines = change_detector.parse_diffs(
            change_detector.get_file_diff(file_path, is_new_file)
        )
        # print("changed_lines:",changed_lines)
        file_handler = FileHandler(repo_path=repo_path, file_path=file_path)
        changes_in_pyfile = change_detector.identify_changes_in_structure(
            changed_lines,
            file_handler.get_functions_and_classes(file_handler.read_file()),
        )
        print(f"Changes in {file_path} Structures:{changes_in_pyfile}\n")



================================================
FILE: repo_agent/chat_engine.py
================================================
from llama_index.llms.openai_like import OpenAILike

from repo_agent.doc_meta_info import DocItem
from repo_agent.log import logger
from repo_agent.prompt import chat_template
from repo_agent.settings import SettingsManager


class ChatEngine:
    """
    ChatEngine is used to generate the doc of functions or classes.
    """

    def __init__(self, project_manager):
        setting = SettingsManager.get_setting()

        self.llm = OpenAILike(
            api_key=setting.chat_completion.openai_api_key.get_secret_value(),
            api_base=setting.chat_completion.openai_base_url,
            timeout=setting.chat_completion.request_timeout,
            model=setting.chat_completion.model,
            temperature=setting.chat_completion.temperature,
            max_retries=1,
            is_chat_model=True,
        )

    def build_prompt(self, doc_item: DocItem):
        """Builds and returns the system and user prompts based on the DocItem."""
        setting = SettingsManager.get_setting()

        code_info = doc_item.content
        referenced = len(doc_item.who_reference_me) > 0

        code_type = code_info["type"]
        code_name = code_info["name"]
        code_content = code_info["code_content"]
        have_return = code_info["have_return"]
        file_path = doc_item.get_full_name()

        def get_referenced_prompt(doc_item: DocItem) -> str:
            if len(doc_item.reference_who) == 0:
                return ""
            prompt = [
                """As you can see, the code calls the following objects, their code and docs are as following:"""
            ]
            for reference_item in doc_item.reference_who:
                instance_prompt = (
                    f"""obj: {reference_item.get_full_name()}\nDocument: \n{reference_item.md_content[-1] if len(reference_item.md_content) > 0 else 'None'}\nRaw code:```\n{reference_item.content['code_content'] if 'code_content' in reference_item.content.keys() else ''}\n```"""
                    + "=" * 10
                )
                prompt.append(instance_prompt)
            return "\n".join(prompt)

        def get_referencer_prompt(doc_item: DocItem) -> str:
            if len(doc_item.who_reference_me) == 0:
                return ""
            prompt = [
                """Also, the code has been called by the following objects, their code and docs are as following:"""
            ]
            for referencer_item in doc_item.who_reference_me:
                instance_prompt = (
                    f"""obj: {referencer_item.get_full_name()}\nDocument: \n{referencer_item.md_content[-1] if len(referencer_item.md_content) > 0 else 'None'}\nRaw code:```\n{referencer_item.content['code_content'] if 'code_content' in referencer_item.content.keys() else 'None'}\n```"""
                    + "=" * 10
                )
                prompt.append(instance_prompt)
            return "\n".join(prompt)

        def get_relationship_description(referencer_content, reference_letter):
            if referencer_content and reference_letter:
                return "And please include the reference relationship with its callers and callees in the project from a functional perspective"
            elif referencer_content:
                return "And please include the relationship with its callers in the project from a functional perspective."
            elif reference_letter:
                return "And please include the relationship with its callees in the project from a functional perspective."
            else:
                return ""

        code_type_tell = "Class" if code_type == "ClassDef" else "Function"
        parameters_or_attribute = (
            "attributes" if code_type == "ClassDef" else "parameters"
        )
        have_return_tell = (
            "**Output Example**: Mock up a possible appearance of the code's return value."
            if have_return
            else ""
        )
        combine_ref_situation = (
            "and combine it with its calling situation in the project,"
            if referenced
            else ""
        )

        referencer_content = get_referencer_prompt(doc_item)
        reference_letter = get_referenced_prompt(doc_item)
        has_relationship = get_relationship_description(
            referencer_content, reference_letter
        )

        project_structure_prefix = ", and the related hierarchical structure of this project is as follows (The current object is marked with an *):"

        return chat_template.format_messages(
            combine_ref_situation=combine_ref_situation,
            file_path=file_path,
            project_structure_prefix=project_structure_prefix,
            code_type_tell=code_type_tell,
            code_name=code_name,
            code_content=code_content,
            have_return_tell=have_return_tell,
            has_relationship=has_relationship,
            reference_letter=reference_letter,
            referencer_content=referencer_content,
            parameters_or_attribute=parameters_or_attribute,
            language=setting.project.language,
        )

    def generate_doc(self, doc_item: DocItem):
        """Generates documentation for a given DocItem."""
        messages = self.build_prompt(doc_item)

        try:
            response = self.llm.chat(messages)
            logger.debug(f"LLM Prompt Tokens: {response.raw.usage.prompt_tokens}")  # type: ignore
            logger.debug(
                f"LLM Completion Tokens: {response.raw.usage.completion_tokens}"  # type: ignore
            )
            logger.debug(
                f"Total LLM Token Count: {response.raw.usage.total_tokens}"  # type: ignore
            )
            return response.message.content
        except Exception as e:
            logger.error(f"Error in llamaindex chat call: {e}")
            raise



================================================
FILE: repo_agent/doc_meta_info.py
================================================
[Binary file]


================================================
FILE: repo_agent/file_handler.py
================================================
# FileHandler 类，实现对文件的读写操作，这里的文件包括markdown文件和python文件
# repo_agent/file_handler.py
import ast
import json
import os

import git
from colorama import Fore, Style
from tqdm import tqdm

from repo_agent.log import logger
from repo_agent.settings import SettingsManager
from repo_agent.utils.gitignore_checker import GitignoreChecker
from repo_agent.utils.meta_info_utils import latest_verison_substring


class FileHandler:
    """
    历变更后的文件的循环中，为每个变更后文件（也就是当前文件）创建一个实例
    """

    def __init__(self, repo_path, file_path):
        self.file_path = file_path  # 这里的file_path是相对于仓库根目录的路径
        self.repo_path = repo_path

        setting = SettingsManager.get_setting()

        self.project_hierarchy = (
            setting.project.target_repo / setting.project.hierarchy_name
        )

    def read_file(self):
        """
        Read the file content

        Returns:
            str: The content of the current changed file
        """
        abs_file_path = os.path.join(self.repo_path, self.file_path)

        with open(abs_file_path, "r", encoding="utf-8") as file:
            content = file.read()
        return content

    def get_obj_code_info(
        self, code_type, code_name, start_line, end_line, params, file_path=None
    ):
        """
        Get the code information for a given object.

        Args:
            code_type (str): The type of the code.
            code_name (str): The name of the code.
            start_line (int): The starting line number of the code.
            end_line (int): The ending line number of the code.
            parent (str): The parent of the code.
            file_path (str, optional): The file path. Defaults to None.

        Returns:
            dict: A dictionary containing the code information.
        """

        code_info = {}
        code_info["type"] = code_type
        code_info["name"] = code_name
        code_info["md_content"] = []
        code_info["code_start_line"] = start_line
        code_info["code_end_line"] = end_line
        code_info["params"] = params

        with open(
            os.path.join(
                self.repo_path, file_path if file_path != None else self.file_path
            ),
            "r",
            encoding="utf-8",
        ) as code_file:
            lines = code_file.readlines()
            code_content = "".join(lines[start_line - 1 : end_line])
            # 获取对象名称在第一行代码中的位置
            name_column = lines[start_line - 1].find(code_name)
            # 判断代码中是否有return字样
            if "return" in code_content:
                have_return = True
            else:
                have_return = False

            code_info["have_return"] = have_return
            # # 使用 json.dumps 来转义字符串，并去掉首尾的引号
            # code_info['code_content'] = json.dumps(code_content)[1:-1]
            code_info["code_content"] = code_content
            code_info["name_column"] = name_column

        return code_info

    def write_file(self, file_path, content):
        """
        Write content to a file.

        Args:
            file_path (str): The relative path of the file.
            content (str): The content to be written to the file.
        """
        # 确保file_path是相对路径
        if file_path.startswith("/"):
            # 移除开头的 '/'
            file_path = file_path[1:]

        abs_file_path = os.path.join(self.repo_path, file_path)
        os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)
        with open(abs_file_path, "w", encoding="utf-8") as file:
            file.write(content)

    def get_modified_file_versions(self):
        """
        Get the current and previous versions of the modified file.

        Returns:
            tuple: A tuple containing the current version and the previous version of the file.
        """
        repo = git.Repo(self.repo_path)

        # Read the file in the current working directory (current version)
        current_version_path = os.path.join(self.repo_path, self.file_path)
        with open(current_version_path, "r", encoding="utf-8") as file:
            current_version = file.read()

        # Get the file version from the last commit (previous version)
        commits = list(repo.iter_commits(paths=self.file_path, max_count=1))
        previous_version = None
        if commits:
            commit = commits[0]
            try:
                previous_version = (
                    (commit.tree / self.file_path).data_stream.read().decode("utf-8")
                )
            except KeyError:
                previous_version = None  # The file may be newly added and not present in previous commits

        return current_version, previous_version

    def get_end_lineno(self, node):
        """
        Get the end line number of a given node.

        Args:
            node: The node for which to find the end line number.

        Returns:
            int: The end line number of the node. Returns -1 if the node does not have a line number.
        """
        if not hasattr(node, "lineno"):
            return -1  # 返回-1表示此节点没有行号

        end_lineno = node.lineno
        for child in ast.iter_child_nodes(node):
            child_end = getattr(child, "end_lineno", None) or self.get_end_lineno(child)
            if child_end > -1:  # 只更新当子节点有有效行号时
                end_lineno = max(end_lineno, child_end)
        return end_lineno

    def add_parent_references(self, node, parent=None):
        """
        Adds a parent reference to each node in the AST.

        Args:
            node: The current node in the AST.

        Returns:
            None
        """
        for child in ast.iter_child_nodes(node):
            child.parent = node
            self.add_parent_references(child, node)

    def get_functions_and_classes(self, code_content):
        """
        Retrieves all functions, classes, their parameters (if any), and their hierarchical relationships.
        Output Examples: [('FunctionDef', 'AI_give_params', 86, 95, None, ['param1', 'param2']), ('ClassDef', 'PipelineEngine', 97, 104, None, []), ('FunctionDef', 'get_all_pys', 99, 104, 'PipelineEngine', ['param1'])]
        On the example above, PipelineEngine is the Father structure for get_all_pys.

        Args:
            code_content: The code content of the whole file to be parsed.

        Returns:
            A list of tuples containing the type of the node (FunctionDef, ClassDef, AsyncFunctionDef),
            the name of the node, the starting line number, the ending line number, the name of the parent node, and a list of parameters (if any).
        """
        tree = ast.parse(code_content)
        self.add_parent_references(tree)
        functions_and_classes = []
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):
                # if node.name == "recursive_check":
                #     import pdb; pdb.set_trace()
                start_line = node.lineno
                end_line = self.get_end_lineno(node)
                # def get_recursive_parent_name(node):
                #     now = node
                #     while "parent" in dir(now):
                #         if isinstance(now.parent, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):
                #             assert 'name' in dir(now.parent)
                #             return now.parent.name
                #         now = now.parent
                #     return None
                # parent_name = get_recursive_parent_name(node)
                parameters = (
                    [arg.arg for arg in node.args.args] if "args" in dir(node) else []
                )
                all_names = [item[1] for item in functions_and_classes]
                # (parent_name == None or parent_name in all_names) and
                functions_and_classes.append(
                    (type(node).__name__, node.name, start_line, end_line, parameters)
                )
        return functions_and_classes

    def generate_file_structure(self, file_path):
        """
        Generates the file structure for the given file path.

        Args:
            file_path (str): The relative path of the file.

        Returns:
            dict: A dictionary containing the file path and the generated file structure.

        Output example:
        {
            "function_name": {
                "type": "function",
                "start_line": 10,
                ··· ···
                "end_line": 20,
                "parent": "class_name"
            },
            "class_name": {
                "type": "class",
                "start_line": 5,
                ··· ···
                "end_line": 25,
                "parent": None
            }
        }
        """
        with open(os.path.join(self.repo_path, file_path), "r", encoding="utf-8") as f:
            content = f.read()
            structures = self.get_functions_and_classes(content)
            file_objects = []  # 以列表的形式存储
            for struct in structures:
                structure_type, name, start_line, end_line, params = struct
                code_info = self.get_obj_code_info(
                    structure_type, name, start_line, end_line, params, file_path
                )
                file_objects.append(code_info)

        return file_objects

    def generate_overall_structure(self, file_path_reflections, jump_files) -> dict:
        """获取目标仓库的文件情况，通过AST-walk获取所有对象等情况。
        对于jump_files: 不会parse，当做不存在
        """
        repo_structure = {}
        gitignore_checker = GitignoreChecker(
            directory=self.repo_path,
            gitignore_path=os.path.join(self.repo_path, ".gitignore"),
        )

        bar = tqdm(gitignore_checker.check_files_and_folders())
        for not_ignored_files in bar:
            normal_file_names = not_ignored_files
            if not_ignored_files in jump_files:
                print(
                    f"{Fore.LIGHTYELLOW_EX}[File-Handler] Unstaged AddFile, ignore this file: {Style.RESET_ALL}{normal_file_names}"
                )
                continue
            elif not_ignored_files.endswith(latest_verison_substring):
                print(
                    f"{Fore.LIGHTYELLOW_EX}[File-Handler] Skip Latest Version, Using Git-Status Version]: {Style.RESET_ALL}{normal_file_names}"
                )
                continue
            # elif not_ignored_files.endswith(latest_version):
            #     """如果某文件被删除但没有暂存，文件系统有fake_file但没有对应的原始文件"""
            #     for k,v in file_path_reflections.items():
            #         if v == not_ignored_files and not os.path.exists(os.path.join(setting.project.target_repo, not_ignored_files)):
            #             print(f"{Fore.LIGHTYELLOW_EX}[Unstaged DeleteFile] load fake-file-content: {Style.RESET_ALL}{k}")
            #             normal_file_names = k #原来的名字
            #             break
            #     if normal_file_names == not_ignored_files:
            #         continue

            # if not_ignored_files in file_path_reflections.keys():
            #     not_ignored_files = file_path_reflections[not_ignored_files] #获取fake_file_path
            #     print(f"{Fore.LIGHTYELLOW_EX}[Unstaged ChangeFile] load fake-file-content: {Style.RESET_ALL}{normal_file_names}")

            try:
                repo_structure[normal_file_names] = self.generate_file_structure(
                    not_ignored_files
                )
            except Exception as e:
                logger.error(
                    f"Alert: An error occurred while generating file structure for {not_ignored_files}: {e}"
                )
                continue
            bar.set_description(f"generating repo structure: {not_ignored_files}")
        return repo_structure

    def convert_to_markdown_file(self, file_path=None):
        """
        Converts the content of a file to markdown format.

        Args:
            file_path (str, optional): The relative path of the file to be converted. If not provided, the default file path, which is None, will be used.

        Returns:
            str: The content of the file in markdown format.

        Raises:
            ValueError: If no file object is found for the specified file path in project_hierarchy.json.
        """
        with open(self.project_hierarchy, "r", encoding="utf-8") as f:
            json_data = json.load(f)

        if file_path is None:
            file_path = self.file_path

        # Find the file object in json_data that matches file_path

        file_dict = json_data.get(file_path)

        if file_dict is None:
            raise ValueError(
                f"No file object found for {self.file_path} in project_hierarchy.json"
            )

        markdown = ""
        parent_dict = {}
        objects = sorted(file_dict.values(), key=lambda obj: obj["code_start_line"])
        for obj in objects:
            if obj["parent"] is not None:
                parent_dict[obj["name"]] = obj["parent"]
        current_parent = None
        for obj in objects:
            level = 1
            parent = obj["parent"]
            while parent is not None:
                level += 1
                parent = parent_dict.get(parent)
            if level == 1 and current_parent is not None:
                markdown += "***\n"
            current_parent = obj["name"]
            params_str = ""
            if obj["type"] in ["FunctionDef", "AsyncFunctionDef"]:
                params_str = "()"
                if obj["params"]:
                    params_str = f"({', '.join(obj['params'])})"
            markdown += f"{'#' * level} {obj['type']} {obj['name']}{params_str}:\n"
            markdown += (
                f"{obj['md_content'][-1] if len(obj['md_content']) >0 else ''}\n"
            )
        markdown += "***\n"

        return markdown



================================================
FILE: repo_agent/log.py
================================================
# repo_agent/log.py
import inspect
import logging
import sys

from loguru import logger

logger = logger.opt(colors=True)
"""
RepoAgent 日志记录器对象。

默认信息:
- 格式: `[%(asctime)s %(name)s] %(levelname)s: %(message)s`
- 等级: `INFO` ，根据 `CONFIG["log_level"]` 配置改变
- 输出: 输出至 stdout

用法示例:
    ```python
    from repo_agent.log import logger
    
    # 基本消息记录
    logger.info("It <green>works</>!") # 使用颜色

    # 记录异常信息
    try:
        1 / 0
    except ZeroDivisionError:
        # 使用 `logger.exception` 可以在记录异常消息时自动附加异常的堆栈跟踪信息。
        logger.exception("ZeroDivisionError occurred")

    # 记录调试信息
    logger.debug(f"Debugging info: {some_debug_variable}")

    # 记录警告信息
    logger.warning("This is a warning message")

    # 记录错误信息
    logger.error("An error occurred")
    ```

"""


class InterceptHandler(logging.Handler):
    def emit(self, record: logging.LogRecord) -> None:
        # Get corresponding Loguru level if it exists.
        level: str | int
        try:
            level = logger.level(record.levelname).name
        except ValueError:
            level = record.levelno

        # Find caller from where originated the logged message.
        frame, depth = inspect.currentframe(), 0
        while frame and (depth == 0 or frame.f_code.co_filename == logging.__file__):
            frame = frame.f_back
            depth += 1

        logger.opt(depth=depth, exception=record.exc_info).log(
            level, record.getMessage()
        )


def set_logger_level_from_config(log_level):
    """
    Configures the loguru logger with specified log level and integrates it with the standard logging module.

    Args:
        log_level (str): The log level to set for loguru (e.g., "DEBUG", "INFO", "WARNING").

    This function:
    - Removes any existing loguru handlers to ensure a clean slate.
    - Adds a new handler to loguru, directing output to stderr with the specified level.
      - `enqueue=True` ensures thread-safe logging by using a queue, helpful in multi-threaded contexts.
      - `backtrace=False` minimizes detailed traceback to prevent overly verbose output.
      - `diagnose=False` suppresses additional loguru diagnostic information for more concise logs.
    - Redirects the standard logging output to loguru using the InterceptHandler, allowing loguru to handle
      all logs consistently across the application.
    """
    logger.remove()
    logger.add(
        sys.stderr, level=log_level, enqueue=True, backtrace=False, diagnose=False
    )

    # Intercept standard logging
    logging.basicConfig(handlers=[InterceptHandler()], level=0, force=True)

    logger.success(f"Log level set to {log_level}!")



================================================
FILE: repo_agent/main.py
================================================
from importlib import metadata

import click
from pydantic import ValidationError

from repo_agent.doc_meta_info import DocItem, MetaInfo
from repo_agent.log import logger, set_logger_level_from_config
from repo_agent.runner import Runner, delete_fake_files
from repo_agent.settings import SettingsManager, LogLevel
from repo_agent.utils.meta_info_utils import delete_fake_files, make_fake_files

try:
    version_number = metadata.version("repoagent")
except metadata.PackageNotFoundError:
    version_number = "0.0.0"


@click.group()
@click.version_option(version_number)
def cli():
    """An LLM-Powered Framework for Repository-level Code Documentation Generation."""
    pass


def handle_setting_error(e: ValidationError):
    """Handle configuration errors for settings."""
    # 输出更详细的字段缺失信息，使用颜色区分
    for error in e.errors():
        field = error["loc"][-1]
        if error["type"] == "missing":
            message = click.style(
                f"Missing required field `{field}`. Please set the `{field}` environment variable.",
                fg="yellow",
            )
        else:
            message = click.style(error["msg"], fg="yellow")
        click.echo(message, err=True, color=True)

    # 使用 ClickException 优雅地退出程序
    raise click.ClickException(
        click.style(
            "Program terminated due to configuration errors.", fg="red", bold=True
        )
    )


@cli.command()
@click.option(
    "--model",
    "-m",
    default="gpt-4o-mini",
    show_default=True,
    help="Specifies the model to use for completion.",
    type=str,
)
@click.option(
    "--temperature",
    "-t",
    default=0.2,
    show_default=True,
    help="Sets the generation temperature for the model. Lower values make the model more deterministic.",
    type=float,
)
@click.option(
    "--request-timeout",
    "-r",
    default=60,
    show_default=True,
    help="Defines the timeout in seconds for the API request.",
    type=int,
)
@click.option(
    "--base-url",
    "-b",
    default="https://api.openai.com/v1",
    show_default=True,
    help="The base URL for the API calls.",
    type=str,
)
@click.option(
    "--target-repo-path",
    "-tp",
    default="",
    show_default=True,
    help="The file system path to the target repository. This path is used as the root for documentation generation.",
    type=click.Path(file_okay=False),
)
@click.option(
    "--hierarchy-path",
    "-hp",
    default=".project_doc_record",
    show_default=True,
    help="The name or path for the project hierarchy file, used to organize documentation structure.",
    type=str,
)
@click.option(
    "--markdown-docs-path",
    "-mdp",
    default="markdown_docs",
    show_default=True,
    help="The folder path where Markdown documentation will be stored or generated.",
    type=str,
)
@click.option(
    "--ignore-list",
    "-i",
    default="",
    help="A comma-separated list of files or directories to ignore during documentation generation.",
)
@click.option(
    "--language",
    "-l",
    default="English",
    show_default=True,
    help="The ISO 639 code or language name for the documentation. ",
    type=str,
)
@click.option(
    "--max-thread-count",
    "-mtc",
    default=4,
    show_default=True,
)
@click.option(
    "--log-level",
    "-ll",
    default="INFO",
    show_default=True,
    help="Sets the logging level (e.g., DEBUG, INFO, WARNING, ERROR, CRITICAL) for the application. Default is INFO.",
    type=click.Choice([level.value for level in LogLevel], case_sensitive=False),
)
@click.option(
    "--print-hierarchy",
    "-pr",
    is_flag=True,
    show_default=True,
    default=False,
    help="If set, prints the hierarchy of the target repository when finished running the main task.",
)
def run(
    model,
    temperature,
    request_timeout,
    base_url,
    target_repo_path,
    hierarchy_path,
    markdown_docs_path,
    ignore_list,
    language,
    max_thread_count,
    log_level,
    print_hierarchy,
):
    """Run the program with the specified parameters."""
    try:
        # Fetch and validate the settings using the SettingsManager
        setting = SettingsManager.initialize_with_params(
            target_repo=target_repo_path,
            hierarchy_name=hierarchy_path,
            markdown_docs_name=markdown_docs_path,
            ignore_list=[item.strip() for item in ignore_list.split(",") if item],
            language=language,
            log_level=log_level,
            model=model,
            temperature=temperature,
            request_timeout=request_timeout,
            openai_base_url=base_url,
            max_thread_count=max_thread_count,
        )
        set_logger_level_from_config(log_level=log_level)
    except ValidationError as e:
        handle_setting_error(e)
        return

    # 如果设置成功，则运行任务
    runner = Runner()
    runner.run()
    logger.success("Documentation task completed.")
    if print_hierarchy:
        runner.meta_info.target_repo_hierarchical_tree.print_recursive()
        logger.success("Hierarchy printed.")


@cli.command()
def clean():
    """Clean the fake files generated by the documentation process."""
    delete_fake_files()
    logger.success("Fake files have been cleaned up.")


@cli.command()
def diff():
    """Check for changes and print which documents will be updated or generated."""
    try:
        # Fetch and validate the settings using the SettingsManager
        setting = SettingsManager.get_setting()
    except ValidationError as e:
        handle_setting_error(e)
        return

    runner = Runner()
    if runner.meta_info.in_generation_process:  # 如果不是在生成过程中，就开始检测变更
        click.echo("This command only supports pre-check")
        raise click.Abort()

    file_path_reflections, jump_files = make_fake_files()
    new_meta_info = MetaInfo.init_meta_info(file_path_reflections, jump_files)
    new_meta_info.load_doc_from_older_meta(runner.meta_info)
    delete_fake_files()

    DocItem.check_has_task(
        new_meta_info.target_repo_hierarchical_tree,
        ignore_list=setting.project.ignore_list,
    )
    if new_meta_info.target_repo_hierarchical_tree.has_task:
        click.echo("The following docs will be generated/updated:")
        new_meta_info.target_repo_hierarchical_tree.print_recursive(
            diff_status=True, ignore_list=setting.project.ignore_list
        )
    else:
        click.echo("No docs will be generated/updated, check your source-code update")


@cli.command()
def chat_with_repo():
    """
    Start an interactive chat session with the repository.
    """
    try:
        # Fetch and validate the settings using the SettingsManager
        setting = SettingsManager.get_setting()
    except ValidationError as e:
        # Handle configuration errors if the settings are invalid
        handle_setting_error(e)
        return

    from repo_agent.chat_with_repo import main

    main()


if __name__ == "__main__":
    cli()



================================================
FILE: repo_agent/multi_task_dispatch.py
================================================
from __future__ import annotations

import random
import threading
import time
from typing import Any, Callable, Dict, List

from colorama import Fore, Style


class Task:
    def __init__(self, task_id: int, dependencies: List[Task], extra_info: Any = None):
        self.task_id = task_id
        self.extra_info = extra_info
        self.dependencies = dependencies
        self.status = 0  # 任务状态：0未开始，1正在进行，2已经完成，3出错了


class TaskManager:
    def __init__(self):
        """
        Initialize a MultiTaskDispatch object.

        This method initializes the MultiTaskDispatch object by setting up the necessary attributes.

        Attributes:
        - task_dict (Dict[int, Task]): A dictionary that maps task IDs to Task objects.
        - task_lock (threading.Lock): A lock used for thread synchronization when accessing the task_dict.
        - now_id (int): The current task ID.
        - query_id (int): The current query ID.
        - sync_func (None): A placeholder for a synchronization function.

        """
        self.task_dict: Dict[int, Task] = {}
        self.task_lock = threading.Lock()
        self.now_id = 0
        self.query_id = 0

    @property
    def all_success(self) -> bool:
        return len(self.task_dict) == 0

    def add_task(self, dependency_task_id: List[int], extra=None) -> int:
        """
        Adds a new task to the task dictionary.

        Args:
            dependency_task_id (List[int]): List of task IDs that the new task depends on.
            extra (Any, optional): Extra information associated with the task. Defaults to None.

        Returns:
            int: The ID of the newly added task.
        """
        with self.task_lock:
            depend_tasks = [self.task_dict[task_id] for task_id in dependency_task_id]
            self.task_dict[self.now_id] = Task(
                task_id=self.now_id, dependencies=depend_tasks, extra_info=extra
            )
            self.now_id += 1
            return self.now_id - 1

    def get_next_task(self, process_id: int):
        """
        Get the next task for a given process ID.

        Args:
            process_id (int): The ID of the process.

        Returns:
            tuple: A tuple containing the next task object and its ID.
                   If there are no available tasks, returns (None, -1).
        """
        with self.task_lock:
            self.query_id += 1
            for task_id in self.task_dict.keys():
                ready = (
                    len(self.task_dict[task_id].dependencies) == 0
                ) and self.task_dict[task_id].status == 0
                if ready:
                    self.task_dict[task_id].status = 1
                    print(
                        f"{Fore.RED}[process {process_id}]{Style.RESET_ALL}: get task({task_id}), remain({len(self.task_dict)})"
                    )
                    return self.task_dict[task_id], task_id
            return None, -1

    def mark_completed(self, task_id: int):
        """
        Marks a task as completed and removes it from the task dictionary.

        Args:
            task_id (int): The ID of the task to mark as completed.

        """
        with self.task_lock:
            target_task = self.task_dict[task_id]
            for task in self.task_dict.values():
                if target_task in task.dependencies:
                    task.dependencies.remove(target_task)
            self.task_dict.pop(task_id)  # 从任务字典中移除


def worker(task_manager, process_id: int, handler: Callable):
    """
    Worker function that performs tasks assigned by the task manager.

    Args:
        task_manager: The task manager object that assigns tasks to workers.
        process_id (int): The ID of the current worker process.
        handler (Callable): The function that handles the tasks.

    Returns:
        None
    """
    while True:
        if task_manager.all_success:
            return
        task, task_id = task_manager.get_next_task(process_id)
        if task is None:
            time.sleep(0.5)
            continue
        # print(f"will perform task: {task_id}")
        handler(task.extra_info)
        task_manager.mark_completed(task.task_id)
        # print(f"task complete: {task_id}")


if __name__ == "__main__":
    task_manager = TaskManager()

    def some_function():  # 随机睡一会
        time.sleep(random.random() * 3)

    # 添加任务，例如：
    i1 = task_manager.add_task(some_function, [])  # type: ignore
    i2 = task_manager.add_task(some_function, [])  # type: ignore
    i3 = task_manager.add_task(some_function, [i1])  # type: ignore
    i4 = task_manager.add_task(some_function, [i2, i3])  # type: ignore
    i5 = task_manager.add_task(some_function, [i2, i3])  # type: ignore
    i6 = task_manager.add_task(some_function, [i1])  # type: ignore

    threads = [threading.Thread(target=worker, args=(task_manager,)) for _ in range(4)]
    for thread in threads:
        thread.start()
    for thread in threads:
        thread.join()



================================================
FILE: repo_agent/project_manager.py
================================================
import os

import jedi


class ProjectManager:
    def __init__(self, repo_path, project_hierarchy):
        self.repo_path = repo_path
        self.project = jedi.Project(self.repo_path)
        self.project_hierarchy = os.path.join(
            self.repo_path, project_hierarchy, "project_hierarchy.json"
        )

    def get_project_structure(self):
        """
        Returns the structure of the project by recursively walking through the directory tree.

        Returns:
            str: The project structure as a string.
        """

        def walk_dir(root, prefix=""):
            structure.append(prefix + os.path.basename(root))
            new_prefix = prefix + "  "
            for name in sorted(os.listdir(root)):
                if name.startswith("."):  # 忽略隐藏文件和目录
                    continue
                path = os.path.join(root, name)
                if os.path.isdir(path):
                    walk_dir(path, new_prefix)
                elif os.path.isfile(path) and name.endswith(".py"):
                    structure.append(new_prefix + name)

        structure = []
        walk_dir(self.repo_path)
        return "\n".join(structure)

    def build_path_tree(self, who_reference_me, reference_who, doc_item_path):
        from collections import defaultdict

        def tree():
            return defaultdict(tree)

        path_tree = tree()

        # 构建 who_reference_me 和 reference_who 的树
        for path_list in [who_reference_me, reference_who]:
            for path in path_list:
                parts = path.split(os.sep)
                node = path_tree
                for part in parts:
                    node = node[part]

        # 处理 doc_item_path
        parts = doc_item_path.split(os.sep)
        parts[-1] = "✳️" + parts[-1]  # 在最后一个对象前面加上星号
        node = path_tree
        for part in parts:
            node = node[part]

        def tree_to_string(tree, indent=0):
            s = ""
            for key, value in sorted(tree.items()):
                s += "    " * indent + key + "\n"
                if isinstance(value, dict):
                    s += tree_to_string(value, indent + 1)
            return s

        return tree_to_string(path_tree)


if __name__ == "__main__":
    project_manager = ProjectManager(repo_path="", project_hierarchy="")
    print(project_manager.get_project_structure())



================================================
FILE: repo_agent/prompt.py
================================================
from llama_index.core import ChatPromptTemplate
from llama_index.core.llms import ChatMessage, MessageRole

doc_generation_instruction = (
    "You are an AI documentation assistant, and your task is to generate documentation based on the given code of an object. "
    "The purpose of the documentation is to help developers and beginners understand the function and specific usage of the code.\n\n"
    "Currently, you are in a project{project_structure_prefix}\n"
    "{project_structure}\n\n"
    "The path of the document you need to generate in this project is {file_path}.\n"
    'Now you need to generate a document for a {code_type_tell}, whose name is "{code_name}".\n\n'
    "The content of the code is as follows:\n"
    "{code_content}\n\n"
    "{reference_letter}\n"
    "{referencer_content}\n\n"
    "Please generate a detailed explanation document for this object based on the code of the target object itself {combine_ref_situation}.\n\n"
    "Please write out the function of this {code_type_tell} in bold plain text, followed by a detailed analysis in plain text "
    "(including all details), in language {language} to serve as the documentation for this part of the code.\n\n"
    "The standard format is as follows:\n\n"
    "**{code_name}**: The function of {code_name} is XXX. (Only code name and one sentence function description are required)\n"
    "**{parameters_or_attribute}**: The {parameters_or_attribute} of this {code_type_tell}.\n"
    "· parameter1: XXX\n"
    "· parameter2: XXX\n"
    "· ...\n"
    "**Code Description**: The description of this {code_type_tell}.\n"
    "(Detailed and CERTAIN code analysis and description...{has_relationship})\n"
    "**Note**: Points to note about the use of the code\n"
    "{have_return_tell}\n\n"
    "Please note:\n"
    "- Any part of the content you generate SHOULD NOT CONTAIN Markdown hierarchical heading and divider syntax.\n"
    "- Write mainly in the desired language. If necessary, you can write with some English words in the analysis and description "
    "to enhance the document's readability because you do not need to translate the function name or variable name into the target language.\n"
)

documentation_guideline = (
    "Keep in mind that your audience is document readers, so use a deterministic tone to generate precise content and don't let them know "
    "you're provided with code snippet and documents. AVOID ANY SPECULATION and inaccurate descriptions! Now, provide the documentation "
    "for the target object in {language} in a professional way."
)


message_templates = [
    ChatMessage(content=doc_generation_instruction, role=MessageRole.SYSTEM),
    ChatMessage(
        content=documentation_guideline,
        role=MessageRole.USER,
    ),
]

chat_template = ChatPromptTemplate(message_templates=message_templates)



================================================
FILE: repo_agent/runner.py
================================================
import json
import os
import shutil
import subprocess
import threading
import time
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from pathlib import Path

from colorama import Fore, Style
from tqdm import tqdm

from repo_agent.change_detector import ChangeDetector
from repo_agent.chat_engine import ChatEngine
from repo_agent.doc_meta_info import DocItem, DocItemStatus, MetaInfo, need_to_generate
from repo_agent.file_handler import FileHandler
from repo_agent.log import logger
from repo_agent.multi_task_dispatch import worker
from repo_agent.project_manager import ProjectManager
from repo_agent.settings import SettingsManager
from repo_agent.utils.meta_info_utils import delete_fake_files, make_fake_files


class Runner:
    def __init__(self):
        self.setting = SettingsManager.get_setting()
        self.absolute_project_hierarchy_path = (
            self.setting.project.target_repo / self.setting.project.hierarchy_name
        )

        self.project_manager = ProjectManager(
            repo_path=self.setting.project.target_repo,
            project_hierarchy=self.setting.project.hierarchy_name,
        )
        self.change_detector = ChangeDetector(
            repo_path=self.setting.project.target_repo
        )
        self.chat_engine = ChatEngine(project_manager=self.project_manager)

        if not self.absolute_project_hierarchy_path.exists():
            file_path_reflections, jump_files = make_fake_files()
            self.meta_info = MetaInfo.init_meta_info(file_path_reflections, jump_files)
            self.meta_info.checkpoint(
                target_dir_path=self.absolute_project_hierarchy_path
            )
        else:  # 如果存在全局结构信息文件夹.project_hierarchy，就从中加载
            self.meta_info = MetaInfo.from_checkpoint_path(
                self.absolute_project_hierarchy_path
            )

        self.meta_info.checkpoint(  # 更新白名单后也要重新将全局信息写入到.project_doc_record文件夹中
            target_dir_path=self.absolute_project_hierarchy_path
        )
        self.runner_lock = threading.Lock()

    def get_all_pys(self, directory):
        """
        Get all Python files in the given directory.

        Args:
            directory (str): The directory to search.

        Returns:
            list: A list of paths to all Python files.
        """
        python_files = []

        for root, dirs, files in os.walk(directory):
            for file in files:
                if file.endswith(".py"):
                    python_files.append(os.path.join(root, file))

        return python_files

    def generate_doc_for_a_single_item(self, doc_item: DocItem):
        """为一个对象生成文档"""
        try:
            if not need_to_generate(doc_item, self.setting.project.ignore_list):
                print(
                    f"Content ignored/Document generated, skipping: {doc_item.get_full_name()}"
                )
            else:
                print(
                    f" -- Generating document  {Fore.LIGHTYELLOW_EX}{doc_item.item_type.name}: {doc_item.get_full_name()}{Style.RESET_ALL}"
                )
                response_message = self.chat_engine.generate_doc(
                    doc_item=doc_item,
                )
                doc_item.md_content.append(response_message)  # type: ignore
                doc_item.item_status = DocItemStatus.doc_up_to_date
                self.meta_info.checkpoint(
                    target_dir_path=self.absolute_project_hierarchy_path
                )
        except Exception:
            logger.exception(
                f"Document generation failed after multiple attempts, skipping: {doc_item.get_full_name()}"
            )
            doc_item.item_status = DocItemStatus.doc_has_not_been_generated

    def first_generate(self):
        """
        生成所有文档，完成后刷新并保存文件系统中的文档信息。
        """
        logger.info("Starting to generate documentation")
        check_task_available_func = partial(
            need_to_generate, ignore_list=self.setting.project.ignore_list
        )
        task_manager = self.meta_info.get_topology(check_task_available_func)
        before_task_len = len(task_manager.task_dict)

        if not self.meta_info.in_generation_process:
            self.meta_info.in_generation_process = True
            logger.info("Init a new task-list")
        else:
            logger.info("Load from an existing task-list")
        self.meta_info.print_task_list(task_manager.task_dict)

        try:
            # 创建并启动线程
            threads = [
                threading.Thread(
                    target=worker,
                    args=(
                        task_manager,
                        process_id,
                        self.generate_doc_for_a_single_item,
                    ),
                )
                for process_id in range(self.setting.project.max_thread_count)
            ]
            for thread in threads:
                thread.start()
            for thread in threads:
                thread.join()

            # 所有任务完成后刷新文档
            self.markdown_refresh()

            # 更新文档版本
            self.meta_info.document_version = (
                self.change_detector.repo.head.commit.hexsha
            )
            self.meta_info.in_generation_process = False
            self.meta_info.checkpoint(
                target_dir_path=self.absolute_project_hierarchy_path
            )
            logger.info(
                f"Successfully generated {before_task_len - len(task_manager.task_dict)} documents."
            )

        except BaseException as e:
            logger.error(
                f"An error occurred: {e}. {before_task_len - len(task_manager.task_dict)} docs are generated at this time"
            )

    def markdown_refresh(self):
        """刷新最新的文档信息到markdown格式文件夹中"""
        with self.runner_lock:
            # 定义 markdown 文件夹路径
            markdown_folder = (
                Path(self.setting.project.target_repo)
                / self.setting.project.markdown_docs_name
            )

            # 删除并重新创建目录
            if markdown_folder.exists():
                logger.debug(f"Deleting existing contents of {markdown_folder}")
                shutil.rmtree(markdown_folder)
            markdown_folder.mkdir(parents=True, exist_ok=True)
            logger.debug(f"Created markdown folder at {markdown_folder}")

        # 遍历文件列表生成 markdown
        file_item_list = self.meta_info.get_all_files()
        logger.debug(f"Found {len(file_item_list)} files to process.")

        for file_item in tqdm(file_item_list):
            # 检查文档内容
            def recursive_check(doc_item) -> bool:
                if doc_item.md_content:
                    return True
                for child in doc_item.children.values():
                    if recursive_check(child):
                        return True
                return False

            if not recursive_check(file_item):
                logger.debug(
                    f"No documentation content for: {file_item.get_full_name()}, skipping."
                )
                continue

            # 生成 markdown 内容
            markdown = ""
            for child in file_item.children.values():
                markdown += self.to_markdown(child, 2)

            if not markdown:
                logger.warning(
                    f"No markdown content generated for: {file_item.get_full_name()}"
                )
                continue

            # 确定并创建文件路径
            file_path = Path(
                self.setting.project.markdown_docs_name
            ) / file_item.get_file_name().replace(".py", ".md")
            abs_file_path = self.setting.project.target_repo / file_path
            logger.debug(f"Writing markdown to: {abs_file_path}")

            # 确保目录存在
            abs_file_path.parent.mkdir(parents=True, exist_ok=True)
            logger.debug(f"Ensured directory exists: {abs_file_path.parent}")

            # 使用锁保护文件写入操作
            with self.runner_lock:
                for attempt in range(3):  # 最多重试3次
                    try:
                        with open(abs_file_path, "w", encoding="utf-8") as file:
                            file.write(markdown)
                        logger.debug(f"Successfully wrote to {abs_file_path}")
                        break
                    except IOError as e:
                        logger.error(
                            f"Failed to write {abs_file_path} on attempt {attempt + 1}: {e}"
                        )
                        time.sleep(1)  # 延迟再试

        logger.info(
            f"Markdown documents have been refreshed at {self.setting.project.markdown_docs_name}"
        )

    def to_markdown(self, item, now_level: int) -> str:
        """将文件内容转化为 markdown 格式的文本"""
        markdown_content = (
            "#" * now_level + f" {item.item_type.to_str()} {item.obj_name}"
        )
        if "params" in item.content.keys() and item.content["params"]:
            markdown_content += f"({', '.join(item.content['params'])})"
        markdown_content += "\n"
        if item.md_content:
            markdown_content += f"{item.md_content[-1]}\n"
        else:
            markdown_content += "Doc is waiting to be generated...\n"
        for child in item.children.values():
            markdown_content += self.to_markdown(child, now_level + 1)
            markdown_content += "***\n"
        return markdown_content

    def git_commit(self, commit_message):
        try:
            subprocess.check_call(
                ["git", "commit", "--no-verify", "-m", commit_message],
                shell=True,
            )
        except subprocess.CalledProcessError as e:
            print(f"An error occurred while trying to commit {str(e)}")

    def run(self):
        """
        Runs the document update process.

        This method detects the changed Python files, processes each file, and updates the documents accordingly.

        Returns:
            None
        """

        if self.meta_info.document_version == "":
            # 根据document version自动检测是否仍在最初生成的process里(是否为第一次生成)
            self.first_generate()  # 如果是第一次做文档生成任务，就通过first_generate生成所有文档
            self.meta_info.checkpoint(
                target_dir_path=self.absolute_project_hierarchy_path,
                flash_reference_relation=True,
            )  # 这一步将生成后的meta信息（包含引用关系）写入到.project_doc_record文件夹中
            return

        if (
            not self.meta_info.in_generation_process
        ):  # 如果不是在生成过程中，就开始检测变更
            logger.info("Starting to detect changes.")

            """采用新的办法
            1.新建一个project-hierachy
            2.和老的hierarchy做merge,处理以下情况：
            - 创建一个新文件：需要生成对应的doc
            - 文件、对象被删除：对应的doc也删除(按照目前的实现，文件重命名算是删除再添加)
            - 引用关系变了：对应的obj-doc需要重新生成
            
            merge后的new_meta_info中：
            1.新建的文件没有文档，因此metainfo merge后还是没有文档
            2.被删除的文件和obj，本来就不在新的meta里面，相当于文档被自动删除了
            3.只需要观察被修改的文件，以及引用关系需要被通知的文件去重新生成文档"""
            file_path_reflections, jump_files = make_fake_files()
            new_meta_info = MetaInfo.init_meta_info(file_path_reflections, jump_files)
            new_meta_info.load_doc_from_older_meta(self.meta_info)

            self.meta_info = new_meta_info  # 更新自身的meta_info信息为new的信息
            self.meta_info.in_generation_process = True  # 将in_generation_process设置为True，表示检测到变更后Generating document 的过程中

        # 处理任务队列
        check_task_available_func = partial(
            need_to_generate, ignore_list=self.setting.project.ignore_list
        )

        task_manager = self.meta_info.get_task_manager(
            self.meta_info.target_repo_hierarchical_tree,
            task_available_func=check_task_available_func,
        )

        for item_name, item_type in self.meta_info.deleted_items_from_older_meta:
            print(
                f"{Fore.LIGHTMAGENTA_EX}[Dir/File/Obj Delete Dected]: {Style.RESET_ALL} {item_type} {item_name}"
            )
        self.meta_info.print_task_list(task_manager.task_dict)
        if task_manager.all_success:
            logger.info(
                "No tasks in the queue, all documents are completed and up to date."
            )

        threads = [
            threading.Thread(
                target=worker,
                args=(task_manager, process_id, self.generate_doc_for_a_single_item),
            )
            for process_id in range(self.setting.project.max_thread_count)
        ]
        for thread in threads:
            thread.start()
        for thread in threads:
            thread.join()

        self.meta_info.in_generation_process = False
        self.meta_info.document_version = self.change_detector.repo.head.commit.hexsha

        self.meta_info.checkpoint(
            target_dir_path=self.absolute_project_hierarchy_path,
            flash_reference_relation=True,
        )
        logger.info(f"Doc has been forwarded to the latest version")

        self.markdown_refresh()
        delete_fake_files()

        logger.info(f"Starting to git-add DocMetaInfo and newly generated Docs")
        time.sleep(1)

        # 将run过程中更新的Markdown文件（未暂存）添加到暂存区
        git_add_result = self.change_detector.add_unstaged_files()

        if len(git_add_result) > 0:
            logger.info(
                f"Added {[file for file in git_add_result]} to the staging area."
            )

        # self.git_commit(f"Update documentation for {file_handler.file_path}") # 提交变更

    def add_new_item(self, file_handler, json_data):
        """
        Add new projects to the JSON file and generate corresponding documentation.

        Args:
            file_handler (FileHandler): The file handler object for reading and writing files.
            json_data (dict): The JSON data storing the project structure information.

        Returns:
            None
        """
        file_dict = {}
        # 因为是新增的项目，所以这个文件里的所有对象都要写一个文档
        for (
            structure_type,
            name,
            start_line,
            end_line,
            parent,
            params,
        ) in file_handler.get_functions_and_classes(file_handler.read_file()):
            code_info = file_handler.get_obj_code_info(
                structure_type, name, start_line, end_line, parent, params
            )
            response_message = self.chat_engine.generate_doc(code_info, file_handler)
            md_content = response_message.content
            code_info["md_content"] = md_content
            # 文件对象file_dict中添加一个新的对象
            file_dict[name] = code_info

        json_data[file_handler.file_path] = file_dict
        # 将新的项写入json文件
        with open(self.project_manager.project_hierarchy, "w", encoding="utf-8") as f:
            json.dump(json_data, f, indent=4, ensure_ascii=False)
        logger.info(
            f"The structural information of the newly added file {file_handler.file_path} has been written into a JSON file."
        )
        # 将变更部分的json文件内容转换成markdown内容
        markdown = file_handler.convert_to_markdown_file(
            file_path=file_handler.file_path
        )
        # 将markdown内容写入.md文件
        file_handler.write_file(
            os.path.join(
                self.project_manager.repo_path,
                self.setting.project.markdown_docs_name,
                file_handler.file_path.replace(".py", ".md"),
            ),
            markdown,
        )
        logger.info(f"已生成新增文件 {file_handler.file_path} 的Markdown文档。")

    def process_file_changes(self, repo_path, file_path, is_new_file):
        """
        This function is called in the loop of detected changed files. Its purpose is to process changed files according to the absolute file path, including new files and existing files.
        Among them, changes_in_pyfile is a dictionary that contains information about the changed structures. An example format is: {'added': {'add_context_stack', '__init__'}, 'removed': set()}

        Args:
            repo_path (str): The path to the repository.
            file_path (str): The relative path to the file.
            is_new_file (bool): Indicates whether the file is new or not.

        Returns:
            None
        """

        file_handler = FileHandler(
            repo_path=repo_path, file_path=file_path
        )  # 变更文件的操作器
        # 获取整个py文件的代码
        source_code = file_handler.read_file()
        changed_lines = self.change_detector.parse_diffs(
            self.change_detector.get_file_diff(file_path, is_new_file)
        )
        changes_in_pyfile = self.change_detector.identify_changes_in_structure(
            changed_lines, file_handler.get_functions_and_classes(source_code)
        )
        logger.info(f"检测到变更对象：\n{changes_in_pyfile}")

        # 判断project_hierarchy.json文件中能否找到对应.py文件路径的项
        with open(self.project_manager.project_hierarchy, "r", encoding="utf-8") as f:
            json_data = json.load(f)

        # 如果找到了对应文件
        if file_handler.file_path in json_data:
            # 更新json文件中的内容
            json_data[file_handler.file_path] = self.update_existing_item(
                json_data[file_handler.file_path], file_handler, changes_in_pyfile
            )
            # 将更新后的file写回到json文件中
            with open(
                self.project_manager.project_hierarchy, "w", encoding="utf-8"
            ) as f:
                json.dump(json_data, f, indent=4, ensure_ascii=False)

            logger.info(f"已更新{file_handler.file_path}文件的json结构信息。")

            # 将变更部分的json文件内容转换成markdown内容
            markdown = file_handler.convert_to_markdown_file(
                file_path=file_handler.file_path
            )
            # 将markdown内容写入.md文件
            file_handler.write_file(
                os.path.join(
                    self.setting.project.markdown_docs_name,
                    file_handler.file_path.replace(".py", ".md"),
                ),
                markdown,
            )
            logger.info(f"已更新{file_handler.file_path}文件的Markdown文档。")

        # 如果没有找到对应的文件，就添加一个新的项
        else:
            self.add_new_item(file_handler, json_data)

        # 将run过程中更新的Markdown文件（未暂存）添加到暂存区
        git_add_result = self.change_detector.add_unstaged_files()

        if len(git_add_result) > 0:
            logger.info(f"已添加 {[file for file in git_add_result]} 到暂存区")

        # self.git_commit(f"Update documentation for {file_handler.file_path}") # 提交变更

    def update_existing_item(self, file_dict, file_handler, changes_in_pyfile):
        """
        Update existing projects.

        Args:
            file_dict (dict): A dictionary containing file structure information.
            file_handler (FileHandler): The file handler object.
            changes_in_pyfile (dict): A dictionary containing information about the objects that have changed in the file.

        Returns:
            dict: The updated file structure information dictionary.
        """
        new_obj, del_obj = self.get_new_objects(file_handler)

        # 处理被删除的对象
        for obj_name in del_obj:  # 真正被删除的对象
            if obj_name in file_dict:
                del file_dict[obj_name]
                logger.info(f"已删除 {obj_name} 对象。")

        referencer_list = []

        # 生成文件的结构信息，获得当前文件中的所有对象， 这里其实就是文件更新之后的结构了
        current_objects = file_handler.generate_file_structure(file_handler.file_path)

        current_info_dict = {obj["name"]: obj for obj in current_objects.values()}

        # 更新全局文件结构信息，比如代码起始行\终止行等
        for current_obj_name, current_obj_info in current_info_dict.items():
            if current_obj_name in file_dict:
                # 如果当前对象在旧对象列表中存在，更新旧对象的信息
                file_dict[current_obj_name]["type"] = current_obj_info["type"]
                file_dict[current_obj_name]["code_start_line"] = current_obj_info[
                    "code_start_line"
                ]
                file_dict[current_obj_name]["code_end_line"] = current_obj_info[
                    "code_end_line"
                ]
                file_dict[current_obj_name]["parent"] = current_obj_info["parent"]
                file_dict[current_obj_name]["name_column"] = current_obj_info[
                    "name_column"
                ]
            else:
                # 如果当前对象在旧对象列表中不存在，将新对象添加到旧对象列表中
                file_dict[current_obj_name] = current_obj_info

        # 对于每一个对象：获取其引用者列表
        for obj_name, _ in changes_in_pyfile["added"]:
            for current_object in current_objects.values():  # 引入new_objects的目的是获取到find_all_referencer中必要的参数信息。在changes_in_pyfile['added']中只有对象和其父级结构的名称，缺少其他参数
                if (
                    obj_name == current_object["name"]
                ):  # 确保只有当added中的对象名称匹配new_objects时才添加引用者
                    # 获取每个需要生成文档的对象的引用者
                    referencer_obj = {
                        "obj_name": obj_name,
                        "obj_referencer_list": self.project_manager.find_all_referencer(
                            variable_name=current_object["name"],
                            file_path=file_handler.file_path,
                            line_number=current_object["code_start_line"],
                            column_number=current_object["name_column"],
                        ),
                    }
                    referencer_list.append(
                        referencer_obj
                    )  # 对于每一个正在处理的对象，添加他的引用者字典到全部对象的应用者列表中

        with ThreadPoolExecutor(max_workers=5) as executor:
            # 通过线程池并发执行
            futures = []
            for changed_obj in changes_in_pyfile["added"]:  # 对于每一个待处理的对象
                for ref_obj in referencer_list:
                    if (
                        changed_obj[0] == ref_obj["obj_name"]
                    ):  # 在referencer_list中找到它的引用者字典！
                        future = executor.submit(
                            self.update_object,
                            file_dict,
                            file_handler,
                            changed_obj[0],
                            ref_obj["obj_referencer_list"],
                        )
                        print(
                            f"正在生成 {Fore.CYAN}{file_handler.file_path}{Style.RESET_ALL}中的{Fore.CYAN}{changed_obj[0]}{Style.RESET_ALL}对象文档."
                        )
                        futures.append(future)

            for future in futures:
                future.result()

        # 更新传入的file参数
        return file_dict

    def update_object(self, file_dict, file_handler, obj_name, obj_referencer_list):
        """
        Generate documentation content and update corresponding field information of the object.

        Args:
            file_dict (dict): A dictionary containing old object information.
            file_handler: The file handler.
            obj_name (str): The object name.
            obj_referencer_list (list): The list of object referencers.

        Returns:
            None
        """
        if obj_name in file_dict:
            obj = file_dict[obj_name]
            response_message = self.chat_engine.generate_doc(
                obj, file_handler, obj_referencer_list
            )
            obj["md_content"] = response_message.content

    def get_new_objects(self, file_handler):
        """
        The function gets the added and deleted objects by comparing the current version and the previous version of the .py file.

        Args:
            file_handler (FileHandler): The file handler object.

        Returns:
            tuple: A tuple containing the added and deleted objects, in the format (new_obj, del_obj)

        Output example:
            new_obj: ['add_context_stack', '__init__']
            del_obj: []
        """
        current_version, previous_version = file_handler.get_modified_file_versions()
        parse_current_py = file_handler.get_functions_and_classes(current_version)
        parse_previous_py = (
            file_handler.get_functions_and_classes(previous_version)
            if previous_version
            else []
        )

        current_obj = {f[1] for f in parse_current_py}
        previous_obj = {f[1] for f in parse_previous_py}

        new_obj = list(current_obj - previous_obj)
        del_obj = list(previous_obj - current_obj)
        return new_obj, del_obj


if __name__ == "__main__":
    runner = Runner()

    runner.run()

    logger.info("文档任务完成。")



================================================
FILE: repo_agent/settings.py
================================================
from enum import StrEnum
from typing import Optional

from iso639 import Language, LanguageNotFoundError
from pydantic import (
    DirectoryPath,
    Field,
    HttpUrl,
    PositiveFloat,
    PositiveInt,
    SecretStr,
    field_validator,
)
from pydantic_settings import BaseSettings
from pathlib import Path


class LogLevel(StrEnum):
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"


class ProjectSettings(BaseSettings):
    target_repo: DirectoryPath = ""  # type: ignore
    hierarchy_name: str = ".project_doc_record"
    markdown_docs_name: str = "markdown_docs"
    ignore_list: list[str] = []
    language: str = "English"
    max_thread_count: PositiveInt = 4
    log_level: LogLevel = LogLevel.INFO

    @field_validator("language")
    @classmethod
    def validate_language_code(cls, v: str) -> str:
        try:
            language_name = Language.match(v).name
            return language_name  # Returning the resolved language name
        except LanguageNotFoundError:
            raise ValueError(
                "Invalid language input. Please enter a valid ISO 639 code or language name."
            )

    @field_validator("log_level", mode="before")
    @classmethod
    def set_log_level(cls, v: str) -> LogLevel:
        if isinstance(v, str):
            v = v.upper()  # Convert input to uppercase
        if (
            v in LogLevel._value2member_map_
        ):  # Check if the converted value is in enum members
            return LogLevel(v)
        raise ValueError(f"Invalid log level: {v}")


class ChatCompletionSettings(BaseSettings):
    model: str = "gpt-4o-mini"  # NOTE: No model restrictions for user flexibility, but it's recommended to use models with a larger context window.
    temperature: PositiveFloat = 0.2
    request_timeout: PositiveInt = 60
    openai_base_url: str = "https://api.openai.com/v1"
    openai_api_key: SecretStr = Field(..., exclude=True)

    @field_validator("openai_base_url", mode="before")
    @classmethod
    def convert_base_url_to_str(cls, openai_base_url: HttpUrl) -> str:
        return str(openai_base_url)


class Setting(BaseSettings):
    project: ProjectSettings = {}  # type: ignore
    chat_completion: ChatCompletionSettings = {}  # type: ignore


class SettingsManager:
    _setting_instance: Optional[Setting] = (
        None  # Private class attribute, initially None
    )

    @classmethod
    def get_setting(cls):
        if cls._setting_instance is None:
            cls._setting_instance = Setting()
        return cls._setting_instance

    @classmethod
    def initialize_with_params(
        cls,
        target_repo: Path,
        markdown_docs_name: str,
        hierarchy_name: str,
        ignore_list: list[str],
        language: str,
        max_thread_count: int,
        log_level: str,
        model: str,
        temperature: float,
        request_timeout: int,
        openai_base_url: str,
    ):
        project_settings = ProjectSettings(
            target_repo=target_repo,
            hierarchy_name=hierarchy_name,
            markdown_docs_name=markdown_docs_name,
            ignore_list=ignore_list,
            language=language,
            max_thread_count=max_thread_count,
            log_level=LogLevel(log_level),
        )

        chat_completion_settings = ChatCompletionSettings(
            model=model,
            temperature=temperature,
            request_timeout=request_timeout,
            openai_base_url=openai_base_url,
        )

        cls._setting_instance = Setting(
            project=project_settings,
            chat_completion=chat_completion_settings,
        )


if __name__ == "__main__":
    setting = SettingsManager.get_setting()
    print(setting.model_dump())



================================================
FILE: repo_agent/chat_with_repo/__init__.py
================================================
# repo_agent/chat_with_repo/__init__.py

from .main import main



================================================
FILE: repo_agent/chat_with_repo/__main__.py
================================================
from .main import main

main()



================================================
FILE: repo_agent/chat_with_repo/gradio_interface.py
================================================
import gradio as gr
import markdown

from repo_agent.log import logger


class GradioInterface:
    def __init__(self, respond_function):
        self.respond = respond_function
        self.cssa = """
                <style>
                        .outer-box {
                            border: 1px solid #333; /* 外框的边框颜色和大小 */
                            border-radius: 10px; /* 外框的边框圆角效果 */
                            padding: 10px; /* 外框的内边距 */
                        }

                        .title {
                            margin-bottom: 10px; /* 标题和内框之间的距离 */
                        }

                        .inner-box {
                            border: 1px solid #555; /* 内框的边框颜色和大小 */
                            border-radius: 5px; /* 内框的边框圆角效果 */
                            padding: 10px; /* 内框的内边距 */
                        }

                        .content {
                            white-space: pre-wrap; /* 保留空白符和换行符 */
                            font-size: 16px; /* 内容文字大小 */
                            height: 405px;
                            overflow: auto;
                        }
                    </style>
                    <div class="outer-box"">
        
        """
        self.cssb = """
                        </div>
                    </div>
                </div>
        """
        self.setup_gradio_interface()

    def wrapper_respond(self, msg_input, system_input):
        # 调用原来的 respond 函数
        msg, output1, output2, output3, code, codex = self.respond(
            msg_input, system_input
        )
        output1 = markdown.markdown(str(output1))
        output2 = markdown.markdown(str(output2))
        code = markdown.markdown(str(code))
        output1 = (
            self.cssa
            + """
                          <div class="title">Response</div>
                            <div class="inner-box">
                                <div class="content">
                """
            + str(output1)
            + """
                        </div>
                    </div>
                </div>
                """
        )
        output2 = (
            self.cssa
            + """
                          <div class="title">Embedding Recall</div>
                            <div class="inner-box">
                                <div class="content">
                """
            + str(output2)
            + self.cssb
        )
        code = (
            self.cssa
            + """
                          <div class="title">Code</div>
                            <div class="inner-box">
                                <div class="content">
                """
            + str(code)
            + self.cssb
        )

        return msg, output1, output2, output3, code, codex

    def clean(self):
        msg = ""
        output1 = gr.HTML(
            self.cssa
            + """
                                        <div class="title">Response</div>
                                            <div class="inner-box">
                                                <div class="content">
                      
                                            """
            + self.cssb
        )
        output2 = gr.HTML(
            self.cssa
            + """
                                        <div class="title">Embedding Recall</div>
                                            <div class="inner-box">
                                                <div class="content">
                                    
                                            """
            + self.cssb
        )
        output3 = ""
        code = gr.HTML(
            self.cssa
            + """
                                        <div class="title">Code</div>
                                            <div class="inner-box">
                                                <div class="content">
                                   
                                            """
            + self.cssb
        )
        codex = ""
        return msg, output1, output2, output3, code, codex

    def setup_gradio_interface(self):
        with gr.Blocks() as demo:
            gr.Markdown("""
                # RepoAgent: Chat with doc
            """)
            with gr.Tab("main chat"):
                with gr.Row():
                    with gr.Column():
                        msg = gr.Textbox(label="Question Input", lines=4)
                        system = gr.Textbox(
                            label="(Optional)insturction editing", lines=4
                        )
                        btn = gr.Button("Submit")
                        btnc = gr.ClearButton()
                        btnr = gr.Button("record")

                    output1 = gr.HTML(
                        self.cssa
                        + """
                                        <div class="title">Response</div>
                                            <div class="inner-box">
                                                <div class="content">
                      
                                            """
                        + self.cssb
                    )
                with gr.Row():
                    with gr.Column():
                        # output2 = gr.Textbox(label = "Embedding recall")
                        output2 = gr.HTML(
                            self.cssa
                            + """
                                        <div class="title">Embedding Recall</div>
                                            <div class="inner-box">
                                                <div class="content">
                                    
                                            """
                            + self.cssb
                        )
                    code = gr.HTML(
                        self.cssa
                        + """
                                        <div class="title">Code</div>
                                            <div class="inner-box">
                                                <div class="content">
                                   
                                            """
                        + self.cssb
                    )
                    with gr.Row():
                        with gr.Column():
                            output3 = gr.Textbox(label="key words", lines=2)
                            output4 = gr.Textbox(label="key words code", lines=14)

            btn.click(
                self.wrapper_respond,
                inputs=[msg, system],
                outputs=[msg, output1, output2, output3, code, output4],
            )
            btnc.click(
                self.clean, outputs=[msg, output1, output2, output3, code, output4]
            )
            msg.submit(
                self.wrapper_respond,
                inputs=[msg, system],
                outputs=[msg, output1, output2, output3, code, output4],
            )  # Press enter to submit

        gr.close_all()
        demo.queue().launch(share=False, height=800)


# 使用方法
if __name__ == "__main__":

    def respond_function(msg, system):
        RAG = """

        
        """
        return msg, RAG, "Embedding_recall_output", "Key_words_output", "Code_output"

    gradio_interface = GradioInterface(respond_function)



================================================
FILE: repo_agent/chat_with_repo/json_handler.py
================================================
import json
import sys

from repo_agent.log import logger


class JsonFileProcessor:
    def __init__(self, file_path):
        self.file_path = file_path

    def read_json_file(self):
        try:
            with open(self.file_path, "r", encoding="utf-8") as file:
                data = json.load(file)
            return data
        except FileNotFoundError:
            logger.exception(f"File not found: {self.file_path}")
            sys.exit(1)

    def extract_data(self):
        # Load JSON data from a file
        json_data = self.read_json_file()
        md_contents = []
        extracted_contents = []
        # Iterate through each file in the JSON data
        for file, items in json_data.items():
            # Check if the value is a list (new format)
            if isinstance(items, list):
                # Iterate through each item in the list
                for item in items:
                    # Check if 'md_content' exists and is not empty
                    if "md_content" in item and item["md_content"]:
                        # Append the first element of 'md_content' to the result list
                        md_contents.append(item["md_content"][0])
                        # Build a dictionary containing the required information
                        item_dict = {
                            "type": item.get("type", "UnknownType"),
                            "name": item.get("name", "Unnamed"),
                            "code_start_line": item.get("code_start_line", -1),
                            "code_end_line": item.get("code_end_line", -1),
                            "have_return": item.get("have_return", False),
                            "code_content": item.get("code_content", "NoContent"),
                            "name_column": item.get("name_column", 0),
                            "item_status": item.get("item_status", "UnknownStatus"),
                            # Adapt or remove fields based on new structure requirements
                        }
                        extracted_contents.append(item_dict)
        return md_contents, extracted_contents

    def recursive_search(self, data_item, search_text, code_results, md_results):
        if isinstance(data_item, dict):
            # Direct comparison is removed as there's no direct key==search_text in the new format
            for key, value in data_item.items():
                # Recursively search through dictionary values and lists
                if isinstance(value, (dict, list)):
                    self.recursive_search(value, search_text, code_results, md_results)
        elif isinstance(data_item, list):
            for item in data_item:
                # Now we check for the 'name' key in each item of the list
                if isinstance(item, dict) and item.get("name") == search_text:
                    # If 'code_content' exists, append it to results
                    if "code_content" in item:
                        code_results.append(item["code_content"])
                        md_results.append(item["md_content"])
                # Recursive call in case of nested lists or dicts
                self.recursive_search(item, search_text, code_results, md_results)

    def search_code_contents_by_name(self, file_path, search_text):
        # Attempt to retrieve code from the JSON file
        try:
            with open(file_path, "r", encoding="utf-8") as file:
                data = json.load(file)
                code_results = []
                md_results = []  # List to store matching items' code_content and md_content
                self.recursive_search(data, search_text, code_results, md_results)
                # 确保无论结果如何都返回两个值
                if code_results or md_results:
                    return code_results, md_results
                else:
                    return ["No matching item found."], ["No matching item found."]
        except FileNotFoundError:
            return "File not found."
        except json.JSONDecodeError:
            return "Invalid JSON file."
        except Exception as e:
            return f"An error occurred: {e}"


if __name__ == "__main__":
    processor = JsonFileProcessor("database.json")
    md_contents, extracted_contents = processor.extract_data()



================================================
FILE: repo_agent/chat_with_repo/main.py
================================================
import time

from repo_agent.chat_with_repo.gradio_interface import GradioInterface
from repo_agent.chat_with_repo.rag import RepoAssistant
from repo_agent.log import logger
from repo_agent.settings import SettingsManager


def main():
    logger.info("Initializing the RepoAgent chat with doc module.")

    # Load settings
    setting = SettingsManager.get_setting()

    api_key = setting.chat_completion.openai_api_key.get_secret_value()
    api_base = str(setting.chat_completion.openai_base_url)
    db_path = (
        setting.project.target_repo
        / setting.project.hierarchy_name
        / "project_hierarchy.json"
    )

    # Initialize RepoAssistant
    assistant = RepoAssistant(api_key, api_base, db_path)

    # Extract data
    md_contents, meta_data = assistant.json_data.extract_data()

    # Create vector store and measure runtime
    logger.info("Starting vector store creation...")
    start_time = time.time()
    assistant.vector_store_manager.create_vector_store(
        md_contents, meta_data, api_key, api_base
    )
    elapsed_time = time.time() - start_time
    logger.info(f"Vector store created successfully in {elapsed_time:.2f} seconds.")

    # Launch Gradio interface
    GradioInterface(assistant.respond)


if __name__ == "__main__":
    main()



================================================
FILE: repo_agent/chat_with_repo/prompt.py
================================================
from llama_index.core import ChatPromptTemplate, PromptTemplate
from llama_index.core.llms import ChatMessage, MessageRole

# Query Generation Prompt
query_generation_prompt_str = (
    "You are a helpful assistant that generates multiple search queries based on a "
    "single input query. Generate {num_queries} search queries, one on each line, "
    "related to the following input query:\n"
    "Query: {query}\n"
    "Queries:\n"
)
query_generation_template = PromptTemplate(query_generation_prompt_str)

# Relevance Ranking Prompt
relevance_ranking_instruction = (
    "You are an expert relevance ranker. Given a list of documents and a query, your job is to determine how relevant each document is for answering the query. "
    "Your output is JSON, which is a list of documents. Each document has two fields, content and relevance_score. relevance_score is from 0.0 to 100.0. "
    "Higher relevance means higher score."
)
relevance_ranking_guideline = "Query: {query} Docs: {docs}"

relevance_ranking_message_template = [
    ChatMessage(content=relevance_ranking_instruction, role=MessageRole.SYSTEM),
    ChatMessage(
        content=relevance_ranking_guideline,
        role=MessageRole.USER,
    ),
]
relevance_ranking_chat_template = ChatPromptTemplate(
    message_templates=relevance_ranking_message_template
)

# RAG (Retrieve and Generate) Prompt
rag_prompt_str = (
    "You are a helpful assistant in repository Q&A. Users will ask questions about something contained in a repository. "
    "You will be shown the user's question, and the relevant information from the repository. Answer the user's question only with information given.\n\n"
    "Question: {query}.\n\n"
    "Information: {information}"
)
rag_template = PromptTemplate(rag_prompt_str)

# RAG_AR (Advanced RAG) Prompt
rag_ar_prompt_str = (
    "You are a helpful Repository-Level Software Q&A assistant. Your task is to answer users' questions based on the given information about a software repository, "
    "including related code and documents.\n\n"
    "Currently, you're in the {project_name} project. The user's question is:\n"
    "{query}\n\n"
    "Now, you are given related code and documents as follows:\n\n"
    "-------------------Code-------------------\n"
    "Some most likely related code snippets recalled by the retriever are:\n"
    "{related_code}\n\n"
    "-------------------Document-------------------\n"
    "Some most relevant documents recalled by the retriever are:\n"
    "{embedding_recall}\n\n"
    "Please note:   \n"
    "1. All the provided recall results are related to the current project {project_name}. Please filter useful information according to the user's question and provide corresponding answers or solutions.\n"
    "2. Ensure that your responses are accurate and detailed. Present specific answers in a professional manner and tone.\n"
    "3. The user's question may be asked in any language. You must respond **in the same language** as the user's question, even if the input language is not English.\n"
    "4. If you find the user's question completely unrelated to the provided information or if you believe you cannot provide an accurate answer, kindly decline. Note: DO NOT fabricate any non-existent information.\n\n"
    "Now, focusing on the user's query, and incorporating the given information to offer a specific, detailed, and professional answer IN THE SAME LANGUAGE AS the user's question."
)


rag_ar_template = PromptTemplate(rag_ar_prompt_str)



================================================
FILE: repo_agent/chat_with_repo/rag.py
================================================
import json

from llama_index.llms.openai import OpenAI

from repo_agent.chat_with_repo.json_handler import JsonFileProcessor
from repo_agent.chat_with_repo.prompt import (
    query_generation_template,
    rag_ar_template,
    rag_template,
    relevance_ranking_chat_template,
)
from repo_agent.chat_with_repo.text_analysis_tool import TextAnalysisTool
from repo_agent.chat_with_repo.vector_store_manager import VectorStoreManager
from repo_agent.log import logger


class RepoAssistant:
    def __init__(self, api_key, api_base, db_path):
        self.db_path = db_path
        self.md_contents = []

        self.weak_model = OpenAI(
            api_key=api_key,
            api_base=api_base,
            model="gpt-4o-mini",
        )
        self.strong_model = OpenAI(
            api_key=api_key,
            api_base=api_base,
            model="gpt-4o",
        )
        self.textanslys = TextAnalysisTool(self.weak_model, db_path)
        self.json_data = JsonFileProcessor(db_path)
        self.vector_store_manager = VectorStoreManager(top_k=5, llm=self.weak_model)

    def generate_queries(self, query_str: str, num_queries: int = 4):
        fmt_prompt = query_generation_template.format(
            num_queries=num_queries - 1, query=query_str
        )
        response = self.weak_model.complete(fmt_prompt)
        queries = response.text.split("\n")
        return queries

    def rerank(self, query, docs):  # 这里要防止返回值格式上出问题
        response = self.weak_model.chat(
            response_format={"type": "json_object"},
            temperature=0,
            messages=relevance_ranking_chat_template.format_messages(
                query=query, docs=docs
            ),
        )
        scores = json.loads(response.message.content)["documents"]  # type: ignore
        logger.debug(f"scores: {scores}")
        sorted_data = sorted(scores, key=lambda x: x["relevance_score"], reverse=True)
        top_5_contents = [doc["content"] for doc in sorted_data[:5]]
        return top_5_contents

    def rag(self, query, retrieved_documents):
        rag_prompt = rag_template.format(
            query=query, information="\n\n".join(retrieved_documents)
        )
        response = self.weak_model.complete(rag_prompt)
        return response.text

    def list_to_markdown(self, list_items):
        markdown_content = ""

        # 对于列表中的每个项目，添加一个带数字的列表项
        for index, item in enumerate(list_items, start=1):
            markdown_content += f"{index}. {item}\n"

        return markdown_content

    def rag_ar(self, query, related_code, embedding_recall, project_name):
        rag_ar_prompt = rag_ar_template.format_messages(
            query=query,
            related_code=related_code,
            embedding_recall=embedding_recall,
            project_name=project_name,
        )
        response = self.strong_model.chat(rag_ar_prompt)
        return response.message.content

    def respond(self, message, instruction):
        """
        Respond to a user query by processing input, querying the vector store,
        reranking results, and generating a final response.
        """
        logger.debug("Starting response generation.")

        # Step 1: Format the chat prompt
        prompt = self.textanslys.format_chat_prompt(message, instruction)
        logger.debug(f"Formatted prompt: {prompt}")

        questions = self.textanslys.keyword(prompt)
        logger.debug(f"Generated keywords from prompt: {questions}")

        # Step 2: Generate additional queries
        prompt_queries = self.generate_queries(prompt, 3)
        logger.debug(f"Generated queries: {prompt_queries}")

        all_results = []
        all_documents = []

        # Step 3: Query the VectorStoreManager for each query
        for query in prompt_queries:
            logger.debug(f"Querying vector store with: {query}")
            query_results = self.vector_store_manager.query_store(query)
            logger.debug(f"Results for query '{query}': {query_results}")
            all_results.extend(query_results)

        # Step 4: Deduplicate results by content
        unique_results = {result["text"]: result for result in all_results}.values()
        unique_documents = [result["text"] for result in unique_results]
        logger.debug(f"Unique documents: {unique_documents}")

        unique_code = [
            result.get("metadata", {}).get("code_content") for result in unique_results
        ]
        logger.debug(f"Unique code content: {unique_code}")

        # Step 5: Rerank documents based on relevance
        retrieved_documents = self.rerank(message, unique_documents)
        logger.debug(f"Reranked documents: {retrieved_documents}")

        # Step 6: Generate a response using RAG (Retrieve and Generate)
        response = self.rag(prompt, retrieved_documents)
        chunkrecall = self.list_to_markdown(retrieved_documents)
        logger.debug(f"RAG-generated response: {response}")
        logger.debug(f"Markdown chunk recall: {chunkrecall}")

        bot_message = str(response)
        logger.debug(f"Initial bot_message: {bot_message}")

        # Step 7: Perform NER and queryblock processing
        keyword = str(self.textanslys.nerquery(bot_message))
        keywords = str(self.textanslys.nerquery(str(prompt) + str(questions)))
        logger.debug(f"Extracted keywords: {keyword}, {keywords}")

        codez, mdz = self.textanslys.queryblock(keyword)
        codey, mdy = self.textanslys.queryblock(keywords)

        # Ensure all returned items are lists
        codez = codez if isinstance(codez, list) else [codez]
        mdz = mdz if isinstance(mdz, list) else [mdz]
        codey = codey if isinstance(codey, list) else [codey]
        mdy = mdy if isinstance(mdy, list) else [mdy]

        # Step 8: Merge and deduplicate results
        codex = list(dict.fromkeys(codez + codey))
        md = list(dict.fromkeys(mdz + mdy))
        unique_mdx = list(set([item for sublist in md for item in sublist]))
        uni_codex = list(dict.fromkeys(codex))
        uni_md = list(dict.fromkeys(unique_mdx))

        # Convert to Markdown format
        codex_md = self.textanslys.list_to_markdown(uni_codex)
        retrieved_documents = list(dict.fromkeys(retrieved_documents + uni_md))

        # Final rerank and response generation
        retrieved_documents = self.rerank(message, retrieved_documents[:6])
        logger.debug(f"Final retrieved documents after rerank: {retrieved_documents}")

        uni_code = self.rerank(
            message, list(dict.fromkeys(uni_codex + unique_code))[:6]
        )
        logger.debug(f"Final unique code after rerank: {uni_code}")

        unique_code_md = self.textanslys.list_to_markdown(unique_code)
        logger.debug(f"Unique code in Markdown: {unique_code_md}")

        # Generate final response using RAG_AR
        bot_message = self.rag_ar(prompt, uni_code, retrieved_documents, "test")
        logger.debug(f"Final bot_message after RAG_AR: {bot_message}")

        return message, bot_message, chunkrecall, questions, unique_code_md, codex_md



================================================
FILE: repo_agent/chat_with_repo/text_analysis_tool.py
================================================
from llama_index.core.llms.function_calling import FunctionCallingLLM
from llama_index.llms.openai import OpenAI

from repo_agent.chat_with_repo.json_handler import JsonFileProcessor


class TextAnalysisTool:
    def __init__(self, llm: FunctionCallingLLM, db_path):
        self.jsonsearch = JsonFileProcessor(db_path)
        self.llm = llm
        self.db_path = db_path

    def keyword(self, query):
        prompt = f"Please provide a list of Code keywords according to the following query, please output no more than 3 keywords, Input: {query}, Output:"
        response = self.llm.complete(prompt)
        return response

    def tree(self, query):
        prompt = f"Please analyze the following text and generate a tree structure based on its hierarchy:\n\n{query}"
        response = self.llm.complete(prompt)
        return response

    def format_chat_prompt(self, message, instruction):
        prompt = f"System:{instruction}\nUser: {message}\nAssistant:"
        return prompt

    def queryblock(self, message):
        search_result, md = self.jsonsearch.search_code_contents_by_name(
            self.db_path, message
        )
        return search_result, md

    def list_to_markdown(self, search_result):
        markdown_str = ""
        # 遍历列表，将每个元素转换为Markdown格式的项
        for index, content in enumerate(search_result, start=1):
            # 添加到Markdown字符串中，每个项后跟一个换行符
            markdown_str += f"{index}. {content}\n\n"

        return markdown_str

    def nerquery(self, message):
        instrcution = """
Extract the most relevant class or function base on the following instrcution:

The output must strictly be a pure function name or class name, without any additional characters.
For example:
Pure function names: calculateSum, processData
Pure class names: MyClass, DataProcessor
The output function name or class name should be only one.
        """
        query = f"{instrcution}\n\nThe input is shown as bellow:\n{message}\n\nAnd now directly give your Output:"
        response = self.llm.complete(query)
        # logger.debug(f"Input: {message}, Output: {response}")
        return response


if __name__ == "__main__":
    api_base = "https://api.openai.com/v1"
    api_key = "your_api_key"
    log_file = "your_logfile_path"
    llm = OpenAI(api_key=api_key, api_base=api_base)
    db_path = "your_database_path"
    test = TextAnalysisTool(llm, db_path)



================================================
FILE: repo_agent/chat_with_repo/vector_store_manager.py
================================================
import chromadb
from llama_index.core import (
    Document,
    StorageContext,
    VectorStoreIndex,
    get_response_synthesizer,
)
from llama_index.core.node_parser import (
    SemanticSplitterNodeParser,
    SentenceSplitter,
)
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore

from repo_agent.log import logger


class VectorStoreManager:
    def __init__(self, top_k, llm):
        """
        Initialize the VectorStoreManager.
        """
        self.query_engine = None  # Initialize as None
        self.chroma_db_path = "./chroma_db"  # Path to Chroma database
        self.collection_name = "test"  # Default collection name
        self.similarity_top_k = top_k
        self.llm = llm

    def create_vector_store(self, md_contents, meta_data, api_key, api_base):
        """
        Add markdown content and metadata to the index.
        """
        if not md_contents or not meta_data:
            logger.warning("No content or metadata provided. Skipping.")
            return

        # Ensure lengths match
        min_length = min(len(md_contents), len(meta_data))
        md_contents = md_contents[:min_length]
        meta_data = meta_data[:min_length]

        logger.debug(f"Number of markdown contents: {len(md_contents)}")
        logger.debug(f"Number of metadata entries: {len(meta_data)}")

        # Initialize Chroma client and collection
        db = chromadb.PersistentClient(path=self.chroma_db_path)
        chroma_collection = db.get_or_create_collection(self.collection_name)

        # Define embedding model
        embed_model = OpenAIEmbedding(
            model_name="text-embedding-3-large",
            api_key=api_key,
            api_base=api_base,
        )

        # Initialize semantic chunker (SimpleNodeParser)
        logger.debug("Initializing semantic chunker (SimpleNodeParser).")
        splitter = SemanticSplitterNodeParser(
            buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model
        )
        base_splitter = SentenceSplitter(chunk_size=1024)

        documents = [
            Document(text=content, extra_info=meta)
            for content, meta in zip(md_contents, meta_data)
        ]

        all_nodes = []
        for i, doc in enumerate(documents):
            logger.debug(
                f"Processing document {i+1}: Content length={len(doc.get_text())}"
            )

            try:
                # Try semantic splitting first
                nodes = splitter.get_nodes_from_documents([doc])
                logger.debug(f"Document {i+1} split into {len(nodes)} semantic chunks.")

            except Exception as e:
                # Fallback to baseline sentence splitting
                logger.warning(
                    f"Semantic splitting failed for document {i+1}, falling back to SentenceSplitter. Error: {e}"
                )
                nodes = base_splitter.get_nodes_from_documents([doc])
                logger.debug(f"Document {i+1} split into {len(nodes)} sentence chunks.")

            all_nodes.extend(nodes)

        if not all_nodes:
            logger.warning("No valid nodes to add to the index after chunking.")
            return

        logger.debug(f"Number of valid chunks: {len(all_nodes)}")

        # Set up ChromaVectorStore and load data
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        index = VectorStoreIndex(
            all_nodes, storage_context=storage_context, embed_model=embed_model
        )
        retriever = VectorIndexRetriever(
            index=index, similarity_top_k=self.similarity_top_k, embed_model=embed_model
        )

        response_synthesizer = get_response_synthesizer(llm=self.llm)

        # Set the query engine
        self.query_engine = RetrieverQueryEngine(
            retriever=retriever,
            response_synthesizer=response_synthesizer,
        )

        logger.info(f"Vector store created and loaded with {len(documents)} documents.")

    def query_store(self, query):
        """
        Query the vector store for relevant documents.
        """
        if not self.query_engine:
            logger.error(
                "Query engine is not initialized. Please create a vector store first."
            )
            return []

        # Query the vector store
        logger.debug(f"Querying vector store with: {query}")
        results = self.query_engine.query(query)

        # Extract relevant information from results
        return [{"text": results.response, "metadata": results.metadata}]



================================================
FILE: repo_agent/utils/gitignore_checker.py
================================================
import fnmatch
import os


class GitignoreChecker:
    def __init__(self, directory: str, gitignore_path: str):
        """
        Initialize the GitignoreChecker with a specific directory and the path to a .gitignore file.

        Args:
            directory (str): The directory to be checked.
            gitignore_path (str): The path to the .gitignore file.
        """
        self.directory = directory
        self.gitignore_path = gitignore_path
        self.folder_patterns, self.file_patterns = self._load_gitignore_patterns()

    def _load_gitignore_patterns(self) -> tuple:
        """
        Load and parse the .gitignore file, then split the patterns into folder and file patterns.

        If the specified .gitignore file is not found, fall back to the default path.

        Returns:
            tuple: A tuple containing two lists - one for folder patterns and one for file patterns.
        """
        try:
            with open(self.gitignore_path, "r", encoding="utf-8") as file:
                gitignore_content = file.read()
        except FileNotFoundError:
            # Fallback to the default .gitignore path if the specified file is not found
            default_path = os.path.join(
                os.path.dirname(__file__), "..", "..", ".gitignore"
            )
            with open(default_path, "r", encoding="utf-8") as file:
                gitignore_content = file.read()

        patterns = self._parse_gitignore(gitignore_content)
        return self._split_gitignore_patterns(patterns)

    @staticmethod
    def _parse_gitignore(gitignore_content: str) -> list:
        """
        Parse the .gitignore content and return patterns as a list.

        Args:
            gitignore_content (str): The content of the .gitignore file.

        Returns:
            list: A list of patterns extracted from the .gitignore content.
        """
        patterns = []
        for line in gitignore_content.splitlines():
            line = line.strip()
            if line and not line.startswith("#"):
                patterns.append(line)
        return patterns

    @staticmethod
    def _split_gitignore_patterns(gitignore_patterns: list) -> tuple:
        """
        Split the .gitignore patterns into folder patterns and file patterns.

        Args:
            gitignore_patterns (list): A list of patterns from the .gitignore file.

        Returns:
            tuple: Two lists, one for folder patterns and one for file patterns.
        """
        folder_patterns = []
        file_patterns = []
        for pattern in gitignore_patterns:
            if pattern.endswith("/"):
                folder_patterns.append(pattern.rstrip("/"))
            else:
                file_patterns.append(pattern)
        return folder_patterns, file_patterns

    @staticmethod
    def _is_ignored(path: str, patterns: list, is_dir: bool = False) -> bool:
        """
        Check if the given path matches any of the patterns.

        Args:
            path (str): The path to check.
            patterns (list): A list of patterns to check against.
            is_dir (bool): True if the path is a directory, False otherwise.

        Returns:
            bool: True if the path matches any pattern, False otherwise.
        """
        for pattern in patterns:
            if fnmatch.fnmatch(path, pattern):
                return True
            if is_dir and pattern.endswith("/") and fnmatch.fnmatch(path, pattern[:-1]):
                return True
        return False

    def check_files_and_folders(self) -> list:
        """
        Check all files and folders in the given directory against the split gitignore patterns.
        Return a list of files that are not ignored and have the '.py' extension.
        The returned file paths are relative to the self.directory.

        Returns:
            list: A list of paths to files that are not ignored and have the '.py' extension.
        """
        not_ignored_files = []
        for root, dirs, files in os.walk(self.directory):
            dirs[:] = [
                d
                for d in dirs
                if not self._is_ignored(d, self.folder_patterns, is_dir=True)
            ]

            for file in files:
                file_path = os.path.join(root, file)
                relative_path = os.path.relpath(file_path, self.directory)
                if not self._is_ignored(
                    file, self.file_patterns
                ) and file_path.endswith(".py"):
                    not_ignored_files.append(relative_path)

        return not_ignored_files


# Example usage:
# gitignore_checker = GitignoreChecker('path_to_directory', 'path_to_gitignore_file')
# not_ignored_files = gitignore_checker.check_files_and_folders()
# print(not_ignored_files)



================================================
FILE: repo_agent/utils/meta_info_utils.py
================================================
[Binary file]


================================================
FILE: tests/__init__.py
================================================
[Empty file]


================================================
FILE: tests/test_change_detector.py
================================================
[Binary file]


================================================
FILE: tests/test_json_handler.py
================================================
import unittest
from unittest.mock import mock_open, patch

from ..repo_agent.chat_with_repo.json_handler import (
    JsonFileProcessor,  # Adjust the import according to your project structure
)


class TestJsonFileProcessor(unittest.TestCase):

    def setUp(self):
        self.processor = JsonFileProcessor("test.json")

    @patch("builtins.open", new_callable=mock_open, read_data='{"files": [{"objects": [{"md_content": "content1"}]}]}')
    def test_read_json_file(self, mock_file):
        # Test read_json_file method
        data = self.processor.read_json_file()
        self.assertEqual(data, {"files": [{"objects": [{"md_content": "content1"}]}]})
        mock_file.assert_called_with("test.json", "r", encoding="utf-8")

    @patch.object(JsonFileProcessor, 'read_json_file')
    def test_extract_md_contents(self, mock_read_json):
        # Test extract_md_contents method
        mock_read_json.return_value = {"files": [{"objects": [{"md_content": "content1"}]}]}
        md_contents = self.processor.extract_md_contents()
        self.assertIn("content1", md_contents)

    @patch("builtins.open", new_callable=mock_open, read_data='{"name": "test", "files": [{"name": "file1"}]}')
    def test_search_in_json_nested(self, mock_file):
        # Test search_in_json_nested method
        result = self.processor.search_in_json_nested("test.json", "file1")
        self.assertEqual(result, {"name": "file1"})
        mock_file.assert_called_with("test.json", "r", encoding="utf-8")

    # Additional tests for error handling (FileNotFoundError, JSONDecodeError, etc.) can be added here

if __name__ == '__main__':
    unittest.main()



================================================
FILE: tests/test_structure_tree.py
================================================
import os
from collections import defaultdict


def build_path_tree(who_reference_me, reference_who, doc_item_path):
    def tree():
        return defaultdict(tree)
    path_tree = tree()

    for path_list in [who_reference_me, reference_who]:
        for path in path_list:
            parts = path.split(os.sep)
            node = path_tree
            for part in parts:
                node = node[part]

    # 处理 doc_item_path
    parts = doc_item_path.split(os.sep)
    parts[-1] = '✳️' + parts[-1]  # 在最后一个对象前面加上星号
    node = path_tree
    for part in parts:
        node = node[part]

    def tree_to_string(tree, indent=0):
        s = ''
        for key, value in sorted(tree.items()):
            s += '    ' * indent + key + '\n'
            if isinstance(value, dict):
                s += tree_to_string(value, indent + 1)
        return s

    return tree_to_string(path_tree)


if "__name__ == main":
    who_reference_me = [
        "repo_agent/file_handler.py/FileHandler/__init__",
        "repo_agent/runner.py/need_to_generate"
    ]
    reference_who = [
        "repo_agent/file_handler.py/FileHandler/__init__",
        "repo_agent/runner.py/need_to_generate",
    ]

    doc_item_path = 'tests/test_change_detector.py/TestChangeDetector'

    result = build_path_tree(who_reference_me,reference_who,doc_item_path)
    print(result)



================================================
FILE: .github/workflows/release.yml
================================================
name: Release

on:
  release:
    types: [published]

jobs:
  pypi-publish:
    name: upload release to PyPI
    runs-on: ubuntu-latest
    permissions:
      # This permission is needed for private repositories.
      contents: read
      # IMPORTANT: this permission is mandatory for trusted publishing
      id-token: write
    steps:
      - uses: actions/checkout@v3

      - uses: pdm-project/setup-pdm@v3

      - name: Publish package distributions to PyPI
        run: pdm publish


