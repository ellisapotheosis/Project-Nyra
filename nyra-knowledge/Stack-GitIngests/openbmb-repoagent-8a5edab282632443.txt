Directory structure:
â””â”€â”€ openbmb-repoagent/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ pyproject.toml
    â”œâ”€â”€ README_CN.md
    â”œâ”€â”€ display/
    â”‚   â”œâ”€â”€ Makefile
    â”‚   â”œâ”€â”€ README_DISPLAY.md
    â”‚   â”œâ”€â”€ book_template/
    â”‚   â”‚   â””â”€â”€ book.json
    â”‚   â”œâ”€â”€ book_tools/
    â”‚   â”‚   â”œâ”€â”€ generate_repoagent_books.py
    â”‚   â”‚   â””â”€â”€ generate_summary_from_book.py
    â”‚   â”œâ”€â”€ books/
    â”‚   â”‚   â””â”€â”€ BOOKS.md
    â”‚   â””â”€â”€ scripts/
    â”‚       â””â”€â”€ install_nodejs.sh
    â”œâ”€â”€ markdown_docs/
    â”‚   â”œâ”€â”€ display/
    â”‚   â”‚   â””â”€â”€ book_tools/
    â”‚   â”‚       â”œâ”€â”€ generate_repoagent_books.md
    â”‚   â”‚       â””â”€â”€ generate_summary_from_book.md
    â”‚   â”œâ”€â”€ repo_agent/
    â”‚   â”‚   â”œâ”€â”€ change_detector.md
    â”‚   â”‚   â”œâ”€â”€ chat_engine.md
    â”‚   â”‚   â”œâ”€â”€ file_handler.md
    â”‚   â”‚   â”œâ”€â”€ log.md
    â”‚   â”‚   â”œâ”€â”€ main.md
    â”‚   â”‚   â”œâ”€â”€ multi_task_dispatch.md
    â”‚   â”‚   â”œâ”€â”€ project_manager.md
    â”‚   â”‚   â”œâ”€â”€ runner.md
    â”‚   â”‚   â”œâ”€â”€ settings.md
    â”‚   â”‚   â””â”€â”€ utils/
    â”‚   â”‚       â”œâ”€â”€ gitignore_checker.md
    â”‚   â”‚       â””â”€â”€ meta_info_utils.md
    â”‚   â””â”€â”€ tests/
    â”‚       â”œâ”€â”€ test_change_detector.md
    â”‚       â”œâ”€â”€ test_json_handler.md
    â”‚       â””â”€â”€ test_structure_tree.md
    â”œâ”€â”€ repo_agent/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ __main__.py
    â”‚   â”œâ”€â”€ change_detector.py
    â”‚   â”œâ”€â”€ chat_engine.py
    â”‚   â”œâ”€â”€ doc_meta_info.py
    â”‚   â”œâ”€â”€ file_handler.py
    â”‚   â”œâ”€â”€ log.py
    â”‚   â”œâ”€â”€ main.py
    â”‚   â”œâ”€â”€ multi_task_dispatch.py
    â”‚   â”œâ”€â”€ project_manager.py
    â”‚   â”œâ”€â”€ prompt.py
    â”‚   â”œâ”€â”€ runner.py
    â”‚   â”œâ”€â”€ settings.py
    â”‚   â”œâ”€â”€ chat_with_repo/
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ __main__.py
    â”‚   â”‚   â”œâ”€â”€ gradio_interface.py
    â”‚   â”‚   â”œâ”€â”€ json_handler.py
    â”‚   â”‚   â”œâ”€â”€ main.py
    â”‚   â”‚   â”œâ”€â”€ prompt.py
    â”‚   â”‚   â”œâ”€â”€ rag.py
    â”‚   â”‚   â”œâ”€â”€ text_analysis_tool.py
    â”‚   â”‚   â””â”€â”€ vector_store_manager.py
    â”‚   â””â”€â”€ utils/
    â”‚       â”œâ”€â”€ gitignore_checker.py
    â”‚       â””â”€â”€ meta_info_utils.py
    â”œâ”€â”€ tests/
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ test_change_detector.py
    â”‚   â”œâ”€â”€ test_json_handler.py
    â”‚   â””â”€â”€ test_structure_tree.py
    â””â”€â”€ .github/
        â””â”€â”€ workflows/
            â””â”€â”€ release.yml

================================================
FILE: README.md
================================================
<h1 align="center">
  <img src="https://github.com/OpenBMB/RepoAgent/assets/138990495/06bc2449-c82d-4b9e-8c83-27640e541451" width="50" alt="RepoAgent logo"/> <em>RepoAgent: An LLM-Powered Framework for Repository-level Code Documentation Generation.</em>
</h1>

<p align="center">
  <img src="https://img.shields.io/pypi/dm/repoagent" alt="PyPI - Downloads"/>
  <a href="https://pypi.org/project/repoagent/">
    <img src="https://img.shields.io/pypi/v/repoagent" alt="PyPI - Version"/>
  </a>
  <a href="Pypi">
    <img src="https://img.shields.io/pypi/pyversions/repoagent" alt="PyPI - Python Version"/>
  </a>
  <img alt="GitHub License" src="https://img.shields.io/github/license/LOGIC-10/RepoAgent">
  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LOGIC-10/RepoAgent?style=social">
  <img alt="GitHub issues" src="https://img.shields.io/github/issues/LOGIC-10/RepoAgent">
  <a href="https://arxiv.org/abs/2402.16667v1">
    <img src="https://img.shields.io/badge/cs.CL-2402.16667-b31b1b?logo=arxiv&logoColor=red" alt="arXiv"/>
  </a>
</p>

<p align="center">
  <img src="https://raw.githubusercontent.com/OpenBMB/RepoAgent/main/assets/images/RepoAgent.png" alt="RepoAgent"/>
</p>

<p align="center">
  <a href="https://github.com/LOGIC-10/RepoAgent/blob/main/README.md">English readme</a>
   â€¢ 
  <a href="https://github.com/LOGIC-10/RepoAgent/blob/main/README_CN.md">ç®€ä½“ä¸­æ–‡ readme</a>
</p>

## :tv: Demo

[![Watch the video](https://img.youtube.com/vi/YPPJBVOP71M/hqdefault.jpg)](https://youtu.be/YPPJBVOP71M)

## ğŸ‘¾ Background

In the realm of computer programming, the significance of comprehensive project documentation, including detailed explanations for each Python file, cannot be overstated. Such documentation serves as the cornerstone for understanding, maintaining, and enhancing the codebase. It provides essential context and rationale for the code, making it easier for current and future developers to comprehend the purpose, functionality, and structure of the software. It not only facilitates current and future developers in grasping the project's purpose and structure but also ensures that the project remains accessible and modifiable over time, significantly easing the learning curve for new team members.

Traditionally, creating and maintaining software documentation demanded significant human effort and expertise, a challenge for small teams without dedicated personnel. The introduction of Large Language Models (LLMs) like GPT has transformed this, enabling AI to handle much of the documentation process. This shift allows human developers to focus on verification and fine-tuning, greatly reducing the manual burden of documentation.

**ğŸ† Our goal is to create an intelligent document assistant that helps people read and understand repositories and generate documents, ultimately helping people improve efficiency and save time.**

## âœ¨ Features

- **ğŸ¤– Automatically detects changes in Git repositories, tracking additions, deletions, and modifications of files.**
- **ğŸ“ Independently analyzes the code structure through AST, generating documents for individual objects.**
- **ğŸ” Accurate identification of inter-object bidirectional invocation relationships, enriching the global perspective of document content.**
- **ğŸ“š Seamlessly replaces Markdown content based on changes, maintaining consistency in documentation.**
- **ğŸ•™ Executes multi-threaded concurrent operations, enhancing the efficiency of document generation.**
- **ğŸ‘­ Offer a sustainable, automated documentation update method for team collaboration.**
- **ğŸ˜ Display Code Documentation in an amazing way. (with document book per project powered by Gitbook)**


## ğŸš€ Getting Started

### Installation Method

#### Using GitHub Actions

This repository supports GitHub Actions for automating workflows such as building, testing, and deploying. For detailed instructions on setting up and using GitHub Actions with this repository, please refer to the [actions/run-repoagent](https://github.com/marketplace/actions/run-repoagent).

#### Using pip (Recommended for Users)

Install the `repoagent` package directly using pip:

```bash
pip install repoagent
```

#### Development Setup Using PDM

If you're looking to contribute or set up a development environment:

- **Install PDM**: If you haven't already, [install PDM](https://pdm-project.org/latest/#installation).
- **Use CodeSpace, or Clone the Repository**:

    - **Use CodeSpace**
    The easiest way to get RepoAgent enviornment. Click below to use the GitHub Codespace, then go to the next step.
  
    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/LOGIC-10/RepoAgent?quickstart=1)
  
    - **Clone the Repository**
  
    ```bash
    git clone https://github.com/LOGIC-10/RepoAgent.git
    cd RepoAgent
    ```

- **Setup with PDM**

    - Initialize the Python virtual environment. Make sure to run the below cmd in `/RepoAgent` directory:
    
      ```bash
      pdm venv create --name repoagent
      ```
    
    - [Activate virtual environment](https://pdm-project.org/latest/usage/venv/#activate-a-virtualenv)
    
    - Install dependencies using PDM
    
      ```bash
       pdm install
      ```

### Configuring RepoAgent

Before configuring specific parameters for RepoAgent, please ensure that the OpenAI API is configured as an environment variable in the command line:

```sh
export OPENAI_API_KEY=YOUR_API_KEY # on Linux/Mac
set OPENAI_API_KEY=YOUR_API_KEY # on Windows
$Env:OPENAI_API_KEY = "YOUR_API_KEY" # on Windows (PowerShell)
```

## Run RepoAgent

Enter the root directory of RepoAgent and try the following command in the terminal:
```sh
repoagent run #this command will generate doc, or update docs(pre-commit-hook will automatically call this)
repoagent run --print-hierarchy # Print how repo-agent parse the target repo
```

The run command supports the following optional flags (if set, will override config defaults):

- `-m`, `--model` TEXT: Specifies the model to use for completion. Default: `gpt-3.5-turbo`
- `-t`, `--temperature` FLOAT: Sets the generation temperature for the model. Lower values make the model more deterministic. Default: `0.2`
- `-r`, `--request-timeout` INTEGER: Defines the timeout in seconds for the API request. Default: `60`
- `-b`, `--base-url` TEXT: The base URL for the API calls. Default: `https://api.openai.com/v1`
- `-tp`, `--target-repo-path` PATH: The file system path to the target repository. Used as the root for documentation generation. Default: `path/to/your/target/repository`
- `-hp`, `--hierarchy-path` TEXT: The name or path for the project hierarchy file, used to organize documentation structure. Default: `.project_doc_record`
- `-mdp`, `--markdown-docs-path` TEXT: The folder path where Markdown documentation will be stored or generated. Default: `markdown_docs`
- `-i`, `--ignore-list` TEXT: A list of files or directories to ignore during documentation generation, separated by commas.
- `-l`, `--language` TEXT: The ISO 639 code or language name for the documentation. Default: `Chinese`
- `-ll`, `--log-level` [DEBUG|INFO|WARNING|ERROR|CRITICAL]: Sets the logging level for the application. Default: `INFO`

You can also try the following feature

```sh
repoagent clean # Remove repoagent-related cache
repoagent diff # Check what docs will be updated/generated based on current code change
```

If it's your first time generating documentation for the target repository, RepoAgent will automatically create a JSON file maintaining the global structure information and a folder named Markdown_Docs in the root directory of the target repository for storing documents.

Once you have initially generated the global documentation for the target repository, or if the project you cloned already contains global documentation information, you can then seamlessly and automatically maintain internal project documentation with your team by configuring the **pre-commit hook** in the target repository! 

### Use `pre-commit` 

RepoAgent currently supports generating documentation for projects, which requires some configuration in the target repository.

First, ensure that the target repository is a git repository and has been initialized.

```sh
git init
```
Install pre-commit in the target repository to detect changes in the git repository.

```sh
pip install pre-commit
```
Create a file named `.pre-commit-config.yaml` in the root directory of the target repository. An example is as follows:

```yml
repos:
  - repo: local
    hooks:
    - id: repo-agent
      name: RepoAgent
      entry: repoagent
      language: system
      pass_filenames: false # prevent from passing filenames to the hook
      # You can specify the file types that trigger the hook, but currently only python is supported.
      types: [python]
```

For specific configuration methods of hooks, please refer to [pre-commit](https://pre-commit.com/#plugins).
After configuring the yaml file, execute the following command to install the hook.

```sh
pre-commit install
```

In this way, each git commit will trigger the RepoAgent's hook, automatically detecting changes in the target repository and generating corresponding documents.
Next, you can make some modifications to the target repository, such as adding a new file to the target repository, or modifying an existing file.
You just need to follow the normal git workflow: git add, git commit -m "your commit message", git push
The RepoAgent hook will automatically trigger at git commit, detect the files you added in the previous step, and generate corresponding documents.

After execution, RepoAgent will automatically modify the staged files in the target repository and formally submit the commit. After the execution is completed, the green "Passed" will be displayed, as shown in the figure below:
![Execution Result](https://raw.githubusercontent.com/OpenBMB/RepoAgent/main/assets/images/ExecutionResult.png)

The generated document will be stored in the specified folder in the root directory of the target warehouse. The rendering of the generated document is as shown below:
![Documentation](https://raw.githubusercontent.com/OpenBMB/RepoAgent/main/assets/images/Doc_example.png)
![Documentation](https://raw.githubusercontent.com/OpenBMB/RepoAgent/main/assets/images/8_documents.png)

We utilized the default model **gpt-3.5-turbo** to generate documentation for the [**XAgent**](https://github.com/OpenBMB/XAgent) project, which comprises approximately **270,000 lines** of code. You can view the results of this generation in the Markdown_Docs directory of the XAgent project on GitHub. For enhanced documentation quality, we suggest considering more advanced models like **gpt-4-1106** or **gpt-4-0125-preview**.

**In the end, you can flexibly adjust the output format, template, and other aspects of the document by customizing the prompt. We are excited about your exploration of a more scientific approach to Automated Technical Writing and your contributions to the community.** 

### Exploring chat with repo

We conceptualize **Chat With Repo** as a unified gateway for these downstream applications, acting as a connector that links RepoAgent to human users and other AI agents. Our future research will focus on adapting the interface to various downstream applications and customizing it to meet their unique characteristics and implementation requirements.

Here we demonstrate a preliminary prototype of one of our downstream tasks: Automatic Q&A for Issues and Code Explanation. You can start the server by running the following code.

```sh
pip install repoagent[chat-with-repo]
repoagent chat-with-repo
```

## âœ… Future Work

- [ ] Generate README.md automatically combining with the global documentation
- [ ] **Multi-programming-language support** Support more programming languages like Java, C or C++, etc.
- [x] Local model support like Llama, chatGLM, Qwen, GLM4, etc.

## ğŸ¥° Featured Cases

Here are featured cases that have adopted RepoAgent.

- [MiniCPM](https://github.com/OpenBMB/MiniCPM): An edge-side LLM of 2B size, comparable to 7B model.
- [ChatDev](https://github.com/OpenBMB/ChatDev): Collaborative AI agents for software development.
- [XAgent](https://github.com/OpenBMB/XAgent): An Autonomous LLM Agent for Complex Task Solving.
- [EasyRL4Rec](https://github.com/chongminggao/EasyRL4Rec): A user-friendly RL library for recommender systems.

## ğŸ“Š Citation

```bibtex
@misc{luo2024repoagent,
      title={RepoAgent: An LLM-Powered Open-Source Framework for Repository-level Code Documentation Generation}, 
      author={Qinyu Luo and Yining Ye and Shihao Liang and Zhong Zhang and Yujia Qin and Yaxi Lu and Yesai Wu and Xin Cong and Yankai Lin and Yingli Zhang and Xiaoyin Che and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2402.16667},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```



================================================
FILE: LICENSE
================================================
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.



================================================
FILE: pyproject.toml
================================================
[project]
authors = [
    {name = "Qinyu Luo", email = "qinyuluo123@gmail.com"},
]
maintainers = [
    {name = "Edwards Arno", email = "Edwards.Arno@outlook.com"},
]
license = {text = "Apache-2.0"}
requires-python = ">=3.11,<4.0"
dependencies = [
    "loguru>=0.7.2",
    "jedi>=0.19.1",
    "GitPython>=3.1.41",
    "prettytable>=3.9.0",
    "python-iso639>=2024.2.7",
    "pydantic-settings>=2.2.1",
    "click>=8.1.7",
    "python-iso639>=2024.10.22",
    "colorama>=0.4.6",
    "llama-index-llms-openai-like>=0.3.3",
]
name = "repoagent"
version = "0.2.0"
description = "An LLM-Powered Framework for Repository-level Code Documentation Generation."
readme = "README.md"
classifiers = [
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Scientific/Engineering :: Artificial Intelligence"
]

[project.urls]
repository = "https://github.com/LOGIC-10/RepoAgent"

[project.scripts]
repoagent = "repo_agent.main:cli"

[project.optional-dependencies]
chat_with_repo = [
    "markdown>=3.7",
    "llama-index-embeddings-openai>=0.2.5",
    "llama-index-vector-stores-chroma>=0.3.0",
    "gradio>=5.6.0",
]

[build-system]
requires = ["pdm-backend"]
build-backend = "pdm.backend"

[tool.pyright]
reportCallIssue="none"

[tool.ruff]
# General ruff settings can stay here.

[tool.ruff.lint]
select = ["I001"] 

[tool.pdm]
[tool.pdm.dev-dependencies]
test = [
    "pytest<8.0.0,>=7.4.4",
    "pytest-mock<4.0.0,>=3.12.0",
]
lint = [
    "ruff>=0.7.4",
]

[tool.pdm.build]
includes = [
    "repo_agent",
]


================================================
FILE: README_CN.md
================================================
<h1 align="center"><em>RepoAgentï¼šä¸€ä¸ªç”¨äºä»£ç åº“çº§åˆ«ä»£ç æ–‡æ¡£ç”Ÿæˆçš„LLMé©±åŠ¨æ¡†æ¶</em></h1>

<p align="center">
  <img src="https://img.shields.io/pypi/dm/repoagent" alt="PyPI - ä¸‹è½½é‡"/>
  <a href="https://pypi.org/project/repoagent/">
    <img src="https://img.shields.io/pypi/v/repoagent" alt="PyPI - ç‰ˆæœ¬"/>
  </a>
  <a href="Pypi">
    <img src="https://img.shields.io/pypi/pyversions/repoagent" alt="PyPI - Pythonç‰ˆæœ¬"/>
  </a>
  <img alt="GitHubæˆæƒè®¸å¯" src="https://img.shields.io/github/license/LOGIC-10/RepoAgent">
  <img alt="GitHubä»“åº“æ˜Ÿæ ‡" src="https://img.shields.io/github/stars/LOGIC-10/RepoAgent?style=social">
  <img alt="GitHubé—®é¢˜" src="https://img.shields.io/github/issues/LOGIC-10/RepoAgent">
  <a href="https://arxiv.org/abs/2402.16667v1">
    <img src="https://img.shields.io/badge/cs.CL-2402.16667-b31b1b?logo=arxiv&logoColor=red" alt="arXiv"/>
  </a>
</p>

<p align="center">
  <img src="https://raw.githubusercontent.com/OpenBMB/RepoAgent/main/assets/images/RepoAgent.png" alt="RepoAgent"/>
</p>

<p align="center">
  <a href="https://github.com/LOGIC-10/RepoAgent/blob/main/README.md">English README</a>
   â€¢ 
  <a href="https://github.com/LOGIC-10/RepoAgent/blob/main/README_CN.md">ç®€ä½“ä¸­æ–‡è¯´æ˜</a>
</p>

## ğŸ‘¾ èƒŒæ™¯

åœ¨è®¡ç®—æœºç¼–ç¨‹é¢†åŸŸï¼Œå…¨é¢çš„é¡¹ç›®æ–‡æ¡£çš„é‡è¦æ€§ï¼ŒåŒ…æ‹¬æ¯ä¸ªPythonæ–‡ä»¶çš„è¯¦ç»†è§£é‡Šï¼Œä¸è¨€è€Œå–»ã€‚è¿™æ ·çš„æ–‡æ¡£æ˜¯ç†è§£ã€ç»´æŠ¤å’Œå¢å¼ºä»£ç åº“çš„åŸºçŸ³ã€‚å®ƒæä¾›äº†ä»£ç çš„å¿…è¦ä¸Šä¸‹æ–‡å’Œç†ç”±ï¼Œä½¿å½“å‰å’Œæœªæ¥çš„å¼€å‘è€…æ›´å®¹æ˜“ç†è§£è½¯ä»¶çš„ç›®çš„ã€åŠŸèƒ½å’Œç»“æ„ã€‚å®ƒä¸ä»…ä¾¿äºå½“å‰å’Œæœªæ¥çš„å¼€å‘è€…ç†è§£é¡¹ç›®çš„ç›®çš„å’Œç»“æ„ï¼Œè¿˜ç¡®ä¿äº†é¡¹ç›®éšæ—¶é—´çš„æ¨ç§»ä¿æŒå¯è®¿é—®å’Œå¯ä¿®æ”¹ï¼Œå¤§å¤§ç®€åŒ–äº†æ–°å›¢é˜Ÿæˆå‘˜çš„å­¦ä¹ æ›²çº¿ã€‚

ä¼ ç»Ÿä¸Šï¼Œåˆ›å»ºå’Œç»´æŠ¤è½¯ä»¶æ–‡æ¡£éœ€è¦å¤§é‡çš„äººåŠ›å’Œä¸“ä¸šçŸ¥è¯†ï¼Œè¿™å¯¹æ²¡æœ‰ä¸“é—¨äººå‘˜çš„å°å›¢é˜Ÿæ¥è¯´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚åƒGPTè¿™æ ·çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¼•å…¥æ”¹å˜äº†è¿™ä¸€ç‚¹ï¼Œä½¿å¾—AIå¯ä»¥å¤„ç†å¤§éƒ¨åˆ†æ–‡æ¡£åŒ–è¿‡ç¨‹ã€‚è¿™ç§è½¬å˜å…è®¸äººç±»å¼€å‘è€…ä¸“æ³¨äºéªŒè¯å’Œå¾®è°ƒï¼Œæå¤§åœ°å‡å°‘äº†æ–‡æ¡£åŒ–çš„æ‰‹åŠ¨è´Ÿæ‹…ã€‚

**ğŸ† æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åˆ›å»ºä¸€ä¸ªæ™ºèƒ½æ–‡æ¡£åŠ©æ‰‹ï¼Œå¸®åŠ©äººä»¬é˜…è¯»å’Œç†è§£ä»“åº“å¹¶ç”Ÿæˆæ–‡æ¡£ï¼Œæœ€ç»ˆå¸®åŠ©äººä»¬æé«˜æ•ˆç‡å’ŒèŠ‚çœæ—¶é—´ã€‚**

## âœ¨ ç‰¹æ€§

- **ğŸ¤– è‡ªåŠ¨æ£€æµ‹Gitä»“åº“ä¸­çš„å˜åŒ–ï¼Œè·Ÿè¸ªæ–‡ä»¶çš„å¢åŠ ã€åˆ é™¤å’Œä¿®æ”¹ã€‚**
- **ğŸ“ é€šè¿‡ASTç‹¬ç«‹åˆ†æä»£ç ç»“æ„ï¼Œä¸ºå„ä¸ªå¯¹è±¡ç”Ÿæˆæ–‡æ¡£ã€‚**
- **ğŸ” å‡†ç¡®è¯†åˆ«å¯¹è±¡é—´çš„åŒå‘è°ƒç”¨å…³ç³»ï¼Œä¸°å¯Œæ–‡æ¡£å†…å®¹çš„å…¨å±€è§†è§’ã€‚**
- **ğŸ“š æ ¹æ®å˜åŒ–æ— ç¼æ›¿æ¢Markdownå†…å®¹ï¼Œä¿æŒæ–‡æ¡£ä¸€è‡´æ€§ã€‚**
- **ğŸ•™ æ‰§è¡Œå¤šçº¿ç¨‹å¹¶å‘æ“ä½œï¼Œæé«˜æ–‡æ¡£ç”Ÿæˆæ•ˆç‡ã€‚**
- **ğŸ‘­ ä¸ºå›¢é˜Ÿåä½œæä¾›å¯æŒç»­çš„è‡ªåŠ¨åŒ–æ–‡æ¡£æ›´æ–°æ–¹æ³•ã€‚**
- **ğŸ˜ ä»¥æƒŠäººçš„æ–¹å¼å±•ç¤ºä»£ç æ–‡æ¡£ï¼ˆæ¯ä¸ªé¡¹ç›®éƒ½æœ‰ç”±Gitbookæä¾›æ”¯æŒçš„æ–‡æ¡£ä¹¦ï¼‰ã€‚**

## ğŸš€ å¼€å§‹ä½¿ç”¨

### å®‰è£…æ–¹æ³•

#### ä½¿ç”¨pipï¼ˆæ™®é€šç”¨æˆ·é¦–é€‰ï¼‰

ç›´æ¥ä½¿ç”¨pipå®‰è£…`repoagent`åŒ…ï¼š

```bash
pip install repoagent
```

#### ä½¿ç”¨PDMè¿›è¡Œå¼€å‘ç¯å¢ƒè®¾ç½®

å¦‚æœæ‚¨æƒ³è¦è´¡çŒ®æˆ–è€…è®¾ç½®ä¸€ä¸ªå¼€å‘ç¯å¢ƒï¼š

- **å®‰è£…PDM**ï¼šå¦‚æœæ‚¨è¿˜æ²¡æœ‰å®‰è£…ï¼Œè¯·[å®‰è£…PDM](https://pdm-project.org/latest/#installation)ã€‚
- **ä½¿ç”¨CodeSpaceæˆ–å…‹éš†ä»“åº“**ï¼š

    - **ä½¿ç”¨CodeSpace**
    è·å–RepoAgentç¯å¢ƒçš„æœ€ç®€å•æ–¹å¼ã€‚ç‚¹å‡»ä¸‹é¢é“¾æ¥ä½¿ç”¨GitHub Codespaceï¼Œç„¶åè¿›è¡Œä¸‹ä¸€æ­¥ã€‚
  
    [![åœ¨GitHub Codespacesä¸­æ‰“å¼€](https://github.com/codespaces/badge.svg)](https://codespaces.new/LOGIC-10/RepoAgent?quickstart=1)
  
    - **å…‹éš†ä»“åº“**
  
    ```bash
    git clone https://github.com/LOGIC-10/RepoAgent.git
    cd RepoAgent
    ```

- **ä½¿ç”¨PDMè®¾ç½®**

    - åˆå§‹åŒ–Pythonè™šæ‹Ÿç¯å¢ƒã€‚ç¡®ä¿åœ¨`/RepoAgent`ç›®å½•ä¸‹è¿è¡Œä¸‹é¢çš„å‘½ä»¤ï¼š
    
      ```bash
      pdm venv create --name repoagent
      ```
    
    - [æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ](https://pdm-project.org/latest/usage/venv/#activate-a-virtualenv)
    
    - ä½¿ç”¨PDMå®‰è£…ä¾èµ–
    
      ```bash
       pdm install
      ```

### é…ç½®RepoAgent

åœ¨é…ç½®RepoAgentå…·ä½“å‚æ•°ä¹‹å‰ï¼Œè¯·å…ˆç¡®ä¿å·²ç»åœ¨å‘½ä»¤è¡Œé…ç½® OpenAI API ä½œä¸ºç¯å¢ƒå˜é‡ï¼š

```sh
export OPENAI_API_KEY=YOUR_API_KEY # on Linux/Mac

set OPENAI_API_KEY=YOUR_API_KEY # on Windows
$Env:OPENAI_API_KEY = "YOUR_API_KEY" # on Windows (PowerShell)
```

## è¿è¡ŒRepoAgent

è¿›å…¥RepoAgentæ ¹ç›®å½•å¹¶åœ¨ç»ˆç«¯å°è¯•ä»¥ä¸‹å‘½ä»¤ï¼š
```sh
repoagent run # è¿™æ¡å‘½ä»¤ä¼šç”Ÿæˆæ–‡æ¡£æˆ–è‡ªåŠ¨æ›´æ–°æ–‡æ¡£ (pre-commit-hook ä¼šè‡ªåŠ¨è°ƒç”¨å®ƒ)
repoagent --print-hierarchy # æ­¤å‘½ä»¤å°†æ‰“å°repoagentè§£æå‡ºçš„ç›®æ ‡ä»“åº“
```

run å‘½ä»¤æ”¯æŒä»¥ä¸‹å¯é€‰æ ‡å¿—ï¼ˆå¦‚æœè®¾ç½®ï¼Œå°†è¦†ç›–é…ç½®é»˜è®¤å€¼ï¼‰ï¼š

- `-m`, `--model` TEXTï¼šæŒ‡å®šç”¨äºå®Œæˆçš„æ¨¡å‹ã€‚é»˜è®¤å€¼ï¼š`gpt-3.5-turbo`
- `-t`, `--temperature` FLOATï¼šè®¾ç½®æ¨¡å‹çš„ç”Ÿæˆæ¸©åº¦ã€‚è¾ƒä½çš„å€¼ä½¿æ¨¡å‹æ›´ç¡®å®šæ€§ã€‚é»˜è®¤å€¼ï¼š`0.2`
- `-r`, `--request-timeout` INTEGERï¼šå®šä¹‰ API è¯·æ±‚çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ã€‚é»˜è®¤å€¼ï¼š`60`
- `-b`, `--base-url` TEXTï¼šAPI è°ƒç”¨çš„åŸºç¡€ URLã€‚é»˜è®¤å€¼ï¼š`https://api.openai.com/v1`
- `-tp`, `--target-repo-path` PATHï¼šç›®æ ‡ä»“åº“çš„æ–‡ä»¶ç³»ç»Ÿè·¯å¾„ã€‚ç”¨ä½œæ–‡æ¡£ç”Ÿæˆçš„æ ¹è·¯å¾„ã€‚é»˜è®¤å€¼ï¼š`path/to/your/target/repository`
- `-hp`, `--hierarchy-path` TEXTï¼šé¡¹ç›®å±‚çº§æ–‡ä»¶çš„åç§°æˆ–è·¯å¾„ï¼Œç”¨äºç»„ç»‡æ–‡æ¡£ç»“æ„ã€‚é»˜è®¤å€¼ï¼š`.project_doc_record`
- `-mdp`, `--markdown-docs-path` TEXTï¼šMarkdown æ–‡æ¡£å°†è¢«å­˜å‚¨æˆ–ç”Ÿæˆçš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚é»˜è®¤å€¼ï¼š`markdown_docs`
- `-i`, `--ignore-list` TEXTï¼šåœ¨æ–‡æ¡£ç”Ÿæˆè¿‡ç¨‹ä¸­è¦å¿½ç•¥çš„æ–‡ä»¶æˆ–ç›®å½•åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”ã€‚
- `-l`, `--language` TEXTï¼šæ–‡æ¡£çš„ ISO 639 ä»£ç æˆ–è¯­è¨€åç§°ã€‚é»˜è®¤å€¼ï¼š`Chinese`
- `-ll`, `--log-level` [DEBUG|INFO|WARNING|ERROR|CRITICAL]ï¼šè®¾ç½®åº”ç”¨ç¨‹åºçš„æ—¥å¿—çº§åˆ«ã€‚é»˜è®¤å€¼ï¼š`INFO`

ä½ ä¹Ÿå¯ä»¥å°è¯•ä»¥ä¸‹åŠŸèƒ½

```sh
repoagent clean # æ­¤å‘½ä»¤å°†åˆ é™¤ä¸repoagentç›¸å…³çš„ç¼“å­˜
repoagent diff # æ­¤å‘½ä»¤å°†æ£€æŸ¥åŸºäºå½“å‰ä»£ç æ›´æ”¹å°†æ›´æ–°/ç”Ÿæˆå“ªäº›æ–‡æ¡£
```

å¦‚æœæ‚¨æ˜¯ç¬¬ä¸€æ¬¡å¯¹ç›®æ ‡ä»“åº“ç”Ÿæˆæ–‡æ¡£ï¼Œæ­¤æ—¶RepoAgentä¼šè‡ªåŠ¨ç”Ÿæˆä¸€ä¸ªç»´æŠ¤å…¨å±€ç»“æ„ä¿¡æ¯çš„jsonæ–‡ä»¶ï¼Œå¹¶åœ¨ç›®æ ‡ä»“åº“æ ¹ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹ç”¨äºå­˜æ”¾æ–‡æ¡£ã€‚
å…¨å±€ç»“æ„ä¿¡æ¯jsonæ–‡ä»¶å’Œæ–‡æ¡£æ–‡ä»¶å¤¹çš„è·¯å¾„éƒ½å¯ä»¥åœ¨`config.yml`ä¸­è¿›è¡Œé…ç½®ã€‚

å½“æ‚¨é¦–æ¬¡å®Œæˆå¯¹ç›®æ ‡ä»“åº“ç”Ÿæˆå…¨å±€æ–‡æ¡£åï¼Œæˆ–æ‚¨cloneä¸‹æ¥çš„é¡¹ç›®å·²ç»åŒ…å«äº†å…¨å±€æ–‡æ¡£ä¿¡æ¯åï¼Œå°±å¯ä»¥é€šè¿‡**pre-commit**é…ç½®ç›®æ ‡ä»“åº“**hook**å’Œå›¢é˜Ÿä¸€èµ·æ— ç¼è‡ªåŠ¨ç»´æŠ¤ä¸€ä¸ªé¡¹ç›®å†…éƒ¨æ–‡æ¡£äº†ï¼

### é…ç½®ç›®æ ‡ä»“åº“

RepoAgentç›®å‰æ”¯æŒå¯¹é¡¹ç›®çš„æ–‡æ¡£ç”Ÿæˆå’Œè‡ªåŠ¨ç»´æŠ¤ï¼Œå› æ­¤éœ€è¦å¯¹ç›®æ ‡ä»“åº“è¿›è¡Œä¸€å®šçš„é…ç½®ã€‚

é¦–å…ˆï¼Œç¡®ä¿ç›®æ ‡ä»“åº“æ˜¯ä¸€ä¸ªgitä»“åº“ï¼Œä¸”å·²ç»åˆå§‹åŒ–ã€‚
```
git init
```
åœ¨ç›®æ ‡ä»“åº“ä¸­å®‰è£…pre-commitï¼Œç”¨äºæ£€æµ‹gitä»“åº“ä¸­çš„å˜æ›´ã€‚
```
pip install pre-commit
```
åœ¨ç›®æ ‡ä»“åº“æ ¹ç›®å½•ä¸‹ï¼Œåˆ›å»ºä¸€ä¸ªåä¸º`.pre-commit-config.yaml`çš„æ–‡ä»¶ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š
```
repos:
  - repo: local
    hooks:
    - id: repo-agent
      name: RepoAgent
      entry: repoagent
      language: system
      pass_filenames: false # é˜»æ­¢pre commitä¼ å…¥æ–‡ä»¶åä½œä¸ºå‚æ•°
      # å¯ä»¥æŒ‡å®šé’©å­è§¦å‘çš„æ–‡ä»¶ç±»å‹ï¼Œä½†æ˜¯ç›®å‰åªæ”¯æŒpython
      types: [python]
```
å…·ä½“hooksçš„é…ç½®æ–¹æ³•è¯·å‚è€ƒ[pre-commit](https://pre-commit.com/#plugins)ã€‚
é…ç½®å¥½yamlæ–‡ä»¶åï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå®‰è£…é’©å­ã€‚
```
pre-commit install
```
è¿™æ ·ï¼Œæ¯æ¬¡git commitæ—¶ï¼Œéƒ½ä¼šè§¦å‘RepoAgentçš„é’©å­ï¼Œè‡ªåŠ¨æ£€æµ‹ç›®æ ‡ä»“åº“ä¸­çš„å˜æ›´ï¼Œå¹¶ç”Ÿæˆå¯¹åº”çš„æ–‡æ¡£ã€‚
æ¥ç€ï¼Œå¯ä»¥å¯¹ç›®æ ‡ä»“åº“è¿›è¡Œä¸€äº›ä¿®æ”¹ï¼Œä¾‹å¦‚åœ¨ç›®æ ‡ä»“åº“ä¸­æ·»åŠ ä¸€ä¸ªæ–°çš„æ–‡ä»¶ï¼Œæˆ–è€…ä¿®æ”¹ä¸€ä¸ªå·²æœ‰çš„æ–‡ä»¶ã€‚
æ‚¨åªéœ€è¦æ­£å¸¸æ‰§è¡Œgitçš„å·¥ä½œæµç¨‹: git add, git commit -m "your commit message", git push
RepoAgent hookä¼šåœ¨git commitæ—¶è‡ªåŠ¨è§¦å‘ï¼Œæ£€æµ‹å‰ä¸€æ­¥æ‚¨git addçš„æ–‡ä»¶ï¼Œå¹¶ç”Ÿæˆå¯¹åº”çš„æ–‡æ¡£ã€‚

æ‰§è¡Œåï¼ŒRepoAgentä¼šè‡ªåŠ¨æ›´æ”¹ç›®æ ‡ä»“åº“ä¸­çš„å·²æš‚å­˜æ–‡ä»¶å¹¶æ­£å¼æäº¤commitï¼Œæ‰§è¡Œå®Œæ¯•åä¼šæ˜¾ç¤ºç»¿è‰²çš„Passedï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
![Execution Result](https://raw.githubusercontent.com/OpenBMB/RepoAgent/main/assets/images/ExecutionResult.png)

ç”Ÿæˆçš„æ–‡æ¡£å°†å­˜æ”¾åœ¨ç›®æ ‡ä»“åº“æ ¹ç›®å½•ä¸‹çš„æŒ‡å®šæ–‡ä»¶å¤¹ä¸­ï¼Œç”Ÿæˆçš„æ–‡æ¡£æ•ˆæœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
![Documentation](https://raw.githubusercontent.com/OpenBMB/RepoAgent/main/assets/images/Doc_example.png)
![Documentation](https://raw.githubusercontent.com/OpenBMB/RepoAgent/main/assets/images/8_documents.png)


æˆ‘ä»¬ä½¿ç”¨é»˜è®¤æ¨¡å‹**gpt-3.5-turbo**å¯¹ä¸€ä¸ªçº¦**27ä¸‡è¡Œ**çš„ä¸­å¤§å‹é¡¹ç›®[**XAgent**](https://github.com/OpenBMB/XAgent)ç”Ÿæˆäº†æ–‡æ¡£ã€‚æ‚¨å¯ä»¥å‰å¾€XAgenté¡¹ç›®çš„Markdown_Docsæ–‡ä»¶ç›®å½•ä¸‹æŸ¥çœ‹ç”Ÿæˆæ•ˆæœã€‚å¦‚æœæ‚¨å¸Œæœ›å¾—åˆ°æ›´å¥½çš„æ–‡æ¡£æ•ˆæœï¼Œæˆ‘ä»¬å»ºè®®æ‚¨ä½¿ç”¨æ›´å…ˆè¿›çš„æ¨¡å‹ï¼Œå¦‚**gpt-4-1106** æˆ– **gpt-4-0125-preview**ã€‚

**æœ€åï¼Œæ‚¨å¯ä»¥é€šè¿‡è‡ªå®šä¹‰Promptæ¥çµæ´»è°ƒæ•´æ–‡æ¡£çš„è¾“å‡ºæ ¼å¼ã€æ¨¡æ¿ç­‰æ–¹é¢çš„æ•ˆæœã€‚ æˆ‘ä»¬å¾ˆé«˜å…´æ‚¨æ¢ç´¢æ›´ç§‘å­¦çš„è‡ªåŠ¨åŒ–Technical Writing Promptså¹¶å¯¹ç¤¾åŒºä½œå‡ºè´¡çŒ®ã€‚**

### æ¢ç´¢ chat with repo

æˆ‘ä»¬å°†ä¸ä»“åº“å¯¹è¯è§†ä¸ºæ‰€æœ‰ä¸‹æ¸¸åº”ç”¨çš„ç»Ÿä¸€å…¥å£ï¼Œä½œä¸ºè¿æ¥RepoAgentä¸äººç±»ç”¨æˆ·å’Œå…¶ä»–AIæ™ºèƒ½ä½“ä¹‹é—´çš„æ¥å£ã€‚æˆ‘ä»¬æœªæ¥çš„ç ”ç©¶å°†æ¢ç´¢é€‚é…å„ç§ä¸‹æ¸¸åº”ç”¨çš„æ¥å£ï¼Œå¹¶å®ç°è¿™äº›ä¸‹æ¸¸ä»»åŠ¡çš„ç‹¬ç‰¹æ€§å’Œç°å®è¦æ±‚ã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„ä¸‹æ¸¸ä»»åŠ¡ä¹‹ä¸€çš„åˆæ­¥åŸå‹ï¼šè‡ªåŠ¨issueé—®é¢˜è§£ç­”å’Œä»£ç è§£é‡Šã€‚æ‚¨å¯ä»¥é€šè¿‡åœ¨ç»ˆç«¯è¿è¡Œä»¥ä¸‹ä»£ç å¯åŠ¨æœåŠ¡ã€‚

```sh
pip install repoagent[chat-with-repo]
repoagent chat-with-repo
```

# âœ… æœªæ¥å·¥ä½œ

- [x] æ”¯æŒé€šè¿‡`pip install repoagent`å°†é¡¹ç›®ä½œä¸ºåŒ…è¿›è¡Œå®‰è£…é…ç½®
- [ ] é€šè¿‡å…¨å±€æ–‡æ¡£ä¿¡æ¯è‡ªåŠ¨ç”Ÿæˆä»“åº“README.mdæ–‡ä»¶
- [ ] **å¤šç¼–ç¨‹è¯­è¨€æ”¯æŒ** æ”¯æŒæ›´å¤šç¼–ç¨‹è¯­è¨€ï¼Œå¦‚Javaã€Cæˆ–C++ç­‰
- [ ] æœ¬åœ°æ¨¡å‹æ”¯æŒå¦‚ Llamaã€chatGLMã€Qianwen ç­‰


# ğŸ¥° ç²¾é€‰æ¡ˆä¾‹

ä»¥ä¸‹æ˜¯é‡‡ç”¨äº†RepoAgentçš„å¼€æºé¡¹ç›®ç²¾é€‰æ¡ˆä¾‹ã€‚

- [MiniCPM](https://github.com/OpenBMB/MiniCPM): ä¸€ä¸ªç«¯ä¾§å¤§è¯­è¨€æ¨¡å‹ï¼Œå¤§å°ä¸º2Bï¼Œæ•ˆæœå¯ä¸7Bæ¨¡å‹åª²ç¾ã€‚
- [ChatDev](https://github.com/OpenBMB/ChatDev): ç”¨äºè½¯ä»¶å¼€å‘çš„åä½œå¼AIæ™ºèƒ½ä½“ã€‚
- [XAgent](https://github.com/OpenBMB/XAgent): ä¸€ä¸ªç”¨äºè§£å†³å¤æ‚ä»»åŠ¡çš„è‡ªä¸»å¤§å‹è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ã€‚
- [EasyRL4Rec](https://github.com/chongminggao/EasyRL4Rec): ä¸€ä¸ªç”¨æˆ·å‹å¥½çš„æ¨èç³»ç»Ÿå¼ºåŒ–å­¦ä¹ åº“ã€‚

# ğŸ“Š å¼•ç”¨æˆ‘ä»¬
```bibtex
@misc{luo2024repoagent,
      title={RepoAgent: An LLM-Powered Open-Source Framework for Repository-level Code Documentation Generation}, 
      author={Qinyu Luo and Yining Ye and Shihao Liang and Zhong Zhang and Yujia Qin and Yaxi Lu and Yesai Wu and Xin Cong and Yankai Lin and Yingli Zhang and Xiaoyin Che and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2402.16667},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```



================================================
FILE: display/Makefile
================================================
# get repo path from ../config.yml
REPO_PATH := $(shell grep 'repo_path:' ../config.yml | awk '{ print $$2 }')

Markdown_Docs_folder := $(shell grep 'Markdown_Docs_folder:' ../config.yml | awk '{ print $$2 }')

# get book name from REPO_PATH
BOOK_NAME := $(notdir $(REPO_PATH))
$(info BOOK_NAME is $(BOOK_NAME))
MARKDOWN_DOCS_FOLDER := $(Markdown_Docs_folder)
$(info MARKDOWN_DOCS_FOLDER is $(MARKDOWN_DOCS_FOLDER))
GITBOOK_PORT := 4000
GITBOOK_LRPORT := 30000

# info colors
GREEN  := $(shell tput -Txterm setaf 2)
YELLOW := $(shell tput -Txterm setaf 3)
WHITE  := $(shell tput -Txterm setaf 7)
RESET  := $(shell tput -Txterm sgr0)


.PHONY: help
.PHONY: init_env env_install
.PHONY: init npm_install clear_book copy_book_json install
.PHONY: generate generate_repo_agent_books generate_summary


################################################################################
# We need nodejs 10.x to run gitbook, this target will install nodejs 10.x
################################################################################
env_install:
	chmod +x ./scripts/install_nodejs.sh
	./scripts/install_nodejs.sh

## init nodejs 10.x env
init_env: env_install
	echo "You have prepared nodejs 10.x  environment."



################################################################################
# The following targets are used to init the gitbook environment
################################################################################
### Install gitbook-cli
npm_install:
	npm install gitbook-cli -g

## clear repo generated book
clear_book:
	-rm -rf ./books/$(BOOK_NAME)

### copy book.json
copy_book_json: clear_book
	mkdir -p ./books/$(BOOK_NAME)
	cp ./book_template/book.json ./books/$(BOOK_NAME)/book.json

### gitbook install plugins
install:
	echo "You need to make sure you have installed nodejs 10.x."
	cd ./books/$(BOOK_NAME) && gitbook install

## gitbook init to install plugins
init: npm_install clear_book copy_book_json install
	@echo Compelete init docs book


################################################################################
# The following targets are used to generate the book and book.json for the gitbook
################################################################################
clear_src:
	-rm -rf ./books/$(BOOK_NAME)/src

generate_repo_agent_books:
	@echo "Generating Repo Agent books..."
	@python ./book_tools/generate_repoagent_books.py $(MARKDOWN_DOCS_FOLDER) $(BOOK_NAME) $(REPO_PATH)

generate_summary:
	@echo "Generating summary..."
	@python ./book_tools/generate_summary_from_book.py $(BOOK_NAME)

## generate repo book
generate: clear_src generate_repo_agent_books generate_summary
	@echo complete repo book: $(BOOK_NAME) generate

## serve gitbook
serve:  generate
	gitbook --port $(GITBOOK_PORT) --lrport $(GITBOOK_LRPORT) serve ./books/$(BOOK_NAME)


TASK_MAX_CHAR_NUM=30
## make help info
help:
	@echo ''
	@echo 'Usage:'
	@echo '  ${YELLOW}make${RESET} ${GREEN}<task>${RESET}'
	@echo ''
	@echo 'Tasks:'
	@awk '/^[a-zA-Z\-\_0-9]+:/ { \
		helpInfo = match(lastLine, /^## (.*)/); \
		if (helpInfo) { \
			helpCommand = substr($$1, 0, index($$1, ":")-1); \
			helpInfo = substr(lastLine, RSTART + 3, RLENGTH); \
			printf "  ${YELLOW}%-$(TASK_MAX_CHAR_NUM)s${RESET} ${GREEN}%s${RESET}\n", helpCommand, helpInfo; \
		} \
	} \
	{ lastLine = $$0 }' $(MAKEFILE_LIST)


================================================
FILE: display/README_DISPLAY.md
================================================
[Binary file]


================================================
FILE: display/book_template/book.json
================================================
{
    
    "title": "RepoAgentæ‰€ç”Ÿæˆæ–‡GitBook",
    "description": "RepoAgentæ ¹æ®é¡¹ç›®repoæ‰€ç”Ÿæˆæ–‡æ¡£GitBook",
    "generator": "site",
    "author": "RepoAgent <x-agent.net>",
    "language": "zh-hans",
    "gitbook": "3.2.3",
    "root": "./src",

    "links": {
        "sidebar": {
            "XAgent": "https://x-agent.net",
            "RepoAgent": "https://github.com/LOGIC-10/RepoAgent.git"
        }
    },

    "pluginsConfig": {
        "github-buttons": {
            "buttons": [
                {
                "repo": "RepoAgent",
                "user": "LOGIC-10",
                "type": "star",
                "count": true,
                "size": "small"
                }
            ]
        },
		
		"image-captions": {
          "caption": "Image _PAGE_LEVEL_._PAGE_IMAGE_NUMBER_ - _CAPTION_"
		},
		"autotheme": {
          "white": [9, 10, 11, 12, 13, 14, 15, 16],
          "sepia": [6, 7, 8, 17, 18, 19],
          "night": [20, 21, 22, 23, 0, 1, 2, 3, 4, 5]
        },
        "callouts": {
            "showTypeInHeader": false
        },
        "theme-default": {
            "showLevel": false
        },
        "disqus": {
            "shortName": ""
        },
        "prism": {
            "css": [
                "prism-themes/themes/prism-atom-dark.css"
            ]
        },
        "sharing": {
            "douban": false,
            "facebook": true,
            "google": false,
            "hatenaBookmark": false,
            "instapaper": false,
            "line": false,
            "linkedin": false,
            "messenger": false,
            "pocket": false,
            "qq": true,
            "qzone": false,
            "stumbleupon": false,
            "twitter": true,
            "viber": false,
            "vk": false,
            "weibo": true,
            "whatsapp": false,
            "all": [
                "douban",
                "facebook",
                "google",
                "instapaper",
                "line",
                "linkedin",
                "messenger",
                "pocket",
                "qq",
                "qzone",
                "stumbleupon",
                "twitter",
                "viber",
                "vk",
                "weibo",
                "whatsapp"
            ]
        },
        "tbfed-pagefooter": {
            "copyright": "https://x-agent.netï¼ŒRepoAgent </a>å‘å¸ƒ",
            "modify_label": "æœ€åæ›´æ–°ï¼š",
            "modify_format": "YYYY-MM-DD HH:mm:ss"
        }
    },
        
    "plugins": [
        "theme-comscore",
        "anchors",
        "-lunr",
        "-search",
        "search-plus",
        "disqus",
        "-highlight",
        "prism",
        "prism-themes",
        "github-buttons",
        "splitter",
        "-sharing",
        "sharing-plus",
        "tbfed-pagefooter",
        "expandable-chapters-small",
        "copy-code-button",
        "callouts",
		"image-captions",
		"autotheme"
    ]
    
}


================================================
FILE: display/book_tools/generate_repoagent_books.py
================================================
import os
import shutil
import sys


def main():
    markdown_docs_folder = sys.argv[1]
    book_name = sys.argv[2]
    repo_path = sys.argv[3]

    # mkdir the book folder
    dst_dir = os.path.join('./books', book_name, 'src')
    docs_dir = os.path.join(repo_path, markdown_docs_folder)

    # check the dst_dir
    if not os.path.exists(dst_dir):
        os.makedirs(dst_dir)
        print("mkdir %s" % dst_dir)

    # cp the Markdown_Docs_folder to dst_dir
    for item in os.listdir(docs_dir):
        src_path = os.path.join(docs_dir, item)
        dst_path = os.path.join(dst_dir, item)

        # check the src_path
        if os.path.isdir(src_path):
            # if the src_path is a folder, use shutil.copytree to copy
            shutil.copytree(src_path, dst_path)
            print("copytree %s to %s" % (src_path, dst_path))
        else:
            # if the src_path is a file, use shutil.copy2 to copy
            shutil.copy2(src_path, dst_path)
            print("copy2 %s to %s" % (src_path, dst_path))

    def create_book_readme_if_not_exist(dire):
        readme_path = os.path.join(dire, 'README.md')

        if not os.path.exists(readme_path):
            with open(readme_path, 'w') as readme_file:
                readme_file.write('# {}\n'.format(book_name))

    # create book README.md if not exist
    create_book_readme_if_not_exist(dst_dir)


if __name__ == '__main__':
    main()



================================================
FILE: display/book_tools/generate_summary_from_book.py
================================================
import os
import re
import sys


def create_readme_if_not_exist(dire):
    readme_path = os.path.join(dire, 'README.md')

    if not os.path.exists(readme_path):
        with open(readme_path, 'w') as readme_file:
            dirname = os.path.basename(dire)
            readme_file.write('# {}\n'.format(dirname))


# def output_markdown(dire, base_dir, output_file, iter_depth=0):
#     for filename in os.listdir(dire):
#         print('add readme ', filename)
#         file_or_path = os.path.join(dire, filename)
#         if os.path.isdir(file_or_path):
#             create_readme_if_not_exist(file_or_path)
#
#     for filename in os.listdir(dire):
#         print('deal with ', filename)
#         file_or_path = os.path.join(dire, filename)
#         if os.path.isdir(file_or_path):
#             # create_readme_if_not_exist(file_or_path)
#
#             if markdown_file_in_dir(file_or_path):
#                 output_file.write('  ' * iter_depth + '- ' + filename + '\n')
#                 output_markdown(file_or_path, base_dir, output_file,
#                                 iter_depth + 1)
#         else:
#             if is_markdown_file(filename):
#                 if (filename not in ['SUMMARY.md',
#                                      'README.md']
#                         or iter_depth != 0):
#                     output_file.write('  ' * iter_depth +
#                                       '- [{}]({})\n'.format(is_markdown_file(filename),
#                                                             os.path.join(os.path.relpath(dire, base_dir),
#                                                                          filename)))

def output_markdown(dire, base_dir, output_file, iter_depth=0):
    for filename in os.listdir(dire):
        print('add readme ', filename)
        file_or_path = os.path.join(dire, filename)
        if os.path.isdir(file_or_path):
            create_readme_if_not_exist(file_or_path)

    for filename in os.listdir(dire):
        print('deal with ', filename)
        file_or_path = os.path.join(dire, filename)
        if os.path.isdir(file_or_path):
            # Check if README.md exists in the directory
            readme_path = os.path.join(file_or_path, 'README.md')
            if os.path.exists(readme_path):
                # If README.md exists, create a markdown link to it
                relative_path = os.path.join(os.path.relpath(file_or_path, base_dir), 'README.md')
                output_file.write('  ' * iter_depth + '- [{}]({})\n'.format(filename, relative_path))
            # Recursively call output_markdown for nested directories
            output_markdown(file_or_path, base_dir, output_file, iter_depth + 1)
        else:
            if is_markdown_file(filename):
                if filename not in ['SUMMARY.md', 'README.md'] or iter_depth != 0 and filename not in ['README.md']:
                    relative_path = os.path.join(os.path.relpath(dire, base_dir), filename)
                    output_file.write('  ' * iter_depth + '- [{}]({})\n'.format(is_markdown_file(filename), relative_path))



def markdown_file_in_dir(dire):
    for root, dirs, files in os.walk(dire):
        for filename in files:
            if re.search('.md$|.markdown$', filename):
                return True
    return False


def is_markdown_file(filename):
    match = re.search('.md$|.markdown$', filename)
    if not match:
        return False
    elif len(match.group()) is len('.md'):
        return filename[:-3]
    elif len(match.group()) is len('.markdown'):
        return filename[:-9]


def main():
    book_name = sys.argv[1]

    # mkdir the book folder
    dir_input = os.path.join('./books', book_name, 'src')

    # check the dst_dir
    if not os.path.exists(dir_input):
        print(dir_input)
        os.makedirs(dir_input)
    # Ensure the directory exists or create it
    if not os.path.exists(dir_input):
        os.makedirs(dir_input)

    # Then proceed to create the file
    output_path = os.path.join(dir_input, 'SUMMARY.md')
    output = open(output_path, 'w')
    # output = open(os.path.join(dir_input, 'SUMMARY.md'), 'w')
    output.write('# Summary\n\n')
    output_markdown(dir_input, dir_input, output)

    print('GitBook auto summary finished:) ')
    return 0


if __name__ == '__main__':
    main()



================================================
FILE: display/books/BOOKS.md
================================================
æ”¯æŒå¤šä¸ª repo book çš„åˆ›å»ºã€‚

ç»Ÿä¸€ book templateï¼Œæ”¯æŒå®Œå…¨è‡ªå®šä¹‰ã€‚

åœ¨ config.yml ä¸­ï¼Œé…ç½®ç”Ÿæˆçš„æ˜¯repo docï¼Œä¸ç”¨æ›´æ”¹config.ymlï¼Œç›´æ¥è¿è¡Œ make å‘½ä»¤å³å¯ç”Ÿæˆå¯¹åº”çš„repo bookå±•ç¤ºã€‚


================================================
FILE: display/scripts/install_nodejs.sh
================================================
#!/bin/bash

export NVM_DIR="$HOME/.nvm"
[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"  # This loads nvm
[ -s "$NVM_DIR/bash_completion" ] && \. "$NVM_DIR/bash_completion"  # This loads nvm bash_completion


# æ£€æŸ¥æ˜¯å¦å·²ç»å®‰è£…äº† nvm
check_nvm_installed() {
    if [ -s "$NVM_DIR/nvm.sh" ]; then
        echo "nvm is already installed"
        return 0
    else
        echo "nvm is not installed"
        return 1
    fi
}

# å®‰è£… nvm
install_nvm_linux() {
    curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash
    source "$NVM_DIR/nvm.sh"
}

install_nvm_mac() {
    curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash
    source "$NVM_DIR/nvm.sh"
}

install_nvm_windows() {
    echo "Downloading nvm for Windows..."
    curl -o nvm-setup.exe -L https://github.com/coreybutler/nvm/releases/download/1.1.12/nvm-setup.exe
    echo "Installing nvm for Windows..."
    ./nvm-setup.exe
    echo "nvm version:"
    nvm -v
    echo "nvm installation for Windows completed."
    rm -f nvm-setup.exe
}

# å®‰è£… Node.js 10
install_nodejs() {
    nvm install 10
    nvm use 10
}

# æ£€æŸ¥ Node.js æ˜¯å¦å®‰è£…æˆåŠŸ
check_node() {
    node_version=$(node -v)
    echo "Installed Node.js version: $node_version"
    if [[ "$node_version" == v10* ]]; then
        echo "Node.js 10 is installed successfully."
    else
        echo "Node.js 10 is not installed."
        exit 1
    fi
}

# æ£€æµ‹æ“ä½œç³»ç»Ÿå¹¶å®‰è£… nvmï¼ˆå¦‚æœéœ€è¦ï¼‰
case "$OSTYPE" in
  linux-gnu*)
    if ! check_nvm_installed; then
        install_nvm_linux
    fi
    ;;
  darwin*)
    if ! check_nvm_installed; then
        install_nvm_mac
    fi
    ;;
  cygwin*|msys*|mingw*|bccwin*|wsl*)
    if ! check_nvm_installed; then
        install_nvm_windows
    fi
    ;;
  *)
    echo "Unsupported OS, You could install nvm manually"
    exit 1
    ;;
esac

# å®‰è£… Node.js 10
install_nodejs

check_node



================================================
FILE: markdown_docs/display/book_tools/generate_repoagent_books.md
================================================
## FunctionDef main
**main**: The function of main is to create a directory structure for a book and copy Markdown documentation files into it.

**parameters**: The parameters of this Function.
Â· parameter1: markdown_docs_folder - The name of the folder containing Markdown documentation files to be copied.
Â· parameter2: book_name - The name of the book for which the directory structure is created.
Â· parameter3: repo_path - The path to the repository where the Markdown documentation folder is located.

**Code Description**: The main function begins by retrieving command-line arguments that specify the folder containing Markdown documentation, the desired book name, and the repository path. It constructs the destination directory path where the book's source files will be stored, specifically under './books/{book_name}/src'. It also constructs the source directory path for the Markdown documentation files based on the provided repository path and the specified folder name.

The function then checks if the destination directory exists. If it does not exist, it creates the directory and prints a confirmation message indicating that the directory has been created. 

Next, the function iterates over each item in the source directory. For each item, it constructs the full source and destination paths. If the item is a directory, it uses `shutil.copytree` to recursively copy the entire directory to the destination. If the item is a file, it uses `shutil.copy2` to copy the file to the destination. For each copy operation, a message is printed to confirm the action taken.

Additionally, the function defines a nested function called `create_book_readme_if_not_exist`, which checks for the existence of a README.md file in the destination directory. If the README.md file does not exist, it creates one and writes the book name as the title in Markdown format.

Finally, the main function calls `create_book_readme_if_not_exist` to ensure that a README.md file is created for the book if it is not already present.

**Note**: It is important to ensure that the specified paths and folder names are valid and accessible. The function relies on the presence of the `shutil` and `os` modules, which must be imported for the code to execute successfully. Additionally, the function assumes that the command-line arguments are provided in the correct order and format.
### FunctionDef create_book_readme_if_not_exist(dire)
**create_book_readme_if_not_exist**: The function of create_book_readme_if_not_exist is to create a README.md file in a specified directory if it does not already exist.

**parameters**: The parameters of this Function.
Â· dire: A string representing the directory path where the README.md file should be created.

**Code Description**: The create_book_readme_if_not_exist function is designed to check for the existence of a README.md file in a specified directory. It takes one parameter, 'dire', which is the path to the directory where the README.md file is intended to be created. 

The function first constructs the full path to the README.md file by joining the provided directory path with the filename 'README.md' using the os.path.join method. It then checks if the file already exists at that path using os.path.exists. If the file does not exist, the function proceeds to create it. 

Within a context manager (using the 'with' statement), the function opens the README.md file in write mode ('w'). This ensures that if the file is created, it will be properly closed after writing. The function writes a header line to the file, formatted as '# {book_name}', where 'book_name' is expected to be a variable that holds the name of the book. However, it is important to note that 'book_name' must be defined in the scope where this function is called, as it is not passed as a parameter to the function itself.

**Note**: It is essential to ensure that the variable 'book_name' is defined before calling this function, as it is used in the content written to the README.md file. Additionally, the function does not handle exceptions that may arise from file operations, so it is advisable to implement error handling if necessary.
***



================================================
FILE: markdown_docs/display/book_tools/generate_summary_from_book.md
================================================
## FunctionDef create_readme_if_not_exist(dire)
**create_readme_if_not_exist**: The function of create_readme_if_not_exist is to create a README.md file in a specified directory if it does not already exist.

**parameters**: The parameters of this Function.
Â· dire: The directory path where the README.md file should be created.

**Code Description**: The create_readme_if_not_exist function checks if a README.md file exists in the specified directory (denoted by the parameter 'dire'). If the file does not exist, the function creates it and writes a header containing the name of the directory as the title. The path for the README.md file is constructed using the os.path.join method, which combines the directory path with the file name 'README.md'. The function uses os.path.exists to verify the existence of the README.md file. If the file is absent, it opens the file in write mode and writes a formatted string that includes the base name of the directory, which is obtained using os.path.basename.

This function is called by the output_markdown function, which iterates through the contents of a specified directory. During its execution, output_markdown checks each item in the directory; if an item is a subdirectory, it invokes create_readme_if_not_exist to ensure that a README.md file is present in that subdirectory. This relationship indicates that create_readme_if_not_exist is a utility function designed to support the documentation generation process by ensuring that each directory has a README.md file, which can be useful for providing context or information about the contents of the directory.

**Note**: It is important to ensure that the directory path provided to the create_readme_if_not_exist function is valid and accessible. Additionally, the function will only create the README.md file if it does not already exist, preventing overwriting any existing documentation.
## FunctionDef output_markdown(dire, base_dir, output_file, iter_depth)
**output_markdown**: The function of output_markdown is to generate a Markdown-formatted summary of files and directories, including links to README.md files and relevant Markdown files.

**parameters**: The parameters of this Function.
Â· dire: A string representing the directory path to be processed for Markdown files and subdirectories.
Â· base_dir: A string representing the base directory path used to create relative paths for the output.
Â· output_file: A file object where the generated Markdown summary will be written.
Â· iter_depth: An integer indicating the current depth of recursion, used for formatting the output.

**Code Description**: The output_markdown function is designed to traverse a specified directory (denoted by the parameter 'dire') and its subdirectories, generating a structured Markdown summary of the contents. The function begins by iterating through the files and directories within 'dire'. For each item, it checks if it is a directory. If it is, the function calls create_readme_if_not_exist to ensure that a README.md file exists in that directory. This utility function is crucial for maintaining documentation consistency across directories.

After ensuring that README.md files are present, the function continues to process each item in the directory. If an item is a directory and contains a README.md file, the function creates a relative Markdown link to that file in the output. The relative path is constructed using os.path.relpath to ensure that the link is correctly formatted based on the base directory.

For files that are not directories, the function utilizes is_markdown_file to determine if the file is a Markdown file. If the file is identified as a Markdown file and is not excluded by specific conditions (such as being named 'SUMMARY.md' or 'README.md' at the top level), the function writes a relative link to that file in the output.

The output_markdown function is called by the main function, which serves as the entry point of the program. In main, the function is invoked after creating the necessary directory structure and opening the output file for writing. This relationship indicates that output_markdown is a critical component of the documentation generation process, responsible for compiling the contents of the specified directory into a cohesive Markdown summary.

**Note**: It is important to ensure that the directory path provided to output_markdown is valid and accessible. The function assumes that the output_file is opened in write mode before being passed to it. Additionally, care should be taken to manage the depth of recursion, as excessive nesting may lead to performance issues or stack overflow errors.
## FunctionDef markdown_file_in_dir(dire)
**markdown_file_in_dir**: The function of markdown_file_in_dir is to check whether any Markdown file (with .md or .markdown extension) exists in a specified directory or its subdirectories.

**parameters**: 
- parameter1: dire (str) - The directory path to be searched for Markdown files.

**Code Description**: 
The function `markdown_file_in_dir` is designed to traverse a specified directory (`dire`) and its subdirectories to check for the existence of files with `.md` or `.markdown` extensions. It utilizes Python's `os.walk` function to walk through the directory tree, where `root` is the current directory path, `dirs` is a list of subdirectories, and `files` is a list of filenames in the current directory.

For each file in the list `files`, the function checks whether the filename matches the regular expression pattern `'.md$|.markdown$'`, which identifies files with the `.md` or `.markdown` extensions. If such a file is found, the function immediately returns `True`, indicating that at least one Markdown file exists within the directory or its subdirectories.

If no Markdown files are found during the entire directory traversal, the function returns `False`.

**Note**: 
- The function stops as soon as a Markdown file is found and returns `True`, which means it does not continue searching further once the condition is met.
- The function uses regular expressions to identify files with `.md` or `.markdown` extensions. Be aware that this check is case-sensitive by default, meaning it will only match lowercase `.md` or `.markdown`. If case-insensitive matching is needed, the regular expression pattern can be modified accordingly.
- This function only returns a Boolean value (True or False). It does not provide any information about the specific files found, just the presence or absence of such files.

**Output Example**:
- If there is at least one `.md` or `.markdown` file in the directory, the return value would be:
  `True`
- If there are no `.md` or `.markdown` files in the directory, the return value would be:
  `False`
## FunctionDef is_markdown_file(filename)
**is_markdown_file**: The function of is_markdown_file is to determine if a given filename corresponds to a Markdown file and return the filename without its extension if it does.

**parameters**: The parameters of this Function.
Â· filename: A string representing the name of the file to be checked.

**Code Description**: The is_markdown_file function uses a regular expression to check if the provided filename ends with either '.md' or '.markdown'. If the filename does not match either of these patterns, the function returns False, indicating that the file is not a Markdown file. If the filename matches '.md', the function returns the filename without the last three characters (the '.md' extension). If the filename matches '.markdown', it returns the filename without the last nine characters (the '.markdown' extension). 

This function is called within the output_markdown function, which is responsible for generating a Markdown-formatted summary of files and directories. In output_markdown, the is_markdown_file function is used to filter out files that are Markdown files. Specifically, it checks each file in the specified directory and its subdirectories. If a file is identified as a Markdown file (and is not 'SUMMARY.md' or 'README.md' under certain conditions), its relative path is formatted and written to the output file. This relationship highlights the utility of is_markdown_file in ensuring that only relevant Markdown files are included in the generated summary.

**Note**: It is important to ensure that the filename passed to the function is a valid string. The function does not handle exceptions for invalid inputs, so care should be taken to validate the input before calling this function.

**Output Example**: 
- If the input is 'example.md', the output will be 'example'.
- If the input is 'document.markdown', the output will be 'document'.
- If the input is 'image.png', the output will be False.
## FunctionDef main
**main**: The function of main is to generate a Markdown summary file for a specified book by creating the necessary directory structure and invoking the output_markdown function.

**parameters**: The parameters of this Function.
Â· book_name: A string representing the name of the book, which is passed as a command-line argument.

**Code Description**: The main function serves as the entry point for the script, responsible for orchestrating the creation of a Markdown summary file for a book. It begins by retrieving the book name from the command-line arguments using `sys.argv[1]`. This book name is then used to construct the path for the source directory where the summary will be generated, specifically `./books/{book_name}/src`.

The function checks if the specified directory exists using `os.path.exists(dir_input)`. If the directory does not exist, it creates the directory structure using `os.makedirs(dir_input)`. This ensures that the environment is prepared for the subsequent operations.

Once the directory is confirmed to exist, the function proceeds to create the summary file named 'SUMMARY.md' within the specified directory. It opens this file in write mode using `open(output_path, 'w')` and writes a header '# Summary\n\n' to initialize the content.

The core functionality of generating the summary is delegated to the `output_markdown` function. This function is called with the parameters `dir_input`, `dir_input` (as the base directory), and the opened output file. The `output_markdown` function is responsible for traversing the directory structure, identifying Markdown files, and generating the appropriate links in the summary file.

After the summary generation process is completed, the function prints a confirmation message indicating that the GitBook auto summary has finished. The function concludes by returning 0, signaling successful execution.

The relationship with the `output_markdown` function is crucial, as it handles the detailed processing of the directory contents and the creation of the Markdown links, making it an integral part of the summary generation workflow.

**Note**: It is important to ensure that the book name provided as a command-line argument is valid and corresponds to an existing book directory structure. The function assumes that the necessary permissions are in place for creating directories and files in the specified path.

**Output Example**: 
When executed with a valid book name, the function will create a directory structure like:
```
./books/
    â””â”€â”€ example_book/
        â””â”€â”€ src/
            â””â”€â”€ SUMMARY.md
```
The content of 'SUMMARY.md' might look like:
```
# Summary

- [Chapter 1](./chapter1.md)
- [Chapter 2](./chapter2.md)
- [Subdirectory](./subdirectory/README.md)
```



================================================
FILE: markdown_docs/repo_agent/change_detector.md
================================================
## ClassDef ChangeDetector
**ChangeDetector**: The function of ChangeDetector is to handle file differences and change detection in a Git repository.

**attributes**: The attributes of this Class.
Â· repo_path: The path to the repository.
Â· repo: An instance of the Git repository initialized with the provided repo_path.

**Code Description**: The ChangeDetector class is designed to facilitate the detection of changes in files within a Git repository. It utilizes the GitPython library to interact with the Git repository, allowing it to track staged and unstaged changes effectively. 

Upon initialization, the class requires a repository path, which it uses to create a Git repository object. This object serves as the primary interface for executing Git commands and retrieving information about the repository's state.

The class includes several methods:

1. **get_staged_pys**: This method retrieves Python files that have been staged for commit. It checks the differences between the staging area and the last commit (HEAD) to identify files that are either newly added or modified. The method returns a dictionary where the keys are the file paths and the values are booleans indicating whether the file is new.

2. **get_file_diff**: This method fetches the differences for a specific file. If the file is new, it stages the file first and then retrieves the differences from the staging area. For existing files, it retrieves the differences from the last commit. The result is a list of changes made to the file.

3. **parse_diffs**: This method processes the list of differences obtained from get_file_diff. It extracts added and removed lines, returning a structured dictionary that categorizes the changes.

4. **identify_changes_in_structure**: This method analyzes the changed lines to determine which functions or classes have been modified. It checks if the changed lines fall within the start and end lines of known structures and records the changes accordingly.

5. **get_to_be_staged_files**: This method identifies files that are modified but not yet staged, based on specific conditions, such as whether a corresponding Markdown file exists for a staged Python file. It returns a list of paths to these files.

6. **add_unstaged_files**: This method stages the identified unstaged files that meet certain conditions, preparing them for the next commit.

The ChangeDetector class is instantiated in the Runner class of the project, where it is used to monitor changes in the repository. The Runner class initializes the ChangeDetector with the target repository path, allowing it to leverage its methods for detecting and managing file changes. This integration ensures that the project can effectively track modifications and prepare files for version control.

**Note**: When using the ChangeDetector class, ensure that the repository path is correctly specified and that the GitPython library is properly installed and configured. The methods are designed to interact with the Git command line, so the underlying Git environment must be accessible.

**Output Example**: A possible output from the get_staged_pys method could be:
```python
{
    'new_test_file.py': True,
    'existing_file.py': False
}
```
This output indicates that 'new_test_file.py' is a newly added file, while 'existing_file.py' has been modified but was already present in the repository.
### FunctionDef __init__(self, repo_path)
**__init__**: __init__çš„åŠŸèƒ½æ˜¯åˆå§‹åŒ–ä¸€ä¸ªChangeDetectorå¯¹è±¡ã€‚

**parameters**: è¯¥å‡½æ•°çš„å‚æ•°ã€‚
Â· repo_path: ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºä»“åº“çš„è·¯å¾„ã€‚

**Code Description**: è¯¥å‡½æ•°æ˜¯ChangeDetectorç±»çš„æ„é€ å‡½æ•°ï¼Œç”¨äºåˆå§‹åŒ–ä¸€ä¸ªChangeDetectorå¯¹è±¡ã€‚åœ¨è°ƒç”¨è¯¥å‡½æ•°æ—¶ï¼Œå¿…é¡»æä¾›ä¸€ä¸ªå‚æ•°repo_pathï¼Œè¯¥å‚æ•°æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºè¦ç›‘æµ‹çš„Gitä»“åº“çš„è·¯å¾„ã€‚å‡½æ•°å†…éƒ¨å°†ä¼ å…¥çš„repo_pathèµ‹å€¼ç»™å®ä¾‹å˜é‡self.repo_pathï¼Œä»¥ä¾¿åœ¨å¯¹è±¡çš„å…¶ä»–æ–¹æ³•ä¸­ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œè¯¥å‡½æ•°è¿˜ä½¿ç”¨gitåº“ä¸­çš„Repoç±»æ¥åˆ›å»ºä¸€ä¸ªæ–°çš„Repoå¯¹è±¡ï¼Œå¹¶å°†å…¶èµ‹å€¼ç»™self.repoï¼Œè¿™æ ·å¯ä»¥é€šè¿‡è¯¥å¯¹è±¡ä¸æŒ‡å®šçš„Gitä»“åº“è¿›è¡Œäº¤äº’ã€‚

**Note**: ä½¿ç”¨è¯¥ä»£ç æ—¶ï¼Œè¯·ç¡®ä¿æä¾›çš„repo_pathæ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„Gitä»“åº“è·¯å¾„ï¼Œå¦åˆ™å°†ä¼šå¼•å‘é”™è¯¯ã€‚ç¡®ä¿åœ¨è°ƒç”¨è¯¥æ„é€ å‡½æ•°ä¹‹å‰ï¼Œå·²å®‰è£…å¹¶æ­£ç¡®é…ç½®äº†gitåº“ã€‚
***
### FunctionDef get_staged_pys(self)
**get_staged_pys**: The function of get_staged_pys is to retrieve a dictionary of Python files that have been staged in the Git repository.

**parameters**: The parameters of this Function.
Â· None

**Code Description**: The get_staged_pys function is designed to identify and return a collection of Python files that have been staged in the Git repository. It utilizes the GitPython library to access the repository's index and compare the current state of staged files against the last commit (HEAD). The function specifically looks for files that have been added or modified, indicated by the change types "A" (added) and "M" (modified). 

The function begins by initializing an empty dictionary called staged_files, which will store the paths of the staged Python files as keys and a boolean value indicating whether each file is newly created as the corresponding value. The core logic of the function involves calling the repo.index.diff("HEAD", R=True) method, which retrieves the differences between the current staging area and the last commit. The R=True parameter is crucial as it reverses the comparison logic, allowing the function to correctly identify newly added files that do not exist in the HEAD commit.

The function then iterates over the differences obtained from the diff call. For each difference, it checks if the change type is either "A" or "M" and if the file path ends with the ".py" extension, ensuring that only Python files are considered. If a file is determined to be newly created (change type "A"), the function marks it as such in the staged_files dictionary.

This function is called within the test_get_staged_pys method of the TestChangeDetector class, which is part of the testing suite for the ChangeDetector functionality. In the test, a new Python file is created and staged using the Git command. The get_staged_pys function is then invoked to verify that the newly created file is correctly identified as staged. The test asserts that the new file appears in the list of staged files, demonstrating the function's effectiveness in tracking changes to Python files in the repository.

**Note**: It is important to ensure that the GitPython library is properly configured and that the repository is in a valid state for the function to operate correctly.

**Output Example**: An example of the return value from get_staged_pys might look like this:
{
    'new_test_file.py': True,
    'existing_file.py': False
}
In this example, 'new_test_file.py' is a newly created file, while 'existing_file.py' has been modified but was already present in the repository.
***
### FunctionDef get_file_diff(self, file_path, is_new_file)
**get_file_diff**: The function of get_file_diff is to retrieve the changes made to a specific file.

**parameters**: The parameters of this Function.
Â· file_path: The relative path of the file.
Â· is_new_file: Indicates whether the file is a new file.

**Code Description**: The get_file_diff function is designed to obtain the differences in a specified file within a Git repository. It takes two parameters: file_path, which is a string representing the relative path of the file in the repository, and is_new_file, a boolean that indicates whether the file is newly created or an existing one.

When is_new_file is set to True, the function first stages the new file by executing a Git command to add it to the staging area. This is done using the subprocess module to run the command `git -C {repo.working_dir} add {file_path}`. After staging the file, it retrieves the differences using `repo.git.diff("--staged", file_path)`, which provides the changes that have been staged for the new file.

If is_new_file is False, the function retrieves the differences from the last committed state (HEAD) using `repo.git.diff("HEAD", file_path)`. The differences are then split into lines and returned as a list.

This function is called by the process_file_changes method in the Runner class. The process_file_changes method is responsible for processing changes in files detected in a repository. It utilizes get_file_diff to obtain the changes in the specified file, which are then parsed and analyzed to identify structural changes in the code. The results are logged and may lead to updates in a JSON file that tracks project hierarchy or the generation of Markdown documentation for the changed file.

**Note**: It is important to ensure that the file path provided is correct and that the Git repository is properly initialized and accessible. Additionally, the subprocess module requires appropriate permissions to execute Git commands.

**Output Example**: An example of the output from get_file_diff might look like the following:
```
[
    "- def old_function():",
    "+ def new_function():",
    "    print('This is a new function')"
]
```
***
### FunctionDef parse_diffs(self, diffs)
**parse_diffs**: The function of parse_diffs is to parse the difference content and extract the added and deleted object information from a list of diffs.

**parameters**: The parameters of this Function.
Â· diffs: A list containing difference content. Obtained by the get_file_diff() function inside the class.

**Code Description**: The parse_diffs function processes a list of differences (diffs) typically generated by a version control system like Git. It identifies lines that have been added or removed in the context of a file's changes. The function initializes a dictionary called changed_lines to store the results, which includes two keys: "added" and "removed". Each key holds a list of tuples, where each tuple contains the line number and the corresponding line content.

The function iterates through each line in the diffs list. It first checks for line number information using a regular expression that matches the format of diff headers (e.g., "@@ -43,33 +43,40 @@"). If a match is found, it updates the current line numbers for both the original and changed content. 

For lines that start with a "+", indicating an addition, the function appends the line number and content (excluding the "+") to the "added" list. Conversely, lines that start with a "-", indicating a removal, are appended to the "removed" list. If a line does not indicate a change, the function increments both line numbers to account for unchanged lines.

The output of this function is a dictionary that provides a structured representation of the changes, allowing other parts of the code to easily access information about what has been added or removed.

The parse_diffs function is called within the process_file_changes method of the Runner class. This method is responsible for processing changes in files detected in a repository. It retrieves the diffs for a specific file using the get_file_diff function and then passes this list to parse_diffs to obtain structured information about the changes. The results are subsequently used to identify changes in the file's structure and update relevant documentation accordingly.

**Note**: It is important to understand that the additions identified by this function do not necessarily indicate newly created objects; modifications in the code are represented as both deletions and additions in the diff output. To determine if an object is newly added, the get_added_objs() function should be used.

**Output Example**: A possible appearance of the code's return value could be:
{
    'added': [
        (86, '    '),
        (87, '    def to_json_new(self, comments = True):'),
        (88, '        data = {'),
        (89, '            "name": self.node_name,'),
        (95, '')
    ],
    'removed': []
}
***
### FunctionDef identify_changes_in_structure(self, changed_lines, structures)
**identify_changes_in_structure**: The function of identify_changes_in_structure is to identify the structures (functions or classes) that have changed in a given set of modified lines of code.

**parameters**: The parameters of this Function.
Â· changed_lines: A dictionary containing the line numbers where changes have occurred, structured as {'added': [(line number, change content)], 'removed': [(line number, change content)]}.
Â· structures: A list of structures (functions or classes) obtained from get_functions_and_classes, where each structure is represented by its type, name, start line number, end line number, and parent structure name.

**Code Description**: The identify_changes_in_structure function processes a dictionary of changed lines and a list of structures to determine which functions or classes have been modified. It initializes a result dictionary, changes_in_structures, with keys 'added' and 'removed', both containing empty sets. The function then iterates through each change type (either 'added' or 'removed') and the corresponding lines. For each line number that has changed, it checks against the list of structures to see if the line number falls within the start and end line numbers of any structure. If a match is found, the structure's name and its parent structure's name are added to the appropriate set in the changes_in_structures dictionary.

This function is called by the process_file_changes method in the Runner class. In that context, it is used to analyze changes detected in a Python file, where it receives the changed lines and the structures of the file. The output of identify_changes_in_structure is then logged and can be used to update project documentation or JSON structure information. This integration ensures that any modifications in the codebase are accurately reflected in the project's metadata and documentation.

**Note**: It is important to ensure that the structures provided to this function are accurate and up-to-date, as any discrepancies may lead to incorrect identification of changes.

**Output Example**: An example of the function's return value could be: {'added': {('NewFunction', 'ParentClass'), ('AnotherFunction', None)}, 'removed': set()}. This indicates that 'NewFunction' was added under 'ParentClass', while no functions were removed.
***
### FunctionDef get_to_be_staged_files(self)
**get_to_be_staged_files**: The function of get_to_be_staged_files is to retrieve all unstaged files in the repository that meet specific conditions for staging.

**parameters**: The parameters of this Function.
Â· No parameters are required for this function.

**Code Description**: The get_to_be_staged_files method is designed to identify and return a list of file paths that are either modified but not staged or untracked, based on certain criteria. The method performs the following operations:

1. It initializes an empty list called to_be_staged_files to store the paths of files that need to be staged.
2. It retrieves a list of already staged files by comparing the current index with the HEAD commit using the Git repository's diff method.
3. The method then fetches the current project settings using the SettingsManager's get_setting method, which provides access to configuration details such as project hierarchy and markdown documentation folder.
4. It gathers a list of all unstaged changes (diffs) in the repository and identifies untracked files that exist in the working directory but have not been added to the staging area.
5. The method iterates through the untracked files and checks if they meet the following conditions:
   - If the untracked file's path starts with the markdown documentation folder name, it is added to the to_be_staged_files list.
   - If the untracked file is a markdown file (.md) and has a corresponding Python file (.py) that is already staged, the markdown file is also added to the list.
   - If the untracked file's path matches the project hierarchy, it is added to the list as well.
6. The method then processes the unstaged files, similarly checking if they are markdown files or match the project hierarchy, and adds them to the to_be_staged_files list if they meet the criteria.
7. Finally, the method returns the list of paths that need to be staged.

This method is called by the add_unstaged_files method within the ChangeDetector class, which utilizes the output of get_to_be_staged_files to determine which files should be added to the staging area. Additionally, it is tested in the TestChangeDetector class through unit tests that verify its functionality by checking if modified markdown files are correctly identified as unstaged.

**Note**: It is important to ensure that the repository is in a clean state and that the project settings are correctly configured before invoking this method, as any discrepancies may lead to inaccurate results.

**Output Example**: A possible appearance of the code's return value when calling get_to_be_staged_files could be:
```
[
    'path/to/repo/markdown_docs/test_file.md',
    'path/to/repo/markdown_docs/another_file.md',
    'path/to/repo/documentation'
]
```
***
### FunctionDef add_unstaged_files(self)
**add_unstaged_files**: The function of add_unstaged_files is to add unstaged files that meet specific conditions to the staging area of a Git repository.

**parameters**: The parameters of this Function.
Â· No parameters are required for this function.

**Code Description**: The add_unstaged_files method is designed to identify and stage files in a Git repository that are currently unstaged but meet certain criteria for staging. This function operates as follows:

1. It first calls the get_to_be_staged_files method, which retrieves a list of file paths for all unstaged files that meet specific conditions. These conditions typically include files that are modified but not staged or untracked files that should be staged based on project settings.

2. The method then iterates over the list of unstaged files obtained from get_to_be_staged_files. For each file path, it constructs a Git command to add the file to the staging area. The command is formatted as `git -C {self.repo.working_dir} add {file_path}`, where `self.repo.working_dir` is the path to the working directory of the repository.

3. The subprocess.run function is used to execute the constructed Git command. The `shell=True` argument allows the command to be run in the shell, and `check=True` ensures that an exception is raised if the command fails.

4. After processing all unstaged files, the method returns the list of file paths that were identified as needing to be staged.

This method is called by the run method in the Runner class, which is responsible for managing the document update process. The run method detects changes in the repository, processes them, and ultimately invokes add_unstaged_files to ensure that any newly generated or modified Markdown files are added to the staging area. Additionally, it is also called in the process_file_changes method, which handles changes to individual files and ensures that any corresponding documentation is updated and staged.

The add_unstaged_files method is crucial for maintaining an accurate staging area in the Git repository, particularly in workflows that involve automatic documentation generation based on changes in Python files.

**Note**: It is important to ensure that the repository is in a clean state and that the project settings are correctly configured before invoking this method, as any discrepancies may lead to inaccurate results.

**Output Example**: A possible appearance of the code's return value when calling add_unstaged_files could be:
```
[
    'path/to/repo/markdown_docs/test_file.md',
    'path/to/repo/markdown_docs/another_file.md',
    'path/to/repo/documentation'
]
```
***



================================================
FILE: markdown_docs/repo_agent/chat_engine.md
================================================
## ClassDef ChatEngine
Doc is waiting to be generated...
### FunctionDef __init__(self, project_manager)
**__init__**: The function of __init__ is to initialize an instance of the ChatEngine class with the necessary configuration settings for the OpenAI API.

**parameters**: The parameters of this Function.
Â· project_manager: An instance of the ProjectManager class that is responsible for managing the overall project workflow and interactions.

**Code Description**: The __init__ method of the ChatEngine class is designed to set up the initial state of the ChatEngine instance by configuring it with the appropriate settings for the OpenAI API. Upon instantiation, the method first retrieves the current configuration settings by calling the `get_setting` method from the SettingsManager class. This method ensures that the settings are accessed in a consistent manner throughout the application, adhering to the Singleton design pattern.

The retrieved settings include critical parameters such as the OpenAI API key, the base URL for API requests, the timeout duration for requests, the model to be used for chat completions, and the temperature setting that influences the randomness of the generated responses. These parameters are essential for the ChatEngine to function correctly and interact with the OpenAI API effectively.

The OpenAI instance is then created using these settings, allowing the ChatEngine to perform chat-related functionalities, such as generating responses based on user input. The integration of the SettingsManager ensures that the ChatEngine is always configured with the latest settings, promoting maintainability and reducing the risk of errors due to misconfiguration.

From a functional perspective, the ChatEngine class relies on the SettingsManager to provide the necessary configuration settings, which are crucial for its operation. This relationship exemplifies the design principle of separation of concerns, where the SettingsManager handles the management of configuration settings, while the ChatEngine focuses on its primary functionality of facilitating chat interactions.

**Note**: It is important to ensure that the SettingsManager is properly configured and that the Setting class contains valid attributes before instantiating the ChatEngine. Any misconfiguration may lead to runtime errors or unexpected behavior when the ChatEngine attempts to utilize the OpenAI API settings.
***
### FunctionDef build_prompt(self, doc_item)
Doc is waiting to be generated...
#### FunctionDef get_referenced_prompt(doc_item)
**get_referenced_prompt**: The function of get_referenced_prompt is to generate a formatted string that summarizes the references made by a given DocItem, including details about the referenced objects and their documentation.

**parameters**: The parameters of this Function.
Â· doc_item: An instance of the DocItem class, which contains information about the documentation item and its references.

**Code Description**: The get_referenced_prompt function is designed to create a prompt that outlines the references associated with a specific DocItem. It first checks if the provided doc_item has any references by evaluating the length of the reference_who attribute, which is a list of DocItem instances that reference the current item. If there are no references, the function returns an empty string.

If references are present, the function initializes a list called prompt with a predefined introductory string. It then iterates over each reference_item in the doc_item.reference_who list. For each reference_item, the function constructs a detailed string (instance_prompt) that includes the full name of the referenced object, its corresponding documentation content, and the raw code associated with it. The get_full_name method of the reference_item is called to retrieve its full hierarchical name, ensuring clarity in the context of the documentation.

The instance_prompt is formatted to include the object's name, its documentation (if available), and the raw code, all separated by a visual divider. Each instance_prompt is appended to the prompt list. Finally, the function joins all elements of the prompt list into a single string, separated by newline characters, and returns this string.

This function is particularly useful in the context of generating documentation, as it provides a clear overview of how different documentation items are interconnected through references. It aids in understanding the relationships between various code elements, which is essential for maintaining comprehensive and accurate documentation.

**Note**: When using the get_referenced_prompt function, ensure that the doc_item passed to it has been properly initialized and contains valid references. This will guarantee that the generated prompt accurately reflects the relationships and documentation of the referenced items.

**Output Example**: An example output of the get_referenced_prompt function for a DocItem with references might look like this:
```
As you can see, the code calls the following objects, their code and docs are as following:
obj: repo_agent/doc_meta_info.py/DocItem
Document: 
**DocItem**: The function of DocItem is to represent individual documentation items within a project, encapsulating their metadata and relationships.
Raw code:```
class DocItem:
    ...
```
obj: repo_agent/another_file.py/AnotherClass
Document: 
**AnotherClass**: This class serves a different purpose within the project.
Raw code:```
class AnotherClass:
    ...
```
```
***
#### FunctionDef get_referencer_prompt(doc_item)
**get_referencer_prompt**: The function of get_referencer_prompt is to generate a prompt string that lists all the objects that reference a given documentation item, along with their associated documentation and code.

**parameters**: The parameters of this Function.
Â· doc_item: An instance of the DocItem class, which represents the documentation item for which the referencing objects are being retrieved.

**Code Description**: The get_referencer_prompt function is designed to create a formatted string that provides information about the objects that reference a specific documentation item. It begins by checking if the provided doc_item has any references in its who_reference_me attribute, which is a list of DocItem instances that reference the current item. If this list is empty, the function returns an empty string, indicating that there are no references to display.

If there are references, the function initializes a prompt list with a header string that introduces the subsequent information. It then iterates over each DocItem in the who_reference_me list. For each referencing item, it constructs a detailed string that includes the full name of the referencing object (obtained by calling the get_full_name method on the referencer_item), the last version of its markdown content (if available), and its raw code content (if present). Each of these details is formatted in a readable manner, separated by line breaks and a visual divider.

Finally, the function joins all the strings in the prompt list into a single string, separated by newline characters, and returns this formatted string. This output serves as a comprehensive reference for developers, allowing them to quickly understand which objects are related to the given documentation item and to access their associated documentation and code.

The get_referencer_prompt function is particularly useful in the context of documentation generation and management, as it helps to clarify the relationships between different code elements. By providing a clear overview of the references, it aids developers in navigating the documentation and understanding the dependencies within the codebase.

**Note**: When using this function, ensure that the doc_item parameter is a properly initialized instance of the DocItem class with an established hierarchy and references. This will ensure accurate and meaningful output.

**Output Example**: An example output of the get_referencer_prompt function might look like this:
```
Also, the code has been called by the following objects, their code and docs are as following:
obj: repo_agent/doc_meta_info.py/DocItem
Document: 
This is a documentation item that describes a specific code element.
Raw code:```
class DocItem:
    ...
```
==========
obj: repo_agent/another_file.py/AnotherClass
Document: 
This class interacts with the DocItem and provides additional functionality.
Raw code:```
class AnotherClass:
    ...
```
```
***
#### FunctionDef get_relationship_description(referencer_content, reference_letter)
**get_relationship_description**: The function of get_relationship_description is to generate a descriptive string regarding the relationship of a referencer with its callers and callees based on the provided inputs.

**parameters**: The parameters of this Function.
Â· referencer_content: A boolean indicating whether there is content related to the referencer.
Â· reference_letter: A boolean indicating whether there is a reference letter available.

**Code Description**: The get_relationship_description function evaluates the presence of two boolean parameters: referencer_content and reference_letter. It constructs and returns a specific string based on the combination of these parameters. 

- If both referencer_content and reference_letter are true, the function returns a string that requests the inclusion of the reference relationship with both callers and callees from a functional perspective.
- If only referencer_content is true, it returns a string that requests the inclusion of the relationship with callers from a functional perspective.
- If only reference_letter is true, it returns a string that requests the inclusion of the relationship with callees from a functional perspective.
- If neither parameter is true, the function returns an empty string.

This design allows for flexible output based on the available information regarding the referencer, ensuring that the user receives relevant instructions based on the context provided.

**Note**: It is important to ensure that the parameters are boolean values, as the function logic relies on their truthiness to determine the appropriate output. Providing non-boolean values may lead to unexpected results.

**Output Example**: 
- If both parameters are true: "And please include the reference relationship with its callers and callees in the project from a functional perspective."
- If only referencer_content is true: "And please include the relationship with its callers in the project from a functional perspective."
- If only reference_letter is true: "And please include the relationship with its callees in the project from a functional perspective."
- If neither parameter is true: "" (an empty string).
***
***
### FunctionDef generate_doc(self, doc_item)
Doc is waiting to be generated...
***



================================================
FILE: markdown_docs/repo_agent/file_handler.md
================================================
## ClassDef FileHandler
# Class `FileHandler`

The `FileHandler` class provides a set of methods to interact with files within a Git repository, specifically for handling changes, reading file contents, extracting code information, and writing back changes to the repository. This class allows for tasks such as retrieving modified file versions, extracting function and class structures from code, and generating project file structures using Abstract Syntax Tree (AST) parsing.

## Methods Overview

### `__init__(self, repo_path, file_path)`
Initializes a `FileHandler` instance with the given repository and file path.

#### Parameters:
- `repo_path` (str): The absolute path to the Git repository.
- `file_path` (str): The relative path of the file within the repository.

### `read_file(self)`
Reads the contents of the file specified by `file_path`.

#### Returns:
- `str`: The content of the current file.

### `get_obj_code_info(self, code_type, code_name, start_line, end_line, params, file_path=None)`
Retrieves detailed information about a given code object (e.g., function or class) in the file.

#### Parameters:
- `code_type` (str): The type of the code object (e.g., 'FunctionDef', 'ClassDef').
- `code_name` (str): The name of the code object.
- `start_line` (int): The starting line number of the code object.
- `end_line` (int): The ending line number of the code object.
- `params` (list): A list of parameters associated with the code object.
- `file_path` (str, optional): The path to the file containing the code object. Defaults to `None`, in which case the `file_path` provided during initialization is used.

#### Returns:
- `dict`: A dictionary containing information about the code object, including its content, line numbers, and parameters.

### `write_file(self, file_path, content)`
Writes the provided content to a file at the specified path.

#### Parameters:
- `file_path` (str): The relative path of the file to write to.
- `content` (str): The content to write into the file.

### `get_modified_file_versions(self)`
Retrieves the current and previous versions of a modified file.

#### Returns:
- `tuple`: A tuple containing:
  - `current_version` (str): The content of the current version of the file.
  - `previous_version` (str): The content of the previous version of the file (from the last Git commit).

### `get_end_lineno(self, node)`
Gets the end line number of a given AST node.

#### Parameters:
- `node`: The AST node for which to determine the end line number.

#### Returns:
- `int`: The end line number of the node, or `-1` if no line number is available.

### `add_parent_references(self, node, parent=None)`
Recursively adds a reference to the parent node for all child nodes in an Abstract Syntax Tree (AST).

#### Parameters:
- `node`: The AST node to start from.
- `parent` (optional): The parent node, which defaults to `None`.

#### Returns:
- `None`

### `get_functions_and_classes(self, code_content)`
Extracts all functions, classes, and their parameters from a given code content, including hierarchical relationships.

#### Parameters:
- `code_content` (str): The code content to parse.

#### Returns:
- `list`: A list of tuples, each containing:
  - The type of the node (e.g., `FunctionDef`, `ClassDef`),
  - The name of the node,
  - The starting line number,
  - The ending line number,
  - The list of parameters (if any).

### `generate_file_structure(self, file_path)`
Generates the file structure of a given file, including all functions, classes, and their parameters.

#### Parameters:
- `file_path` (str): The relative path of the file to process.

#### Returns:
- `list`: A list of dictionaries, each containing code information for a function or class in the file.

### `generate_overall_structure(self, file_path_reflections, jump_files)`
Generates the overall file structure for a repository, parsing all relevant files and skipping files that are either ignored or not staged.

#### Parameters:
- `file_path_reflections` (dict): A dictionary mapping file paths to their corresponding reflections (for handling fake files or renamed files).
- `jump_files` (list): A list of files to skip during processing.

#### Returns:
- `dict`: A dictionary representing the overall structure of the repository, with file paths as keys and lists of code object information as values.

### `convert_to_markdown_file(self, file_path=None)`
Converts the content of a file to markdown format.

#### Parameters:
- `file_path` (str, optional): The relative path of the file to convert. If not provided, the default `file_path` will be used.

#### Returns:
- `str`: The content of the file in markdown format.

#### Raises:
- `ValueError`: If no file object is found for the specified file path.

---

## Usage Example

```python
# Initialize the FileHandler with the repository path and file path
file_handler = FileHandler(repo_path="/path/to/repo", file_path="src/example.py")

# Read the content of the file
file_content = file_handler.read_file()

# Get code information for a function named 'example_function'
code_info = file_handler.get_obj_code_info(
    code_type="FunctionDef",
    code_name="example_function",
    start_line=10,
    end_line=20,
    params=["param1", "param2"]
)

# Write new content to the file
file_handler.write_file(file_path="src/example.py", content="new content")

# Get the current and previous versions of the modified file
current_version, previous_version = file_handler.get_modified_file_versions()

# Generate the file structure for a given file
file_structure = file_handler.generate_file_structure(file_path="src/example.py")

# Generate the overall file structure for the repository, skipping specified files
repo_structure = file_handler.generate_overall_structure(file_path_reflections={}, jump_files=["skip_file.py"])

# Convert the file content to markdown
markdown_content = file_handler.convert_to_markdown_file(file_path="src/example.py")
```

## Dependencies
- `os`: For file path manipulation and file operations.
- `gitpython`: For interacting with the Git repository.
- `ast`: For parsing Python code into an Abstract Syntax Tree.
- `tqdm`: For progress bar display during repository processing.
- `logging`: For logging error messages.

The `FileHandler` class provides an effective set of utilities for managing and analyzing code files in a Git repository, making it ideal for scenarios involving file change tracking, code analysis, and file versioning.
### FunctionDef __init__(self, repo_path, file_path)
**__init__**: The function of __init__ is to initialize an instance of the FileHandler class with the specified repository and file paths.

**parameters**: The parameters of this Function.
Â· repo_path: This parameter represents the path to the repository where the project files are located. It is expected to be an absolute or relative path that points to the root of the repository.
Â· file_path: This parameter is the path to a specific file within the repository. It should be a path relative to the root directory of the repository.

**Code Description**: The __init__ method serves as the constructor for the FileHandler class. It initializes the instance by setting two attributes: `file_path` and `repo_path`. The `file_path` attribute is assigned the value of the `file_path` parameter, which is intended to be relative to the root directory of the repository. The `repo_path` attribute is similarly assigned the value of the `repo_path` parameter, establishing a reference to the repository's location.

Additionally, the method retrieves the current project settings by invoking the `get_setting` method from the SettingsManager class. This call ensures that the FileHandler instance has access to the latest configuration settings defined for the project. The retrieved settings are then used to construct the `project_hierarchy` attribute, which combines the target repository path with the hierarchy name specified in the project settings. This hierarchical structure is essential for managing files and directories within the project context.

The relationship with the SettingsManager is critical, as it centralizes the configuration management for the project. By utilizing the `get_setting` method, the FileHandler class ensures that it operates with the most up-to-date settings, which may include paths, logging configurations, and other project-specific parameters. This design promotes consistency and reduces the risk of errors that could arise from hardcoded values or outdated configurations.

**Note**: It is important to ensure that the SettingsManager is properly configured before instantiating the FileHandler class. Any misconfiguration in the settings may lead to runtime errors or unexpected behavior when accessing the project hierarchy or file paths.
***
### FunctionDef read_file(self)
**read_file**: read_fileçš„åŠŸèƒ½æ˜¯è¯»å–å½“å‰æ›´æ”¹æ–‡ä»¶çš„å†…å®¹ã€‚

**parameters**: è¯¥å‡½æ•°æ²¡æœ‰å‚æ•°ã€‚

**Code Description**: 
read_fileå‡½æ•°ç”¨äºè¯»å–æŒ‡å®šè·¯å¾„çš„æ–‡ä»¶å†…å®¹ã€‚å®ƒé¦–å…ˆé€šè¿‡os.path.joinæ–¹æ³•å°†å­˜å‚¨åº“è·¯å¾„ï¼ˆrepo_pathï¼‰å’Œæ–‡ä»¶è·¯å¾„ï¼ˆfile_pathï¼‰ç»„åˆæˆä¸€ä¸ªç»å¯¹æ–‡ä»¶è·¯å¾„ï¼ˆabs_file_pathï¼‰ã€‚æ¥ç€ï¼Œå‡½æ•°ä»¥åªè¯»æ¨¡å¼æ‰“å¼€è¯¥æ–‡ä»¶ï¼Œå¹¶ä½¿ç”¨UTF-8ç¼–ç è¯»å–æ–‡ä»¶çš„å…¨éƒ¨å†…å®¹ã€‚è¯»å–å®Œæˆåï¼Œå‡½æ•°å°†æ–‡ä»¶å†…å®¹ä½œä¸ºå­—ç¬¦ä¸²è¿”å›ã€‚

åœ¨é¡¹ç›®ä¸­ï¼Œread_fileå‡½æ•°è¢«å¤šä¸ªå¯¹è±¡è°ƒç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨repo_agent/runner.pyä¸­çš„add_new_itemå’Œprocess_file_changesæ–¹æ³•ä¸­éƒ½æœ‰è°ƒç”¨ã€‚add_new_itemæ–¹æ³•ä½¿ç”¨read_fileå‡½æ•°æ¥è·å–æ–‡ä»¶çš„æºä»£ç ï¼Œä»¥ä¾¿æå–æ–‡ä»¶ä¸­çš„å‡½æ•°å’Œç±»ä¿¡æ¯ï¼Œå¹¶ç”Ÿæˆç›¸åº”çš„æ–‡æ¡£ã€‚process_file_changesæ–¹æ³•åˆ™åœ¨å¤„ç†æ–‡ä»¶å˜æ›´æ—¶è°ƒç”¨read_fileï¼Œè·å–æ•´ä¸ªPythonæ–‡ä»¶çš„ä»£ç ï¼Œä»¥ä¾¿åˆ†ææ–‡ä»¶çš„å˜æ›´æƒ…å†µã€‚è¿™è¡¨æ˜read_fileå‡½æ•°åœ¨æ–‡ä»¶å¤„ç†å’Œæ–‡æ¡£ç”Ÿæˆçš„è¿‡ç¨‹ä¸­èµ·åˆ°äº†å…³é”®ä½œç”¨ã€‚

**Note**: ä½¿ç”¨è¯¥å‡½æ•°æ—¶ï¼Œè¯·ç¡®ä¿æä¾›çš„repo_pathå’Œfile_pathæ˜¯æœ‰æ•ˆçš„è·¯å¾„ï¼Œä»¥é¿å…æ–‡ä»¶è¯»å–é”™è¯¯ã€‚

**Output Example**: å‡è®¾æ–‡ä»¶å†…å®¹ä¸ºâ€œHello, World!â€ï¼Œåˆ™è¯¥å‡½æ•°çš„è¿”å›å€¼å°†æ˜¯å­—ç¬¦ä¸²â€œHello, World!â€ã€‚
***
### FunctionDef get_obj_code_info(self, code_type, code_name, start_line, end_line, params, file_path)
**get_obj_code_info**: The function of get_obj_code_info is to retrieve detailed information about a specific code segment within a file.

**parameters**: The parameters of this Function.
Â· code_type: A string representing the type of the code being analyzed.
Â· code_name: A string indicating the name of the code object.
Â· start_line: An integer specifying the starting line number of the code segment.
Â· end_line: An integer specifying the ending line number of the code segment.
Â· params: A collection of parameters associated with the code.
Â· file_path: An optional string that provides the path to the file. If not specified, it defaults to None.

**Code Description**: The get_obj_code_info function is designed to extract and return information about a specific segment of code from a file. It takes in several parameters that define the characteristics of the code segment, including its type, name, and the range of lines it occupies. The function initializes a dictionary, code_info, to store various attributes related to the code segment.

The function opens the specified file in read mode and reads all lines into a list. It then concatenates the lines from start_line to end_line to form the complete code content. Additionally, it checks for the presence of the code_name in the first line of the specified range to determine its column position. The function also checks if the code segment contains a return statement, which is a common indicator of a function's output.

Finally, the function populates the code_info dictionary with the gathered information, including the type, name, start and end lines, parameters, the presence of a return statement, the code content, and the column position of the code name. The populated dictionary is then returned as the output of the function.

**Note**: It is important to ensure that the specified start_line and end_line are valid and within the bounds of the file's total line count to avoid potential errors when reading the file. The file_path parameter should be correctly set to point to the desired file location.

**Output Example**: A possible return value of the function could look like this:
{
    "type": "function",
    "name": "calculate_sum",
    "md_content": [],
    "code_start_line": 10,
    "code_end_line": 15,
    "params": ["a", "b"],
    "have_return": true,
    "code_content": "def calculate_sum(a, b):\n    return a + b\n",
    "name_column": 4
}
***
### FunctionDef write_file(self, file_path, content)
**write_file**: write_fileçš„åŠŸèƒ½æ˜¯å°†å†…å®¹å†™å…¥æŒ‡å®šè·¯å¾„çš„æ–‡ä»¶ä¸­ã€‚

**parameters**: è¯¥å‡½æ•°çš„å‚æ•°å¦‚ä¸‹ï¼š
Â· parameter1: file_path (str) - æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„ã€‚
Â· parameter2: content (str) - è¦å†™å…¥æ–‡ä»¶çš„å†…å®¹ã€‚

**Code Description**: write_fileå‡½æ•°ç”¨äºå°†æŒ‡å®šå†…å®¹å†™å…¥åˆ°ç»™å®šçš„æ–‡ä»¶è·¯å¾„ã€‚é¦–å…ˆï¼Œè¯¥å‡½æ•°ä¼šæ£€æŸ¥file_pathæ˜¯å¦ä¸ºç»å¯¹è·¯å¾„ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™å»æ‰è·¯å¾„å¼€å¤´çš„æ–œæ ï¼Œä»¥ç¡®ä¿file_pathæ˜¯ç›¸å¯¹è·¯å¾„ã€‚æ¥ç€ï¼Œå‡½æ•°é€šè¿‡os.path.joinå°†repo_pathä¸file_pathç»„åˆæˆç»å¯¹è·¯å¾„abs_file_pathï¼Œå¹¶ä½¿ç”¨os.makedirsç¡®ä¿è¯¥è·¯å¾„çš„ç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºå®ƒã€‚ç„¶åï¼Œå‡½æ•°ä»¥å†™å…¥æ¨¡å¼æ‰“å¼€æ–‡ä»¶ï¼Œå¹¶å°†å†…å®¹å†™å…¥è¯¥æ–‡ä»¶ï¼Œä½¿ç”¨utf-8ç¼–ç æ ¼å¼ã€‚

åœ¨é¡¹ç›®ä¸­ï¼Œwrite_fileå‡½æ•°è¢«Runnerç±»ä¸­çš„add_new_itemå’Œprocess_file_changesä¸¤ä¸ªæ–¹æ³•è°ƒç”¨ã€‚åœ¨add_new_itemæ–¹æ³•ä¸­ï¼Œwrite_fileç”¨äºå°†ç”Ÿæˆçš„Markdownæ–‡æ¡£å†™å…¥åˆ°æŒ‡å®šçš„.mdæ–‡ä»¶ä¸­ï¼Œç¡®ä¿æ–°æ·»åŠ çš„é¡¹ç›®çš„æ–‡æ¡£èƒ½å¤Ÿè¢«æ­£ç¡®ä¿å­˜ã€‚è€Œåœ¨process_file_changesæ–¹æ³•ä¸­ï¼Œwrite_fileåŒæ ·ç”¨äºæ›´æ–°Markdownæ–‡æ¡£ï¼Œç¡®ä¿åœ¨æ–‡ä»¶å˜æ›´åï¼Œæ–‡æ¡£å†…å®¹èƒ½å¤ŸåŠæ—¶åæ˜ æœ€æ–°çš„ä»£ç ç»“æ„ä¿¡æ¯ã€‚è¿™ä¸¤ä¸ªè°ƒç”¨åœºæ™¯è¡¨æ˜ï¼Œwrite_fileå‡½æ•°åœ¨æ–‡ä»¶å¤„ç†å’Œæ–‡æ¡£ç”Ÿæˆä¸­èµ·åˆ°äº†é‡è¦çš„ä½œç”¨ã€‚

**Note**: ä½¿ç”¨è¯¥å‡½æ•°æ—¶ï¼Œè¯·ç¡®ä¿æä¾›çš„file_pathæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå¹¶ä¸”ç¡®ä¿repo_pathå·²æ­£ç¡®è®¾ç½®ï¼Œä»¥é¿å…æ–‡ä»¶å†™å…¥é”™è¯¯ã€‚
***
### FunctionDef get_modified_file_versions(self)
**get_modified_file_versions**: get_modified_file_versionsçš„åŠŸèƒ½æ˜¯è·å–è¢«ä¿®æ”¹æ–‡ä»¶çš„å½“å‰ç‰ˆæœ¬å’Œä¹‹å‰ç‰ˆæœ¬ã€‚

**parameters**: è¯¥å‡½æ•°æ²¡æœ‰å‚æ•°ã€‚

**Code Description**: get_modified_file_versionså‡½æ•°ç”¨äºè·å–æŒ‡å®šæ–‡ä»¶çš„å½“å‰ç‰ˆæœ¬å’Œä¸Šä¸€ä¸ªç‰ˆæœ¬ã€‚é¦–å…ˆï¼Œå®ƒé€šè¿‡gitåº“è·å–å½“å‰å·¥ä½œç›®å½•ä¸­æŒ‡å®šæ–‡ä»¶çš„å†…å®¹ï¼Œä½œä¸ºå½“å‰ç‰ˆæœ¬ã€‚ç„¶åï¼Œå®ƒé€šè¿‡è®¿é—®gitæäº¤å†å²è®°å½•ï¼Œè·å–è¯¥æ–‡ä»¶åœ¨æœ€è¿‘ä¸€æ¬¡æäº¤ä¸­çš„å†…å®¹ï¼Œä½œä¸ºä¹‹å‰ç‰ˆæœ¬ã€‚å¦‚æœæ–‡ä»¶åœ¨ä¹‹å‰çš„æäº¤ä¸­ä¸å­˜åœ¨ï¼ˆä¾‹å¦‚ï¼Œæ–‡ä»¶æ˜¯æ–°æ·»åŠ çš„ï¼‰ï¼Œåˆ™ä¹‹å‰ç‰ˆæœ¬å°†è¢«è®¾ç½®ä¸ºNoneã€‚æœ€ç»ˆï¼Œè¯¥å‡½æ•°è¿”å›ä¸€ä¸ªåŒ…å«å½“å‰ç‰ˆæœ¬å’Œä¹‹å‰ç‰ˆæœ¬çš„å…ƒç»„ã€‚

è¯¥å‡½æ•°åœ¨é¡¹ç›®ä¸­çš„è°ƒç”¨åœºæ™¯ä¸»è¦å‡ºç°åœ¨Runnerç±»çš„get_new_objectsæ–¹æ³•ä¸­ã€‚åœ¨è¯¥æ–¹æ³•ä¸­ï¼Œget_modified_file_versionsè¢«ç”¨æ¥è·å–å½“å‰å’Œä¹‹å‰ç‰ˆæœ¬çš„æ–‡ä»¶å†…å®¹ï¼Œä»¥ä¾¿æ¯”è¾ƒè¿™ä¸¤ä¸ªç‰ˆæœ¬ä¹‹é—´çš„å·®å¼‚ã€‚å…·ä½“æ¥è¯´ï¼Œget_new_objectsæ–¹æ³•åˆ©ç”¨å½“å‰ç‰ˆæœ¬å’Œä¹‹å‰ç‰ˆæœ¬çš„ä¿¡æ¯ï¼Œè§£æå‡ºæ–°å¢å’Œåˆ é™¤çš„å¯¹è±¡ï¼Œä»è€Œå®ç°å¯¹æ–‡ä»¶å†…å®¹å˜åŒ–çš„æ£€æµ‹ã€‚

**Note**: ä½¿ç”¨è¯¥å‡½æ•°æ—¶ï¼Œè¯·ç¡®ä¿æŒ‡å®šçš„æ–‡ä»¶è·¯å¾„æ­£ç¡®ï¼Œå¹¶ä¸”è¯¥æ–‡ä»¶åœ¨gitä»“åº“ä¸­å­˜åœ¨ï¼Œä»¥é¿å…KeyErrorå¼‚å¸¸ã€‚

**Output Example**: å¯èƒ½çš„è¿”å›å€¼ç¤ºä¾‹ä¸ºï¼š
```
(
    "def new_function():\n    pass\n", 
    "def old_function():\n    pass\n"
)
```
***
### FunctionDef get_end_lineno(self, node)
**get_end_lineno**: get_end_linenoçš„åŠŸèƒ½æ˜¯è·å–ç»™å®šèŠ‚ç‚¹çš„ç»“æŸè¡Œå·ã€‚

**parameters**: æ­¤å‡½æ•°çš„å‚æ•°ã€‚
Â· parameter1: node - è¦æŸ¥æ‰¾ç»“æŸè¡Œå·çš„èŠ‚ç‚¹ã€‚

**Code Description**: get_end_linenoå‡½æ•°ç”¨äºè·å–ASTï¼ˆæŠ½è±¡è¯­æ³•æ ‘ï¼‰èŠ‚ç‚¹çš„ç»“æŸè¡Œå·ã€‚é¦–å…ˆï¼Œè¯¥å‡½æ•°æ£€æŸ¥ä¼ å…¥çš„èŠ‚ç‚¹æ˜¯å¦å…·æœ‰è¡Œå·å±æ€§ã€‚å¦‚æœèŠ‚ç‚¹æ²¡æœ‰è¡Œå·ï¼Œåˆ™è¿”å›-1ï¼Œè¡¨ç¤ºè¯¥èŠ‚ç‚¹æ²¡æœ‰æœ‰æ•ˆçš„è¡Œå·ã€‚æ¥ä¸‹æ¥ï¼Œå‡½æ•°åˆå§‹åŒ–ä¸€ä¸ªå˜é‡end_linenoä¸ºèŠ‚ç‚¹çš„è¡Œå·ï¼Œå¹¶éå†è¯¥èŠ‚ç‚¹çš„æ‰€æœ‰å­èŠ‚ç‚¹ã€‚å¯¹äºæ¯ä¸ªå­èŠ‚ç‚¹ï¼Œå‡½æ•°å°è¯•è·å–å…¶ç»“æŸè¡Œå·ï¼Œå¦‚æœå­èŠ‚ç‚¹æ²¡æœ‰ç»“æŸè¡Œå·ï¼Œåˆ™é€’å½’è°ƒç”¨get_end_linenoå‡½æ•°æ¥è·å–å…¶ç»“æŸè¡Œå·ã€‚åªæœ‰å½“å­èŠ‚ç‚¹çš„ç»“æŸè¡Œå·æœ‰æ•ˆæ—¶ï¼Œend_linenoæ‰ä¼šè¢«æ›´æ–°ä¸ºå­èŠ‚ç‚¹çš„ç»“æŸè¡Œå·å’Œå½“å‰èŠ‚ç‚¹çš„ç»“æŸè¡Œå·ä¸­çš„è¾ƒå¤§å€¼ã€‚æœ€ç»ˆï¼Œå‡½æ•°è¿”å›è®¡ç®—å¾—åˆ°çš„ç»“æŸè¡Œå·ã€‚

è¯¥å‡½æ•°åœ¨get_functions_and_classeså‡½æ•°ä¸­è¢«è°ƒç”¨ï¼Œç”¨äºè·å–æ¯ä¸ªå‡½æ•°æˆ–ç±»èŠ‚ç‚¹çš„ç»“æŸè¡Œå·ã€‚get_functions_and_classeså‡½æ•°è§£ææ•´ä¸ªä»£ç å†…å®¹ï¼Œéå†ASTæ ‘ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹ï¼Œå¹¶å°†æ¯ä¸ªå‡½æ•°å’Œç±»çš„ç›¸å…³ä¿¡æ¯ï¼ˆåŒ…æ‹¬å¼€å§‹è¡Œå·å’Œç»“æŸè¡Œå·ï¼‰æ”¶é›†åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­ã€‚é€šè¿‡è°ƒç”¨get_end_linenoï¼Œget_functions_and_classesèƒ½å¤Ÿå‡†ç¡®åœ°è·å–æ¯ä¸ªèŠ‚ç‚¹çš„ç»“æŸè¡Œå·ï¼Œä»è€Œæä¾›æ›´å®Œæ•´çš„èŠ‚ç‚¹ä¿¡æ¯ã€‚

**Note**: ä½¿ç”¨æ­¤ä»£ç æ—¶ï¼Œè¯·ç¡®ä¿ä¼ å…¥çš„èŠ‚ç‚¹æ˜¯æœ‰æ•ˆçš„ASTèŠ‚ç‚¹ï¼Œå¹¶ä¸”å…·æœ‰ç›¸åº”çš„è¡Œå·å±æ€§ï¼Œä»¥é¿å…è¿”å›-1çš„æƒ…å†µã€‚

**Output Example**: å‡è®¾ä¼ å…¥çš„èŠ‚ç‚¹çš„è¡Œå·ä¸º10ï¼Œä¸”å…¶å­èŠ‚ç‚¹çš„ç»“æŸè¡Œå·ä¸º15ï¼Œåˆ™è¯¥å‡½æ•°çš„è¿”å›å€¼å°†ä¸º15ã€‚
***
### FunctionDef add_parent_references(self, node, parent)
**add_parent_references**: add_parent_referencesçš„åŠŸèƒ½æ˜¯ä¸ºæŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTï¼‰ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹æ·»åŠ çˆ¶å¼•ç”¨ã€‚

**parameters**: è¯¥å‡½æ•°çš„å‚æ•°å¦‚ä¸‹ï¼š
Â· parameter1: node - å½“å‰åœ¨ASTä¸­çš„èŠ‚ç‚¹ã€‚
Â· parameter2: parent - å½“å‰èŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹ï¼Œé»˜è®¤ä¸ºNoneã€‚

**Code Description**: add_parent_referenceså‡½æ•°ç”¨äºéå†ç»™å®šçš„æŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTï¼‰èŠ‚ç‚¹ï¼Œå¹¶ä¸ºæ¯ä¸ªèŠ‚ç‚¹æ·»åŠ ä¸€ä¸ªæŒ‡å‘å…¶çˆ¶èŠ‚ç‚¹çš„å¼•ç”¨ã€‚å‡½æ•°é¦–å…ˆé€šè¿‡ast.iter_child_nodes(node)è·å–å½“å‰èŠ‚ç‚¹çš„æ‰€æœ‰å­èŠ‚ç‚¹ï¼Œç„¶åå°†å½“å‰èŠ‚ç‚¹ï¼ˆnodeï¼‰èµ‹å€¼ç»™æ¯ä¸ªå­èŠ‚ç‚¹çš„parentå±æ€§ã€‚æ¥ç€ï¼Œå‡½æ•°é€’å½’è°ƒç”¨è‡ªèº«ä»¥å¤„ç†æ¯ä¸ªå­èŠ‚ç‚¹ï¼Œç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½èƒ½æ­£ç¡®åœ°å¼•ç”¨å…¶çˆ¶èŠ‚ç‚¹ã€‚

è¯¥å‡½æ•°åœ¨get_functions_and_classesæ–¹æ³•ä¸­è¢«è°ƒç”¨ã€‚get_functions_and_classesçš„ä¸»è¦åŠŸèƒ½æ˜¯è§£æç»™å®šçš„ä»£ç å†…å®¹ï¼Œæå–å‡ºæ‰€æœ‰å‡½æ•°å’Œç±»åŠå…¶å‚æ•°ï¼Œå¹¶å»ºç«‹å®ƒä»¬ä¹‹é—´çš„å±‚çº§å…³ç³»ã€‚åœ¨è§£æASTæ ‘æ—¶ï¼Œé¦–å…ˆè°ƒç”¨add_parent_referenceså‡½æ•°ï¼Œä»¥ç¡®ä¿æ¯ä¸ªèŠ‚ç‚¹éƒ½èƒ½è®¿é—®åˆ°å…¶çˆ¶èŠ‚ç‚¹çš„ä¿¡æ¯ï¼Œè¿™å¯¹äºåç»­çš„å±‚çº§å…³ç³»åˆ†æè‡³å…³é‡è¦ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œget_functions_and_classesèƒ½å¤Ÿå‡†ç¡®åœ°æ„å»ºå‡ºå‡½æ•°å’Œç±»çš„å±‚çº§ç»“æ„ï¼Œæä¾›æ›´æ¸…æ™°çš„ä»£ç è§£æç»“æœã€‚

**Note**: ä½¿ç”¨è¯¥å‡½æ•°æ—¶ï¼Œè¯·ç¡®ä¿ä¼ å…¥çš„èŠ‚ç‚¹æ˜¯æœ‰æ•ˆçš„ASTèŠ‚ç‚¹ï¼Œå¹¶æ³¨æ„åœ¨é€’å½’è°ƒç”¨æ—¶å¯èƒ½å¯¼è‡´çš„æ ˆæº¢å‡ºé—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†æ·±å±‚åµŒå¥—çš„ASTæ—¶ã€‚
***
### FunctionDef get_functions_and_classes(self, code_content)
**get_functions_and_classes**: get_functions_and_classesçš„åŠŸèƒ½æ˜¯æ£€ç´¢æ‰€æœ‰å‡½æ•°ã€ç±»åŠå…¶å‚æ•°ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ä»¥åŠå®ƒä»¬çš„å±‚çº§å…³ç³»ã€‚

**parameters**: æ­¤å‡½æ•°çš„å‚æ•°å¦‚ä¸‹ï¼š
Â· parameter1: code_content - è¦è§£æçš„æ•´ä¸ªæ–‡ä»¶çš„ä»£ç å†…å®¹ã€‚

**Code Description**: get_functions_and_classeså‡½æ•°ç”¨äºè§£æç»™å®šçš„ä»£ç å†…å®¹ï¼Œæå–å‡ºæ‰€æœ‰å‡½æ•°å’Œç±»çš„ç›¸å…³ä¿¡æ¯ï¼ŒåŒ…æ‹¬å®ƒä»¬çš„åç§°ã€èµ·å§‹è¡Œå·ã€ç»“æŸè¡Œå·ã€çˆ¶èŠ‚ç‚¹åç§°ä»¥åŠå‚æ•°åˆ—è¡¨ã€‚è¯¥å‡½æ•°é¦–å…ˆä½¿ç”¨ast.parseå°†ä»£ç å†…å®¹è½¬æ¢ä¸ºæŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTï¼‰ï¼Œç„¶åè°ƒç”¨add_parent_referenceså‡½æ•°ä¸ºæ¯ä¸ªèŠ‚ç‚¹æ·»åŠ çˆ¶å¼•ç”¨ï¼Œä»¥ä¾¿åç»­åˆ†ææ—¶èƒ½å¤Ÿè®¿é—®åˆ°çˆ¶èŠ‚ç‚¹çš„ä¿¡æ¯ã€‚

æ¥ä¸‹æ¥ï¼Œå‡½æ•°éå†ASTæ ‘ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹ï¼Œæ£€æŸ¥æ¯ä¸ªèŠ‚ç‚¹æ˜¯å¦ä¸ºå‡½æ•°å®šä¹‰ï¼ˆFunctionDefï¼‰ã€ç±»å®šä¹‰ï¼ˆClassDefï¼‰æˆ–å¼‚æ­¥å‡½æ•°å®šä¹‰ï¼ˆAsyncFunctionDefï¼‰ã€‚å¯¹äºæ¯ä¸ªç¬¦åˆæ¡ä»¶çš„èŠ‚ç‚¹ï¼Œå‡½æ•°è·å–å…¶èµ·å§‹è¡Œå·å’Œç»“æŸè¡Œå·ï¼Œå¹¶æå–å‚æ•°åˆ—è¡¨ã€‚æœ€ç»ˆï¼Œæ‰€æœ‰æ”¶é›†åˆ°çš„ä¿¡æ¯ä»¥å…ƒç»„çš„å½¢å¼å­˜å‚¨åœ¨ä¸€ä¸ªåˆ—è¡¨ä¸­å¹¶è¿”å›ã€‚

è¯¥å‡½æ•°åœ¨å¤šä¸ªåœ°æ–¹è¢«è°ƒç”¨ï¼Œä¾‹å¦‚åœ¨generate_file_structureå‡½æ•°ä¸­ç”¨äºç”Ÿæˆæ–‡ä»¶ç»“æ„æ—¶ï¼Œå’Œåœ¨add_new_itemå‡½æ•°ä¸­ç”¨äºå¤„ç†æ–°å¢é¡¹ç›®æ—¶ã€‚é€šè¿‡è°ƒç”¨get_functions_and_classesï¼Œå…¶ä»–å‡½æ•°èƒ½å¤Ÿè·å–åˆ°ä»£ç ä¸­çš„ç»“æ„ä¿¡æ¯ï¼Œä»è€Œè¿›è¡Œè¿›ä¸€æ­¥çš„å¤„ç†å’Œæ–‡æ¡£ç”Ÿæˆã€‚

**Note**: ä½¿ç”¨æ­¤å‡½æ•°æ—¶ï¼Œè¯·ç¡®ä¿ä¼ å…¥çš„ä»£ç å†…å®¹æ˜¯æœ‰æ•ˆçš„Pythonä»£ç ï¼Œä»¥ä¾¿èƒ½å¤Ÿæ­£ç¡®è§£æASTå¹¶æå–ä¿¡æ¯ã€‚

**Output Example**: å‡è®¾ä¼ å…¥çš„ä»£ç å†…å®¹åŒ…å«ä»¥ä¸‹å‡½æ•°å’Œç±»å®šä¹‰ï¼Œå‡½æ•°çš„è¿”å›å€¼å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š
[
    ('FunctionDef', 'AI_give_params', 86, 95, None, ['param1', 'param2']),
    ('ClassDef', 'PipelineEngine', 97, 104, None, []),
    ('FunctionDef', 'get_all_pys', 99, 104, 'PipelineEngine', ['param1'])
]
***
### FunctionDef generate_file_structure(self, file_path)
**generate_file_structure**: generate_file_structureçš„åŠŸèƒ½æ˜¯ç”Ÿæˆç»™å®šæ–‡ä»¶è·¯å¾„çš„æ–‡ä»¶ç»“æ„ã€‚

**parameters**: æ­¤å‡½æ•°çš„å‚æ•°å¦‚ä¸‹ï¼š
Â· parameter1: file_path (str): æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„ã€‚

**Code Description**: generate_file_structureå‡½æ•°ç”¨äºç”ŸæˆæŒ‡å®šæ–‡ä»¶è·¯å¾„çš„æ–‡ä»¶ç»“æ„ä¿¡æ¯ã€‚è¯¥å‡½æ•°é¦–å…ˆæ‰“å¼€æŒ‡å®šè·¯å¾„çš„æ–‡ä»¶ï¼Œå¹¶è¯»å–å…¶å†…å®¹ã€‚æ¥ç€ï¼Œå®ƒè°ƒç”¨get_functions_and_classesæ–¹æ³•æ¥è§£ææ–‡ä»¶å†…å®¹ï¼Œæå–å‡ºæ‰€æœ‰å‡½æ•°å’Œç±»çš„ç›¸å…³ä¿¡æ¯ï¼ŒåŒ…æ‹¬å®ƒä»¬çš„åç§°ã€èµ·å§‹è¡Œå·ã€ç»“æŸè¡Œå·åŠå‚æ•°åˆ—è¡¨ã€‚è§£æå¾—åˆ°çš„ç»“æ„ä¿¡æ¯ä»¥å…ƒç»„çš„å½¢å¼å­˜å‚¨åœ¨ä¸€ä¸ªåˆ—è¡¨ä¸­ã€‚

åœ¨è·å–åˆ°æ‰€æœ‰ç»“æ„ä¿¡æ¯åï¼Œå‡½æ•°ä¼šéå†è¿™äº›ä¿¡æ¯ï¼Œå¹¶è°ƒç”¨get_obj_code_infoæ–¹æ³•æ¥è·å–æ¯ä¸ªå¯¹è±¡çš„è¯¦ç»†ä»£ç ä¿¡æ¯ï¼ŒåŒ…æ‹¬å¯¹è±¡çš„ç±»å‹ã€åç§°ã€èµ·å§‹å’Œç»“æŸè¡Œå·ã€å‚æ•°ç­‰ã€‚æœ€ç»ˆï¼Œæ‰€æœ‰æ”¶é›†åˆ°çš„å¯¹è±¡ä¿¡æ¯ä»¥åˆ—è¡¨çš„å½¢å¼è¿”å›ã€‚

è¯¥å‡½æ•°è¢«generate_overall_structureå‡½æ•°è°ƒç”¨ï¼Œç”¨äºç”Ÿæˆç›®æ ‡ä»“åº“ä¸­æ‰€æœ‰æ–‡ä»¶çš„ç»“æ„ä¿¡æ¯ã€‚generate_overall_structureå‡½æ•°ä¼šéå†æ‰€æœ‰æœªè¢«å¿½ç•¥çš„æ–‡ä»¶ï¼Œå¹¶å¯¹æ¯ä¸ªæ–‡ä»¶è°ƒç”¨generate_file_structureï¼Œä»¥è·å–å…¶ç»“æ„ä¿¡æ¯å¹¶å­˜å‚¨åœ¨repo_structureå­—å…¸ä¸­ã€‚

**Note**: ä½¿ç”¨æ­¤å‡½æ•°æ—¶ï¼Œè¯·ç¡®ä¿ä¼ å…¥çš„æ–‡ä»¶è·¯å¾„æ˜¯æœ‰æ•ˆçš„ï¼Œå¹¶ä¸”æ–‡ä»¶å†…å®¹æ˜¯æœ‰æ•ˆçš„Pythonä»£ç ï¼Œä»¥ä¾¿èƒ½å¤Ÿæ­£ç¡®è§£æå¹¶æå–ä¿¡æ¯ã€‚

**Output Example**: å‡è®¾ä¼ å…¥çš„æ–‡ä»¶è·¯å¾„å¯¹åº”çš„æ–‡ä»¶å†…å®¹åŒ…å«ä»¥ä¸‹å‡½æ•°å’Œç±»å®šä¹‰ï¼Œå‡½æ•°çš„è¿”å›å€¼å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š
[
    {
        "function_name": {
            "type": "function",
            "start_line": 10,
            "end_line": 20,
            "parent": "class_name"
        },
        "class_name": {
            "type": "class",
            "start_line": 5,
            "end_line": 25,
            "parent": None
        }
    }
]
***
### FunctionDef generate_overall_structure(self, file_path_reflections, jump_files)
**generate_overall_structure**: The function of generate_overall_structure is to retrieve the file structure of a target repository by analyzing its contents while excluding certain files based on specified criteria.

**parameters**: The parameters of this Function.
Â· parameter1: file_path_reflections (dict) - A dictionary mapping original file paths to their reflections, used to identify files that may have been renamed or moved.
Â· parameter2: jump_files (list) - A list of file names that should be ignored during the processing, as they are not to be parsed.

**Code Description**: The generate_overall_structure method is designed to construct a comprehensive representation of the file structure within a specified repository. It begins by initializing an empty dictionary called repo_structure, which will ultimately hold the file paths and their corresponding structures.

The method instantiates a GitignoreChecker object, which is responsible for checking the repository directory against patterns defined in a .gitignore file. This checker is crucial for filtering out files and folders that should be ignored based on the project's version control settings.

The method then utilizes the tqdm library to create a progress bar that reflects the ongoing process of checking files and folders. It iterates over the list of non-ignored files provided by the GitignoreChecker's check_files_and_folders method. For each file, the following checks are performed:

1. If the file is present in the jump_files list, it is skipped, and a message is printed to indicate that the file will not be processed.
2. If the file name ends with a specific substring indicating a "latest version," it is also skipped, with a corresponding message printed to the console.

If the file passes these checks, the method attempts to generate its structure by calling the generate_file_structure method, passing the file name as an argument. If an error occurs during this process, it is logged, and the method continues to the next file.

The progress bar is updated to reflect the current file being processed, and once all files have been evaluated, the method returns the repo_structure dictionary, which contains the paths of the files and their respective structures.

This method is integral to the FileHandler class, as it consolidates the information about the repository's file structure while adhering to the rules defined in the .gitignore file and respecting the files specified in the jump_files list.

**Note**: It is essential to ensure that the .gitignore file is correctly formatted and accessible to avoid unintended exclusions of files. Additionally, the jump_files list should be accurately populated to ensure that the intended files are ignored during processing.

**Output Example**: An example output of the generate_overall_structure method might look like this:
```
{
    "src/module1.py": { ... },  # Structure of module1.py
    "src/module2.py": { ... },  # Structure of module2.py
    "tests/test_module1.py": { ... }  # Structure of test_module1.py
}
```
This output indicates that the method has successfully generated the structures for the specified files, with each file path mapped to its corresponding structure representation.
***
### FunctionDef convert_to_markdown_file(self, file_path)
**convert_to_markdown_file**: The function of convert_to_markdown_file is to convert the content of a specified file into markdown format.

**parameters**: The parameters of this Function.
Â· file_path: (str, optional) The relative path of the file to be converted. If not provided, the default file path will be used.

**Code Description**: The convert_to_markdown_file function is designed to read a file's metadata from a JSON structure and convert it into a markdown representation. The function begins by opening a JSON file that contains the project hierarchy, which is expected to be structured in a way that associates file paths with their corresponding metadata. If the file_path parameter is not provided, the function defaults to using an internal file path attribute.

The function retrieves the relevant file object from the loaded JSON data using the specified or default file path. If no matching file object is found, it raises a ValueError, indicating that the specified file path does not exist in the project hierarchy.

Once the file object is successfully located, the function initializes an empty string to accumulate the markdown content. It sorts the objects associated with the file based on their starting line numbers in the code. The function then constructs a parent-child relationship mapping for the objects, which is crucial for determining the hierarchy levels in the markdown output.

For each object, the function calculates its level in the hierarchy by traversing the parent dictionary. It constructs the markdown string by appending the object's type, name, and parameters, formatted according to its level. The markdown content includes the last piece of markdown content associated with the object, if available. Finally, the function appends a closing separator to the markdown string and returns the complete markdown representation.

**Note**: It is important to ensure that the project_hierarchy.json file is correctly formatted and accessible, as the function relies on this data to perform its operations. Additionally, the function expects the objects within the JSON to have specific attributes such as "type", "name", "params", and "md_content" for proper markdown generation.

**Output Example**: 
A possible appearance of the code's return value could be:
```
# FunctionDef my_function(param1, param2):
This function does something important.

# AsyncFunctionDef my_async_function():
This async function handles asynchronous operations.

***
```
***



================================================
FILE: markdown_docs/repo_agent/log.md
================================================
## ClassDef InterceptHandler
**InterceptHandler**: The function of InterceptHandler is to redirect standard logging output to the Loguru logging system.

**attributes**: The attributes of this Class.
Â· record: logging.LogRecord - This parameter represents the log record containing all the information pertinent to the event being logged.

**Code Description**: The InterceptHandler class extends the logging.Handler class to facilitate the integration of Python's standard logging module with the Loguru logging system. The primary method of this class is `emit`, which is responsible for processing log records. 

When a log record is received, the `emit` method first attempts to map the standard logging level (e.g., DEBUG, INFO, WARNING) to the corresponding Loguru level. If the mapping fails, it defaults to using the numeric level of the log record. This ensures that all log messages are appropriately categorized according to their severity.

Next, the method identifies the caller of the log message by traversing the call stack. It uses the `inspect.currentframe()` function to obtain the current stack frame and iterates through the frames to find the origin of the log message. This is particularly useful for debugging, as it provides context about where the log message was generated.

Finally, the method logs the message using Loguru's logging capabilities, including any exception information if present. The `logger.opt()` method is utilized to set the depth of the stack trace and to include exception details, ensuring that the log output is informative and relevant.

The InterceptHandler is specifically invoked within the `set_logger_level_from_config` function. This function configures the Loguru logger with a specified log level and integrates it with the standard logging module. By calling `logging.basicConfig()` with an instance of InterceptHandler, it effectively redirects all standard logging output to Loguru, allowing for a unified logging approach across the application. This integration is crucial for maintaining consistent logging behavior, especially in applications that utilize both standard logging and Loguru.

**Note**: When using the InterceptHandler, it is important to ensure that the logging configuration is set up correctly to avoid conflicts between standard logging and Loguru. Additionally, developers should be aware of the performance implications of logging, particularly in multi-threaded environments, where the `enqueue=True` option in Loguru can help manage log messages safely.
### FunctionDef emit(self, record)
**emit**: emitå‡½æ•°çš„åŠŸèƒ½æ˜¯å°†æ—¥å¿—è®°å½•å‘é€åˆ°Loguruæ—¥å¿—ç³»ç»Ÿã€‚

**parameters**: è¯¥å‡½æ•°çš„å‚æ•°ã€‚
Â· record: logging.LogRecord - åŒ…å«æ—¥å¿—è®°å½•ä¿¡æ¯çš„å¯¹è±¡ã€‚

**Code Description**: emitå‡½æ•°é¦–å…ˆå°è¯•è·å–ä¸ä¼ å…¥çš„æ—¥å¿—è®°å½•çš„çº§åˆ«ç›¸å¯¹åº”çš„Loguruçº§åˆ«ã€‚å¦‚æœæˆåŠŸï¼Œåˆ™ä½¿ç”¨è¯¥çº§åˆ«ï¼›å¦‚æœå¤±è´¥ï¼Œåˆ™ä½¿ç”¨è®°å½•çš„çº§åˆ«å·ã€‚æ¥ç€ï¼Œå‡½æ•°é€šè¿‡inspectæ¨¡å—è·å–å½“å‰è°ƒç”¨æ ˆçš„å¸§ä¿¡æ¯ï¼Œä»¥ç¡®å®šæ—¥å¿—æ¶ˆæ¯çš„æ¥æºã€‚å®ƒä¼šéå†è°ƒç”¨æ ˆï¼Œç›´åˆ°æ‰¾åˆ°ä¸€ä¸ªéloggingæ¨¡å—çš„å¸§ï¼Œä»è€Œç¡®å®šæ—¥å¿—æ¶ˆæ¯çš„æ·±åº¦ã€‚æœ€åï¼Œä½¿ç”¨Loguruçš„loggerå¯¹è±¡ï¼Œç»“åˆæ·±åº¦å’Œå¼‚å¸¸ä¿¡æ¯ï¼Œè®°å½•æ—¥å¿—æ¶ˆæ¯ã€‚

å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š
1. ä½¿ç”¨logger.levelæ–¹æ³•è·å–ä¸record.levelnameå¯¹åº”çš„Loguruçº§åˆ«åç§°ã€‚å¦‚æœè¯¥çº§åˆ«ä¸å­˜åœ¨ï¼Œåˆ™ä½¿ç”¨record.levelnoä½œä¸ºçº§åˆ«ã€‚
2. é€šè¿‡inspect.currentframe()è·å–å½“å‰å¸§ï¼Œå¹¶åˆå§‹åŒ–æ·±åº¦ä¸º0ã€‚ç„¶åï¼Œä½¿ç”¨whileå¾ªç¯éå†è°ƒç”¨æ ˆï¼Œç›´åˆ°æ‰¾åˆ°ä¸€ä¸ªéloggingæ¨¡å—çš„å¸§ã€‚
3. ä½¿ç”¨logger.optæ–¹æ³•è®°å½•æ—¥å¿—ï¼Œä¼ å…¥æ·±åº¦å’Œå¼‚å¸¸ä¿¡æ¯ï¼Œå¹¶è°ƒç”¨record.getMessage()è·å–æ—¥å¿—æ¶ˆæ¯çš„å†…å®¹ã€‚

**Note**: ä½¿ç”¨è¯¥å‡½æ•°æ—¶ï¼Œè¯·ç¡®ä¿ä¼ å…¥çš„recordå¯¹è±¡æ˜¯æœ‰æ•ˆçš„logging.LogRecordå®ä¾‹ï¼Œä»¥é¿å…æ½œåœ¨çš„é”™è¯¯ã€‚åŒæ—¶ï¼Œç¡®ä¿Loguruåº“å·²æ­£ç¡®é…ç½®ï¼Œä»¥ä¾¿èƒ½å¤Ÿå¤„ç†æ—¥å¿—è®°å½•ã€‚
***
## FunctionDef set_logger_level_from_config(log_level)
**set_logger_level_from_config**: The function of set_logger_level_from_config is to configure the loguru logger with a specified log level and integrate it with the standard logging module.

**parameters**: The parameters of this Function.
Â· log_level: str - The log level to set for loguru (e.g., "DEBUG", "INFO", "WARNING").

**Code Description**: The set_logger_level_from_config function is designed to set the logging level for the loguru logger based on the provided log_level argument. It begins by removing any existing loguru handlers to ensure that there are no conflicts or duplications in logging output. Following this, it adds a new handler to the loguru logger that directs output to stderr at the specified log level. The parameters `enqueue=True`, `backtrace=False`, and `diagnose=False` are used to ensure that logging is thread-safe, minimizes detailed traceback information, and suppresses additional diagnostic information, respectively.

Additionally, the function redirects the standard logging output to the loguru logger by utilizing the InterceptHandler class. This integration allows loguru to handle all logging consistently across the application, which is particularly useful in scenarios where both standard logging and loguru are used. The function concludes by logging a success message indicating that the log level has been set.

The set_logger_level_from_config function is called within the run function located in the repo_agent/main.py file. In this context, it retrieves the logging configuration from the SettingsManager and applies it by calling set_logger_level_from_config with the appropriate log level. This ensures that the logging configuration is established before any tasks are executed, allowing for consistent logging behavior throughout the application.

**Note**: When using the set_logger_level_from_config function, it is essential to ensure that the logging configuration is correctly set up to avoid conflicts between standard logging and loguru. Developers should also consider the implications of logging performance, especially in multi-threaded environments, where the `enqueue=True` option can help manage log messages safely.



================================================
FILE: markdown_docs/repo_agent/main.md
================================================
## FunctionDef cli
**cli**: cliå‡½æ•°çš„åŠŸèƒ½æ˜¯ä¸ºåŸºäºLLMçš„æ¡†æ¶æä¾›ä»“åº“çº§ä»£ç æ–‡æ¡£ç”Ÿæˆã€‚

**parameters**: è¯¥å‡½æ•°æ²¡æœ‰å‚æ•°ã€‚

**Code Description**: cliå‡½æ•°æ˜¯ä¸€ä¸ªç©ºå‡½æ•°ï¼Œå½“å‰æ²¡æœ‰å®ç°ä»»ä½•å…·ä½“çš„åŠŸèƒ½ã€‚æ ¹æ®å…¶æ–‡æ¡£å­—ç¬¦ä¸²ï¼Œcliçš„ç›®çš„æ˜¯ä¸ºä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„æ¡†æ¶æä¾›ä»“åº“çº§åˆ«çš„ä»£ç æ–‡æ¡£ç”Ÿæˆã€‚è¿™è¡¨æ˜è¯¥å‡½æ•°å¯èƒ½æ˜¯æœªæ¥æ‰©å±•çš„åŸºç¡€ï¼Œæ—¨åœ¨å¤„ç†ä¸ä»£ç æ–‡æ¡£ç”Ÿæˆç›¸å…³çš„ä»»åŠ¡ã€‚

åœ¨é¡¹ç›®ç»“æ„ä¸­ï¼Œcliå‡½æ•°è¢«è°ƒç”¨äºrepo_agent/__main__.pyæ–‡ä»¶ä¸­ã€‚è™½ç„¶åœ¨__main__.pyä¸­æ²¡æœ‰æä¾›å…·ä½“çš„è°ƒç”¨ä»£ç ï¼Œä½†é€šå¸¸æƒ…å†µä¸‹ï¼Œ__main__.pyæ–‡ä»¶æ˜¯Pythonç¨‹åºçš„å…¥å£ç‚¹ï¼Œcliå‡½æ•°å¯èƒ½ä¼šåœ¨ç¨‹åºå¯åŠ¨æ—¶è¢«è°ƒç”¨ï¼Œä»¥åˆå§‹åŒ–æˆ–é…ç½®æ–‡æ¡£ç”Ÿæˆçš„ç›¸å…³åŠŸèƒ½ã€‚

**Note**: ç”±äºcliå‡½æ•°ç›®å‰æœªå®ç°ä»»ä½•åŠŸèƒ½ï¼Œå¼€å‘è€…åœ¨ä½¿ç”¨æ—¶åº”æ³¨æ„è¯¥å‡½æ•°å°šæœªå®Œæˆï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥çš„å¼€å‘å’Œå®ç°æ‰èƒ½è¾¾åˆ°é¢„æœŸçš„æ–‡æ¡£ç”Ÿæˆæ•ˆæœã€‚
## FunctionDef handle_setting_error(e)
**handle_setting_error**: handle_setting_errorçš„åŠŸèƒ½æ˜¯å¤„ç†è®¾ç½®ä¸­çš„é…ç½®é”™è¯¯ã€‚

**parameters**: è¯¥å‡½æ•°çš„å‚æ•°ã€‚
Â· e: ValidationError - è¡¨ç¤ºéªŒè¯é”™è¯¯çš„å¼‚å¸¸å¯¹è±¡ï¼ŒåŒ…å«æœ‰å…³é…ç½®é”™è¯¯çš„è¯¦ç»†ä¿¡æ¯ã€‚

**Code Description**: handle_setting_errorå‡½æ•°ç”¨äºå¤„ç†åœ¨ç¨‹åºè¿è¡Œè¿‡ç¨‹ä¸­é‡åˆ°çš„é…ç½®é”™è¯¯ã€‚å½“ç¨‹åºå°è¯•è·å–è®¾ç½®æ—¶ï¼Œå¦‚æœå‡ºç°ValidationErrorå¼‚å¸¸ï¼Œè¯¥å‡½æ•°å°†è¢«è°ƒç”¨ã€‚å‡½æ•°é¦–å…ˆé€šè¿‡clickåº“æ‰“å°ä¸€æ¡é€šç”¨çš„é”™è¯¯æ¶ˆæ¯ï¼Œæç¤ºç”¨æˆ·æ£€æŸ¥å…¶è®¾ç½®ã€‚æ¥ç€ï¼Œå‡½æ•°éå†ValidationErrorå¯¹è±¡ä¸­çš„é”™è¯¯ä¿¡æ¯ï¼Œé’ˆå¯¹æ¯ä¸ªé”™è¯¯è¾“å‡ºæ›´è¯¦ç»†çš„å­—æ®µç¼ºå¤±ä¿¡æ¯ï¼Œå¹¶ä½¿ç”¨ä¸åŒçš„é¢œè‰²è¿›è¡ŒåŒºåˆ†ã€‚

å¦‚æœé”™è¯¯ç±»å‹ä¸ºâ€œmissingâ€ï¼Œå‡½æ•°ä¼šæç¤ºç”¨æˆ·ç¼ºå°‘å¿…éœ€çš„å­—æ®µï¼Œå¹¶å»ºè®®è®¾ç½®ç›¸åº”çš„ç¯å¢ƒå˜é‡ï¼›å¦‚æœæ˜¯å…¶ä»–ç±»å‹çš„é”™è¯¯ï¼Œåˆ™ç›´æ¥è¾“å‡ºé”™è¯¯æ¶ˆæ¯ã€‚æœ€åï¼Œå‡½æ•°é€šè¿‡æŠ›å‡ºclick.ClickExceptionä¼˜é›…åœ°ç»ˆæ­¢ç¨‹åºï¼Œå¹¶æ˜¾ç¤ºä¸€æ¡ç»ˆæ­¢ç¨‹åºçš„é”™è¯¯æ¶ˆæ¯ã€‚

åœ¨é¡¹ç›®ä¸­ï¼Œhandle_setting_errorå‡½æ•°è¢«å¤šä¸ªå‡½æ•°è°ƒç”¨ï¼ŒåŒ…æ‹¬runã€print_hierarchyå’Œdiffã€‚è¿™äº›å‡½æ•°åœ¨å°è¯•è·å–è®¾ç½®æ—¶ï¼Œå¦‚æœé‡åˆ°ValidationErrorå¼‚å¸¸ï¼Œéƒ½ä¼šè°ƒç”¨handle_setting_erroræ¥å¤„ç†é”™è¯¯å¹¶è¾“å‡ºç›¸å…³ä¿¡æ¯ï¼Œä»è€Œç¡®ä¿ç”¨æˆ·èƒ½å¤ŸåŠæ—¶äº†è§£é…ç½®é—®é¢˜å¹¶è¿›è¡Œä¿®æ­£ã€‚

**Note**: ä½¿ç”¨è¯¥å‡½æ•°æ—¶ï¼Œè¯·ç¡®ä¿ä¼ å…¥çš„å‚æ•°æ˜¯ValidationErrorç±»å‹çš„å¼‚å¸¸å¯¹è±¡ï¼Œä»¥ä¾¿æ­£ç¡®å¤„ç†å’Œè¾“å‡ºé”™è¯¯ä¿¡æ¯ã€‚
## FunctionDef run
Doc is waiting to be generated...
## FunctionDef clean
**clean**: The function of clean is to remove the fake files generated by the documentation process.

**parameters**: The parameters of this Function.
Â· No parameters are required for this function.

**Code Description**: The clean function is designed to facilitate the cleanup of temporary files, referred to as "fake files," that are created during the documentation generation process. This function achieves its purpose by invoking the delete_fake_files function, which is responsible for identifying and removing these temporary files.

When the clean function is called, it executes the delete_fake_files function, which performs a thorough search through the project's directory structure to locate and delete any files that match specific criteria indicative of temporary files. Upon successful completion of the deletion process, the clean function logs a success message indicating that the fake files have been cleaned up.

The delete_fake_files function operates by first retrieving the project settings through the SettingsManager's get_setting method. It then utilizes a nested helper function, gci, to recursively traverse the specified directory. The gci function checks each file and directory, identifying those that are temporary based on their naming conventions. If a temporary file is found, it either deletes it if it is empty or renames it back to its original name if it contains content.

The clean function is crucial in ensuring that the workspace remains free of unnecessary files after documentation tasks are completed. It is typically called at the end of the documentation process to maintain an organized project structure.

**Note**: It is important to ensure that the project settings are correctly configured and that the target repository is accessible before invoking the clean function. Any issues related to file permissions or incorrect paths may lead to errors during the cleanup process.
## FunctionDef print_hierarchy
Doc is waiting to be generated...
## FunctionDef diff
Doc is waiting to be generated...



================================================
FILE: markdown_docs/repo_agent/multi_task_dispatch.md
================================================
## ClassDef Task
**Task**: The function of Task is to represent a unit of work with its dependencies and status.

**attributes**: The attributes of this Class.
Â· task_id: An integer that uniquely identifies the task.
Â· dependencies: A list of Task objects that this task depends on.
Â· extra_info: Any additional information associated with the task, which can be of any type.
Â· status: An integer representing the current status of the task (0 for not started, 1 for in progress, 2 for completed, 3 for error).

**Code Description**: The Task class is designed to encapsulate the concept of a task within a multi-tasking framework. Each Task object is initialized with a unique identifier (task_id), a list of dependencies that must be completed before this task can start, and optional extra information that can provide context or metadata about the task. The status attribute tracks the current state of the task, allowing for management and monitoring of its progress.

The Task class is utilized within the MultiTaskDispatch system, where it plays a crucial role in task management. Specifically, the TaskManager class, which is responsible for managing multiple tasks, creates instances of the Task class when new tasks are added. The add_task method in TaskManager takes a list of dependency task IDs and creates a new Task object, linking it to its dependencies. This relationship ensures that tasks are executed in the correct order based on their dependencies.

Furthermore, the Task class is referenced in the print_task_list method of the MetaInfo class, which formats and displays a list of tasks along with their statuses and dependencies. This integration highlights the importance of the Task class in providing a structured way to manage and visualize tasks within the system.

**Note**: When using the Task class, it is important to ensure that the dependencies are properly managed to avoid circular dependencies, which could lead to errors in task execution. Additionally, the status attribute should be updated appropriately to reflect the current state of the task throughout its lifecycle.
### FunctionDef __init__(self, task_id, dependencies, extra_info)
**__init__**: The function of __init__ is åˆå§‹åŒ–ä»»åŠ¡å¯¹è±¡ã€‚

**parameters**: The parameters of this Function.
Â· parameter1: task_id (int) - ä»»åŠ¡çš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚
Â· parameter2: dependencies (List[Task]) - è¯¥ä»»åŠ¡æ‰€ä¾èµ–çš„å…¶ä»–ä»»åŠ¡åˆ—è¡¨ã€‚
Â· parameter3: extra_info (Any, å¯é€‰) - é¢å¤–çš„ä¿¡æ¯ï¼Œå¯ä»¥æ˜¯ä»»ä½•ç±»å‹çš„æ•°æ®ï¼Œé»˜è®¤ä¸ºNoneã€‚

**Code Description**: è¯¥__init__å‡½æ•°æ˜¯ä»»åŠ¡ç±»çš„æ„é€ å‡½æ•°ï¼Œç”¨äºåˆå§‹åŒ–ä»»åŠ¡å¯¹è±¡çš„åŸºæœ¬å±æ€§ã€‚é¦–å…ˆï¼Œå®ƒæ¥æ”¶ä¸€ä¸ªæ•´æ•°ç±»å‹çš„å‚æ•°task_idï¼Œç”¨äºå”¯ä¸€æ ‡è¯†è¯¥ä»»åŠ¡ã€‚æ¥ç€ï¼Œdependencieså‚æ•°æ˜¯ä¸€ä¸ªä»»åŠ¡å¯¹è±¡çš„åˆ—è¡¨ï¼Œè¡¨ç¤ºå½“å‰ä»»åŠ¡æ‰€ä¾èµ–çš„å…¶ä»–ä»»åŠ¡ï¼Œè¿™å¯¹äºä»»åŠ¡è°ƒåº¦å’Œæ‰§è¡Œé¡ºåºéå¸¸é‡è¦ã€‚extra_infoå‚æ•°æ˜¯ä¸€ä¸ªå¯é€‰å‚æ•°ï¼Œå¯ä»¥å­˜å‚¨ä¸ä»»åŠ¡ç›¸å…³çš„é¢å¤–ä¿¡æ¯ï¼Œé»˜è®¤ä¸ºNoneã€‚æœ€åï¼Œstatuså±æ€§è¢«åˆå§‹åŒ–ä¸º0ï¼Œè¡¨ç¤ºä»»åŠ¡çš„åˆå§‹çŠ¶æ€ä¸ºâ€œæœªå¼€å§‹â€ã€‚çŠ¶æ€å€¼çš„å®šä¹‰å¦‚ä¸‹ï¼š0è¡¨ç¤ºæœªå¼€å§‹ï¼Œ1è¡¨ç¤ºæ­£åœ¨è¿›è¡Œï¼Œ2è¡¨ç¤ºå·²ç»å®Œæˆï¼Œ3è¡¨ç¤ºå‡ºé”™äº†ã€‚

**Note**: åœ¨ä½¿ç”¨è¯¥æ„é€ å‡½æ•°æ—¶ï¼Œç¡®ä¿ä¼ å…¥çš„dependencieså‚æ•°æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ä»»åŠ¡åˆ—è¡¨ï¼Œä»¥é¿å…åœ¨åç»­ä»»åŠ¡è°ƒåº¦ä¸­å‡ºç°é”™è¯¯ã€‚åŒæ—¶ï¼Œtask_idåº”ä¿æŒå”¯ä¸€æ€§ï¼Œä»¥ç¡®ä¿ä»»åŠ¡çš„æ­£ç¡®è¯†åˆ«å’Œç®¡ç†ã€‚
***
## ClassDef TaskManager
**TaskManager**: The function of TaskManager is to manage and dispatch multiple tasks based on their dependencies.

**attributes**: The attributes of this Class.
Â· task_dict: A dictionary that maps task IDs to Task objects.  
Â· task_lock: A threading.Lock used for thread synchronization when accessing the task_dict.  
Â· now_id: An integer representing the current task ID.  
Â· query_id: An integer representing the current query ID.  
Â· sync_func: A placeholder for a synchronization function, initially set to None.  

**Code Description**: The TaskManager class is designed to facilitate the management of multiple tasks in a concurrent environment. It initializes with an empty task dictionary (task_dict) that will hold Task objects indexed by their unique IDs. The class employs a threading lock (task_lock) to ensure that access to the task dictionary is thread-safe, preventing race conditions when multiple threads attempt to modify the task list simultaneously.

The now_id attribute keeps track of the next available task ID, while query_id is used to track the number of queries made for tasks. The sync_func attribute is intended to hold a function that can be called for synchronization purposes, though it is not defined upon initialization.

The class provides several key methods:
- **all_success**: A property that checks if all tasks have been completed by verifying if the task dictionary is empty.
- **add_task**: This method allows the addition of a new task to the task dictionary. It takes a list of dependency task IDs and optional extra information. The method locks the task dictionary during the addition process to ensure thread safety, creates a new Task object, and increments the now_id for the next task.
- **get_next_task**: This method retrieves the next available task for a specified process ID. It checks the task dictionary for tasks that have no dependencies and are not currently in progress. If a task is found, it updates its status to indicate that it is now being processed and may call the sync_func every ten queries.
- **mark_completed**: This method marks a specified task as completed and removes it from the task dictionary. It also updates the dependencies of other tasks that may rely on the completed task.

The TaskManager class is utilized within the MetaInfo class in the repo_agent/doc_meta_info.py file. Specifically, it is called in the methods get_task_manager and get_topology. The get_task_manager method constructs a TaskManager instance and populates it with tasks based on the dependencies of document items in a hierarchical structure. The get_topology method orchestrates the overall process of calculating the topological order of all objects in a repository, leveraging the TaskManager to manage the tasks that arise from this calculation.

**Note**: When using the TaskManager, ensure that the sync_func is properly defined if synchronization is required during task processing. Additionally, be aware of potential circular dependencies in task management, which may complicate the task retrieval process.

**Output Example**: A possible return value from the get_next_task method could be a tuple containing a Task object and its ID, such as (Task(task_id=0, dependencies=[], extra_info=None), 0), indicating that the task with ID 0 is ready for processing.
### FunctionDef __init__(self)
**__init__**: The function of __init__ is to initialize a MultiTaskDispatch object.

**parameters**: The __init__ function does not take any parameters.

**Code Description**: The __init__ method is responsible for setting up a new instance of the MultiTaskDispatch class. It initializes several key attributes that are essential for managing multiple tasks within a multi-tasking framework. 

- `task_dict`: This attribute is a dictionary that maps integer task IDs to Task objects. It serves as a central repository for all tasks being managed, allowing for efficient retrieval and management of tasks based on their unique identifiers.

- `task_lock`: This attribute is an instance of `threading.Lock`, which is utilized for thread synchronization. It ensures that access to the `task_dict` is thread-safe, preventing race conditions that could occur when multiple threads attempt to modify or access the task dictionary simultaneously.

- `now_id`: This integer attribute keeps track of the current task ID being processed. It is initialized to zero, indicating that no tasks have been processed yet.

- `query_id`: Similar to `now_id`, this integer attribute is used to track the current query ID. It is also initialized to zero.

- `sync_func`: This attribute is initialized to None and serves as a placeholder for a synchronization function that may be defined later. This allows for flexibility in managing task synchronization as needed.

The initialization of these attributes is crucial for the proper functioning of the MultiTaskDispatch system, as they lay the groundwork for task management and synchronization. The MultiTaskDispatch class relies on the Task class to represent individual tasks, which are stored in `task_dict`. The relationship between MultiTaskDispatch and Task is fundamental, as MultiTaskDispatch orchestrates the execution and management of these Task objects, ensuring that tasks are executed in accordance with their dependencies and statuses.

**Note**: When using the MultiTaskDispatch class, it is important to ensure that the task management system is properly configured, particularly with respect to thread safety and the handling of task dependencies. Proper initialization of the attributes is essential for the smooth operation of the task management framework.
***
### FunctionDef all_success(self)
**all_success**: all_successçš„åŠŸèƒ½æ˜¯æ£€æŸ¥ä»»åŠ¡ç®¡ç†å™¨ä¸­çš„ä»»åŠ¡å­—å…¸æ˜¯å¦ä¸ºç©ºã€‚

**parameters**: æ­¤å‡½æ•°æ²¡æœ‰å‚æ•°ã€‚

**Code Description**: all_successå‡½æ•°ç”¨äºåˆ¤æ–­ä»»åŠ¡ç®¡ç†å™¨ä¸­çš„ä»»åŠ¡å­—å…¸ï¼ˆtask_dictï¼‰æ˜¯å¦ä¸ºç©ºã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒé€šè¿‡è®¡ç®—ä»»åŠ¡å­—å…¸çš„é•¿åº¦æ¥å®ç°è¿™ä¸€ç‚¹ã€‚å¦‚æœä»»åŠ¡å­—å…¸çš„é•¿åº¦ä¸ºé›¶ï¼Œè¡¨ç¤ºæ²¡æœ‰å¾…å¤„ç†çš„ä»»åŠ¡ï¼Œå‡½æ•°å°†è¿”å›Trueï¼›å¦åˆ™ï¼Œè¿”å›Falseã€‚

åœ¨é¡¹ç›®ä¸­ï¼Œall_successå‡½æ•°è¢«è°ƒç”¨äºrepo_agent/runner.pyä¸­çš„Runnerç±»çš„runæ–¹æ³•ã€‚åœ¨runæ–¹æ³•ä¸­ï¼Œä»»åŠ¡ç®¡ç†å™¨çš„çŠ¶æ€è¢«æ£€æŸ¥ï¼Œä»¥ç¡®å®šæ˜¯å¦æ‰€æœ‰æ–‡æ¡£ç”Ÿæˆä»»åŠ¡éƒ½å·²å®Œæˆã€‚å¦‚æœall_successè¿”å›Trueï¼Œè¡¨ç¤ºä»»åŠ¡é˜Ÿåˆ—ä¸­æ²¡æœ‰ä»»åŠ¡ï¼Œæ‰€æœ‰æ–‡æ¡£éƒ½å·²å®Œæˆä¸”æ˜¯æœ€æ–°çš„ï¼Œè¿™æ—¶ä¼šè®°å½•ä¸€æ¡æ—¥å¿—ï¼Œè¡¨æ˜æ²¡æœ‰ä»»åŠ¡åœ¨é˜Ÿåˆ—ä¸­ã€‚

**Note**: ä½¿ç”¨æ­¤å‡½æ•°æ—¶ï¼Œè¯·ç¡®ä¿ä»»åŠ¡å­—å…¸çš„çŠ¶æ€å·²æ­£ç¡®æ›´æ–°ï¼Œä»¥é¿å…è¯¯åˆ¤ä»»åŠ¡æ˜¯å¦å®Œæˆã€‚

**Output Example**: å‡è®¾ä»»åŠ¡å­—å…¸ä¸ºç©ºï¼Œè°ƒç”¨all_successå°†è¿”å›Trueã€‚
***
### FunctionDef add_task(self, dependency_task_id, extra)
**add_task**: The function of add_task is to add a new task to the task dictionary while managing its dependencies.

**parameters**: The parameters of this Function.
Â· dependency_task_id: List[int] - A list of task IDs that the new task depends on.
Â· extra: Any, optional - Extra information associated with the task. Defaults to None.

**Code Description**: The add_task method is responsible for creating and adding a new task to the task manager's internal dictionary of tasks. It takes a list of dependency task IDs, which represent other tasks that must be completed before the new task can start. The method also accepts an optional parameter, extra, which can hold any additional information related to the task.

When the add_task method is invoked, it first acquires a lock (self.task_lock) to ensure thread safety while modifying the task dictionary. It then retrieves the Task objects corresponding to the provided dependency_task_id list. These Task objects are stored in the depend_tasks list. 

Next, a new Task object is instantiated using the current task ID (self.now_id), the list of dependencies (depend_tasks), and the optional extra information. This new Task object is then added to the task dictionary with the current task ID as the key. After successfully adding the task, the method increments the now_id counter to ensure that the next task added will have a unique identifier. Finally, the method returns the ID of the newly added task.

The add_task method is called within the get_task_manager method of the MetaInfo class. In this context, get_task_manager is responsible for constructing a TaskManager instance and populating it with tasks based on the relationships between various document items. As it processes each document item, it determines the dependencies for the task to be created and invokes add_task to register the new task in the TaskManager. This integration highlights the role of add_task in establishing the task management framework, ensuring that tasks are created with the correct dependencies and are properly tracked within the system.

**Note**: When using the add_task method, it is essential to ensure that the dependency_task_id list does not contain circular references, as this could lead to issues in task execution. Additionally, the extra parameter should be used judiciously to provide relevant context for the task without introducing unnecessary complexity.

**Output Example**: A possible return value of the add_task method could be an integer representing the ID of the newly added task, such as 5, indicating that the task has been successfully added to the task manager with that identifier.
***
### FunctionDef get_next_task(self, process_id)
**get_next_task**: get_next_taskçš„åŠŸèƒ½æ˜¯ä¸ºç»™å®šçš„è¿›ç¨‹IDè·å–ä¸‹ä¸€ä¸ªä»»åŠ¡ã€‚

**parameters**: è¯¥å‡½æ•°çš„å‚æ•°ã€‚
Â· parameter1: process_id (int) - è¿›ç¨‹çš„IDã€‚

**Code Description**: 
get_next_taskå‡½æ•°ç”¨äºæ ¹æ®æä¾›çš„è¿›ç¨‹IDè·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„ä»»åŠ¡ã€‚å‡½æ•°é¦–å…ˆé€šè¿‡self.task_locké”å®šä»»åŠ¡ï¼Œä»¥ç¡®ä¿åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸­å¯¹ä»»åŠ¡çš„å®‰å…¨è®¿é—®ã€‚æ¥ç€ï¼Œquery_idè‡ªå¢1ï¼Œç”¨äºè·Ÿè¸ªæŸ¥è¯¢æ¬¡æ•°ã€‚å‡½æ•°éå†task_dictå­—å…¸ä¸­çš„æ‰€æœ‰ä»»åŠ¡IDï¼Œæ£€æŸ¥æ¯ä¸ªä»»åŠ¡çš„ä¾èµ–å…³ç³»å’ŒçŠ¶æ€ã€‚åªæœ‰å½“ä»»åŠ¡çš„ä¾èµ–å…³ç³»ä¸ºç©ºä¸”çŠ¶æ€ä¸º0ï¼ˆè¡¨ç¤ºä»»åŠ¡å¯ç”¨ï¼‰æ—¶ï¼Œæ‰å°†å…¶æ ‡è®°ä¸ºå·²è·å–ï¼ˆçŠ¶æ€è®¾ç½®ä¸º1ï¼‰ã€‚åœ¨è·å–ä»»åŠ¡æ—¶ï¼Œå‡½æ•°ä¼šæ‰“å°å‡ºå½“å‰è¿›ç¨‹IDã€è·å–çš„ä»»åŠ¡IDä»¥åŠå‰©ä½™ä»»åŠ¡çš„æ•°é‡ã€‚å¦‚æœquery_idæ˜¯10çš„å€æ•°ï¼Œåˆ™è°ƒç”¨sync_funcå‡½æ•°è¿›è¡ŒåŒæ­¥ã€‚æœ€åï¼Œå‡½æ•°è¿”å›è·å–çš„ä»»åŠ¡å¯¹è±¡åŠå…¶IDã€‚å¦‚æœæ²¡æœ‰å¯ç”¨çš„ä»»åŠ¡ï¼Œå‡½æ•°å°†è¿”å›(None, -1)ã€‚

**Note**: ä½¿ç”¨è¯¥å‡½æ•°æ—¶ï¼Œè¯·ç¡®ä¿åœ¨è°ƒç”¨å‰å·²æ­£ç¡®åˆå§‹åŒ–task_dictï¼Œå¹¶ä¸”åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸­ä½¿ç”¨task_lockæ¥é¿å…ç«äº‰æ¡ä»¶ã€‚

**Output Example**: 
å‡è®¾æœ‰ä¸€ä¸ªå¯ç”¨çš„ä»»åŠ¡ï¼Œå…¶IDä¸º5ï¼Œè¿”å›å€¼å¯èƒ½ä¸ºï¼š
(task_object, 5)  # å…¶ä¸­task_objectæ˜¯è·å–çš„ä»»åŠ¡å¯¹è±¡ã€‚ 

å¦‚æœæ²¡æœ‰å¯ç”¨çš„ä»»åŠ¡ï¼Œè¿”å›å€¼å°†ä¸ºï¼š
(None, -1)
***
### FunctionDef mark_completed(self, task_id)
**mark_completed**: mark_completedçš„åŠŸèƒ½æ˜¯å°†æŒ‡å®šä»»åŠ¡æ ‡è®°ä¸ºå·²å®Œæˆï¼Œå¹¶ä»ä»»åŠ¡å­—å…¸ä¸­ç§»é™¤è¯¥ä»»åŠ¡ã€‚

**parameters**: è¯¥å‡½æ•°çš„å‚æ•°ã€‚
Â· parameter1: task_id (int) - è¦æ ‡è®°ä¸ºå·²å®Œæˆçš„ä»»åŠ¡çš„IDã€‚

**Code Description**: mark_completedå‡½æ•°ç”¨äºå°†æŒ‡å®šçš„ä»»åŠ¡æ ‡è®°ä¸ºå·²å®Œæˆï¼Œå¹¶ä»ä»»åŠ¡ç®¡ç†å™¨çš„ä»»åŠ¡å­—å…¸ä¸­åˆ é™¤è¯¥ä»»åŠ¡ã€‚å‡½æ•°æ¥æ”¶ä¸€ä¸ªæ•´æ•°ç±»å‹çš„å‚æ•°task_idï¼Œè¡¨ç¤ºè¦å¤„ç†çš„ä»»åŠ¡çš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚å‡½æ•°å†…éƒ¨é¦–å…ˆé€šè¿‡è‡ªæˆ‘é”å®šï¼ˆself.task_lockï¼‰æ¥ç¡®ä¿åœ¨å¤šçº¿ç¨‹ç¯å¢ƒä¸‹å¯¹ä»»åŠ¡å­—å…¸çš„å®‰å…¨è®¿é—®ã€‚æ¥ç€ï¼Œå‡½æ•°é€šè¿‡task_idä»ä»»åŠ¡å­—å…¸ä¸­è·å–ç›®æ ‡ä»»åŠ¡ï¼ˆtarget_taskï¼‰ã€‚ç„¶åï¼Œå‡½æ•°éå†ä»»åŠ¡å­—å…¸ä¸­çš„æ‰€æœ‰ä»»åŠ¡ï¼Œæ£€æŸ¥ç›®æ ‡ä»»åŠ¡æ˜¯å¦åœ¨å…¶ä»–ä»»åŠ¡çš„ä¾èµ–åˆ—è¡¨ä¸­ã€‚å¦‚æœç›®æ ‡ä»»åŠ¡å­˜åœ¨äºå…¶ä»–ä»»åŠ¡çš„ä¾èµ–ä¸­ï¼Œåˆ™å°†å…¶ä»ä¾èµ–åˆ—è¡¨ä¸­ç§»é™¤ã€‚æœ€åï¼Œå‡½æ•°è°ƒç”¨popæ–¹æ³•ä»ä»»åŠ¡å­—å…¸ä¸­åˆ é™¤è¯¥ä»»åŠ¡ï¼Œç¡®ä¿ä»»åŠ¡ä¸å†è¢«ç®¡ç†ã€‚

**Note**: ä½¿ç”¨è¯¥å‡½æ•°æ—¶ï¼Œè¯·ç¡®ä¿ä¼ å…¥çš„task_idæ˜¯æœ‰æ•ˆçš„ï¼Œå¹¶ä¸”å¯¹åº”çš„ä»»åŠ¡åœ¨ä»»åŠ¡å­—å…¸ä¸­å­˜åœ¨ã€‚è°ƒç”¨æ­¤å‡½æ•°åï¼Œç›¸å…³ä¾èµ–å…³ç³»ä¹Ÿä¼šè¢«æ›´æ–°ï¼Œå› æ­¤åœ¨è°ƒç”¨ä¹‹å‰åº”è€ƒè™‘ä»»åŠ¡ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚
***
## FunctionDef worker(task_manager, process_id, handler)
**worker**: workerå‡½æ•°ç”¨äºæ‰§è¡Œç”±ä»»åŠ¡ç®¡ç†å™¨åˆ†é…çš„ä»»åŠ¡ã€‚

**parameters**: è¯¥å‡½æ•°çš„å‚æ•°å¦‚ä¸‹ï¼š
Â· parameter1: task_manager - ä»»åŠ¡ç®¡ç†å™¨å¯¹è±¡ï¼Œç”¨äºåˆ†é…ä»»åŠ¡ç»™å·¥ä½œçº¿ç¨‹ã€‚
Â· parameter2: process_id (int) - å½“å‰å·¥ä½œè¿›ç¨‹çš„IDã€‚
Â· parameter3: handler (Callable) - å¤„ç†ä»»åŠ¡çš„å‡½æ•°ã€‚

**Code Description**: workerå‡½æ•°æ˜¯ä¸€ä¸ªæ— é™å¾ªç¯çš„å·¥ä½œçº¿ç¨‹ï¼Œå®ƒä»ä»»åŠ¡ç®¡ç†å™¨ä¸­è·å–ä»»åŠ¡å¹¶æ‰§è¡Œã€‚é¦–å…ˆï¼Œå®ƒä¼šæ£€æŸ¥ä»»åŠ¡ç®¡ç†å™¨çš„çŠ¶æ€ï¼Œå¦‚æœæ‰€æœ‰ä»»åŠ¡éƒ½å·²æˆåŠŸå®Œæˆï¼Œåˆ™å‡½æ•°è¿”å›ï¼Œç»“æŸæ‰§è¡Œã€‚æ¥ç€ï¼Œworkerè°ƒç”¨task_managerçš„get_next_taskæ–¹æ³•ï¼Œè·å–å½“å‰è¿›ç¨‹IDå¯¹åº”çš„ä¸‹ä¸€ä¸ªä»»åŠ¡åŠå…¶IDã€‚å¦‚æœæ²¡æœ‰å¯ç”¨çš„ä»»åŠ¡ï¼Œworkerä¼šæš‚åœ0.5ç§’åç»§ç»­å¾ªç¯ã€‚

ä¸€æ—¦è·å–åˆ°ä»»åŠ¡ï¼Œworkerä¼šè°ƒç”¨ä¼ å…¥çš„handlerå‡½æ•°ï¼Œå¤„ç†ä»»åŠ¡çš„é¢å¤–ä¿¡æ¯ã€‚å¤„ç†å®Œæˆåï¼Œworkerä¼šè°ƒç”¨task_managerçš„mark_completedæ–¹æ³•ï¼Œæ ‡è®°è¯¥ä»»åŠ¡ä¸ºå·²å®Œæˆã€‚æ­¤å‡½æ•°çš„è®¾è®¡å…è®¸å¤šä¸ªworkerå¹¶è¡Œå¤„ç†ä»»åŠ¡ï¼Œæå‡äº†ä»»åŠ¡æ‰§è¡Œçš„æ•ˆç‡ã€‚

åœ¨é¡¹ç›®ä¸­ï¼Œworkerå‡½æ•°è¢«repo_agent/runner.pyä¸­çš„first_generateå’Œrunæ–¹æ³•è°ƒç”¨ã€‚å…·ä½“æ¥è¯´ï¼Œè¿™ä¸¤ä¸ªæ–¹æ³•åœ¨ç”Ÿæˆæ–‡æ¡£çš„è¿‡ç¨‹ä¸­ä¼šåˆ›å»ºå¤šä¸ªçº¿ç¨‹ï¼Œæ¯ä¸ªçº¿ç¨‹éƒ½è¿è¡Œworkerå‡½æ•°ï¼Œä»¥å¹¶è¡Œå¤„ç†ä»»åŠ¡ã€‚first_generateæ–¹æ³•è´Ÿè´£åˆå§‹åŒ–ä»»åŠ¡åˆ—è¡¨å¹¶å¯åŠ¨workerçº¿ç¨‹ï¼Œè€Œrunæ–¹æ³•åˆ™åœ¨æ£€æµ‹åˆ°æ–‡ä»¶å˜æ›´æ—¶é‡æ–°ç”Ÿæˆæ–‡æ¡£ï¼Œå¹¶åŒæ ·å¯åŠ¨workerçº¿ç¨‹æ¥å¤„ç†ä»»åŠ¡ã€‚

**Note**: ä½¿ç”¨è¯¥å‡½æ•°æ—¶ï¼Œéœ€è¦ç¡®ä¿ä»»åŠ¡ç®¡ç†å™¨çš„çŠ¶æ€æ­£ç¡®ï¼Œä»¥é¿å…åœ¨æ²¡æœ‰ä»»åŠ¡å¯æ‰§è¡Œæ—¶é€ æˆä¸å¿…è¦çš„ç­‰å¾…ã€‚

**Output Example**: å‡è®¾ä»»åŠ¡ç®¡ç†å™¨åˆ†é…äº†ä¸€ä¸ªä»»åŠ¡ï¼Œworkerå‡½æ•°åœ¨å¤„ç†åå¯èƒ½ä¼šè¿”å›å¦‚ä¸‹ä¿¡æ¯ï¼š
```
ä»»åŠ¡ID: 12345 å·²æˆåŠŸå®Œæˆã€‚
```
## FunctionDef some_function
**some_function**: some_functionçš„åŠŸèƒ½æ˜¯éšæœºæš‚åœä¸€æ®µæ—¶é—´ã€‚

**parameters**: è¯¥å‡½æ•°æ²¡æœ‰å‚æ•°ã€‚

**Code Description**: some_functionæ˜¯ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼Œå…¶ä¸»è¦åŠŸèƒ½æ˜¯ä½¿ç¨‹åºéšæœºæš‚åœä¸€æ®µæ—¶é—´ã€‚å…·ä½“å®ç°ä¸Šï¼Œå‡½æ•°å†…éƒ¨è°ƒç”¨äº†time.sleep()æ–¹æ³•ï¼Œä¼ å…¥çš„å‚æ•°æ˜¯ä¸€ä¸ªéšæœºç”Ÿæˆçš„æµ®ç‚¹æ•°ï¼Œè¯¥æµ®ç‚¹æ•°çš„èŒƒå›´æ˜¯0åˆ°3ç§’ä¹‹é—´ã€‚è¿™ä¸ªéšæœºæ•°æ˜¯é€šè¿‡random.random()ç”Ÿæˆçš„ï¼Œrandom.random()è¿”å›ä¸€ä¸ªåœ¨[0.0, 1.0)èŒƒå›´å†…çš„éšæœºæµ®ç‚¹æ•°ï¼Œå› æ­¤ä¹˜ä»¥3åï¼Œæœ€ç»ˆçš„æš‚åœæ—¶é—´ä¼šåœ¨0åˆ°3ç§’ä¹‹é—´å˜åŒ–ã€‚è¿™ç§éšæœºæš‚åœçš„åŠŸèƒ½å¯ä»¥ç”¨äºéœ€è¦æ¨¡æ‹Ÿå»¶è¿Ÿæˆ–ç­‰å¾…çš„åœºæ™¯ï¼Œä¾‹å¦‚åœ¨å¤šçº¿ç¨‹æˆ–å¼‚æ­¥ç¼–ç¨‹ä¸­ï¼Œå¯èƒ½éœ€è¦éšæœºå»¶è¿Ÿä»¥é¿å…èµ„æºç«äº‰æˆ–æ¨¡æ‹ŸçœŸå®ç”¨æˆ·çš„æ“ä½œè¡Œä¸ºã€‚

**Note**: ä½¿ç”¨è¯¥å‡½æ•°æ—¶ï¼Œè¯·æ³¨æ„å®ƒä¼šå¯¼è‡´ç¨‹åºæš‚åœï¼Œå› æ­¤åœ¨éœ€è¦é«˜æ€§èƒ½æˆ–å®æ—¶å“åº”çš„åœºæ™¯ä¸­åº”è°¨æ…ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œç”±äºæš‚åœæ—¶é—´æ˜¯éšæœºçš„ï¼Œå¯èƒ½ä¼šå½±å“ç¨‹åºçš„å¯é¢„æµ‹æ€§ã€‚



================================================
FILE: markdown_docs/repo_agent/project_manager.md
================================================
## ClassDef ProjectManager
**ProjectManager**: The function of ProjectManager is to manage and retrieve the structure of a project repository.

**attributes**: The attributes of this Class.
Â· repo_path: The file path to the project repository.
Â· project: An instance of the Jedi Project class, initialized with the repo_path.
Â· project_hierarchy: The file path to the project hierarchy JSON file, constructed from the repo_path and project_hierarchy parameter.

**Code Description**: The ProjectManager class is designed to facilitate the management of a project repository by providing methods to retrieve the project's directory structure and build a reference path tree. Upon initialization, the class requires two parameters: `repo_path`, which specifies the location of the project repository, and `project_hierarchy`, which indicates the name of the hierarchy to be used. The class constructs the path to the project hierarchy JSON file by combining the repo_path with the project_hierarchy name.

The `get_project_structure` method is responsible for returning the structure of the project by recursively traversing the directory tree starting from the repo_path. It constructs a string representation of the project structure, including all directories and Python files, while ignoring hidden files and directories. This method utilizes a nested function `walk_dir` to perform the recursive traversal.

The `build_path_tree` method creates a hierarchical tree structure based on two lists of paths: `who_reference_me` and `reference_who`, as well as a specific `doc_item_path`. It constructs a nested dictionary using `defaultdict` to represent the tree structure. The method modifies the last part of the `doc_item_path` to indicate a specific item with a star symbol. Finally, it converts the tree structure into a string format for easier visualization.

The ProjectManager class is instantiated within the Runner class, where it is initialized with the target repository and hierarchy name obtained from the SettingsManager. This integration allows the Runner to leverage the ProjectManager's capabilities to manage and retrieve project structure information, which is essential for the overall functionality of the application.

**Note**: When using the ProjectManager class, ensure that the provided repo_path is valid and accessible. The project_hierarchy should correspond to an existing hierarchy name to avoid file path errors.

**Output Example**: A possible output of the `get_project_structure` method might look like this:
```
project_root
  src
    main.py
    utils.py
  tests
    test_main.py
```
### FunctionDef __init__(self, repo_path, project_hierarchy)
**__init__**: __init__çš„åŠŸèƒ½æ˜¯åˆå§‹åŒ–ProjectManagerç±»çš„å®ä¾‹ã€‚

**parameters**: è¯¥å‡½æ•°çš„å‚æ•°å¦‚ä¸‹ï¼š
Â· parameter1: repo_path - æŒ‡å®šé¡¹ç›®çš„å­˜å‚¨åº“è·¯å¾„ã€‚
Â· parameter2: project_hierarchy - æŒ‡å®šé¡¹ç›®å±‚æ¬¡ç»“æ„çš„è·¯å¾„ã€‚

**Code Description**: è¯¥__init__å‡½æ•°ç”¨äºåˆå§‹åŒ–ProjectManagerç±»çš„å®ä¾‹ã€‚åœ¨å‡½æ•°å†…éƒ¨ï¼Œé¦–å…ˆå°†ä¼ å…¥çš„repo_pathå‚æ•°èµ‹å€¼ç»™å®ä¾‹å˜é‡self.repo_pathï¼Œä»¥ä¾¿åœ¨ç±»çš„å…¶ä»–æ–¹æ³•ä¸­ä½¿ç”¨ã€‚æ¥ç€ï¼Œä½¿ç”¨jediåº“åˆ›å»ºä¸€ä¸ªæ–°çš„Projectå¯¹è±¡ï¼Œå¹¶å°†å…¶èµ‹å€¼ç»™self.projectï¼Œä¼ å…¥çš„repo_pathä½œä¸ºå‚æ•°ã€‚è¿™ä½¿å¾—ProjectManagerèƒ½å¤Ÿåˆ©ç”¨jediåº“æä¾›çš„åŠŸèƒ½æ¥å¤„ç†ä»£ç åˆ†æå’Œè‡ªåŠ¨è¡¥å…¨ç­‰ä»»åŠ¡ã€‚æœ€åï¼Œå‡½æ•°é€šè¿‡os.path.joinæ–¹æ³•æ„å»ºé¡¹ç›®å±‚æ¬¡ç»“æ„çš„å®Œæ•´è·¯å¾„ï¼Œå°†å…¶èµ‹å€¼ç»™self.project_hierarchyã€‚è¯¥è·¯å¾„ç”±repo_pathã€project_hierarchyå‚æ•°å’Œä¸€ä¸ªåä¸º"project_hierarchy.json"çš„æ–‡ä»¶åç»„æˆï¼Œè¿™æ ·å¯ä»¥æ–¹ä¾¿åœ°è®¿é—®é¡¹ç›®çš„å±‚æ¬¡ç»“æ„æ•°æ®ã€‚

**Note**: ä½¿ç”¨è¯¥ä»£ç æ—¶ï¼Œè¯·ç¡®ä¿ä¼ å…¥çš„repo_pathæ˜¯æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„ï¼Œå¹¶ä¸”project_hierarchyå‚æ•°æŒ‡å‘çš„ç›®å½•ä¸­å­˜åœ¨"project_hierarchy.json"æ–‡ä»¶ï¼Œä»¥é¿å…åœ¨å®ä¾‹åŒ–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ã€‚
***
### FunctionDef get_project_structure(self)
**get_project_structure**: The function of get_project_structure is to return the structure of the project by recursively walking through the directory tree.

**parameters**: The parameters of this Function.
Â· There are no parameters for this function.

**Code Description**: The get_project_structure function is designed to generate a string representation of the project's directory structure. It does this by defining an inner function called walk_dir, which takes two arguments: root (the current directory being processed) and prefix (a string used to format the output). The function initializes an empty list called structure to hold the formatted directory and file names.

The walk_dir function begins by appending the base name of the current directory (root) to the structure list, prefixed by the provided prefix. It then creates a new prefix by adding two spaces to the existing prefix to indicate a deeper level in the directory hierarchy. The function proceeds to iterate over the sorted list of items in the current directory, skipping any hidden files or directories (those starting with a dot).

For each item, it constructs the full path and checks if it is a directory or a Python file (ending with ".py"). If it is a directory, the function calls itself recursively with the new prefix. If it is a Python file, it appends the file name to the structure list with the new prefix.

Finally, after the walk_dir function has processed all directories and files, the get_project_structure function joins the elements of the structure list into a single string, separated by newline characters, and returns this string.

**Note**: It is important to ensure that the repo_path attribute of the class instance is correctly set to the root directory of the project before calling this function. The function will only include Python files in the output, ignoring other file types.

**Output Example**: 
```
project_name
  module1
    file1.py
    file2.py
  module2
    file3.py
  README.md
```
#### FunctionDef walk_dir(root, prefix)
**walk_dir**: walk_dirçš„åŠŸèƒ½æ˜¯éå†æŒ‡å®šç›®å½•åŠå…¶å­ç›®å½•ï¼Œå¹¶æ”¶é›†æ‰€æœ‰Pythonæ–‡ä»¶çš„ç»“æ„ä¿¡æ¯ã€‚

**parameters**: æ­¤å‡½æ•°çš„å‚æ•°å¦‚ä¸‹ï¼š
Â· parameter1: root - è¦éå†çš„æ ¹ç›®å½•çš„è·¯å¾„ã€‚
Â· parameter2: prefix - ç”¨äºæ ¼å¼åŒ–è¾“å‡ºçš„å‰ç¼€å­—ç¬¦ä¸²ï¼Œé»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²ã€‚

**Code Description**: 
walk_dirå‡½æ•°ç”¨äºé€’å½’éå†ç»™å®šçš„ç›®å½•ï¼ˆrootï¼‰åŠå…¶æ‰€æœ‰å­ç›®å½•ã€‚å®ƒé¦–å…ˆå°†å½“å‰ç›®å½•çš„åç§°ï¼ˆé€šè¿‡os.path.basename(root)è·å–ï¼‰æ·»åŠ åˆ°ç»“æ„åˆ—è¡¨ä¸­ï¼ˆstructureï¼‰ï¼Œå¹¶åœ¨å‰ç¼€å­—ç¬¦ä¸²ï¼ˆprefixï¼‰åæ·»åŠ ç©ºæ ¼ä»¥ä¾¿äºæ ¼å¼åŒ–ã€‚æ¥ç€ï¼Œå‡½æ•°ä½¿ç”¨os.listdir(root)åˆ—å‡ºå½“å‰ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶å’Œå­ç›®å½•ï¼Œå¹¶å¯¹è¿™äº›åç§°è¿›è¡Œæ’åºã€‚

åœ¨éå†æ¯ä¸ªåç§°æ—¶ï¼Œå‡½æ•°ä¼šæ£€æŸ¥åç§°æ˜¯å¦ä»¥ç‚¹ï¼ˆ.ï¼‰å¼€å¤´ï¼Œä»¥æ­¤æ¥å¿½ç•¥éšè—æ–‡ä»¶å’Œç›®å½•ã€‚å¦‚æœåç§°ä¸æ˜¯éšè—çš„ï¼Œå‡½æ•°ä¼šæ„é€ è¯¥åç§°çš„å®Œæ•´è·¯å¾„ï¼ˆpathï¼‰ã€‚å¦‚æœè¯¥è·¯å¾„æ˜¯ä¸€ä¸ªç›®å½•ï¼Œå‡½æ•°ä¼šé€’å½’è°ƒç”¨walk_dirï¼Œä¼ å…¥æ–°çš„å‰ç¼€ï¼ˆnew_prefixï¼‰ã€‚å¦‚æœè¯¥è·¯å¾„æ˜¯ä¸€ä¸ªæ–‡ä»¶ä¸”æ–‡ä»¶åä»¥â€œ.pyâ€ç»“å°¾ï¼Œå‡½æ•°åˆ™å°†è¯¥æ–‡ä»¶çš„åç§°æ·»åŠ åˆ°ç»“æ„åˆ—è¡¨ä¸­ï¼Œå‰é¢åŠ ä¸Šæ–°çš„å‰ç¼€ã€‚

è¯¥å‡½æ•°çš„è®¾è®¡ä½¿å¾—å®ƒèƒ½å¤Ÿæœ‰æ•ˆåœ°æ”¶é›†æŒ‡å®šç›®å½•ä¸‹æ‰€æœ‰Pythonæ–‡ä»¶çš„ç»“æ„ä¿¡æ¯ï¼Œå¹¶ä»¥å±‚çº§æ–¹å¼å±•ç¤ºã€‚

**Note**: ä½¿ç”¨æ­¤ä»£ç æ—¶ï¼Œè¯·ç¡®ä¿ä¼ å…¥çš„æ ¹ç›®å½•è·¯å¾„æ˜¯æœ‰æ•ˆçš„ï¼Œå¹¶ä¸”å…·æœ‰è¯»å–æƒé™ã€‚æ­¤å¤–ï¼Œå‡½æ•°ä¼šå¿½ç•¥æ‰€æœ‰ä»¥ç‚¹å¼€å¤´çš„æ–‡ä»¶å’Œç›®å½•ï¼Œå› æ­¤å¦‚æœéœ€è¦å¤„ç†è¿™äº›æ–‡ä»¶ï¼Œéœ€è°ƒæ•´ç›¸å…³é€»è¾‘ã€‚
***
***
### FunctionDef build_path_tree(self, who_reference_me, reference_who, doc_item_path)
**build_path_tree**: The function of build_path_tree is to construct a hierarchical representation of file paths based on two reference lists and a specific document item path.

**parameters**: The parameters of this Function.
Â· who_reference_me: A list of file paths that reference the current object.
Â· reference_who: A list of file paths that are referenced by the current object.
Â· doc_item_path: A specific file path that needs to be highlighted in the tree structure.

**Code Description**: The build_path_tree function creates a nested dictionary structure representing a tree of file paths. It utilizes the `defaultdict` from the `collections` module to facilitate the creation of this tree. The function begins by defining an inner function, `tree`, which initializes a new `defaultdict` that can recursively create nested dictionaries.

The function then processes the two input lists, `who_reference_me` and `reference_who`. For each path in these lists, it splits the path into its components using the operating system's path separator (`os.sep`). It traverses the tree structure, creating a new node for each part of the path.

Next, the function processes the `doc_item_path`. It splits this path into components as well, but modifies the last component by prefixing it with a star symbol (âœ³ï¸) to indicate that it is the item of interest. This modified path is then added to the tree in the same manner as the previous paths.

Finally, the function defines another inner function, `tree_to_string`, which converts the nested dictionary structure into a formatted string representation. This function recursively traverses the tree, indenting each level of the hierarchy for clarity. The resulting string is returned as the output of the build_path_tree function.

**Note**: It is important to ensure that the paths provided in `who_reference_me` and `reference_who` are valid and correctly formatted. The function assumes that the paths are well-structured and uses the operating system's path separator for splitting.

**Output Example**: 
Given the following inputs:
- who_reference_me: ["folder1/fileA.txt", "folder1/folder2/fileB.txt"]
- reference_who: ["folder3/fileC.txt"]
- doc_item_path: "folder1/folder2/fileB.txt"

The output of the function might look like this:
```
folder1
    fileA.txt
    folder2
        âœ³ï¸fileB.txt
folder3
    fileC.txt
```
#### FunctionDef tree
**tree**: treeå‡½æ•°çš„åŠŸèƒ½æ˜¯è¿”å›ä¸€ä¸ªé»˜è®¤å­—å…¸ï¼Œè¯¥å­—å…¸çš„é»˜è®¤å€¼æ˜¯ä¸€ä¸ªæ–°çš„treeå‡½æ•°ã€‚

**parameters**: è¯¥å‡½æ•°æ²¡æœ‰å‚æ•°ã€‚

**Code Description**: treeå‡½æ•°ä½¿ç”¨äº†Pythonçš„defaultdictç±»ã€‚defaultdictæ˜¯collectionsæ¨¡å—ä¸­çš„ä¸€ä¸ªå­—å…¸å­ç±»ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªé»˜è®¤å€¼ï¼Œå½“è®¿é—®çš„é”®ä¸å­˜åœ¨æ—¶ï¼Œä¼šè‡ªåŠ¨è°ƒç”¨ä¸€ä¸ªæŒ‡å®šçš„å·¥å‚å‡½æ•°æ¥ç”Ÿæˆè¿™ä¸ªé»˜è®¤å€¼ã€‚åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œtreeå‡½æ•°æœ¬èº«è¢«ç”¨ä½œå·¥å‚å‡½æ•°ï¼Œè¿™æ„å‘³ç€æ¯å½“è®¿é—®ä¸€ä¸ªä¸å­˜åœ¨çš„é”®æ—¶ï¼Œdefaultdictå°†ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„treeå¯¹è±¡ã€‚è¿™ç§é€’å½’çš„ç»“æ„ä½¿å¾—è¿”å›çš„å­—å…¸å¯ä»¥ç”¨äºæ„å»ºæ ‘å½¢ç»“æ„ï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹éƒ½å¯ä»¥æœ‰å¤šä¸ªå­èŠ‚ç‚¹ï¼Œä¸”å­èŠ‚ç‚¹çš„æ•°é‡å’Œå†…å®¹æ˜¯åŠ¨æ€ç”Ÿæˆçš„ã€‚

**Note**: ä½¿ç”¨è¯¥å‡½æ•°æ—¶ï¼Œè¯·æ³¨æ„é¿å…æ— é™é€’å½’çš„æƒ…å†µã€‚ç”±äºtreeå‡½æ•°è¿”å›çš„æ˜¯ä¸€ä¸ªdefaultdictï¼Œå…¶é»˜è®¤å€¼ä¹Ÿæ˜¯treeå‡½æ•°æœ¬èº«ï¼Œå› æ­¤åœ¨è®¿é—®æœªå®šä¹‰çš„é”®æ—¶ä¼šä¸æ–­åˆ›å»ºæ–°çš„defaultdictï¼Œå¯èƒ½å¯¼è‡´å†…å­˜æ¶ˆè€—è¿‡å¤§ã€‚

**Output Example**: è°ƒç”¨treeå‡½æ•°åï¼Œå¯èƒ½çš„è¿”å›å€¼å¦‚ä¸‹ï¼š
```
defaultdict(<function tree at 0x...>, {})
```
æ­¤è¿”å›å€¼è¡¨ç¤ºä¸€ä¸ªç©ºçš„defaultdictï¼Œä¸”å…¶é»˜è®¤å€¼æ˜¯treeå‡½æ•°æœ¬èº«ã€‚è‹¥è®¿é—®ä¸€ä¸ªä¸å­˜åœ¨çš„é”®ï¼Œä¾‹å¦‚`my_tree['a']`ï¼Œåˆ™ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„defaultdictï¼Œä½œä¸º'a'çš„å€¼ã€‚
***
#### FunctionDef tree_to_string(tree, indent)
**tree_to_string**: tree_to_stringçš„åŠŸèƒ½æ˜¯å°†æ ‘å½¢ç»“æ„è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼Œä¾¿äºå¯è§†åŒ–å±•ç¤ºã€‚

**parameters**: æ­¤å‡½æ•°çš„å‚æ•°å¦‚ä¸‹ï¼š
Â· parameter1: tree - ä¸€ä¸ªå­—å…¸ç±»å‹çš„æ ‘å½¢ç»“æ„ï¼Œå…¶ä¸­é”®è¡¨ç¤ºèŠ‚ç‚¹ï¼Œå€¼å¯ä»¥æ˜¯å­èŠ‚ç‚¹çš„å­—å…¸æˆ–å…¶ä»–ç±»å‹ã€‚
Â· parameter2: indent - ä¸€ä¸ªæ•´æ•°ï¼Œè¡¨ç¤ºå½“å‰èŠ‚ç‚¹çš„ç¼©è¿›çº§åˆ«ï¼Œé»˜è®¤ä¸º0ã€‚

**Code Description**: tree_to_stringå‡½æ•°é€šè¿‡é€’å½’çš„æ–¹å¼å°†æ ‘å½¢ç»“æ„è½¬æ¢ä¸ºå­—ç¬¦ä¸²ã€‚é¦–å…ˆï¼Œå‡½æ•°åˆå§‹åŒ–ä¸€ä¸ªç©ºå­—ç¬¦ä¸²sã€‚ç„¶åï¼Œå®ƒå¯¹ä¼ å…¥çš„treeå­—å…¸è¿›è¡Œæ’åºï¼Œå¹¶éå†æ¯ä¸€ä¸ªé”®å€¼å¯¹ã€‚åœ¨éå†è¿‡ç¨‹ä¸­ï¼Œå‡½æ•°å°†å½“å‰é”®ï¼ˆèŠ‚ç‚¹ï¼‰æ·»åŠ åˆ°å­—ç¬¦ä¸²sä¸­ï¼Œå¹¶æ ¹æ®indentå‚æ•°æ·»åŠ ç›¸åº”æ•°é‡çš„ç©ºæ ¼ä»¥å®ç°ç¼©è¿›ã€‚å¦‚æœå½“å‰å€¼æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œè¡¨ç¤ºè¯¥èŠ‚ç‚¹æœ‰å­èŠ‚ç‚¹ï¼Œå‡½æ•°ä¼šé€’å½’è°ƒç”¨tree_to_stringï¼Œå°†å­èŠ‚ç‚¹è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶æ·»åŠ åˆ°sä¸­ã€‚æœ€ç»ˆï¼Œå‡½æ•°è¿”å›æ„å»ºå¥½çš„å­—ç¬¦ä¸²sã€‚

**Note**: ä½¿ç”¨æ­¤å‡½æ•°æ—¶ï¼Œè¯·ç¡®ä¿ä¼ å…¥çš„treeå‚æ•°æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„å­—å…¸ç»“æ„ï¼Œå¹¶ä¸”å¯ä»¥åŒ…å«åµŒå¥—çš„å­—å…¸ã€‚indentå‚æ•°ç”¨äºæ§åˆ¶è¾“å‡ºçš„æ ¼å¼ï¼Œé€šå¸¸ä¸éœ€è¦æ‰‹åŠ¨è®¾ç½®ï¼Œé™¤éåœ¨ç‰¹å®šæƒ…å†µä¸‹éœ€è¦è°ƒæ•´ç¼©è¿›ã€‚

**Output Example**: å‡è®¾è¾“å…¥çš„treeä¸ºå¦‚ä¸‹ç»“æ„ï¼š
{
    "æ ¹èŠ‚ç‚¹": {
        "å­èŠ‚ç‚¹1": {},
        "å­èŠ‚ç‚¹2": {
            "å­™èŠ‚ç‚¹1": {}
        }
    }
}
è°ƒç”¨tree_to_string(tree)å°†è¿”å›ï¼š
æ ¹èŠ‚ç‚¹
    å­èŠ‚ç‚¹1
    å­èŠ‚ç‚¹2
        å­™èŠ‚ç‚¹1
***
***



================================================
FILE: markdown_docs/repo_agent/runner.md
================================================
[Binary file]


================================================
FILE: markdown_docs/repo_agent/settings.md
================================================
## ClassDef LogLevel
**LogLevel**: LogLevel çš„åŠŸèƒ½æ˜¯å®šä¹‰æ—¥å¿—çº§åˆ«çš„æšä¸¾ç±»å‹ã€‚

**attributes**: è¯¥ç±»çš„å±æ€§åŒ…æ‹¬ï¼š
Â· DEBUG: è¡¨ç¤ºè°ƒè¯•ä¿¡æ¯çš„æ—¥å¿—çº§åˆ«ã€‚
Â· INFO: è¡¨ç¤ºä¸€èˆ¬ä¿¡æ¯çš„æ—¥å¿—çº§åˆ«ã€‚
Â· WARNING: è¡¨ç¤ºè­¦å‘Šä¿¡æ¯çš„æ—¥å¿—çº§åˆ«ã€‚
Â· ERROR: è¡¨ç¤ºé”™è¯¯ä¿¡æ¯çš„æ—¥å¿—çº§åˆ«ã€‚
Â· CRITICAL: è¡¨ç¤ºä¸¥é‡é”™è¯¯ä¿¡æ¯çš„æ—¥å¿—çº§åˆ«ã€‚

**Code Description**: LogLevel ç±»ç»§æ‰¿è‡ª StrEnumï¼Œå®šä¹‰äº†ä¸€ç»„å¸¸é‡ï¼Œç”¨äºè¡¨ç¤ºä¸åŒçš„æ—¥å¿—çº§åˆ«ã€‚è¿™äº›æ—¥å¿—çº§åˆ«åŒ…æ‹¬ DEBUGã€INFOã€WARNINGã€ERROR å’Œ CRITICALï¼Œåˆ†åˆ«å¯¹åº”ä¸åŒçš„æ—¥å¿—è®°å½•é‡è¦æ€§ã€‚ä½¿ç”¨æšä¸¾ç±»å‹çš„å¥½å¤„åœ¨äºï¼Œå®ƒæä¾›äº†ä¸€ç§æ¸…æ™°ä¸”ç±»å‹å®‰å…¨çš„æ–¹å¼æ¥å¤„ç†æ—¥å¿—çº§åˆ«ï¼Œé¿å…äº†ä½¿ç”¨å­—ç¬¦ä¸²å¸¸é‡å¯èƒ½å¸¦æ¥çš„é”™è¯¯ã€‚

åœ¨é¡¹ç›®ä¸­ï¼ŒLogLevel ç±»è¢« ProjectSettings ç±»å¼•ç”¨ï¼Œä½œä¸º log_level å±æ€§çš„ç±»å‹ã€‚ProjectSettings ç±»æ˜¯ä¸€ä¸ªé…ç½®ç±»ï¼Œè´Ÿè´£ç®¡ç†é¡¹ç›®çš„è®¾ç½®ï¼Œå…¶ä¸­ log_level å±æ€§é»˜è®¤è®¾ç½®ä¸º LogLevel.INFOã€‚è¿™æ„å‘³ç€åœ¨æ²¡æœ‰ç‰¹åˆ«æŒ‡å®šçš„æƒ…å†µä¸‹ï¼Œé¡¹ç›®çš„æ—¥å¿—çº§åˆ«å°†ä¸ºä¿¡æ¯çº§åˆ«ã€‚

æ­¤å¤–ï¼ŒProjectSettings ç±»ä¸­çš„ set_log_level æ–¹æ³•ç”¨äºéªŒè¯å’Œè®¾ç½®æ—¥å¿—çº§åˆ«ã€‚è¯¥æ–¹æ³•ä¼šå°†è¾“å…¥çš„å­—ç¬¦ä¸²è½¬æ¢ä¸ºå¤§å†™ï¼Œå¹¶æ£€æŸ¥å…¶æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ—¥å¿—çº§åˆ«ã€‚å¦‚æœè¾“å…¥çš„å€¼ä¸åœ¨ LogLevel çš„å®šä¹‰èŒƒå›´å†…ï¼Œå°†ä¼šæŠ›å‡ºä¸€ä¸ª ValueError å¼‚å¸¸ã€‚è¿™ç¡®ä¿äº†åœ¨é¡¹ç›®ä¸­ä½¿ç”¨çš„æ—¥å¿—çº§åˆ«å§‹ç»ˆæ˜¯æœ‰æ•ˆä¸”ä¸€è‡´çš„ã€‚

**Note**: ä½¿ç”¨ LogLevel æ—¶ï¼Œè¯·ç¡®ä¿æ‰€ä½¿ç”¨çš„æ—¥å¿—çº§åˆ«æ˜¯é¢„å®šä¹‰çš„å¸¸é‡ä¹‹ä¸€ï¼Œä»¥é¿å…è¿è¡Œæ—¶é”™è¯¯ã€‚åœ¨è®¾ç½®æ—¥å¿—çº§åˆ«æ—¶ï¼Œå»ºè®®ä½¿ç”¨å¤§å†™å­—æ¯è¾“å…¥ï¼Œä»¥ç¬¦åˆæšä¸¾çš„å®šä¹‰ã€‚
## ClassDef ProjectSettings
**ProjectSettings**: The function of ProjectSettings is to manage the configuration settings for the project.

**attributes**: The attributes of this Class.
Â· target_repo: DirectoryPath - Specifies the target repository directory path.
Â· hierarchy_name: str - Defines the name of the hierarchy for project documentation.
Â· markdown_docs_name: str - Indicates the name of the directory where markdown documentation is stored.
Â· ignore_list: list[str] - A list of items to be ignored in the project settings.
Â· language: str - Specifies the language used in the project, defaulting to "Chinese".
Â· max_thread_count: PositiveInt - Sets the maximum number of threads allowed, defaulting to 4.
Â· log_level: LogLevel - Defines the logging level for the project, defaulting to LogLevel.INFO.

**Code Description**: The ProjectSettings class inherits from BaseSettings and serves as a configuration class that encapsulates various settings required for the project. It includes attributes that define the target repository, documentation hierarchy, language preferences, and logging configurations. 

The class utilizes field validators to ensure that the values assigned to certain attributes are valid. For instance, the `validate_language_code` method checks if the provided language code corresponds to a valid ISO 639 code or language name, raising a ValueError if the input is invalid. This ensures that only recognized language codes are accepted, enhancing the robustness of the configuration.

Similarly, the `set_log_level` method validates the log level input, converting it to uppercase and checking its validity against the predefined LogLevel enumeration. If the input does not match any of the defined log levels, a ValueError is raised, ensuring that the logging configuration remains consistent and valid throughout the project.

The ProjectSettings class is referenced by the Setting class, which aggregates various settings for the project, including ProjectSettings and ChatCompletionSettings. This hierarchical structure allows for organized management of project configurations, where ProjectSettings plays a crucial role in defining the core settings that govern the behavior of the application.

**Note**: When using the ProjectSettings class, ensure that the values assigned to attributes like language and log_level are valid to avoid runtime errors. It is recommended to use the predefined constants for log levels and valid ISO codes for languages to maintain consistency and reliability in the project's configuration.

**Output Example**: An instance of ProjectSettings might look like this:
```
ProjectSettings(
    target_repo="/path/to/repo",
    hierarchy_name=".project_doc_record",
    markdown_docs_name="markdown_docs",
    ignore_list=["temp", "cache"],
    language="English",
    max_thread_count=4,
    log_level=LogLevel.INFO
)
```
### FunctionDef validate_language_code(cls, v)
**validate_language_code**: validate_language_codeçš„åŠŸèƒ½æ˜¯éªŒè¯å¹¶è¿”å›æœ‰æ•ˆçš„è¯­è¨€åç§°ã€‚

**parameters**: è¯¥å‡½æ•°çš„å‚æ•°ã€‚
Â· v: å­—ç¬¦ä¸²ç±»å‹ï¼Œè¡¨ç¤ºå¾…éªŒè¯çš„è¯­è¨€ä»£ç æˆ–è¯­è¨€åç§°ã€‚

**Code Description**: validate_language_codeæ˜¯ä¸€ä¸ªç±»æ–¹æ³•ï¼Œç”¨äºéªŒè¯è¾“å…¥çš„è¯­è¨€ä»£ç æˆ–è¯­è¨€åç§°æ˜¯å¦æœ‰æ•ˆã€‚è¯¥æ–¹æ³•æ¥å—ä¸€ä¸ªå­—ç¬¦ä¸²å‚æ•°vï¼Œè¡¨ç¤ºç”¨æˆ·è¾“å…¥çš„è¯­è¨€ä»£ç æˆ–åç§°ã€‚å‡½æ•°å†…éƒ¨ä½¿ç”¨Language.match(v)æ¥å°è¯•åŒ¹é…è¾“å…¥çš„è¯­è¨€ã€‚å¦‚æœåŒ¹é…æˆåŠŸï¼Œå°†è¿”å›å¯¹åº”çš„è¯­è¨€åç§°ã€‚å¦‚æœè¾“å…¥çš„è¯­è¨€ä»£ç æˆ–åç§°æ— æ•ˆï¼Œåˆ™ä¼šå¼•å‘LanguageNotFoundErrorå¼‚å¸¸ï¼Œè¿›è€ŒæŠ›å‡ºä¸€ä¸ªValueErrorï¼Œæç¤ºç”¨æˆ·è¾“å…¥æœ‰æ•ˆçš„ISO 639ä»£ç æˆ–è¯­è¨€åç§°ã€‚

è¯¥å‡½æ•°çš„ä¸»è¦ç›®çš„æ˜¯ç¡®ä¿ç”¨æˆ·è¾“å…¥çš„è¯­è¨€ä¿¡æ¯æ˜¯æœ‰æ•ˆçš„ï¼Œå¹¶æä¾›ç›¸åº”çš„åé¦ˆï¼Œä»¥ä¾¿ç”¨æˆ·èƒ½å¤Ÿçº æ­£è¾“å…¥é”™è¯¯ã€‚

**Note**: ä½¿ç”¨è¯¥å‡½æ•°æ—¶ï¼Œè¯·ç¡®ä¿ä¼ å…¥çš„å‚æ•°æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œå¹¶ä¸”ç¬¦åˆISO 639æ ‡å‡†æˆ–å·²çŸ¥çš„è¯­è¨€åç§°ã€‚è‹¥è¾“å…¥æ— æ•ˆï¼Œå‡½æ•°å°†æŠ›å‡ºå¼‚å¸¸ï¼Œéœ€åœ¨è°ƒç”¨æ—¶åšå¥½å¼‚å¸¸å¤„ç†ã€‚

**Output Example**: å‡è®¾è¾“å…¥å‚æ•°ä¸º"en"ï¼Œå‡½æ•°å°†è¿”å›"English"ã€‚å¦‚æœè¾“å…¥å‚æ•°ä¸º"invalid_code"ï¼Œåˆ™å°†æŠ›å‡ºValueErrorï¼Œæç¤º"Invalid language input. Please enter a valid ISO 639 code or language name."
***
### FunctionDef set_log_level(cls, v)
**set_log_level**: The function of set_log_level is to validate and set the logging level for the application.

**parameters**: The parameters of this Function.
Â· cls: This parameter refers to the class itself, allowing the method to be called on the class rather than an instance.
Â· v: A string that represents the desired logging level to be set.

**Code Description**: The set_log_level function is a class method designed to validate and convert a provided string input into a corresponding LogLevel enumeration value. The function first checks if the input value v is of type string. If it is, the function converts the string to uppercase to ensure consistency with the predefined log level constants. 

Next, the function checks if the uppercase version of v exists within the members of the LogLevel enumeration, specifically by referencing LogLevel._value2member_map_. This mapping allows the function to verify if the provided value corresponds to one of the valid log levels defined in the LogLevel class, which includes DEBUG, INFO, WARNING, ERROR, and CRITICAL.

If the value is valid, the function returns the corresponding LogLevel enumeration member. However, if the value does not match any of the predefined log levels, the function raises a ValueError, indicating that the provided log level is invalid. This mechanism ensures that only valid log levels are accepted, maintaining the integrity of the logging configuration within the application.

The set_log_level function is closely related to the LogLevel class, which defines the valid logging levels as an enumeration. This relationship is crucial as it ensures that the logging level set by the ProjectSettings class is always one of the predefined constants, thus preventing runtime errors associated with invalid log levels.

**Note**: When using the set_log_level function, it is important to provide the log level as a string in uppercase to match the enumeration definitions. This practice helps avoid errors and ensures that the logging configuration is set correctly.

**Output Example**: If the input value is "info", the function will convert it to "INFO" and return LogLevel.INFO. If the input value is "verbose", the function will raise a ValueError with the message "Invalid log level: VERBOSE".
***
## ClassDef MaxInputTokens
**MaxInputTokens**: The function of MaxInputTokens is to define and manage the token limits for various AI models.

**attributes**: The attributes of this Class.
Â· gpt_4o_mini: int - Represents the token limit for the "gpt-4o-mini" model, defaulting to 128,000 tokens.  
Â· gpt_4o: int - Represents the token limit for the "gpt-4o" model, defaulting to 128,000 tokens.  
Â· o1_preview: int - Represents the token limit for the "o1-preview" model, defaulting to 128,000 tokens.  
Â· o1_mini: int - Represents the token limit for the "o1-mini" model, defaulting to 128,000 tokens.  

**Code Description**: The MaxInputTokens class is a subclass of BaseModel, which is likely part of a data validation library such as Pydantic. This class is designed to encapsulate the configuration of token limits for different AI models. Each model has a predefined token limit set to 128,000 tokens. The class utilizes the `Field` function to define these attributes, allowing for the specification of aliases that can be used to refer to these fields in a more user-friendly manner.

The class includes two class methods: `get_valid_models` and `get_token_limit`. The `get_valid_models` method returns a list of valid model names by iterating over the model fields and extracting their aliases. This is useful for validating model names against a known set of options. The `get_token_limit` method takes a model name as an argument, creates an instance of the MaxInputTokens class, and retrieves the corresponding token limit by accessing the attribute that matches the model name (with hyphens replaced by underscores).

The MaxInputTokens class is utilized by other components in the project, specifically in the ChatCompletionSettings class. The `validate_model` method in ChatCompletionSettings calls `MaxInputTokens.get_valid_models()` to ensure that the provided model name is valid. If the model name is not found in the list of valid models, a ValueError is raised, ensuring that only acceptable model names are processed.

Additionally, the `get_token_limit` method in ChatCompletionSettings leverages `MaxInputTokens.get_token_limit(self.model)` to retrieve the token limit for the model specified in the settings. This integration ensures that the token limits are consistently applied and validated across the application.

**Note**: It is important to ensure that the model names used in the application match the aliases defined in the MaxInputTokens class to avoid validation errors. 

**Output Example**: For a valid model name "gpt-4o", calling `MaxInputTokens.get_token_limit("gpt-4o")` would return 128000, indicating the token limit for that model.
### FunctionDef get_valid_models(cls)
**get_valid_models**: get_valid_modelsçš„åŠŸèƒ½æ˜¯è¿”å›æ‰€æœ‰æœ‰æ•ˆæ¨¡å‹çš„åç§°æˆ–åˆ«ååˆ—è¡¨ã€‚

**parameters**: æ­¤å‡½æ•°æ²¡æœ‰å‚æ•°ã€‚

**Code Description**: get_valid_modelsæ˜¯ä¸€ä¸ªç±»æ–¹æ³•ï¼Œä¸»è¦ç”¨äºè·å–ä¸æ¨¡å‹ç›¸å…³çš„æ‰€æœ‰å­—æ®µçš„åˆ«åæˆ–åç§°ã€‚å®ƒé€šè¿‡è®¿é—®ç±»çš„model_fieldså±æ€§ï¼Œéå†å…¶ä¸­çš„æ¯ä¸€ä¸ªå­—æ®µï¼Œæå–å‡ºå­—æ®µçš„åˆ«åï¼ˆå¦‚æœå­˜åœ¨ï¼‰æˆ–å­—æ®µçš„åç§°ã€‚è¿”å›çš„ç»“æœæ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼ŒåŒ…å«äº†æ‰€æœ‰æœ‰æ•ˆæ¨¡å‹çš„åç§°æˆ–åˆ«åã€‚

åœ¨é¡¹ç›®ä¸­ï¼Œget_valid_modelså‡½æ•°è¢«ChatCompletionSettingsç±»çš„validate_modelæ–¹æ³•è°ƒç”¨ã€‚validate_modelæ–¹æ³•çš„ä½œç”¨æ˜¯éªŒè¯ä¼ å…¥çš„æ¨¡å‹åç§°æ˜¯å¦åœ¨æœ‰æ•ˆæ¨¡å‹åˆ—è¡¨ä¸­ã€‚å¦‚æœä¼ å…¥çš„æ¨¡å‹åç§°ä¸åœ¨ç”±get_valid_modelsè¿”å›çš„æœ‰æ•ˆæ¨¡å‹åˆ—è¡¨ä¸­ï¼Œvalidate_modelå°†æŠ›å‡ºä¸€ä¸ªValueErrorå¼‚å¸¸ï¼Œæç¤ºç”¨æˆ·è¾“å…¥çš„æ¨¡å‹æ— æ•ˆï¼Œå¹¶åˆ—å‡ºæ‰€æœ‰æœ‰æ•ˆæ¨¡å‹ã€‚è¿™ç§è®¾è®¡ç¡®ä¿äº†åªæœ‰æœ‰æ•ˆçš„æ¨¡å‹åç§°æ‰èƒ½è¢«ä½¿ç”¨ï¼Œä»è€Œæé«˜äº†ä»£ç çš„å¥å£®æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚

**Note**: ä½¿ç”¨æ­¤ä»£ç æ—¶ï¼Œè¯·ç¡®ä¿model_fieldså±æ€§å·²æ­£ç¡®å®šä¹‰å¹¶åŒ…å«æ‰€éœ€çš„å­—æ®µä¿¡æ¯ï¼Œä»¥é¿å…è¿è¡Œæ—¶é”™è¯¯ã€‚

**Output Example**: å‡è®¾model_fieldsåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- name: "gpt-3.5-turbo", alias: "gpt-3.5"
- name: "gpt-4", alias: None

é‚£ä¹ˆget_valid_modelsçš„è¿”å›å€¼å°†æ˜¯ï¼š
["gpt-3.5", "gpt-4"]
***
### FunctionDef get_token_limit(cls, model_name)
**get_token_limit**: get_token_limitçš„åŠŸèƒ½æ˜¯æ ¹æ®ç»™å®šçš„æ¨¡å‹åç§°è¿”å›ç›¸åº”çš„ä»¤ç‰Œé™åˆ¶å€¼ã€‚

**parameters**: è¯¥å‡½æ•°çš„å‚æ•°ã€‚
Â· model_name: å­—ç¬¦ä¸²ç±»å‹ï¼Œè¡¨ç¤ºæ¨¡å‹çš„åç§°ã€‚

**Code Description**: get_token_limitæ˜¯ä¸€ä¸ªç±»æ–¹æ³•ï¼Œæ¥å—ä¸€ä¸ªå­—ç¬¦ä¸²å‚æ•°model_nameã€‚è¯¥æ–¹æ³•é¦–å…ˆåˆ›å»ºå½“å‰ç±»çš„ä¸€ä¸ªå®ä¾‹ï¼Œç„¶åé€šè¿‡å°†model_nameä¸­çš„çŸ­æ¨ªçº¿ï¼ˆ-ï¼‰æ›¿æ¢ä¸ºä¸‹åˆ’çº¿ï¼ˆ_ï¼‰æ¥è·å–ç›¸åº”çš„å±æ€§å€¼ã€‚æœ€ç»ˆï¼Œå®ƒè¿”å›è¯¥å±æ€§çš„å€¼ï¼Œè¯¥å€¼é€šå¸¸ä»£è¡¨ä¸æŒ‡å®šæ¨¡å‹ç›¸å…³çš„ä»¤ç‰Œé™åˆ¶ã€‚æ­¤æ–¹æ³•çš„è®¾è®¡ä½¿å¾—å¯ä»¥çµæ´»åœ°æ ¹æ®ä¸åŒçš„æ¨¡å‹åç§°åŠ¨æ€è·å–å…¶å¯¹åº”çš„ä»¤ç‰Œé™åˆ¶ã€‚

**Note**: ä½¿ç”¨è¯¥ä»£ç æ—¶ï¼Œè¯·ç¡®ä¿model_nameå‚æ•°å¯¹åº”çš„å±æ€§åœ¨ç±»ä¸­æ˜¯å­˜åœ¨çš„ï¼Œå¦åˆ™å°†å¼•å‘AttributeErrorã€‚ç¡®ä¿ä¼ å…¥çš„æ¨¡å‹åç§°æ ¼å¼æ­£ç¡®ï¼Œä»¥é¿å…ä¸å¿…è¦çš„é”™è¯¯ã€‚

**Output Example**: å‡è®¾è°ƒç”¨get_token_limit("gpt-3")ï¼Œå¦‚æœgpt-3å¯¹åº”çš„å±æ€§å€¼ä¸º4096ï¼Œåˆ™è¿”å›å€¼å°†æ˜¯4096ã€‚
***
## ClassDef ChatCompletionSettings
**ChatCompletionSettings**: The function of ChatCompletionSettings is to manage and validate settings related to chat completion models used in the application.

**attributes**: The attributes of this Class.
Â· model: str - The model to be used for chat completion, defaulting to "gpt-4o-mini".  
Â· temperature: PositiveFloat - A float value that influences the randomness of the model's output, defaulting to 0.2.  
Â· request_timeout: PositiveFloat - The timeout duration for requests, defaulting to 5 seconds.  
Â· openai_base_url: str - The base URL for the OpenAI API, defaulting to "https://api.openai.com/v1".  
Â· openai_api_key: SecretStr - The API key required for authentication with the OpenAI service, marked to be excluded from certain outputs.

**Code Description**: The ChatCompletionSettings class inherits from BaseSettings and is designed to encapsulate the configuration settings necessary for interacting with OpenAI's chat completion models. It includes attributes for specifying the model type, temperature, request timeout, base URL, and API key. The class employs field validators to ensure that the provided values for the model and base URL conform to expected formats and constraints.

The `convert_base_url_to_str` method is a class method that converts the base URL into a string format before validation, ensuring that the URL is correctly formatted. The `validate_model` method checks if the specified model is valid by comparing it against a list of acceptable models obtained from the MaxInputTokens class. If the model is invalid, it raises a ValueError with a descriptive message.

Additionally, the class includes a method `get_token_limit`, which retrieves the token limit based on the specified model. This method interacts with the MaxInputTokens class to determine the appropriate limit for the current model setting.

In the context of the project, the ChatCompletionSettings class is instantiated within the Setting class, where it is used to define the chat completion settings for the application. This relationship indicates that any instance of Setting will have a corresponding ChatCompletionSettings object, allowing for structured management of chat-related configurations.

**Note**: It is important to ensure that the model specified is valid and that the API key is securely managed, as it is critical for authenticating requests to the OpenAI service.

**Output Example**: An example of the output when retrieving the token limit for a valid model might look like this:
```
{
  "model": "gpt-4o-mini",
  "token_limit": 4096
}
```
### FunctionDef convert_base_url_to_str(cls, openai_base_url)
**convert_base_url_to_str**: convert_base_url_to_str çš„åŠŸèƒ½æ˜¯å°†ç»™å®šçš„ openai_base_url è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ã€‚

**parameters**: æ­¤å‡½æ•°çš„å‚æ•°ã€‚
Â· openai_base_url: ç±»å‹ä¸º HttpUrl çš„å‚æ•°ï¼Œè¡¨ç¤º OpenAI çš„åŸºç¡€ URLã€‚

**Code Description**: convert_base_url_to_str æ˜¯ä¸€ä¸ªç±»æ–¹æ³•ï¼Œæ¥å—ä¸€ä¸ª HttpUrl ç±»å‹çš„å‚æ•° openai_base_urlï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²ã€‚è¯¥æ–¹æ³•ä½¿ç”¨ Python çš„å†…ç½® str() å‡½æ•°æ¥å®ç°è½¬æ¢ã€‚HttpUrl æ˜¯ä¸€ä¸ªç±»å‹æç¤ºï¼Œé€šå¸¸ç”¨äºç¡®ä¿ä¼ å…¥çš„ URL æ˜¯æœ‰æ•ˆçš„æ ¼å¼ã€‚æ­¤æ–¹æ³•çš„ä¸»è¦ç”¨é€”æ˜¯åœ¨éœ€è¦å°† URL ä½œä¸ºå­—ç¬¦ä¸²å¤„ç†æ—¶ï¼Œç¡®ä¿ç±»å‹çš„ä¸€è‡´æ€§å’Œæ­£ç¡®æ€§ã€‚

**Note**: ä½¿ç”¨æ­¤ä»£ç æ—¶ï¼Œè¯·ç¡®ä¿ä¼ å…¥çš„ openai_base_url æ˜¯æœ‰æ•ˆçš„ HttpUrl ç±»å‹ï¼Œä»¥é¿å…ç±»å‹é”™è¯¯æˆ–å¼‚å¸¸ã€‚

**Output Example**: å‡è®¾ä¼ å…¥çš„ openai_base_url ä¸º "https://api.openai.com/v1/", åˆ™è¯¥å‡½æ•°çš„è¿”å›å€¼å°†æ˜¯ "https://api.openai.com/v1/"ã€‚
***
### FunctionDef validate_model(cls, value)
**validate_model**: The function of validate_model is to ensure that a given model name is valid by checking it against a list of predefined valid models.

**parameters**:
Â· value: str - A string representing the model name to be validated.

**Code Description**:  
The `validate_model` method is a class method that verifies if a given model name is part of the set of valid model names. This function accepts a single parameter, `value`, which is expected to be a string representing the model name.

1. **Validation Process**:  
   The function calls the `get_valid_models` method from the `MaxInputTokens` class. This method returns a list of valid model names, which includes the aliases of the models defined in the `MaxInputTokens` class. 

2. **Comparison**:  
   The provided `value` (the model name to be validated) is then checked to see if it exists within the list of valid models. If the model name is not found, the function raises a `ValueError`, indicating that the provided model is invalid and listing the valid options.

3. **Return**:  
   If the model name is valid (i.e., it exists in the list of valid models), the function returns the same model name (`value`).

The `validate_model` function is used primarily to ensure that only models which are defined as valid in the system are accepted for further processing. By calling the `MaxInputTokens.get_valid_models()` method, the function directly leverages the list of predefined models to perform this check.

**Note**:  
- It is important to ensure that the `MaxInputTokens.get_valid_models()` method correctly returns the list of valid model names, including any aliases or variations. If the model name provided to `validate_model` does not match a valid entry, a `ValueError` will be raised, which could interrupt the workflow.
- This function expects the model names to be exactly as defined in the valid models list, and does not perform any automatic corrections or formatting on the input value.

**Output Example**:  
For a valid input model name "gpt-4o", assuming this model is present in the valid models list returned by `MaxInputTokens.get_valid_models()`, the function would simply return "gpt-4o".

In the case of an invalid model name like "gpt-5", the function would raise an exception:
```
ValueError: Invalid model 'gpt-5'. Must be one of ['gpt-4o', 'gpt-4o-mini', 'o1-preview', 'o1-mini'].
```
***
### FunctionDef get_token_limit(self)
**get_token_limit**: The function of get_token_limit is to retrieve the token limit associated with a specified AI model.

**parameters**: 
Â· None.

**Code Description**:  
The `get_token_limit` function is a method defined within the `ChatCompletionSettings` class. It is responsible for retrieving the token limit corresponding to the model specified in the instance's `model` attribute. 

The function works by calling the `get_token_limit` method of the `MaxInputTokens` class, which is designed to return the token limit for a given AI model. The method passes the value of `self.model` (which represents the model name) to `MaxInputTokens.get_token_limit()`. The `get_token_limit` method in `MaxInputTokens` is a class method that accepts a model name as a string and returns the token limit for that model. It does this by accessing the appropriate attribute in the `MaxInputTokens` class, which corresponds to the given model name (with hyphens replaced by underscores).

The relationship with other components in the project is as follows:  
1. The `ChatCompletionSettings` class utilizes the `get_token_limit` method to dynamically fetch the token limit for the model specified in its settings. 
2. The method relies on the `MaxInputTokens` class, which encapsulates predefined token limits for different models. This connection ensures that the `get_token_limit` function in `ChatCompletionSettings` accurately reflects the correct token limit based on the specified model.
3. In the `MaxInputTokens` class, the `get_token_limit` method is a class method that matches model names with their corresponding attributes and retrieves the token limit (defaulting to 128,000 tokens for each model).

**Note**:  
It is important to ensure that the model name specified in `self.model` matches one of the valid model names defined in the `MaxInputTokens` class, such as "gpt-4o" or "o1-mini", to avoid errors. If an invalid model name is provided, the method will raise an exception when attempting to fetch the token limit.

**Output Example**:  
If the `model` attribute of the `ChatCompletionSettings` instance is set to `"gpt-4o"`, calling `get_token_limit()` will return `128000`, which is the token limit for the "gpt-4o" model as defined in the `MaxInputTokens` class.
***
## ClassDef Setting
**Setting**: The function of Setting is to aggregate and manage configuration settings for the project, including project-specific and chat completion settings.

**attributes**: The attributes of this Class.
Â· project: ProjectSettings - An instance that holds the configuration settings related to the project, including repository paths, documentation hierarchy, language preferences, and logging configurations.  
Â· chat_completion: ChatCompletionSettings - An instance that manages settings related to chat completion models, including model type, temperature, request timeout, and API key.

**Code Description**: The Setting class inherits from BaseSettings and serves as a central configuration class that encapsulates various settings required for the project. It contains two primary attributes: `project`, which is an instance of the ProjectSettings class, and `chat_completion`, which is an instance of the ChatCompletionSettings class. 

The ProjectSettings class is responsible for managing the configuration settings specific to the project, such as the target repository directory path, hierarchy name for documentation, language preferences, maximum thread count, and logging level. It ensures that the values assigned to these attributes are valid through field validators, enhancing the robustness of the configuration.

The ChatCompletionSettings class, on the other hand, manages settings related to chat completion models used in the application. It includes attributes for specifying the model type, temperature, request timeout, base URL for the OpenAI API, and the API key required for authentication. This class also employs field validators to ensure that the provided values conform to expected formats and constraints.

The Setting class is referenced by the SettingsManager class, which is responsible for managing the instantiation of the Setting object. The SettingsManager maintains a private class attribute `_setting_instance` that holds the instance of the Setting class. The `get_setting` class method checks if the `_setting_instance` has been initialized; if not, it creates a new instance of Setting. This design pattern ensures that there is a single instance of the Setting class throughout the application, promoting consistent access to configuration settings.

**Note**: When using the Setting class, it is important to ensure that the values assigned to the attributes of ProjectSettings and ChatCompletionSettings are valid to avoid runtime errors. Proper management of the API key in ChatCompletionSettings is crucial for secure authentication with the OpenAI service.
## ClassDef SettingsManager
**SettingsManager**: The function of SettingsManager is to manage the instantiation and access to the configuration settings for the project.

**attributes**: The attributes of this Class.
Â· _setting_instance: Optional[Setting] - A private class attribute that holds the singleton instance of the Setting class, initially set to None.

**Code Description**: The SettingsManager class is designed to provide a centralized access point for the configuration settings of the project. It utilizes a class method, `get_setting`, to ensure that there is only one instance of the Setting class throughout the application, implementing the Singleton design pattern.

The class maintains a private class attribute, `_setting_instance`, which is initially set to None. When the `get_setting` method is called, it first checks if `_setting_instance` is None, indicating that the Setting object has not yet been instantiated. If this is the case, it creates a new instance of the Setting class and assigns it to `_setting_instance`. This ensures that subsequent calls to `get_setting` return the same instance of the Setting class, thereby promoting consistent access to configuration settings across the application.

The SettingsManager class is called by various components within the project, including the ChangeDetector, ChatEngine, and MetaInfo classes. For instance, in the `get_to_be_staged_files` method of the ChangeDetector class, the SettingsManager is invoked to retrieve the current settings, which are then used to determine the project hierarchy and manage file staging. Similarly, in the ChatEngine's `__init__` method, the SettingsManager is used to access the OpenAI API settings, ensuring that the chat engine is configured correctly with the necessary parameters.

This design allows for a clear separation of concerns, where the SettingsManager handles the instantiation and retrieval of settings, while other components focus on their specific functionalities. By centralizing the configuration management, the SettingsManager enhances the maintainability and scalability of the project.

**Note**: It is important to ensure that the Setting class is properly configured before accessing its attributes through the SettingsManager. Any misconfiguration may lead to runtime errors when the application attempts to utilize the settings.

**Output Example**: A possible appearance of the code's return value when calling `SettingsManager.get_setting()` could be an instance of the Setting class containing project-specific configurations such as project paths, logging levels, and chat completion settings.
### FunctionDef get_setting(cls)
**get_setting**: The function of get_setting is to provide a singleton instance of the Setting class, ensuring that configuration settings are consistently accessed throughout the application.

**parameters**: The parameters of this Function.
Â· No parameters are required for this function.

**Code Description**: The get_setting class method is a crucial component of the SettingsManager class, designed to manage the instantiation of the Setting object. This method first checks if the class attribute `_setting_instance` is None, indicating that the Setting instance has not yet been created. If it is None, the method initializes `_setting_instance` by creating a new instance of the Setting class. This ensures that only one instance of the Setting class exists, adhering to the singleton design pattern. The method then returns the `_setting_instance`, allowing other parts of the application to access the configuration settings encapsulated within the Setting instance.

The Setting class itself is responsible for managing various configuration settings for the project, including project-specific settings and chat completion settings. It contains attributes that hold instances of ProjectSettings and ChatCompletionSettings, which further manage specific configurations related to the project and chat functionalities, respectively.

The get_setting method is called by various components within the project, such as the ChangeDetector, ChatEngine, and MetaInfo classes. For instance, in the ChangeDetector's get_to_be_staged_files method, get_setting is invoked to retrieve the current project settings, which are then used to determine which files need to be staged based on the project's hierarchy and markdown documentation requirements. Similarly, in the ChatEngine's __init__ method, get_setting is called to configure the OpenAI API settings, ensuring that the chat functionalities are properly initialized with the correct parameters.

This method plays a vital role in maintaining a centralized access point for configuration settings, promoting consistency and reducing the risk of errors that may arise from multiple instances of the Setting class.

**Note**: It is important to ensure that the Setting class is properly configured before accessing its attributes through get_setting. Any misconfiguration may lead to runtime errors or unexpected behavior in the application.

**Output Example**: A possible appearance of the code's return value could be an instance of the Setting class containing initialized attributes for project settings and chat completion settings, such as:
```
Setting(
    project=ProjectSettings(
        target_repo='path/to/repo',
        hierarchy_name='documentation',
        log_level='INFO',
        ignore_list=['*.pyc', '__pycache__']
    ),
    chat_completion=ChatCompletionSettings(
        openai_api_key='your_api_key',
        openai_base_url='https://api.openai.com',
        request_timeout=30,
        model='gpt-3.5-turbo',
        temperature=0.7
    )
)
```
***



================================================
FILE: markdown_docs/repo_agent/utils/gitignore_checker.md
================================================
## ClassDef GitignoreChecker
**GitignoreChecker**: The function of GitignoreChecker is to check files and folders in a specified directory against patterns defined in a .gitignore file, identifying which files are not ignored and have a specific extension.

**attributes**: The attributes of this Class.
Â· directory: The directory to be checked for files and folders.
Â· gitignore_path: The path to the .gitignore file.
Â· folder_patterns: A list of folder patterns extracted from the .gitignore file.
Â· file_patterns: A list of file patterns extracted from the .gitignore file.

**Code Description**: The GitignoreChecker class is designed to facilitate the checking of files and folders in a specified directory against the rules defined in a .gitignore file. Upon initialization, it requires two parameters: the directory to be checked and the path to the .gitignore file. The constructor reads the .gitignore file, parsing its contents to separate folder patterns from file patterns.

The class contains several methods:
- `_load_gitignore_patterns`: This method attempts to load the .gitignore file from the specified path. If the file is not found, it falls back to a default .gitignore file located two directories up from the current file. It returns a tuple containing lists of folder and file patterns.
- `_parse_gitignore`: This static method processes the content of the .gitignore file, extracting valid patterns while ignoring comments and empty lines.
- `_split_gitignore_patterns`: This static method takes a list of patterns and categorizes them into folder patterns (ending with a '/') and file patterns.
- `_is_ignored`: This static method checks if a given path matches any of the provided patterns, determining if the path should be ignored based on whether it is a directory or a file.
- `check_files_and_folders`: This method walks through the specified directory, checking each file and folder against the extracted patterns. It returns a list of file paths that are not ignored and have a '.py' extension, with paths being relative to the specified directory.

The GitignoreChecker is utilized in the `generate_overall_structure` method of the FileHandler class. In this context, it is instantiated to check the repository's directory for files that are not ignored by the .gitignore rules. The method iterates over the list of non-ignored files, performing additional checks and processing for each file, ultimately contributing to the generation of the repository's overall structure.

**Note**: When using the GitignoreChecker, ensure that the specified .gitignore file is accessible and correctly formatted to avoid falling back to the default path unintentionally.

**Output Example**: An example output of the `check_files_and_folders` method might look like this:
```
[
    "src/module1.py",
    "src/module2.py",
    "tests/test_module1.py"
]
``` 
This output indicates that the listed Python files are not ignored according to the rules defined in the .gitignore file.
### FunctionDef __init__(self, directory, gitignore_path)
**__init__**: The function of __init__ is to initialize the GitignoreChecker with a specific directory and the path to a .gitignore file.

**parameters**: The parameters of this Function.
Â· directory: The directory to be checked.
Â· gitignore_path: The path to the .gitignore file.

**Code Description**: The __init__ method is the constructor for the GitignoreChecker class. It takes two parameters: `directory`, which specifies the directory that will be checked for files and folders to ignore, and `gitignore_path`, which indicates the location of the .gitignore file that contains the ignore patterns. Upon initialization, these parameters are assigned to instance variables `self.directory` and `self.gitignore_path`, respectively.

Additionally, the constructor calls the private method `_load_gitignore_patterns`, which is responsible for loading and parsing the .gitignore file. This method returns a tuple containing two lists: one for folder patterns and another for file patterns. These lists are then assigned to the instance variables `self.folder_patterns` and `self.file_patterns`. This structured approach ensures that the GitignoreChecker has immediate access to the relevant patterns for processing files and directories according to the rules defined in the .gitignore file.

The `_load_gitignore_patterns` method is crucial for the initialization process, as it ensures that the patterns are correctly loaded and categorized. If the specified .gitignore file is not found, the method will attempt to load a default .gitignore file from a predetermined location, ensuring that the GitignoreChecker can still function even in the absence of a user-defined file.

**Note**: It is important to ensure that the provided .gitignore file is correctly formatted and accessible at the specified path to avoid falling back to the default file unintentionally. Proper handling of file paths and existence checks is essential for the reliable operation of the GitignoreChecker.
***
### FunctionDef _load_gitignore_patterns(self)
**_load_gitignore_patterns**: The function of _load_gitignore_patterns is to load and parse the .gitignore file, then split the patterns into folder and file patterns.

**parameters**: The parameters of this Function.
Â· self: An instance of the GitignoreChecker class, which contains the attributes necessary for loading the .gitignore file.

**Code Description**: The _load_gitignore_patterns method is responsible for reading the content of a .gitignore file from a specified path. If the specified file is not found, it falls back to a default .gitignore file located two directories up from the current file's directory. The method attempts to open the .gitignore file in read mode with UTF-8 encoding. If successful, it reads the entire content of the file into a string variable named gitignore_content. In the event of a FileNotFoundError, the method constructs a default path and attempts to read from that file instead.

Once the content of the .gitignore file is obtained, the method calls the _parse_gitignore function, passing the gitignore_content as an argument. This function processes the content and returns a list of patterns that are relevant for ignoring files and directories. Subsequently, the _load_gitignore_patterns method calls the _split_gitignore_patterns function, providing it with the list of patterns. This function categorizes the patterns into two separate lists: one for folder patterns and another for file patterns. Finally, _load_gitignore_patterns returns a tuple containing these two lists.

This method is invoked during the initialization of the GitignoreChecker class, where it is used to populate the folder_patterns and file_patterns attributes with the relevant patterns extracted from the .gitignore file. This structured approach ensures that the patterns are readily available for further processing or application within the project.

**Note**: It is essential to ensure that the .gitignore file is properly formatted and accessible at the specified path to avoid falling back to the default file unintentionally.

**Output Example**: An example of the return value from _load_gitignore_patterns could be:
```python
(['src', 'docs'], ['README.md', 'LICENSE'])
```
In this example, the method would return a tuple where the first list contains folder patterns 'src' and 'docs', while the second list contains file patterns 'README.md' and 'LICENSE'.
***
### FunctionDef _parse_gitignore(gitignore_content)
**_parse_gitignore**: The function of _parse_gitignore is to parse the content of a .gitignore file and return a list of patterns.

**parameters**: The parameters of this Function.
Â· gitignore_content: A string representing the content of the .gitignore file.

**Code Description**: The _parse_gitignore function is designed to process the content of a .gitignore file, which typically contains rules for ignoring files and directories in a Git repository. The function takes a single argument, gitignore_content, which is expected to be a string containing the raw text of the .gitignore file.

The function begins by initializing an empty list called patterns. It then splits the gitignore_content into individual lines using the splitlines() method. For each line, it performs the following operations:
1. It trims any leading or trailing whitespace using the strip() method.
2. It checks if the line is not empty and does not start with a "#" character, which denotes a comment in .gitignore files.
3. If the line meets these criteria, it appends the line to the patterns list.

Once all lines have been processed, the function returns the patterns list, which contains only the relevant patterns extracted from the .gitignore content.

The _parse_gitignore function is called by the _load_gitignore_patterns method within the GitignoreChecker class. The _load_gitignore_patterns method is responsible for loading the content of a .gitignore file from a specified path. After reading the file content, it invokes _parse_gitignore to extract the patterns before further processing them. This relationship highlights the utility of _parse_gitignore as a helper function that simplifies the task of filtering out valid patterns from the potentially noisy content of a .gitignore file.

**Note**: It is important to ensure that the input to _parse_gitignore is a properly formatted string representing the content of a .gitignore file. Lines that are empty or comments will be ignored in the output.

**Output Example**: An example of the return value from _parse_gitignore could be:
```python
["*.log", "temp/", "build/", "# Ignore all .env files"]
```
In this example, the function would return a list containing the patterns that are relevant for ignoring files and directories, excluding any comments or empty lines.
***
### FunctionDef _split_gitignore_patterns(gitignore_patterns)
**_split_gitignore_patterns**: The function of _split_gitignore_patterns is to separate .gitignore patterns into distinct lists for folder patterns and file patterns.

**parameters**: The parameters of this Function.
Â· gitignore_patterns: A list of patterns extracted from the .gitignore file.

**Code Description**: The _split_gitignore_patterns function takes a list of patterns from a .gitignore file as input. It iterates through each pattern and checks whether it ends with a forward slash ("/"). If a pattern ends with "/", it is identified as a folder pattern, and the trailing slash is removed before appending it to the folder_patterns list. If a pattern does not end with "/", it is treated as a file pattern and is added to the file_patterns list. The function ultimately returns a tuple containing two lists: the first list includes all folder patterns, while the second list contains all file patterns.

This function is called by the _load_gitignore_patterns method within the GitignoreChecker class. The _load_gitignore_patterns method is responsible for loading and parsing the contents of a .gitignore file. After reading the file, it utilizes the _parse_gitignore method to extract the patterns from the content. Once the patterns are obtained, _load_gitignore_patterns calls _split_gitignore_patterns to categorize these patterns into folder and file patterns before returning them as a tuple. This structured approach ensures that the patterns are organized for further processing or application within the project.

**Note**: It is important to ensure that the input list of gitignore_patterns is properly formatted according to .gitignore syntax to achieve accurate results when splitting the patterns.

**Output Example**: An example of the function's return value could be:
```python
(['src', 'docs'], ['README.md', 'LICENSE'])
```
In this example, the first list contains folder patterns 'src' and 'docs', while the second list contains file patterns 'README.md' and 'LICENSE'.
***
### FunctionDef _is_ignored(path, patterns, is_dir)
**_is_ignored**: The function of _is_ignored is to determine if a given path matches any specified patterns, indicating whether the path should be ignored.

**parameters**: The parameters of this Function.
Â· parameter1: path (str) - The path to check against the patterns.
Â· parameter2: patterns (list) - A list of patterns that the path will be checked against.
Â· parameter3: is_dir (bool) - A boolean indicating if the path is a directory; defaults to False.

**Code Description**: The _is_ignored function checks if the provided path matches any of the patterns in the given list. It utilizes the fnmatch module to perform pattern matching. The function iterates through each pattern in the patterns list and checks if the path matches the pattern directly. If the path is a directory (indicated by the is_dir parameter being True), it also checks if the pattern ends with a slash ("/") and if the path matches the pattern without the trailing slash. If any match is found, the function returns True, indicating that the path should be ignored. If no matches are found after checking all patterns, it returns False.

This function is called by the check_files_and_folders method within the GitignoreChecker class. The check_files_and_folders method is responsible for traversing a specified directory and checking each file and folder against the patterns defined for files and folders. It uses _is_ignored to filter out any directories and files that should be ignored based on the patterns provided. The result of this method is a list of files that are not ignored and have a '.py' extension, thus ensuring that only relevant files are returned for further processing.

**Note**: It is important to ensure that the patterns provided are correctly formatted for fnmatch to work as expected. Additionally, the is_dir parameter should be set appropriately when checking directory paths to ensure accurate matching.

**Output Example**: If the function is called with the path "src/main.py" and the patterns ["*.py", "test/"], the expected return value would be True if "src/main.py" matches any of the patterns, indicating that it is not ignored. If the path were "src/test.py" and the patterns were ["test/"], the function would return True, indicating that it should be ignored.
***
### FunctionDef check_files_and_folders(self)
**check_files_and_folders**: The function of check_files_and_folders is to check all files and folders in the specified directory against the defined gitignore patterns and return a list of files that are not ignored and have the '.py' extension.

**parameters**: The parameters of this Function.
Â· parameter1: None - This function does not take any parameters directly as it operates on the instance's attributes.

**Code Description**: The check_files_and_folders method is responsible for traversing the directory specified by the instance variable self.directory. It utilizes the os.walk function to iterate through all directories and files within the specified path. For each directory, it filters out those that should be ignored based on the patterns defined in self.folder_patterns by calling the _is_ignored method with the is_dir parameter set to True.

For each file encountered, the method constructs the full file path and its relative path to the base directory. It then checks if the file should be ignored by calling the _is_ignored method again, this time with the file name and the patterns defined in self.file_patterns. Additionally, it checks if the file has a '.py' extension. If both conditions are satisfied (the file is not ignored and has a '.py' extension), the relative path of the file is added to the not_ignored_files list.

The method ultimately returns a list of paths to Python files that are not ignored, allowing further processing of relevant files in the project.

This method is called by the generate_overall_structure method in the FileHandler class. In this context, it is used to gather a list of files that should be processed from a repository, excluding any files that are ignored according to the gitignore patterns. The results from check_files_and_folders are then iterated over, and each file is further processed to generate the overall structure of the repository.

**Note**: It is essential to ensure that the gitignore patterns are correctly defined and formatted for accurate matching. The method relies on the _is_ignored function to determine which files and directories should be excluded based on these patterns.

**Output Example**: If the method is executed in a directory containing files such as "script.py", "test_script.py", and "README.md", and the gitignore patterns include "*.py", the expected return value would be a list like ["script.py", "test_script.py"] if those files are not ignored.
***



================================================
FILE: markdown_docs/repo_agent/utils/meta_info_utils.md
================================================
## FunctionDef make_fake_files
**make_fake_files**: The function of make_fake_files is to analyze the git status of a repository and create temporary files that reflect the current state of the working directory, specifically for untracked and unstaged changes.

**parameters**: The parameters of this Function.
Â· No parameters are required for this function.

**Code Description**: The make_fake_files function is designed to interact with a Git repository to detect changes in the working directory that have not been staged for commit. It performs the following key operations:

1. **Delete Existing Fake Files**: The function begins by calling delete_fake_files to ensure that any previously created temporary files are removed before generating new ones.

2. **Retrieve Project Settings**: It retrieves the current project settings using the SettingsManager's get_setting method, which ensures consistent access to configuration settings throughout the application.

3. **Initialize Git Repository**: The function initializes a Git repository object using the target repository path specified in the project settings.

4. **Detect Unstaged Changes**: It identifies unstaged changes in the repository using the index.diff method, which returns a list of modified files that have not been added to the staging area. Additionally, it collects untracked files that exist in the file system but are not tracked by Git.

5. **Skip Untracked Python Files**: The function iterates through the list of untracked files and skips any that have a ".py" extension, logging a message for each skipped file.

6. **Handle New and Modified Files**: For files that have been modified (but not staged), the function checks if they end with a specific substring (latest_verison_substring). If they do, an error is logged, and the function exits. Otherwise, it renames the original file to include the latest version substring and creates a new file with the original name, writing the original content back into it.

7. **Return Values**: Finally, the function returns a dictionary mapping the original file paths to their corresponding fake file paths, along with a list of files that were skipped during processing.

The make_fake_files function is called within the diff function in the main.py file. This function is responsible for checking for changes in the repository and determining which documents need to be updated or generated. By calling make_fake_files, the diff function ensures that the current state of the repository is accurately reflected in the documentation process.

**Note**: It is crucial to ensure that the target repository is properly configured and that the latest_verison_substring does not conflict with existing file names. Any misconfiguration may lead to runtime errors or unexpected behavior during the execution of this function.

**Output Example**: A possible appearance of the code's return value when calling make_fake_files could be:
```
({
    'original_file_path.py': 'original_file_path.latest_version',
    'another_file.py': 'another_file.latest_version'
}, ['skipped_file.py'])
```
## FunctionDef delete_fake_files
**delete_fake_files**: The function of delete_fake_files is to remove temporary files generated during the documentation process after the task execution is completed.

**parameters**: The parameters of this Function.
Â· No parameters are required for this function.

**Code Description**: The delete_fake_files function is responsible for cleaning up temporary files, referred to as "fake files," that are created during the documentation generation process. This function utilizes a nested helper function, gci, which performs a recursive traversal of the directory specified by the project settings to identify and delete or rename files based on specific criteria.

The function begins by retrieving the project settings through the SettingsManager's get_setting method, which ensures that the configuration settings are consistently accessed throughout the application. The gci function is then called with the target repository path, which is obtained from the settings.

Within the gci function, the following operations are performed:
1. The function lists all files and directories within the specified filepath.
2. For each file, it checks if it is a directory and recursively calls itself if it is.
3. If the file ends with a specific substring (latest_verison_substring), it indicates that it is a temporary file. The function then constructs the original file name by replacing the substring with ".py".
4. If the original file exists and is successfully deleted, the function prints a message indicating that the temporary file has been deleted. If the temporary file is empty, it is also deleted.
5. If the original file exists but is not empty, the temporary file is renamed back to the original file name, and a message is printed indicating that the latest version has been recovered.

The delete_fake_files function is called in various parts of the project, including the clean function in main.py, which explicitly invokes delete_fake_files to ensure that all temporary files are removed after the documentation process. Additionally, it is called within the diff function to clean up any fake files before checking for changes in the repository. The run method of the Runner class also calls delete_fake_files after completing the document update process, ensuring that any temporary files created during the run are cleaned up.

**Note**: It is important to ensure that the target repository is correctly configured and accessible before invoking delete_fake_files. Any issues with file permissions or incorrect paths may lead to runtime errors during the deletion or renaming processes.
### FunctionDef gci(filepath)
**gci**: The function of gci is to traverse a specified directory and its subdirectories to delete or rename files based on specific criteria.

**parameters**: The parameters of this Function.
Â· filepath: A string representing the path of the directory to be traversed.

**Code Description**: The gci function begins by listing all files and directories within the specified filepath. It iterates through each item found in the directory. If an item is a directory, the function calls itself recursively to traverse that subdirectory. For files, it checks if the filename ends with a specific substring defined as `latest_verison_substring`. If this condition is met, the function constructs an original filename by replacing the substring with ".py". 

The function then checks the size of the file. If the file size is zero, it indicates that the file is empty, and the function proceeds to delete both the empty file and its corresponding original file. A message is printed to the console indicating the deletion of the temporary file. Conversely, if the file is not empty, the function renames the temporary file back to its original name and prints a message indicating that the latest version is being recovered.

This function effectively manages temporary files by either deleting them if they are empty or restoring the original file if they contain data, ensuring that the directory remains clean and organized.

**Note**: It is important to ensure that the `latest_verison_substring` variable is defined in the scope where this function is used, as it is crucial for determining which files to process. Additionally, the function relies on the presence of the `setting.project.target_repo` variable to format the output messages correctly.
***



================================================
FILE: markdown_docs/tests/test_change_detector.md
================================================
## ClassDef TestChangeDetector
**TestChangeDetector**: The function of TestChangeDetector is to perform unit tests on the ChangeDetector class, specifically focusing on the detection and management of staged and unstaged files in a Git repository.

**attributes**: The attributes of this Class.
Â· test_repo_path: The file path to the test repository created for the unit tests.
Â· repo: The initialized Git repository object used for testing.

**Code Description**: The TestChangeDetector class is a unit test case that inherits from unittest.TestCase, providing a framework for testing the functionality of the ChangeDetector class. The class includes setup and teardown methods to prepare and clean up the test environment, specifically a Git repository used for testing file changes.

The setUpClass method is a class method that initializes the test environment before any tests are run. It defines the path for the test repository, creates the directory if it does not exist, initializes a new Git repository, and configures user information for Git operations. It also creates two test files: a Python file and a Markdown file, and performs an initial commit to the repository.

The class contains three test methods:
1. test_get_staged_pys: This method tests the ChangeDetector's ability to identify staged Python files. It creates a new Python file, stages it, and asserts that the file is included in the list of staged files returned by the ChangeDetector.
   
2. test_get_unstaged_mds: This method tests the ChangeDetector's ability to identify unstaged Markdown files. It modifies an existing Markdown file without staging it and asserts that the modified file is included in the list of unstaged files returned by the ChangeDetector.

3. test_add_unstaged_mds: This method ensures that there are unstaged Markdown files and then uses the ChangeDetector to stage them. It checks that after the staging operation, there are no remaining unstaged Markdown files, asserting that the operation was successful.

The tearDownClass method is a class method that cleans up the test environment after all tests have been executed. It closes the Git repository and removes the test repository directory to ensure no residual files remain.

**Note**: It is important to ensure that the ChangeDetector class is properly implemented and available in the testing environment for these tests to execute successfully. Additionally, the tests rely on the presence of the Git command-line tools and the appropriate permissions to create and manipulate files and directories.
### FunctionDef setUpClass(cls)
**setUpClass**: setUpClassçš„åŠŸèƒ½æ˜¯ä¸ºæµ‹è¯•å‡†å¤‡ä¸€ä¸ªGitä»“åº“åŠç›¸å…³æ–‡ä»¶ã€‚

**parameters**: æ­¤å‡½æ•°æ²¡æœ‰å‚æ•°ã€‚

**Code Description**: 
setUpClassæ˜¯ä¸€ä¸ªç±»æ–¹æ³•ï¼Œç”¨äºåœ¨æµ‹è¯•ç±»æ‰§è¡Œä¹‹å‰è®¾ç½®æµ‹è¯•ç¯å¢ƒã€‚è¯¥æ–¹æ³•é¦–å…ˆå®šä¹‰äº†æµ‹è¯•ä»“åº“çš„è·¯å¾„ï¼Œå°†å…¶è®¾ç½®ä¸ºå½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•ä¸‹çš„'test_repo'æ–‡ä»¶å¤¹ã€‚å¦‚æœè¯¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œæ–¹æ³•ä¼šåˆ›å»ºå®ƒã€‚æ¥ç€ï¼Œä½¿ç”¨GitPythonåº“åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„Gitä»“åº“ï¼Œå¹¶å°†å…¶ä¸æŒ‡å®šçš„è·¯å¾„å…³è”ã€‚

åœ¨åˆå§‹åŒ–Gitä»“åº“åï¼Œæ–¹æ³•é…ç½®äº†Gitç”¨æˆ·ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç”¨æˆ·çš„ç”µå­é‚®ä»¶å’Œå§“åï¼Œä»¥ä¾¿åœ¨åç»­çš„Gitæ“ä½œä¸­ä½¿ç”¨ã€‚æ¥ä¸‹æ¥ï¼Œæ–¹æ³•åˆ›å»ºäº†ä¸¤ä¸ªæµ‹è¯•æ–‡ä»¶ï¼šä¸€ä¸ªPythonæ–‡ä»¶'test_file.py'ï¼Œå…¶ä¸­åŒ…å«ä¸€è¡Œæ‰“å°è¯­å¥ï¼›å¦ä¸€ä¸ªMarkdownæ–‡ä»¶'test_file.md'ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªMarkdownæ ‡é¢˜ã€‚

æœ€åï¼Œæ–¹æ³•æ¨¡æ‹Ÿäº†Gitæ“ä½œï¼Œé€šè¿‡å°†æ‰€æœ‰æ–‡ä»¶æ·»åŠ åˆ°æš‚å­˜åŒºå¹¶æäº¤ä¸€ä¸ªåˆå§‹æäº¤ï¼Œå®Œæˆäº†æµ‹è¯•ç¯å¢ƒçš„è®¾ç½®ã€‚è¿™äº›æ“ä½œç¡®ä¿äº†åœ¨æµ‹è¯•æ‰§è¡Œæ—¶ï¼Œæµ‹è¯•ç¯å¢ƒæ˜¯å¹²å‡€ä¸”å¯æ§çš„ã€‚

**Note**: ä½¿ç”¨æ­¤æ–¹æ³•æ—¶ï¼Œè¯·ç¡®ä¿åœ¨æµ‹è¯•ç±»ä¸­è°ƒç”¨setUpClassï¼Œä»¥ä¾¿åœ¨æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹æ‰§è¡Œä¹‹å‰æ­£ç¡®è®¾ç½®æµ‹è¯•ç¯å¢ƒã€‚åŒæ—¶ï¼Œç¡®ä¿å·²å®‰è£…GitPythonåº“ï¼Œä»¥æ”¯æŒGitæ“ä½œã€‚
***
### FunctionDef test_get_staged_pys(self)
**test_get_staged_pys**: The function of test_get_staged_pys is to verify that a newly created Python file is correctly identified as staged in the Git repository.

**parameters**: The parameters of this Function.
Â· None

**Code Description**: The test_get_staged_pys function is a unit test designed to validate the functionality of the ChangeDetector class, specifically its ability to detect staged Python files within a Git repository. The function begins by creating a new Python file named 'new_test_file.py' in a specified test repository path. This file contains a simple print statement. Once the file is created, it is added to the staging area of the Git repository using the Git command `git add`.

Following the staging of the new file, an instance of the ChangeDetector class is instantiated with the test repository path. The method get_staged_pys of the ChangeDetector instance is then called to retrieve a list of Python files that are currently staged for commit. This method is responsible for checking the differences between the staging area and the last commit (HEAD) to identify which files have been added or modified.

The test then asserts that 'new_test_file.py' is included in the list of staged files returned by get_staged_pys. This assertion confirms that the ChangeDetector class is functioning as expected, accurately tracking the newly staged Python file. Additionally, the function prints the list of staged Python files for verification purposes.

This test is crucial for ensuring that the ChangeDetector class operates correctly in identifying changes within a Git repository, particularly for Python files. It serves as a safeguard against potential regressions in the functionality of the change detection mechanism.

**Note**: It is important to ensure that the test environment is properly set up, including the availability of a valid Git repository and the necessary permissions to create and stage files. The GitPython library must also be correctly configured to facilitate interaction with the Git repository.
***
### FunctionDef test_get_unstaged_mds(self)
**test_get_unstaged_mds**: The function of test_get_unstaged_mds is to verify that a modified Markdown file, which has not been staged, is correctly identified as an unstaged file by the ChangeDetector class.

**parameters**: The parameters of this Function.
Â· No parameters are required for this function.

**Code Description**: The test_get_unstaged_mds function is a unit test designed to validate the functionality of the ChangeDetector class, specifically its ability to identify unstaged Markdown files in a Git repository. The function performs the following operations:

1. It begins by defining the path to a Markdown file named 'test_file.md' within a test repository directory specified by `self.test_repo_path`.
2. The function opens this Markdown file in append mode and writes additional content to it, simulating a modification that has not yet been staged.
3. An instance of the ChangeDetector class is then created, initialized with the path to the test repository.
4. The method `get_to_be_staged_files` of the ChangeDetector instance is called to retrieve a list of files that have been modified but not staged.
5. The function asserts that 'test_file.md' is included in the list of unstaged files by checking if its basename is present in the returned list.
6. Finally, it prints the list of unstaged Markdown files for verification.

This function is called within the test_add_unstaged_mds function, which ensures that there is at least one unstaged Markdown file before attempting to add unstaged files to the staging area. The test_add_unstaged_mds function relies on the successful execution of test_get_unstaged_mds to confirm that the ChangeDetector can accurately identify unstaged files, thereby establishing a dependency between these two test functions.

**Note**: It is essential to ensure that the test repository is correctly set up and that the necessary files exist before running this test. The test environment should be clean to avoid false positives or negatives in the test results.
***
### FunctionDef test_add_unstaged_mds(self)
**test_add_unstaged_mds**: The function of test_add_unstaged_mds is to verify that the ChangeDetector class correctly stages unstaged Markdown files in a Git repository.

**parameters**: The parameters of this Function.
Â· No parameters are required for this function.

**Code Description**: The test_add_unstaged_mds function is a unit test designed to validate the functionality of the ChangeDetector class, specifically its ability to add unstaged Markdown files to the staging area of a Git repository. The function performs the following operations:

1. It first ensures that there is at least one unstaged Markdown file by invoking the test_get_unstaged_mds function. This function modifies a Markdown file in the test repository, ensuring that it is recognized as unstaged.

2. An instance of the ChangeDetector class is created, initialized with the path to the test repository specified by `self.test_repo_path`. This instance will be used to manage the staging of files.

3. The add_unstaged_files method of the ChangeDetector instance is called. This method identifies all unstaged files that meet specific criteria and stages them in the Git repository.

4. After attempting to stage the files, the function retrieves the list of files that are still unstaged by calling the get_to_be_staged_files method. This method checks for any files that remain unstaged after the add operation.

5. The function asserts that the length of the list of unstaged files after the add operation is zero, indicating that all unstaged Markdown files have been successfully staged.

6. Finally, it prints the number of remaining unstaged Markdown files, which should be zero if the test passes.

This function is dependent on the successful execution of the test_get_unstaged_mds function, which ensures that there is at least one unstaged Markdown file before the add operation is attempted. The relationship between these two functions is crucial, as test_add_unstaged_mds relies on the outcome of test_get_unstaged_mds to validate the staging functionality of the ChangeDetector class.

**Note**: It is essential to ensure that the test repository is correctly set up and that the necessary files exist before running this test. The test environment should be clean to avoid false positives or negatives in the test results.
***
### FunctionDef tearDownClass(cls)
**tearDownClass**: tearDownClassçš„åŠŸèƒ½æ˜¯æ¸…ç†æµ‹è¯•ä»“åº“ã€‚

**parameters**: è¯¥å‡½æ•°æ²¡æœ‰å‚æ•°ã€‚

**Code Description**: 
tearDownClassæ˜¯ä¸€ä¸ªç±»æ–¹æ³•ï¼Œç”¨äºåœ¨æµ‹è¯•ç±»çš„æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹æ‰§è¡Œå®Œæ¯•åè¿›è¡Œæ¸…ç†å·¥ä½œã€‚è¯¥æ–¹æ³•é¦–å…ˆè°ƒç”¨cls.repo.close()ï¼Œç”¨äºå…³é—­ä¸æµ‹è¯•ä»“åº“ç›¸å…³çš„èµ„æºï¼Œç¡®ä¿æ²¡æœ‰æœªå…³é—­çš„è¿æ¥æˆ–æ–‡ä»¶å¥æŸ„ã€‚æ¥ç€ï¼Œä½¿ç”¨os.system('rm -rf ' + cls.test_repo_path)å‘½ä»¤åˆ é™¤æµ‹è¯•ä»“åº“çš„æ–‡ä»¶å¤¹åŠå…¶å†…å®¹ã€‚è¿™é‡Œçš„cls.test_repo_pathæ˜¯ä¸€ä¸ªç±»å±æ€§ï¼ŒæŒ‡å‘æµ‹è¯•ä»“åº“çš„è·¯å¾„ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒtearDownClassç¡®ä¿äº†æµ‹è¯•ç¯å¢ƒçš„æ•´æ´ï¼Œé¿å…äº†åç»­æµ‹è¯•å—åˆ°ä¹‹å‰æµ‹è¯•çš„å½±å“ã€‚

**Note**: ä½¿ç”¨è¯¥å‡½æ•°æ—¶ï¼Œè¯·ç¡®ä¿åœ¨æµ‹è¯•ç”¨ä¾‹æ‰§è¡Œåè°ƒç”¨ï¼Œä»¥é¿å…èµ„æºæ³„æ¼æˆ–æ–‡ä»¶å†²çªã€‚åŒæ—¶ï¼Œæ³¨æ„ä½¿ç”¨os.systemåˆ é™¤æ–‡ä»¶æ—¶è¦å°å¿ƒï¼Œä»¥å…è¯¯åˆ å…¶ä»–é‡è¦æ–‡ä»¶ã€‚
***



================================================
FILE: markdown_docs/tests/test_json_handler.md
================================================
## ClassDef TestJsonFileProcessor
**TestJsonFileProcessor**: The function of TestJsonFileProcessor is to test the functionalities of the JsonFileProcessor class, specifically its methods for reading and extracting data from JSON files.

**attributes**: The attributes of this Class.
Â· processor: An instance of the JsonFileProcessor class initialized with the filename "test.json".

**Code Description**: The TestJsonFileProcessor class is a unit test case that inherits from unittest.TestCase. It is designed to validate the behavior of the JsonFileProcessor class, which is responsible for handling JSON file operations. The class contains several test methods that utilize the unittest framework's features, such as setup methods and mocking.

The setUp method initializes an instance of JsonFileProcessor with a test JSON file named "test.json". This setup is executed before each test method runs, ensuring that each test has a fresh instance of the processor.

The test_read_json_file method tests the read_json_file method of the JsonFileProcessor class. It uses the @patch decorator to mock the built-in open function, simulating the reading of a JSON file containing a specific structure. The test asserts that the data returned by read_json_file matches the expected dictionary structure and verifies that the open function was called with the correct parameters.

The test_extract_md_contents method tests the extract_md_contents method of the JsonFileProcessor class. It mocks the read_json_file method to return a predefined JSON structure. The test checks that the extracted markdown content includes the expected value "content1".

The test_search_in_json_nested method tests the search_in_json_nested method of the JsonFileProcessor class. Similar to the previous tests, it mocks the open function to provide a different JSON structure. The test asserts that the result of the search matches the expected dictionary for the specified file name and verifies the correct invocation of the open function.

**Note**: It is important to ensure that the JsonFileProcessor class is implemented correctly for these tests to pass. The tests rely on the structure of the JSON data being consistent with the expectations set in the test cases.

**Output Example**: 
For the test_read_json_file method, the expected output when read_json_file is called would be:
{"files": [{"objects": [{"md_content": "content1"}]}]} 

For the test_extract_md_contents method, the expected output for md_contents would include:
["content1"]

For the test_search_in_json_nested method, the expected output when searching for "file1" would be:
{"name": "file1"}
### FunctionDef setUp(self)
**setUp**: setUpçš„åŠŸèƒ½æ˜¯åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒã€‚

**parameters**: è¯¥å‡½æ•°æ²¡æœ‰å‚æ•°ã€‚

**Code Description**: setUpå‡½æ•°æ˜¯ä¸€ä¸ªæµ‹è¯•å‡†å¤‡å‡½æ•°ï¼Œé€šå¸¸åœ¨å•å…ƒæµ‹è¯•æ¡†æ¶ä¸­ä½¿ç”¨ã€‚åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œåˆ›å»ºäº†ä¸€ä¸ªåä¸ºprocessorçš„å®ä¾‹ï¼Œç±»å‹ä¸ºJsonFileProcessorï¼Œå¹¶ä¼ å…¥äº†ä¸€ä¸ªå­—ç¬¦ä¸²å‚æ•°"test.json"ã€‚è¿™ä¸ªå®ä¾‹åŒ–çš„è¿‡ç¨‹æ„å‘³ç€åœ¨æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹æ‰§è¡Œä¹‹å‰ï¼Œéƒ½ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„JsonFileProcessorå¯¹è±¡ï¼Œç¡®ä¿æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹éƒ½åœ¨ä¸€ä¸ªå¹²å‡€çš„çŠ¶æ€ä¸‹è¿è¡Œã€‚JsonFileProcessorç±»çš„å…·ä½“åŠŸèƒ½å’Œå®ç°ç»†èŠ‚å¹¶æœªåœ¨æ­¤ä»£ç ç‰‡æ®µä¸­æä¾›ï¼Œä½†å¯ä»¥æ¨æµ‹å®ƒä¸å¤„ç†JSONæ–‡ä»¶ç›¸å…³ã€‚

**Note**: ä½¿ç”¨setUpå‡½æ•°æ—¶ï¼Œç¡®ä¿JsonFileProcessorç±»å·²æ­£ç¡®å®ç°ï¼Œå¹¶ä¸”"test.json"æ–‡ä»¶å­˜åœ¨äºé¢„æœŸçš„è·¯å¾„ä¸­ï¼Œä»¥é¿å…åœ¨æµ‹è¯•æ‰§è¡Œæ—¶å‡ºç°æ–‡ä»¶æœªæ‰¾åˆ°çš„é”™è¯¯ã€‚
***
### FunctionDef test_read_json_file(self, mock_file)
**test_read_json_file**: The function of test_read_json_file is æµ‹è¯• read_json_file æ–¹æ³•çš„åŠŸèƒ½ã€‚

**parameters**: æ­¤å‡½æ•°çš„å‚æ•°ã€‚
Â· mock_file: ä¸€ä¸ªæ¨¡æ‹Ÿæ–‡ä»¶å¯¹è±¡ï¼Œç”¨äºæµ‹è¯•æ–‡ä»¶è¯»å–æ“ä½œã€‚

**Code Description**: è¯¥å‡½æ•°ç”¨äºæµ‹è¯• `read_json_file` æ–¹æ³•çš„æ­£ç¡®æ€§ã€‚é¦–å…ˆï¼Œå®ƒè°ƒç”¨ `self.processor.read_json_file()` æ–¹æ³•ä»¥è¯»å– JSON æ–‡ä»¶çš„æ•°æ®ã€‚æ¥ç€ï¼Œä½¿ç”¨ `self.assertEqual` æ–¹æ³•éªŒè¯è¯»å–çš„æ•°æ®æ˜¯å¦ä¸é¢„æœŸçš„å­—å…¸ç»“æ„ç›¸ç¬¦ï¼Œå³ `{"files": [{"objects": [{"md_content": "content1"}]}]}`ã€‚æœ€åï¼Œ`mock_file.assert_called_with("test.json", "r", encoding="utf-8")` ç”¨äºç¡®è®¤åœ¨è¯»å–æ–‡ä»¶æ—¶ï¼Œæ˜¯å¦ä»¥æ­£ç¡®çš„å‚æ•°è°ƒç”¨äº†æ¨¡æ‹Ÿçš„æ–‡ä»¶å¯¹è±¡ï¼Œç¡®ä¿æ–‡ä»¶åä¸º "test.json"ï¼Œæ¨¡å¼ä¸ºåªè¯»ï¼ˆ"r"ï¼‰ï¼Œå¹¶ä¸”ä½¿ç”¨ UTF-8 ç¼–ç ã€‚

**Note**: ä½¿ç”¨æ­¤ä»£ç æ—¶ï¼Œè¯·ç¡®ä¿å·²æ­£ç¡®è®¾ç½®æ¨¡æ‹Ÿæ–‡ä»¶å¯¹è±¡ï¼Œä»¥ä¾¿èƒ½å¤Ÿå‡†ç¡®æµ‹è¯•æ–‡ä»¶è¯»å–åŠŸèƒ½ã€‚åŒæ—¶ï¼Œç¡®ä¿ `read_json_file` æ–¹æ³•èƒ½å¤Ÿå¤„ç†é¢„æœŸçš„æ–‡ä»¶æ ¼å¼å’Œå†…å®¹ã€‚
***
### FunctionDef test_extract_md_contents(self, mock_read_json)
**test_extract_md_contents**: The function of test_extract_md_contents is æµ‹è¯• extract_md_contents æ–¹æ³•çš„åŠŸèƒ½ã€‚

**parameters**: æ­¤å‡½æ•°çš„å‚æ•°ã€‚
Â· mock_read_json: ä¸€ä¸ªæ¨¡æ‹Ÿçš„å‡½æ•°ï¼Œç”¨äºæ›¿ä»£å®é™…çš„ JSON è¯»å–æ“ä½œã€‚

**Code Description**: 
è¯¥å‡½æ•°ä¸»è¦ç”¨äºæµ‹è¯• `extract_md_contents` æ–¹æ³•çš„æ­£ç¡®æ€§ã€‚é¦–å…ˆï¼Œä½¿ç”¨ `mock_read_json` æ¨¡æ‹Ÿè¯»å– JSON æ–‡ä»¶çš„æ“ä½œï¼Œè¿”å›ä¸€ä¸ªåŒ…å«æ–‡ä»¶ä¿¡æ¯çš„å­—å…¸ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªå¯¹è±¡åˆ—è¡¨ï¼Œåˆ—è¡¨ä¸­çš„æ¯ä¸ªå¯¹è±¡éƒ½æœ‰ä¸€ä¸ª `md_content` å­—æ®µã€‚å…·ä½“æ¥è¯´ï¼Œæ¨¡æ‹Ÿè¿”å›çš„ JSON æ•°æ®ç»“æ„ä¸ºï¼š
```json
{
  "files": [
    {
      "objects": [
        {
          "md_content": "content1"
        }
      ]
    }
  ]
}
```
æ¥ä¸‹æ¥ï¼Œè°ƒç”¨ `self.processor.extract_md_contents()` æ–¹æ³•ï¼Œè¯¥æ–¹æ³•çš„ç›®çš„æ˜¯æå–æ‰€æœ‰çš„ `md_content` å†…å®¹ã€‚æœ€åï¼Œä½¿ç”¨ `self.assertIn("content1", md_contents)` æ–­è¨€æ¥éªŒè¯æå–çš„å†…å®¹ä¸­æ˜¯å¦åŒ…å« "content1"ã€‚å¦‚æœåŒ…å«ï¼Œåˆ™æµ‹è¯•é€šè¿‡ï¼Œè¡¨æ˜ `extract_md_contents` æ–¹æ³•èƒ½å¤Ÿæ­£ç¡®æå–å‡º JSON æ•°æ®ä¸­çš„ Markdown å†…å®¹ã€‚

**Note**: ä½¿ç”¨æ­¤ä»£ç æ—¶ï¼Œè¯·ç¡®ä¿ `extract_md_contents` æ–¹æ³•èƒ½å¤Ÿå¤„ç†æ¨¡æ‹Ÿçš„ JSON æ•°æ®ç»“æ„ï¼Œå¹¶ä¸”åœ¨æµ‹è¯•ç¯å¢ƒä¸­æ­£ç¡®é…ç½®äº† `mock_read_json`ã€‚

**Output Example**: è¯¥å‡½æ•°çš„è¿”å›å€¼å¯èƒ½ç±»ä¼¼äºä»¥ä¸‹ç»“æ„ï¼š
```python
["content1"]
```
***
### FunctionDef test_search_in_json_nested(self, mock_file)
**test_search_in_json_nested**: The function of test_search_in_json_nested is æµ‹è¯• search_in_json_nested æ–¹æ³•çš„åŠŸèƒ½ã€‚

**parameters**: è¯¥å‡½æ•°çš„å‚æ•°ã€‚
Â· parameter1: mock_file - ä¸€ä¸ªæ¨¡æ‹Ÿæ–‡ä»¶å¯¹è±¡ï¼Œç”¨äºæµ‹è¯•æ–‡ä»¶æ“ä½œã€‚

**Code Description**: è¯¥å‡½æ•°ç”¨äºæµ‹è¯• `search_in_json_nested` æ–¹æ³•çš„åŠŸèƒ½ã€‚é¦–å…ˆï¼Œå®ƒè°ƒç”¨ `self.processor.search_in_json_nested` æ–¹æ³•ï¼Œä¼ å…¥ä¸¤ä¸ªå‚æ•°ï¼šæ–‡ä»¶å `"test.json"` å’Œè¦æœç´¢çš„å…³é”®å­— `"file1"`ã€‚è¯¥æ–¹æ³•çš„é¢„æœŸç»“æœæ˜¯è¿”å›ä¸€ä¸ªå­—å…¸ `{"name": "file1"}`ï¼Œè¡¨ç¤ºåœ¨ JSON æ–‡ä»¶ä¸­æˆåŠŸæ‰¾åˆ°ä¸å…³é”®å­—åŒ¹é…çš„æ¡ç›®ã€‚æ¥ç€ï¼Œä½¿ç”¨ `self.assertEqual` æ–¹æ³•éªŒè¯è¿”å›ç»“æœæ˜¯å¦ä¸é¢„æœŸç»“æœç›¸ç¬¦ã€‚å¦‚æœç»“æœåŒ¹é…ï¼Œåˆ™æµ‹è¯•é€šè¿‡ã€‚æœ€åï¼Œ`mock_file.assert_called_with` ç”¨äºéªŒè¯åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­æ˜¯å¦ä»¥æ­£ç¡®çš„å‚æ•°è°ƒç”¨äº†æ–‡ä»¶æ‰“å¼€æ–¹æ³•ï¼Œç¡®ä¿æ–‡ä»¶ `"test.json"` ä»¥åªè¯»æ¨¡å¼ï¼ˆ"r"ï¼‰å’Œ UTF-8 ç¼–ç æ‰“å¼€ã€‚

**Note**: ä½¿ç”¨è¯¥ä»£ç æ—¶ï¼Œè¯·ç¡®ä¿ `mock_file` å·²æ­£ç¡®é…ç½®ä¸ºæ¨¡æ‹Ÿæ–‡ä»¶æ“ä½œï¼Œä»¥é¿å…å®é™…æ–‡ä»¶çš„è¯»å†™æ“ä½œå½±å“æµ‹è¯•ç»“æœã€‚åŒæ—¶ï¼Œç¡®ä¿ `search_in_json_nested` æ–¹æ³•çš„å®ç°èƒ½å¤Ÿæ­£ç¡®å¤„ç†åµŒå¥— JSON æ•°æ®ï¼Œä»¥ä¾¿è¿”å›é¢„æœŸçš„ç»“æœã€‚
***



================================================
FILE: markdown_docs/tests/test_structure_tree.md
================================================
## FunctionDef build_path_tree(who_reference_me, reference_who, doc_item_path)
**build_path_tree**: The function of build_path_tree is to create a hierarchical representation of file paths based on provided references and a specific document item path.

**parameters**: The parameters of this Function.
Â· parameter1: who_reference_me - A list of file paths that reference the current entity.
Â· parameter2: reference_who - A list of file paths that reference another entity.
Â· parameter3: doc_item_path - A specific file path that needs to be highlighted in the output.

**Code Description**: The build_path_tree function constructs a nested dictionary structure representing a tree of file paths. It begins by defining an inner function, tree, which initializes a defaultdict that allows for the creation of nested dictionaries automatically. The variable path_tree is then assigned the result of calling this inner function.

The function processes two lists of paths: who_reference_me and reference_who. For each path in these lists, it splits the path into its components using the operating system's path separator (os.sep). It then traverses the path_tree structure, creating nested dictionaries for each part of the path.

After processing the reference paths, the function handles the doc_item_path. It splits this path into components as well, but modifies the last component by prefixing it with a star symbol (âœ³ï¸) to indicate it as a special item. The function again traverses the path_tree to include this modified path.

Finally, the function defines another inner function, tree_to_string, which recursively converts the tree structure into a string representation. This function sorts the keys at each level and adds indentation based on the depth of the tree. The resulting string representation of the path_tree is returned as the output of the build_path_tree function.

**Note**: It is important to ensure that the input paths are formatted correctly and that the os module is imported for the path separator to function properly. The output string will visually represent the hierarchy of paths, with the doc_item_path clearly marked.

**Output Example**: 
Assuming the following inputs:
who_reference_me = ["folder1/fileA.txt", "folder1/folder2/fileB.txt"]
reference_who = ["folder3/fileC.txt"]
doc_item_path = "folder1/folder2/fileB.txt"

The output of the function could look like this:
folder1
    fileA.txt
    folder2
        âœ³ï¸fileB.txt
folder3
    fileC.txt
### FunctionDef tree
**tree**: treeå‡½æ•°çš„åŠŸèƒ½æ˜¯è¿”å›ä¸€ä¸ªé»˜è®¤å­—å…¸ï¼Œè¯¥å­—å…¸çš„é»˜è®¤å€¼æ˜¯ä¸€ä¸ªæ–°çš„treeå‡½æ•°ã€‚

**parameters**: è¯¥å‡½æ•°æ²¡æœ‰å‚æ•°ã€‚

**Code Description**: treeå‡½æ•°ä½¿ç”¨äº†Pythonçš„collectionsæ¨¡å—ä¸­çš„defaultdictã€‚defaultdictæ˜¯ä¸€ä¸ªå­—å…¸å­ç±»ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªé»˜è®¤å€¼ï¼Œå½“è®¿é—®ä¸€ä¸ªä¸å­˜åœ¨çš„é”®æ—¶ï¼Œä¼šè‡ªåŠ¨åˆ›å»ºä¸€ä¸ªæ–°çš„å€¼ã€‚åœ¨è¿™ä¸ªå®ç°ä¸­ï¼Œtreeå‡½æ•°è¿”å›ä¸€ä¸ªdefaultdictï¼Œå…¶ä¸­çš„é»˜è®¤å€¼æ˜¯è°ƒç”¨treeå‡½æ•°æœ¬èº«ã€‚è¿™æ„å‘³ç€æ¯å½“è®¿é—®ä¸€ä¸ªä¸å­˜åœ¨çš„é”®æ—¶ï¼Œdefaultdictä¼šè‡ªåŠ¨åˆ›å»ºä¸€ä¸ªæ–°çš„defaultdictã€‚è¿™ç§é€’å½’çš„ç»“æ„å¯ä»¥ç”¨äºæ„å»ºæ ‘å½¢æ•°æ®ç»“æ„ï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹å¯ä»¥æœ‰å¤šä¸ªå­èŠ‚ç‚¹ï¼Œä¸”å­èŠ‚ç‚¹çš„æ•°é‡å’Œåç§°æ˜¯åŠ¨æ€ç”Ÿæˆçš„ã€‚

**Note**: ä½¿ç”¨æ­¤ä»£ç æ—¶ï¼Œè¯·æ³¨æ„é¿å…æ— é™é€’å½’çš„æƒ…å†µã€‚ç”±äºtreeå‡½æ•°è¿”å›çš„defaultdictçš„é»˜è®¤å€¼æ˜¯treeå‡½æ•°æœ¬èº«ï¼Œå› æ­¤åœ¨è®¿é—®ä¸å­˜åœ¨çš„é”®æ—¶ï¼Œä¼šä¸æ–­åˆ›å»ºæ–°çš„defaultdictï¼Œç›´åˆ°è¾¾åˆ°æŸç§æ¡ä»¶æˆ–é™åˆ¶ã€‚

**Output Example**: è°ƒç”¨treeå‡½æ•°åï¼Œå¯èƒ½ä¼šå¾—åˆ°å¦‚ä¸‹ç»“æ„ï¼š
```
defaultdict(<function tree at 0x...>, {
    'key1': defaultdict(<function tree at 0x...>, {
        'subkey1': defaultdict(<function tree at 0x...>, {}),
        'subkey2': defaultdict(<function tree at 0x...>, {})
    }),
    'key2': defaultdict(<function tree at 0x...>, {})
})
``` 
åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ'key1'å’Œ'key2'æ˜¯é¡¶å±‚é”®ï¼Œè€Œ'subkey1'å’Œ'subkey2'æ˜¯'key1'ä¸‹çš„å­é”®ã€‚
***
### FunctionDef tree_to_string(tree, indent)
**tree_to_string**: tree_to_string å‡½æ•°çš„åŠŸèƒ½æ˜¯å°†æ ‘å½¢ç»“æ„è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼ï¼Œä¾¿äºå¯è§†åŒ–å±•ç¤ºã€‚

**parameters**: æ­¤å‡½æ•°çš„å‚æ•°å¦‚ä¸‹ï¼š
Â· parameter1: tree - ä¸€ä¸ªå­—å…¸ç±»å‹çš„æ ‘å½¢ç»“æ„ï¼Œå…¶ä¸­åŒ…å«é”®å€¼å¯¹ï¼Œé”®ä¸ºèŠ‚ç‚¹åç§°ï¼Œå€¼ä¸ºå­èŠ‚ç‚¹ï¼ˆå¯ä»¥æ˜¯å­—å…¸æˆ–å…¶ä»–ç±»å‹ï¼‰ã€‚
Â· parameter2: indent - ä¸€ä¸ªæ•´æ•°ï¼Œè¡¨ç¤ºå½“å‰èŠ‚ç‚¹çš„ç¼©è¿›çº§åˆ«ï¼Œé»˜è®¤ä¸º0ã€‚

**Code Description**: tree_to_string å‡½æ•°é€šè¿‡é€’å½’çš„æ–¹å¼éå†ç»™å®šçš„æ ‘å½¢ç»“æ„ï¼Œå¹¶å°†å…¶æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ã€‚å‡½æ•°é¦–å…ˆåˆå§‹åŒ–ä¸€ä¸ªç©ºå­—ç¬¦ä¸² sï¼Œç”¨äºå­˜å‚¨æœ€ç»ˆçš„ç»“æœã€‚æ¥ç€ï¼Œå‡½æ•°å¯¹æ ‘ä¸­çš„æ¯ä¸ªé”®å€¼å¯¹è¿›è¡Œæ’åºï¼Œå¹¶é€ä¸ªå¤„ç†æ¯ä¸ªé”®ã€‚å¯¹äºæ¯ä¸ªé”®ï¼Œå‡½æ•°ä¼šåœ¨å­—ç¬¦ä¸²ä¸­æ·»åŠ ç›¸åº”æ•°é‡çš„ç©ºæ ¼ï¼ˆç”± indent å‚æ•°æ§åˆ¶ï¼‰ï¼Œç„¶åæ·»åŠ é”®çš„åç§°ï¼Œå¹¶æ¢è¡Œã€‚å¦‚æœè¯¥é”®å¯¹åº”çš„å€¼æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå‡½æ•°ä¼šé€’å½’è°ƒç”¨è‡ªèº«ï¼Œå¢åŠ ç¼©è¿›çº§åˆ«ï¼ˆindent + 1ï¼‰ï¼Œä»¥å¤„ç†å­æ ‘ã€‚æœ€ç»ˆï¼Œå‡½æ•°è¿”å›æ„å»ºå¥½çš„å­—ç¬¦ä¸²ï¼Œå±•ç¤ºäº†æ ‘å½¢ç»“æ„çš„å±‚æ¬¡å…³ç³»ã€‚

**Note**: ä½¿ç”¨æ­¤å‡½æ•°æ—¶ï¼Œè¯·ç¡®ä¿ä¼ å…¥çš„ tree å‚æ•°ä¸ºå­—å…¸ç±»å‹ï¼Œå¹¶ä¸”å…¶å€¼å¯ä»¥æ˜¯å­—å…¸æˆ–å…¶ä»–ç±»å‹ã€‚ç¼©è¿›å‚æ•° indent åº”ä¸ºéè´Ÿæ•´æ•°ï¼Œä»¥ç¡®ä¿è¾“å‡ºæ ¼å¼æ­£ç¡®ã€‚

**Output Example**: å‡è®¾è¾“å…¥çš„æ ‘å½¢ç»“æ„ä¸ºï¼š
{
    "æ ¹èŠ‚ç‚¹": {
        "å­èŠ‚ç‚¹1": {},
        "å­èŠ‚ç‚¹2": {
            "å­™èŠ‚ç‚¹1": {}
        }
    },
    "å¦ä¸€ä¸ªæ ¹èŠ‚ç‚¹": {}
}
è°ƒç”¨ tree_to_string å‡½æ•°åï¼Œè¿”å›çš„å­—ç¬¦ä¸²å¯èƒ½å¦‚ä¸‹æ‰€ç¤ºï¼š
æ ¹èŠ‚ç‚¹
    å­èŠ‚ç‚¹1
    å­èŠ‚ç‚¹2
        å­™èŠ‚ç‚¹1
å¦ä¸€ä¸ªæ ¹èŠ‚ç‚¹
***



================================================
FILE: repo_agent/__init__.py
================================================
[Empty file]


================================================
FILE: repo_agent/__main__.py
================================================
from .main import cli

if __name__ == "__main__":
    cli()



================================================
FILE: repo_agent/change_detector.py
================================================
import os
import re
import subprocess

import git
from colorama import Fore, Style

from repo_agent.file_handler import FileHandler
from repo_agent.settings import SettingsManager


class ChangeDetector:
    """
    è¿™ä¸ªç±»éœ€è¦å¤„ç†æ–‡ä»¶çš„å·®å¼‚å’Œå˜æ›´æ£€æµ‹ï¼Œå®ƒå¯èƒ½ä¼šç”¨åˆ° FileHandler ç±»æ¥è®¿é—®æ–‡ä»¶ç³»ç»Ÿã€‚
    ChangeDetector ç±»çš„æ ¸å¿ƒåœ¨äºèƒ½å¤Ÿè¯†åˆ«è‡ªä¸Šæ¬¡æäº¤ä»¥æ¥æ–‡ä»¶çš„å˜æ›´ã€‚
    """

    def __init__(self, repo_path):
        """
        Initializes a ChangeDetector object.

        Parameters:
        repo_path (str): The path to the repository.

        Returns:
        None
        """
        self.repo_path = repo_path
        self.repo = git.Repo(repo_path)

    def get_staged_pys(self):
        """
        Get added python files in the repository that have been staged.

        This function only tracks the changes of Python files in Git that have been staged,
        i.e., the files that have been added using `git add`.

        Returns:
            dict: A dictionary of changed Python files, where the keys are the file paths and the values are booleans indicating whether the file is newly created or not.

        """
        repo = self.repo
        staged_files = {}
        # Detect Staged Changes
        # Please note! The logic of the GitPython library is different from git. Here, the R=True parameter is used to reverse the version comparison logic.
        # In the GitPython library, repo.index.diff('HEAD') compares the staging area (index) as the new state with the original HEAD commit (old state). This means that if there is a new file in the current staging area, it will be shown as non-existent in HEAD, i.e., "deleted".
        # R=True reverses this logic, correctly treating the last commit (HEAD) as the old state and comparing it with the current staging area (new state) (Index). In this case, a new file in the staging area will correctly show as added because it does not exist in HEAD.
        diffs = repo.index.diff("HEAD", R=True)

        for diff in diffs:
            if diff.change_type in ["A", "M"] and diff.a_path.endswith(".py"):
                is_new_file = diff.change_type == "A"
                staged_files[diff.a_path] = is_new_file

        return staged_files

    def get_file_diff(self, file_path, is_new_file):
        """
        The function's purpose is to retrieve the changes made to a specific file. For new files, it uses git diff --staged to get the differences.
        Args:
            file_path (str): The relative path of the file
            is_new_file (bool): Indicates whether the file is a new file
        Returns:
            list: List of changes made to the file
        """
        repo = self.repo

        if is_new_file:
            # For new files, first add them to the staging area.
            add_command = f"git -C {repo.working_dir} add {file_path}"
            subprocess.run(add_command, shell=True, check=True)

            # Get the diff from the staging area.
            diffs = repo.git.diff("--staged", file_path).splitlines()
        else:
            # For non-new files, get the diff from HEAD.
            diffs = repo.git.diff("HEAD", file_path).splitlines()

        return diffs

    def parse_diffs(self, diffs):
        """
        Parse the difference content, extract the added and deleted object information, the object can be a class or a function.
        Output example: {'added': [(86, '    '), (87, '    def to_json_new(self, comments = True):'), (88, '        data = {'), (89, '            "name": self.node_name,')...(95, '')], 'removed': []}
        In the above example, PipelineEngine and AI_give_params are added objects, and there are no deleted objects.
        But the addition here does not mean that it is a newly added object, because in git diff, the modification of a line is represented as deletion and addition in diff.
        So for the modified content, it will also be represented as this object has undergone an added operation.

        If you need to know clearly that an object is newly added, you need to use the get_added_objs() function.
        Args:
            diffs (list): A list containing difference content. Obtained by the get_file_diff() function inside the class.

        Returns:
            dict: A dictionary containing added and deleted line information, the format is {'added': set(), 'removed': set()}
        """
        changed_lines = {"added": [], "removed": []}
        line_number_current = 0
        line_number_change = 0

        for line in diffs:
            # æ£€æµ‹è¡Œå·ä¿¡æ¯ï¼Œä¾‹å¦‚ "@@ -43,33 +43,40 @@"
            line_number_info = re.match(r"@@ \-(\d+),\d+ \+(\d+),\d+ @@", line)
            if line_number_info:
                line_number_current = int(line_number_info.group(1))
                line_number_change = int(line_number_info.group(2))
                continue

            if line.startswith("+") and not line.startswith("+++"):
                changed_lines["added"].append((line_number_change, line[1:]))
                line_number_change += 1
            elif line.startswith("-") and not line.startswith("---"):
                changed_lines["removed"].append((line_number_current, line[1:]))
                line_number_current += 1
            else:
                # å¯¹äºæ²¡æœ‰å˜åŒ–çš„è¡Œï¼Œä¸¤è€…çš„è¡Œå·éƒ½éœ€è¦å¢åŠ 
                line_number_current += 1
                line_number_change += 1

        return changed_lines

    # TODO: The key issue is that the changed line numbers correspond to the old function names (i.e., those removed) and the new function names (i.e., those added), and the current implementation does not handle this correctly.
    # We need a way to associate the changed line numbers with their function or class names before and after the change. One method is to build a mapping before processing changed_lines, which can map the names after the change back to the names before the change based on the line number.
    # Then, in the identify_changes_in_structure function, this mapping can be used to correctly identify the changed structure.
    def identify_changes_in_structure(self, changed_lines, structures):
        """
        Identify the structure of the function or class where changes have occurred: Traverse all changed lines, for each line, it checks whether this line is between the start line and the end line of a structure (function or class).
        If so, then this structure is considered to have changed, and its name and the name of the parent structure are added to the corresponding set in the result dictionary changes_in_structures (depending on whether this line is added or deleted).

        Output example: {'added': {('PipelineAutoMatNode', None), ('to_json_new', 'PipelineAutoMatNode')}, 'removed': set()}

        Args:
            changed_lines (dict): A dictionary containing the line numbers where changes have occurred, {'added': [(line number, change content)], 'removed': [(line number, change content)]}
            structures (list): The received is a list of function or class structures from get_functions_and_classes, each structure is composed of structure type, name, start line number, end line number, and parent structure name.

        Returns:
            dict: A dictionary containing the structures where changes have occurred, the key is the change type, and the value is a set of structure names and parent structure names.
                Possible change types are 'added' (new) and 'removed' (removed).
        """
        changes_in_structures = {"added": set(), "removed": set()}
        for change_type, lines in changed_lines.items():
            for line_number, _ in lines:
                for (
                    structure_type,
                    name,
                    start_line,
                    end_line,
                    parent_structure,
                ) in structures:
                    if start_line <= line_number <= end_line:
                        changes_in_structures[change_type].add((name, parent_structure))
        return changes_in_structures

    # TODO:å¯èƒ½æœ‰é”™ï¼Œéœ€è¦å•å…ƒæµ‹è¯•è¦†ç›–ï¼› å¯èƒ½æœ‰æ›´å¥½çš„å®ç°æ–¹å¼
    def get_to_be_staged_files(self):
        """
        This method retrieves all unstaged files in the repository that meet one of the following conditions:
        1. The file, when its extension is changed to .md, corresponds to a file that is already staged.
        2. The file's path is the same as the 'project_hierarchy' field in the CONFIG.

        It returns a list of the paths of these files.

        :return: A list of relative file paths to the repo that are either modified but not staged, or untracked, and meet one of the conditions above.
        """
        # å·²ç»æ›´æ”¹ä½†æ˜¯æš‚æœªæš‚å­˜çš„æ–‡ä»¶ï¼Œè¿™é‡Œåªèƒ½æ˜¯.mdæ–‡ä»¶ï¼Œå› ä¸ºä½œè€…ä¸æäº¤çš„.pyæ–‡ä»¶ï¼ˆå³ä½¿å‘ç”Ÿå˜æ›´ï¼‰æˆ‘ä»¬ä¸åšå¤„ç†ã€‚
        to_be_staged_files = []
        # staged_filesæ˜¯å·²ç»æš‚å­˜çš„æ–‡ä»¶ï¼Œé€šå¸¸è¿™é‡Œæ˜¯ä½œè€…åšäº†æ›´æ”¹ågit add çš„.pyæ–‡ä»¶ æˆ–å…¶ä»–æ–‡ä»¶
        staged_files = [item.a_path for item in self.repo.index.diff("HEAD")]
        print(
            f"{Fore.LIGHTYELLOW_EX}target_repo_path{Style.RESET_ALL}: {self.repo_path}"
        )
        print(
            f"{Fore.LIGHTMAGENTA_EX}already_staged_files{Style.RESET_ALL}:{staged_files}"
        )

        setting = SettingsManager.get_setting()

        project_hierarchy = setting.project.hierarchy_name
        # diffsæ˜¯æ‰€æœ‰æœªæš‚å­˜æ›´æ”¹æ–‡ä»¶çš„åˆ—è¡¨ã€‚è¿™äº›æ›´æ”¹æ–‡ä»¶æ˜¯ç›¸å¯¹äºå·¥ä½œåŒºï¼ˆworking directoryï¼‰çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä»¬æ˜¯è‡ªä¸Šæ¬¡æäº¤ï¼ˆcommitï¼‰ä»¥æ¥åœ¨å·¥ä½œåŒºå‘ç”Ÿçš„æ›´æ”¹ï¼Œä½†è¿˜æ²¡æœ‰è¢«æ·»åŠ åˆ°æš‚å­˜åŒºï¼ˆstaging areaï¼‰
        # æ¯”å¦‚åŸæœ¬å­˜åœ¨çš„mdæ–‡ä»¶ç°åœ¨ç”±äºä»£ç çš„å˜æ›´å‘ç”Ÿäº†æ›´æ–°ï¼Œå°±ä¼šæ ‡è®°ä¸ºæœªæš‚å­˜diff
        diffs = self.repo.index.diff(None)
        # untracked_filesæ˜¯ä¸€ä¸ªåŒ…å«äº†æ‰€æœ‰æœªè·Ÿè¸ªæ–‡ä»¶çš„åˆ—è¡¨ã€‚æ¯”å¦‚è¯´ç”¨æˆ·æ·»åŠ äº†æ–°çš„.pyæ–‡ä»¶åé¡¹ç›®è‡ªå·±ç”Ÿæˆçš„å¯¹åº”.mdæ–‡æ¡£ã€‚å®ƒä»¬æ˜¯åœ¨å·¥ä½œåŒºä¸­å­˜åœ¨ä½†è¿˜æ²¡æœ‰è¢«æ·»åŠ åˆ°æš‚å­˜åŒºï¼ˆstaging areaï¼‰çš„æ–‡ä»¶ã€‚
        # untracked_filesä¸­çš„æ–‡ä»¶è·¯å¾„æ˜¯ç»å¯¹è·¯å¾„
        untracked_files = self.repo.untracked_files
        print(f"{Fore.LIGHTCYAN_EX}untracked_files{Style.RESET_ALL}: {untracked_files}")

        # å¤„ç†untrack_filesä¸­çš„å†…å®¹
        for untracked_file in untracked_files:
            # è¿æ¥repo_pathå’Œuntracked_fileä»¥è·å–å®Œæ•´çš„ç»å¯¹è·¯å¾„
            if untracked_file.startswith(setting.project.markdown_docs_name):
                to_be_staged_files.append(untracked_file)
            continue
            print(f"rel_untracked_file:{rel_untracked_file}")
            # import pdb; pdb.set_trace()
            # åˆ¤æ–­è¿™ä¸ªæ–‡ä»¶çš„ç±»å‹ï¼š
            if rel_untracked_file.endswith(".md"):
                # æŠŠrel_untracked_fileä»CONFIG['Markdown_Docs_folder']ä¸­æ‹†ç¦»å‡ºæ¥ã€‚åˆ¤æ–­æ˜¯å¦èƒ½è·Ÿæš‚å­˜åŒºä¸­çš„æŸä¸€ä¸ª.pyæ–‡ä»¶å¯¹åº”ä¸Š
                rel_untracked_file = os.path.relpath(
                    rel_untracked_file, setting.project.markdown_docs_name
                )
                corresponding_py_file = os.path.splitext(rel_untracked_file)[0] + ".py"
                print(
                    f"corresponding_py_file in untracked_files:{corresponding_py_file}"
                )
                if corresponding_py_file in staged_files:
                    # å¦‚æœæ˜¯ï¼Œé‚£ä¹ˆå°±æŠŠè¿™ä¸ªmdæ–‡ä»¶ä¹ŸåŠ å…¥åˆ°unstaged_filesä¸­
                    to_be_staged_files.append(
                        os.path.join(
                            self.repo_path.lstrip("/"),
                            setting.project.markdown_docs_name,
                            rel_untracked_file,
                        )
                    )
            elif rel_untracked_file == project_hierarchy:
                to_be_staged_files.append(rel_untracked_file)

        # å¤„ç†å·²è¿½è¸ªä½†æ˜¯æœªæš‚å­˜çš„å†…å®¹
        unstaged_files = [diff.b_path for diff in diffs]
        print(f"{Fore.LIGHTCYAN_EX}unstaged_files{Style.RESET_ALL}: {unstaged_files}")

        for unstaged_file in unstaged_files:
            # è¿æ¥repo_pathå’Œunstaged_fileä»¥è·å–å®Œæ•´çš„ç»å¯¹è·¯å¾„
            if unstaged_file.startswith(
                setting.project.markdown_docs_name
            ) or unstaged_file.startswith(setting.project.hierarchy_name):
                # abs_unstaged_file = os.path.join(self.repo_path, unstaged_file)
                # # # è·å–ç›¸å¯¹äºä»“åº“æ ¹ç›®å½•çš„ç›¸å¯¹è·¯å¾„
                # # rel_unstaged_file = os.path.relpath(abs_unstaged_file, self.repo_path)
                to_be_staged_files.append(unstaged_file)
            elif unstaged_file == project_hierarchy:  # project_hierarchyæ°¸è¿œadd
                to_be_staged_files.append(unstaged_file)
            continue
            abs_unstaged_file = os.path.join(self.repo_path, unstaged_file)
            # è·å–ç›¸å¯¹äºä»“åº“æ ¹ç›®å½•çš„ç›¸å¯¹è·¯å¾„
            rel_unstaged_file = os.path.relpath(abs_unstaged_file, self.repo_path)
            print(f"rel_unstaged_file:{rel_unstaged_file}")
            # å¦‚æœå®ƒæ˜¯mdæ–‡ä»¶
            if unstaged_file.endswith(".md"):
                # æŠŠrel_unstaged_fileä»CONFIG['Markdown_Docs_folder']ä¸­æ‹†ç¦»å‡ºæ¥ã€‚åˆ¤æ–­æ˜¯å¦èƒ½è·Ÿæš‚å­˜åŒºä¸­çš„æŸä¸€ä¸ª.pyæ–‡ä»¶å¯¹åº”ä¸Š
                rel_unstaged_file = os.path.relpath(
                    rel_unstaged_file, setting.project.markdown_docs_name
                )
                corresponding_py_file = os.path.splitext(rel_unstaged_file)[0] + ".py"
                print(f"corresponding_py_file:{corresponding_py_file}")
                if corresponding_py_file in staged_files:
                    # å¦‚æœæ˜¯ï¼Œé‚£ä¹ˆå°±æŠŠè¿™ä¸ªmdæ–‡ä»¶ä¹ŸåŠ å…¥åˆ°unstaged_filesä¸­
                    to_be_staged_files.append(
                        os.path.join(
                            self.repo_path.lstrip("/"),
                            setting.project.markdown_docs_name,
                            rel_unstaged_file,
                        )
                    )
            elif unstaged_file == project_hierarchy:  # project_hierarchyæ°¸è¿œadd
                to_be_staged_files.append(unstaged_file)
        print(
            f"{Fore.LIGHTRED_EX}newly_staged_files{Style.RESET_ALL}: {to_be_staged_files}"
        )
        return to_be_staged_files

    def add_unstaged_files(self):
        """
        Add unstaged files which meet the condition to the staging area.
        """
        unstaged_files_meeting_conditions = self.get_to_be_staged_files()
        for file_path in unstaged_files_meeting_conditions:
            add_command = f"git -C {self.repo.working_dir} add {file_path}"
            subprocess.run(add_command, shell=True, check=True)
        return unstaged_files_meeting_conditions


if __name__ == "__main__":
    repo_path = "/path/to/your/repo/"
    change_detector = ChangeDetector(repo_path)
    changed_files = change_detector.get_staged_pys()
    print(f"\nchanged_files:{changed_files}\n\n")
    for file_path, is_new_file in changed_files.items():
        changed_lines = change_detector.parse_diffs(
            change_detector.get_file_diff(file_path, is_new_file)
        )
        # print("changed_lines:",changed_lines)
        file_handler = FileHandler(repo_path=repo_path, file_path=file_path)
        changes_in_pyfile = change_detector.identify_changes_in_structure(
            changed_lines,
            file_handler.get_functions_and_classes(file_handler.read_file()),
        )
        print(f"Changes in {file_path} Structures:{changes_in_pyfile}\n")



================================================
FILE: repo_agent/chat_engine.py
================================================
from llama_index.llms.openai_like import OpenAILike

from repo_agent.doc_meta_info import DocItem
from repo_agent.log import logger
from repo_agent.prompt import chat_template
from repo_agent.settings import SettingsManager


class ChatEngine:
    """
    ChatEngine is used to generate the doc of functions or classes.
    """

    def __init__(self, project_manager):
        setting = SettingsManager.get_setting()

        self.llm = OpenAILike(
            api_key=setting.chat_completion.openai_api_key.get_secret_value(),
            api_base=setting.chat_completion.openai_base_url,
            timeout=setting.chat_completion.request_timeout,
            model=setting.chat_completion.model,
            temperature=setting.chat_completion.temperature,
            max_retries=1,
            is_chat_model=True,
        )

    def build_prompt(self, doc_item: DocItem):
        """Builds and returns the system and user prompts based on the DocItem."""
        setting = SettingsManager.get_setting()

        code_info = doc_item.content
        referenced = len(doc_item.who_reference_me) > 0

        code_type = code_info["type"]
        code_name = code_info["name"]
        code_content = code_info["code_content"]
        have_return = code_info["have_return"]
        file_path = doc_item.get_full_name()

        def get_referenced_prompt(doc_item: DocItem) -> str:
            if len(doc_item.reference_who) == 0:
                return ""
            prompt = [
                """As you can see, the code calls the following objects, their code and docs are as following:"""
            ]
            for reference_item in doc_item.reference_who:
                instance_prompt = (
                    f"""obj: {reference_item.get_full_name()}\nDocument: \n{reference_item.md_content[-1] if len(reference_item.md_content) > 0 else 'None'}\nRaw code:```\n{reference_item.content['code_content'] if 'code_content' in reference_item.content.keys() else ''}\n```"""
                    + "=" * 10
                )
                prompt.append(instance_prompt)
            return "\n".join(prompt)

        def get_referencer_prompt(doc_item: DocItem) -> str:
            if len(doc_item.who_reference_me) == 0:
                return ""
            prompt = [
                """Also, the code has been called by the following objects, their code and docs are as following:"""
            ]
            for referencer_item in doc_item.who_reference_me:
                instance_prompt = (
                    f"""obj: {referencer_item.get_full_name()}\nDocument: \n{referencer_item.md_content[-1] if len(referencer_item.md_content) > 0 else 'None'}\nRaw code:```\n{referencer_item.content['code_content'] if 'code_content' in referencer_item.content.keys() else 'None'}\n```"""
                    + "=" * 10
                )
                prompt.append(instance_prompt)
            return "\n".join(prompt)

        def get_relationship_description(referencer_content, reference_letter):
            if referencer_content and reference_letter:
                return "And please include the reference relationship with its callers and callees in the project from a functional perspective"
            elif referencer_content:
                return "And please include the relationship with its callers in the project from a functional perspective."
            elif reference_letter:
                return "And please include the relationship with its callees in the project from a functional perspective."
            else:
                return ""

        code_type_tell = "Class" if code_type == "ClassDef" else "Function"
        parameters_or_attribute = (
            "attributes" if code_type == "ClassDef" else "parameters"
        )
        have_return_tell = (
            "**Output Example**: Mock up a possible appearance of the code's return value."
            if have_return
            else ""
        )
        combine_ref_situation = (
            "and combine it with its calling situation in the project,"
            if referenced
            else ""
        )

        referencer_content = get_referencer_prompt(doc_item)
        reference_letter = get_referenced_prompt(doc_item)
        has_relationship = get_relationship_description(
            referencer_content, reference_letter
        )

        project_structure_prefix = ", and the related hierarchical structure of this project is as follows (The current object is marked with an *):"

        return chat_template.format_messages(
            combine_ref_situation=combine_ref_situation,
            file_path=file_path,
            project_structure_prefix=project_structure_prefix,
            code_type_tell=code_type_tell,
            code_name=code_name,
            code_content=code_content,
            have_return_tell=have_return_tell,
            has_relationship=has_relationship,
            reference_letter=reference_letter,
            referencer_content=referencer_content,
            parameters_or_attribute=parameters_or_attribute,
            language=setting.project.language,
        )

    def generate_doc(self, doc_item: DocItem):
        """Generates documentation for a given DocItem."""
        messages = self.build_prompt(doc_item)

        try:
            response = self.llm.chat(messages)
            logger.debug(f"LLM Prompt Tokens: {response.raw.usage.prompt_tokens}")  # type: ignore
            logger.debug(
                f"LLM Completion Tokens: {response.raw.usage.completion_tokens}"  # type: ignore
            )
            logger.debug(
                f"Total LLM Token Count: {response.raw.usage.total_tokens}"  # type: ignore
            )
            return response.message.content
        except Exception as e:
            logger.error(f"Error in llamaindex chat call: {e}")
            raise



================================================
FILE: repo_agent/doc_meta_info.py
================================================
[Binary file]


================================================
FILE: repo_agent/file_handler.py
================================================
# FileHandler ç±»ï¼Œå®ç°å¯¹æ–‡ä»¶çš„è¯»å†™æ“ä½œï¼Œè¿™é‡Œçš„æ–‡ä»¶åŒ…æ‹¬markdownæ–‡ä»¶å’Œpythonæ–‡ä»¶
# repo_agent/file_handler.py
import ast
import json
import os

import git
from colorama import Fore, Style
from tqdm import tqdm

from repo_agent.log import logger
from repo_agent.settings import SettingsManager
from repo_agent.utils.gitignore_checker import GitignoreChecker
from repo_agent.utils.meta_info_utils import latest_verison_substring


class FileHandler:
    """
    å†å˜æ›´åçš„æ–‡ä»¶çš„å¾ªç¯ä¸­ï¼Œä¸ºæ¯ä¸ªå˜æ›´åæ–‡ä»¶ï¼ˆä¹Ÿå°±æ˜¯å½“å‰æ–‡ä»¶ï¼‰åˆ›å»ºä¸€ä¸ªå®ä¾‹
    """

    def __init__(self, repo_path, file_path):
        self.file_path = file_path  # è¿™é‡Œçš„file_pathæ˜¯ç›¸å¯¹äºä»“åº“æ ¹ç›®å½•çš„è·¯å¾„
        self.repo_path = repo_path

        setting = SettingsManager.get_setting()

        self.project_hierarchy = (
            setting.project.target_repo / setting.project.hierarchy_name
        )

    def read_file(self):
        """
        Read the file content

        Returns:
            str: The content of the current changed file
        """
        abs_file_path = os.path.join(self.repo_path, self.file_path)

        with open(abs_file_path, "r", encoding="utf-8") as file:
            content = file.read()
        return content

    def get_obj_code_info(
        self, code_type, code_name, start_line, end_line, params, file_path=None
    ):
        """
        Get the code information for a given object.

        Args:
            code_type (str): The type of the code.
            code_name (str): The name of the code.
            start_line (int): The starting line number of the code.
            end_line (int): The ending line number of the code.
            parent (str): The parent of the code.
            file_path (str, optional): The file path. Defaults to None.

        Returns:
            dict: A dictionary containing the code information.
        """

        code_info = {}
        code_info["type"] = code_type
        code_info["name"] = code_name
        code_info["md_content"] = []
        code_info["code_start_line"] = start_line
        code_info["code_end_line"] = end_line
        code_info["params"] = params

        with open(
            os.path.join(
                self.repo_path, file_path if file_path != None else self.file_path
            ),
            "r",
            encoding="utf-8",
        ) as code_file:
            lines = code_file.readlines()
            code_content = "".join(lines[start_line - 1 : end_line])
            # è·å–å¯¹è±¡åç§°åœ¨ç¬¬ä¸€è¡Œä»£ç ä¸­çš„ä½ç½®
            name_column = lines[start_line - 1].find(code_name)
            # åˆ¤æ–­ä»£ç ä¸­æ˜¯å¦æœ‰returnå­—æ ·
            if "return" in code_content:
                have_return = True
            else:
                have_return = False

            code_info["have_return"] = have_return
            # # ä½¿ç”¨ json.dumps æ¥è½¬ä¹‰å­—ç¬¦ä¸²ï¼Œå¹¶å»æ‰é¦–å°¾çš„å¼•å·
            # code_info['code_content'] = json.dumps(code_content)[1:-1]
            code_info["code_content"] = code_content
            code_info["name_column"] = name_column

        return code_info

    def write_file(self, file_path, content):
        """
        Write content to a file.

        Args:
            file_path (str): The relative path of the file.
            content (str): The content to be written to the file.
        """
        # ç¡®ä¿file_pathæ˜¯ç›¸å¯¹è·¯å¾„
        if file_path.startswith("/"):
            # ç§»é™¤å¼€å¤´çš„ '/'
            file_path = file_path[1:]

        abs_file_path = os.path.join(self.repo_path, file_path)
        os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)
        with open(abs_file_path, "w", encoding="utf-8") as file:
            file.write(content)

    def get_modified_file_versions(self):
        """
        Get the current and previous versions of the modified file.

        Returns:
            tuple: A tuple containing the current version and the previous version of the file.
        """
        repo = git.Repo(self.repo_path)

        # Read the file in the current working directory (current version)
        current_version_path = os.path.join(self.repo_path, self.file_path)
        with open(current_version_path, "r", encoding="utf-8") as file:
            current_version = file.read()

        # Get the file version from the last commit (previous version)
        commits = list(repo.iter_commits(paths=self.file_path, max_count=1))
        previous_version = None
        if commits:
            commit = commits[0]
            try:
                previous_version = (
                    (commit.tree / self.file_path).data_stream.read().decode("utf-8")
                )
            except KeyError:
                previous_version = None  # The file may be newly added and not present in previous commits

        return current_version, previous_version

    def get_end_lineno(self, node):
        """
        Get the end line number of a given node.

        Args:
            node: The node for which to find the end line number.

        Returns:
            int: The end line number of the node. Returns -1 if the node does not have a line number.
        """
        if not hasattr(node, "lineno"):
            return -1  # è¿”å›-1è¡¨ç¤ºæ­¤èŠ‚ç‚¹æ²¡æœ‰è¡Œå·

        end_lineno = node.lineno
        for child in ast.iter_child_nodes(node):
            child_end = getattr(child, "end_lineno", None) or self.get_end_lineno(child)
            if child_end > -1:  # åªæ›´æ–°å½“å­èŠ‚ç‚¹æœ‰æœ‰æ•ˆè¡Œå·æ—¶
                end_lineno = max(end_lineno, child_end)
        return end_lineno

    def add_parent_references(self, node, parent=None):
        """
        Adds a parent reference to each node in the AST.

        Args:
            node: The current node in the AST.

        Returns:
            None
        """
        for child in ast.iter_child_nodes(node):
            child.parent = node
            self.add_parent_references(child, node)

    def get_functions_and_classes(self, code_content):
        """
        Retrieves all functions, classes, their parameters (if any), and their hierarchical relationships.
        Output Examples: [('FunctionDef', 'AI_give_params', 86, 95, None, ['param1', 'param2']), ('ClassDef', 'PipelineEngine', 97, 104, None, []), ('FunctionDef', 'get_all_pys', 99, 104, 'PipelineEngine', ['param1'])]
        On the example above, PipelineEngine is the Father structure for get_all_pys.

        Args:
            code_content: The code content of the whole file to be parsed.

        Returns:
            A list of tuples containing the type of the node (FunctionDef, ClassDef, AsyncFunctionDef),
            the name of the node, the starting line number, the ending line number, the name of the parent node, and a list of parameters (if any).
        """
        tree = ast.parse(code_content)
        self.add_parent_references(tree)
        functions_and_classes = []
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):
                # if node.name == "recursive_check":
                #     import pdb; pdb.set_trace()
                start_line = node.lineno
                end_line = self.get_end_lineno(node)
                # def get_recursive_parent_name(node):
                #     now = node
                #     while "parent" in dir(now):
                #         if isinstance(now.parent, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):
                #             assert 'name' in dir(now.parent)
                #             return now.parent.name
                #         now = now.parent
                #     return None
                # parent_name = get_recursive_parent_name(node)
                parameters = (
                    [arg.arg for arg in node.args.args] if "args" in dir(node) else []
                )
                all_names = [item[1] for item in functions_and_classes]
                # (parent_name == None or parent_name in all_names) and
                functions_and_classes.append(
                    (type(node).__name__, node.name, start_line, end_line, parameters)
                )
        return functions_and_classes

    def generate_file_structure(self, file_path):
        """
        Generates the file structure for the given file path.

        Args:
            file_path (str): The relative path of the file.

        Returns:
            dict: A dictionary containing the file path and the generated file structure.

        Output example:
        {
            "function_name": {
                "type": "function",
                "start_line": 10,
                Â·Â·Â· Â·Â·Â·
                "end_line": 20,
                "parent": "class_name"
            },
            "class_name": {
                "type": "class",
                "start_line": 5,
                Â·Â·Â· Â·Â·Â·
                "end_line": 25,
                "parent": None
            }
        }
        """
        with open(os.path.join(self.repo_path, file_path), "r", encoding="utf-8") as f:
            content = f.read()
            structures = self.get_functions_and_classes(content)
            file_objects = []  # ä»¥åˆ—è¡¨çš„å½¢å¼å­˜å‚¨
            for struct in structures:
                structure_type, name, start_line, end_line, params = struct
                code_info = self.get_obj_code_info(
                    structure_type, name, start_line, end_line, params, file_path
                )
                file_objects.append(code_info)

        return file_objects

    def generate_overall_structure(self, file_path_reflections, jump_files) -> dict:
        """è·å–ç›®æ ‡ä»“åº“çš„æ–‡ä»¶æƒ…å†µï¼Œé€šè¿‡AST-walkè·å–æ‰€æœ‰å¯¹è±¡ç­‰æƒ…å†µã€‚
        å¯¹äºjump_files: ä¸ä¼šparseï¼Œå½“åšä¸å­˜åœ¨
        """
        repo_structure = {}
        gitignore_checker = GitignoreChecker(
            directory=self.repo_path,
            gitignore_path=os.path.join(self.repo_path, ".gitignore"),
        )

        bar = tqdm(gitignore_checker.check_files_and_folders())
        for not_ignored_files in bar:
            normal_file_names = not_ignored_files
            if not_ignored_files in jump_files:
                print(
                    f"{Fore.LIGHTYELLOW_EX}[File-Handler] Unstaged AddFile, ignore this file: {Style.RESET_ALL}{normal_file_names}"
                )
                continue
            elif not_ignored_files.endswith(latest_verison_substring):
                print(
                    f"{Fore.LIGHTYELLOW_EX}[File-Handler] Skip Latest Version, Using Git-Status Version]: {Style.RESET_ALL}{normal_file_names}"
                )
                continue
            # elif not_ignored_files.endswith(latest_version):
            #     """å¦‚æœæŸæ–‡ä»¶è¢«åˆ é™¤ä½†æ²¡æœ‰æš‚å­˜ï¼Œæ–‡ä»¶ç³»ç»Ÿæœ‰fake_fileä½†æ²¡æœ‰å¯¹åº”çš„åŸå§‹æ–‡ä»¶"""
            #     for k,v in file_path_reflections.items():
            #         if v == not_ignored_files and not os.path.exists(os.path.join(setting.project.target_repo, not_ignored_files)):
            #             print(f"{Fore.LIGHTYELLOW_EX}[Unstaged DeleteFile] load fake-file-content: {Style.RESET_ALL}{k}")
            #             normal_file_names = k #åŸæ¥çš„åå­—
            #             break
            #     if normal_file_names == not_ignored_files:
            #         continue

            # if not_ignored_files in file_path_reflections.keys():
            #     not_ignored_files = file_path_reflections[not_ignored_files] #è·å–fake_file_path
            #     print(f"{Fore.LIGHTYELLOW_EX}[Unstaged ChangeFile] load fake-file-content: {Style.RESET_ALL}{normal_file_names}")

            try:
                repo_structure[normal_file_names] = self.generate_file_structure(
                    not_ignored_files
                )
            except Exception as e:
                logger.error(
                    f"Alert: An error occurred while generating file structure for {not_ignored_files}: {e}"
                )
                continue
            bar.set_description(f"generating repo structure: {not_ignored_files}")
        return repo_structure

    def convert_to_markdown_file(self, file_path=None):
        """
        Converts the content of a file to markdown format.

        Args:
            file_path (str, optional): The relative path of the file to be converted. If not provided, the default file path, which is None, will be used.

        Returns:
            str: The content of the file in markdown format.

        Raises:
            ValueError: If no file object is found for the specified file path in project_hierarchy.json.
        """
        with open(self.project_hierarchy, "r", encoding="utf-8") as f:
            json_data = json.load(f)

        if file_path is None:
            file_path = self.file_path

        # Find the file object in json_data that matches file_path

        file_dict = json_data.get(file_path)

        if file_dict is None:
            raise ValueError(
                f"No file object found for {self.file_path} in project_hierarchy.json"
            )

        markdown = ""
        parent_dict = {}
        objects = sorted(file_dict.values(), key=lambda obj: obj["code_start_line"])
        for obj in objects:
            if obj["parent"] is not None:
                parent_dict[obj["name"]] = obj["parent"]
        current_parent = None
        for obj in objects:
            level = 1
            parent = obj["parent"]
            while parent is not None:
                level += 1
                parent = parent_dict.get(parent)
            if level == 1 and current_parent is not None:
                markdown += "***\n"
            current_parent = obj["name"]
            params_str = ""
            if obj["type"] in ["FunctionDef", "AsyncFunctionDef"]:
                params_str = "()"
                if obj["params"]:
                    params_str = f"({', '.join(obj['params'])})"
            markdown += f"{'#' * level} {obj['type']} {obj['name']}{params_str}:\n"
            markdown += (
                f"{obj['md_content'][-1] if len(obj['md_content']) >0 else ''}\n"
            )
        markdown += "***\n"

        return markdown



================================================
FILE: repo_agent/log.py
================================================
# repo_agent/log.py
import inspect
import logging
import sys

from loguru import logger

logger = logger.opt(colors=True)
"""
RepoAgent æ—¥å¿—è®°å½•å™¨å¯¹è±¡ã€‚

é»˜è®¤ä¿¡æ¯:
- æ ¼å¼: `[%(asctime)s %(name)s] %(levelname)s: %(message)s`
- ç­‰çº§: `INFO` ï¼Œæ ¹æ® `CONFIG["log_level"]` é…ç½®æ”¹å˜
- è¾“å‡º: è¾“å‡ºè‡³ stdout

ç”¨æ³•ç¤ºä¾‹:
    ```python
    from repo_agent.log import logger
    
    # åŸºæœ¬æ¶ˆæ¯è®°å½•
    logger.info("It <green>works</>!") # ä½¿ç”¨é¢œè‰²

    # è®°å½•å¼‚å¸¸ä¿¡æ¯
    try:
        1 / 0
    except ZeroDivisionError:
        # ä½¿ç”¨ `logger.exception` å¯ä»¥åœ¨è®°å½•å¼‚å¸¸æ¶ˆæ¯æ—¶è‡ªåŠ¨é™„åŠ å¼‚å¸¸çš„å †æ ˆè·Ÿè¸ªä¿¡æ¯ã€‚
        logger.exception("ZeroDivisionError occurred")

    # è®°å½•è°ƒè¯•ä¿¡æ¯
    logger.debug(f"Debugging info: {some_debug_variable}")

    # è®°å½•è­¦å‘Šä¿¡æ¯
    logger.warning("This is a warning message")

    # è®°å½•é”™è¯¯ä¿¡æ¯
    logger.error("An error occurred")
    ```

"""


class InterceptHandler(logging.Handler):
    def emit(self, record: logging.LogRecord) -> None:
        # Get corresponding Loguru level if it exists.
        level: str | int
        try:
            level = logger.level(record.levelname).name
        except ValueError:
            level = record.levelno

        # Find caller from where originated the logged message.
        frame, depth = inspect.currentframe(), 0
        while frame and (depth == 0 or frame.f_code.co_filename == logging.__file__):
            frame = frame.f_back
            depth += 1

        logger.opt(depth=depth, exception=record.exc_info).log(
            level, record.getMessage()
        )


def set_logger_level_from_config(log_level):
    """
    Configures the loguru logger with specified log level and integrates it with the standard logging module.

    Args:
        log_level (str): The log level to set for loguru (e.g., "DEBUG", "INFO", "WARNING").

    This function:
    - Removes any existing loguru handlers to ensure a clean slate.
    - Adds a new handler to loguru, directing output to stderr with the specified level.
      - `enqueue=True` ensures thread-safe logging by using a queue, helpful in multi-threaded contexts.
      - `backtrace=False` minimizes detailed traceback to prevent overly verbose output.
      - `diagnose=False` suppresses additional loguru diagnostic information for more concise logs.
    - Redirects the standard logging output to loguru using the InterceptHandler, allowing loguru to handle
      all logs consistently across the application.
    """
    logger.remove()
    logger.add(
        sys.stderr, level=log_level, enqueue=True, backtrace=False, diagnose=False
    )

    # Intercept standard logging
    logging.basicConfig(handlers=[InterceptHandler()], level=0, force=True)

    logger.success(f"Log level set to {log_level}!")



================================================
FILE: repo_agent/main.py
================================================
from importlib import metadata

import click
from pydantic import ValidationError

from repo_agent.doc_meta_info import DocItem, MetaInfo
from repo_agent.log import logger, set_logger_level_from_config
from repo_agent.runner import Runner, delete_fake_files
from repo_agent.settings import SettingsManager, LogLevel
from repo_agent.utils.meta_info_utils import delete_fake_files, make_fake_files

try:
    version_number = metadata.version("repoagent")
except metadata.PackageNotFoundError:
    version_number = "0.0.0"


@click.group()
@click.version_option(version_number)
def cli():
    """An LLM-Powered Framework for Repository-level Code Documentation Generation."""
    pass


def handle_setting_error(e: ValidationError):
    """Handle configuration errors for settings."""
    # è¾“å‡ºæ›´è¯¦ç»†çš„å­—æ®µç¼ºå¤±ä¿¡æ¯ï¼Œä½¿ç”¨é¢œè‰²åŒºåˆ†
    for error in e.errors():
        field = error["loc"][-1]
        if error["type"] == "missing":
            message = click.style(
                f"Missing required field `{field}`. Please set the `{field}` environment variable.",
                fg="yellow",
            )
        else:
            message = click.style(error["msg"], fg="yellow")
        click.echo(message, err=True, color=True)

    # ä½¿ç”¨ ClickException ä¼˜é›…åœ°é€€å‡ºç¨‹åº
    raise click.ClickException(
        click.style(
            "Program terminated due to configuration errors.", fg="red", bold=True
        )
    )


@cli.command()
@click.option(
    "--model",
    "-m",
    default="gpt-4o-mini",
    show_default=True,
    help="Specifies the model to use for completion.",
    type=str,
)
@click.option(
    "--temperature",
    "-t",
    default=0.2,
    show_default=True,
    help="Sets the generation temperature for the model. Lower values make the model more deterministic.",
    type=float,
)
@click.option(
    "--request-timeout",
    "-r",
    default=60,
    show_default=True,
    help="Defines the timeout in seconds for the API request.",
    type=int,
)
@click.option(
    "--base-url",
    "-b",
    default="https://api.openai.com/v1",
    show_default=True,
    help="The base URL for the API calls.",
    type=str,
)
@click.option(
    "--target-repo-path",
    "-tp",
    default="",
    show_default=True,
    help="The file system path to the target repository. This path is used as the root for documentation generation.",
    type=click.Path(file_okay=False),
)
@click.option(
    "--hierarchy-path",
    "-hp",
    default=".project_doc_record",
    show_default=True,
    help="The name or path for the project hierarchy file, used to organize documentation structure.",
    type=str,
)
@click.option(
    "--markdown-docs-path",
    "-mdp",
    default="markdown_docs",
    show_default=True,
    help="The folder path where Markdown documentation will be stored or generated.",
    type=str,
)
@click.option(
    "--ignore-list",
    "-i",
    default="",
    help="A comma-separated list of files or directories to ignore during documentation generation.",
)
@click.option(
    "--language",
    "-l",
    default="English",
    show_default=True,
    help="The ISO 639 code or language name for the documentation. ",
    type=str,
)
@click.option(
    "--max-thread-count",
    "-mtc",
    default=4,
    show_default=True,
)
@click.option(
    "--log-level",
    "-ll",
    default="INFO",
    show_default=True,
    help="Sets the logging level (e.g., DEBUG, INFO, WARNING, ERROR, CRITICAL) for the application. Default is INFO.",
    type=click.Choice([level.value for level in LogLevel], case_sensitive=False),
)
@click.option(
    "--print-hierarchy",
    "-pr",
    is_flag=True,
    show_default=True,
    default=False,
    help="If set, prints the hierarchy of the target repository when finished running the main task.",
)
def run(
    model,
    temperature,
    request_timeout,
    base_url,
    target_repo_path,
    hierarchy_path,
    markdown_docs_path,
    ignore_list,
    language,
    max_thread_count,
    log_level,
    print_hierarchy,
):
    """Run the program with the specified parameters."""
    try:
        # Fetch and validate the settings using the SettingsManager
        setting = SettingsManager.initialize_with_params(
            target_repo=target_repo_path,
            hierarchy_name=hierarchy_path,
            markdown_docs_name=markdown_docs_path,
            ignore_list=[item.strip() for item in ignore_list.split(",") if item],
            language=language,
            log_level=log_level,
            model=model,
            temperature=temperature,
            request_timeout=request_timeout,
            openai_base_url=base_url,
            max_thread_count=max_thread_count,
        )
        set_logger_level_from_config(log_level=log_level)
    except ValidationError as e:
        handle_setting_error(e)
        return

    # å¦‚æœè®¾ç½®æˆåŠŸï¼Œåˆ™è¿è¡Œä»»åŠ¡
    runner = Runner()
    runner.run()
    logger.success("Documentation task completed.")
    if print_hierarchy:
        runner.meta_info.target_repo_hierarchical_tree.print_recursive()
        logger.success("Hierarchy printed.")


@cli.command()
def clean():
    """Clean the fake files generated by the documentation process."""
    delete_fake_files()
    logger.success("Fake files have been cleaned up.")


@cli.command()
def diff():
    """Check for changes and print which documents will be updated or generated."""
    try:
        # Fetch and validate the settings using the SettingsManager
        setting = SettingsManager.get_setting()
    except ValidationError as e:
        handle_setting_error(e)
        return

    runner = Runner()
    if runner.meta_info.in_generation_process:  # å¦‚æœä¸æ˜¯åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œå°±å¼€å§‹æ£€æµ‹å˜æ›´
        click.echo("This command only supports pre-check")
        raise click.Abort()

    file_path_reflections, jump_files = make_fake_files()
    new_meta_info = MetaInfo.init_meta_info(file_path_reflections, jump_files)
    new_meta_info.load_doc_from_older_meta(runner.meta_info)
    delete_fake_files()

    DocItem.check_has_task(
        new_meta_info.target_repo_hierarchical_tree,
        ignore_list=setting.project.ignore_list,
    )
    if new_meta_info.target_repo_hierarchical_tree.has_task:
        click.echo("The following docs will be generated/updated:")
        new_meta_info.target_repo_hierarchical_tree.print_recursive(
            diff_status=True, ignore_list=setting.project.ignore_list
        )
    else:
        click.echo("No docs will be generated/updated, check your source-code update")


@cli.command()
def chat_with_repo():
    """
    Start an interactive chat session with the repository.
    """
    try:
        # Fetch and validate the settings using the SettingsManager
        setting = SettingsManager.get_setting()
    except ValidationError as e:
        # Handle configuration errors if the settings are invalid
        handle_setting_error(e)
        return

    from repo_agent.chat_with_repo import main

    main()


if __name__ == "__main__":
    cli()



================================================
FILE: repo_agent/multi_task_dispatch.py
================================================
from __future__ import annotations

import random
import threading
import time
from typing import Any, Callable, Dict, List

from colorama import Fore, Style


class Task:
    def __init__(self, task_id: int, dependencies: List[Task], extra_info: Any = None):
        self.task_id = task_id
        self.extra_info = extra_info
        self.dependencies = dependencies
        self.status = 0  # ä»»åŠ¡çŠ¶æ€ï¼š0æœªå¼€å§‹ï¼Œ1æ­£åœ¨è¿›è¡Œï¼Œ2å·²ç»å®Œæˆï¼Œ3å‡ºé”™äº†


class TaskManager:
    def __init__(self):
        """
        Initialize a MultiTaskDispatch object.

        This method initializes the MultiTaskDispatch object by setting up the necessary attributes.

        Attributes:
        - task_dict (Dict[int, Task]): A dictionary that maps task IDs to Task objects.
        - task_lock (threading.Lock): A lock used for thread synchronization when accessing the task_dict.
        - now_id (int): The current task ID.
        - query_id (int): The current query ID.
        - sync_func (None): A placeholder for a synchronization function.

        """
        self.task_dict: Dict[int, Task] = {}
        self.task_lock = threading.Lock()
        self.now_id = 0
        self.query_id = 0

    @property
    def all_success(self) -> bool:
        return len(self.task_dict) == 0

    def add_task(self, dependency_task_id: List[int], extra=None) -> int:
        """
        Adds a new task to the task dictionary.

        Args:
            dependency_task_id (List[int]): List of task IDs that the new task depends on.
            extra (Any, optional): Extra information associated with the task. Defaults to None.

        Returns:
            int: The ID of the newly added task.
        """
        with self.task_lock:
            depend_tasks = [self.task_dict[task_id] for task_id in dependency_task_id]
            self.task_dict[self.now_id] = Task(
                task_id=self.now_id, dependencies=depend_tasks, extra_info=extra
            )
            self.now_id += 1
            return self.now_id - 1

    def get_next_task(self, process_id: int):
        """
        Get the next task for a given process ID.

        Args:
            process_id (int): The ID of the process.

        Returns:
            tuple: A tuple containing the next task object and its ID.
                   If there are no available tasks, returns (None, -1).
        """
        with self.task_lock:
            self.query_id += 1
            for task_id in self.task_dict.keys():
                ready = (
                    len(self.task_dict[task_id].dependencies) == 0
                ) and self.task_dict[task_id].status == 0
                if ready:
                    self.task_dict[task_id].status = 1
                    print(
                        f"{Fore.RED}[process {process_id}]{Style.RESET_ALL}: get task({task_id}), remain({len(self.task_dict)})"
                    )
                    return self.task_dict[task_id], task_id
            return None, -1

    def mark_completed(self, task_id: int):
        """
        Marks a task as completed and removes it from the task dictionary.

        Args:
            task_id (int): The ID of the task to mark as completed.

        """
        with self.task_lock:
            target_task = self.task_dict[task_id]
            for task in self.task_dict.values():
                if target_task in task.dependencies:
                    task.dependencies.remove(target_task)
            self.task_dict.pop(task_id)  # ä»ä»»åŠ¡å­—å…¸ä¸­ç§»é™¤


def worker(task_manager, process_id: int, handler: Callable):
    """
    Worker function that performs tasks assigned by the task manager.

    Args:
        task_manager: The task manager object that assigns tasks to workers.
        process_id (int): The ID of the current worker process.
        handler (Callable): The function that handles the tasks.

    Returns:
        None
    """
    while True:
        if task_manager.all_success:
            return
        task, task_id = task_manager.get_next_task(process_id)
        if task is None:
            time.sleep(0.5)
            continue
        # print(f"will perform task: {task_id}")
        handler(task.extra_info)
        task_manager.mark_completed(task.task_id)
        # print(f"task complete: {task_id}")


if __name__ == "__main__":
    task_manager = TaskManager()

    def some_function():  # éšæœºç¡ä¸€ä¼š
        time.sleep(random.random() * 3)

    # æ·»åŠ ä»»åŠ¡ï¼Œä¾‹å¦‚ï¼š
    i1 = task_manager.add_task(some_function, [])  # type: ignore
    i2 = task_manager.add_task(some_function, [])  # type: ignore
    i3 = task_manager.add_task(some_function, [i1])  # type: ignore
    i4 = task_manager.add_task(some_function, [i2, i3])  # type: ignore
    i5 = task_manager.add_task(some_function, [i2, i3])  # type: ignore
    i6 = task_manager.add_task(some_function, [i1])  # type: ignore

    threads = [threading.Thread(target=worker, args=(task_manager,)) for _ in range(4)]
    for thread in threads:
        thread.start()
    for thread in threads:
        thread.join()



================================================
FILE: repo_agent/project_manager.py
================================================
import os

import jedi


class ProjectManager:
    def __init__(self, repo_path, project_hierarchy):
        self.repo_path = repo_path
        self.project = jedi.Project(self.repo_path)
        self.project_hierarchy = os.path.join(
            self.repo_path, project_hierarchy, "project_hierarchy.json"
        )

    def get_project_structure(self):
        """
        Returns the structure of the project by recursively walking through the directory tree.

        Returns:
            str: The project structure as a string.
        """

        def walk_dir(root, prefix=""):
            structure.append(prefix + os.path.basename(root))
            new_prefix = prefix + "  "
            for name in sorted(os.listdir(root)):
                if name.startswith("."):  # å¿½ç•¥éšè—æ–‡ä»¶å’Œç›®å½•
                    continue
                path = os.path.join(root, name)
                if os.path.isdir(path):
                    walk_dir(path, new_prefix)
                elif os.path.isfile(path) and name.endswith(".py"):
                    structure.append(new_prefix + name)

        structure = []
        walk_dir(self.repo_path)
        return "\n".join(structure)

    def build_path_tree(self, who_reference_me, reference_who, doc_item_path):
        from collections import defaultdict

        def tree():
            return defaultdict(tree)

        path_tree = tree()

        # æ„å»º who_reference_me å’Œ reference_who çš„æ ‘
        for path_list in [who_reference_me, reference_who]:
            for path in path_list:
                parts = path.split(os.sep)
                node = path_tree
                for part in parts:
                    node = node[part]

        # å¤„ç† doc_item_path
        parts = doc_item_path.split(os.sep)
        parts[-1] = "âœ³ï¸" + parts[-1]  # åœ¨æœ€åä¸€ä¸ªå¯¹è±¡å‰é¢åŠ ä¸Šæ˜Ÿå·
        node = path_tree
        for part in parts:
            node = node[part]

        def tree_to_string(tree, indent=0):
            s = ""
            for key, value in sorted(tree.items()):
                s += "    " * indent + key + "\n"
                if isinstance(value, dict):
                    s += tree_to_string(value, indent + 1)
            return s

        return tree_to_string(path_tree)


if __name__ == "__main__":
    project_manager = ProjectManager(repo_path="", project_hierarchy="")
    print(project_manager.get_project_structure())



================================================
FILE: repo_agent/prompt.py
================================================
from llama_index.core import ChatPromptTemplate
from llama_index.core.llms import ChatMessage, MessageRole

doc_generation_instruction = (
    "You are an AI documentation assistant, and your task is to generate documentation based on the given code of an object. "
    "The purpose of the documentation is to help developers and beginners understand the function and specific usage of the code.\n\n"
    "Currently, you are in a project{project_structure_prefix}\n"
    "{project_structure}\n\n"
    "The path of the document you need to generate in this project is {file_path}.\n"
    'Now you need to generate a document for a {code_type_tell}, whose name is "{code_name}".\n\n'
    "The content of the code is as follows:\n"
    "{code_content}\n\n"
    "{reference_letter}\n"
    "{referencer_content}\n\n"
    "Please generate a detailed explanation document for this object based on the code of the target object itself {combine_ref_situation}.\n\n"
    "Please write out the function of this {code_type_tell} in bold plain text, followed by a detailed analysis in plain text "
    "(including all details), in language {language} to serve as the documentation for this part of the code.\n\n"
    "The standard format is as follows:\n\n"
    "**{code_name}**: The function of {code_name} is XXX. (Only code name and one sentence function description are required)\n"
    "**{parameters_or_attribute}**: The {parameters_or_attribute} of this {code_type_tell}.\n"
    "Â· parameter1: XXX\n"
    "Â· parameter2: XXX\n"
    "Â· ...\n"
    "**Code Description**: The description of this {code_type_tell}.\n"
    "(Detailed and CERTAIN code analysis and description...{has_relationship})\n"
    "**Note**: Points to note about the use of the code\n"
    "{have_return_tell}\n\n"
    "Please note:\n"
    "- Any part of the content you generate SHOULD NOT CONTAIN Markdown hierarchical heading and divider syntax.\n"
    "- Write mainly in the desired language. If necessary, you can write with some English words in the analysis and description "
    "to enhance the document's readability because you do not need to translate the function name or variable name into the target language.\n"
)

documentation_guideline = (
    "Keep in mind that your audience is document readers, so use a deterministic tone to generate precise content and don't let them know "
    "you're provided with code snippet and documents. AVOID ANY SPECULATION and inaccurate descriptions! Now, provide the documentation "
    "for the target object in {language} in a professional way."
)


message_templates = [
    ChatMessage(content=doc_generation_instruction, role=MessageRole.SYSTEM),
    ChatMessage(
        content=documentation_guideline,
        role=MessageRole.USER,
    ),
]

chat_template = ChatPromptTemplate(message_templates=message_templates)



================================================
FILE: repo_agent/runner.py
================================================
import json
import os
import shutil
import subprocess
import threading
import time
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from pathlib import Path

from colorama import Fore, Style
from tqdm import tqdm

from repo_agent.change_detector import ChangeDetector
from repo_agent.chat_engine import ChatEngine
from repo_agent.doc_meta_info import DocItem, DocItemStatus, MetaInfo, need_to_generate
from repo_agent.file_handler import FileHandler
from repo_agent.log import logger
from repo_agent.multi_task_dispatch import worker
from repo_agent.project_manager import ProjectManager
from repo_agent.settings import SettingsManager
from repo_agent.utils.meta_info_utils import delete_fake_files, make_fake_files


class Runner:
    def __init__(self):
        self.setting = SettingsManager.get_setting()
        self.absolute_project_hierarchy_path = (
            self.setting.project.target_repo / self.setting.project.hierarchy_name
        )

        self.project_manager = ProjectManager(
            repo_path=self.setting.project.target_repo,
            project_hierarchy=self.setting.project.hierarchy_name,
        )
        self.change_detector = ChangeDetector(
            repo_path=self.setting.project.target_repo
        )
        self.chat_engine = ChatEngine(project_manager=self.project_manager)

        if not self.absolute_project_hierarchy_path.exists():
            file_path_reflections, jump_files = make_fake_files()
            self.meta_info = MetaInfo.init_meta_info(file_path_reflections, jump_files)
            self.meta_info.checkpoint(
                target_dir_path=self.absolute_project_hierarchy_path
            )
        else:  # å¦‚æœå­˜åœ¨å…¨å±€ç»“æ„ä¿¡æ¯æ–‡ä»¶å¤¹.project_hierarchyï¼Œå°±ä»ä¸­åŠ è½½
            self.meta_info = MetaInfo.from_checkpoint_path(
                self.absolute_project_hierarchy_path
            )

        self.meta_info.checkpoint(  # æ›´æ–°ç™½åå•åä¹Ÿè¦é‡æ–°å°†å…¨å±€ä¿¡æ¯å†™å…¥åˆ°.project_doc_recordæ–‡ä»¶å¤¹ä¸­
            target_dir_path=self.absolute_project_hierarchy_path
        )
        self.runner_lock = threading.Lock()

    def get_all_pys(self, directory):
        """
        Get all Python files in the given directory.

        Args:
            directory (str): The directory to search.

        Returns:
            list: A list of paths to all Python files.
        """
        python_files = []

        for root, dirs, files in os.walk(directory):
            for file in files:
                if file.endswith(".py"):
                    python_files.append(os.path.join(root, file))

        return python_files

    def generate_doc_for_a_single_item(self, doc_item: DocItem):
        """ä¸ºä¸€ä¸ªå¯¹è±¡ç”Ÿæˆæ–‡æ¡£"""
        try:
            if not need_to_generate(doc_item, self.setting.project.ignore_list):
                print(
                    f"Content ignored/Document generated, skipping: {doc_item.get_full_name()}"
                )
            else:
                print(
                    f" -- Generating document  {Fore.LIGHTYELLOW_EX}{doc_item.item_type.name}: {doc_item.get_full_name()}{Style.RESET_ALL}"
                )
                response_message = self.chat_engine.generate_doc(
                    doc_item=doc_item,
                )
                doc_item.md_content.append(response_message)  # type: ignore
                doc_item.item_status = DocItemStatus.doc_up_to_date
                self.meta_info.checkpoint(
                    target_dir_path=self.absolute_project_hierarchy_path
                )
        except Exception:
            logger.exception(
                f"Document generation failed after multiple attempts, skipping: {doc_item.get_full_name()}"
            )
            doc_item.item_status = DocItemStatus.doc_has_not_been_generated

    def first_generate(self):
        """
        ç”Ÿæˆæ‰€æœ‰æ–‡æ¡£ï¼Œå®Œæˆååˆ·æ–°å¹¶ä¿å­˜æ–‡ä»¶ç³»ç»Ÿä¸­çš„æ–‡æ¡£ä¿¡æ¯ã€‚
        """
        logger.info("Starting to generate documentation")
        check_task_available_func = partial(
            need_to_generate, ignore_list=self.setting.project.ignore_list
        )
        task_manager = self.meta_info.get_topology(check_task_available_func)
        before_task_len = len(task_manager.task_dict)

        if not self.meta_info.in_generation_process:
            self.meta_info.in_generation_process = True
            logger.info("Init a new task-list")
        else:
            logger.info("Load from an existing task-list")
        self.meta_info.print_task_list(task_manager.task_dict)

        try:
            # åˆ›å»ºå¹¶å¯åŠ¨çº¿ç¨‹
            threads = [
                threading.Thread(
                    target=worker,
                    args=(
                        task_manager,
                        process_id,
                        self.generate_doc_for_a_single_item,
                    ),
                )
                for process_id in range(self.setting.project.max_thread_count)
            ]
            for thread in threads:
                thread.start()
            for thread in threads:
                thread.join()

            # æ‰€æœ‰ä»»åŠ¡å®Œæˆååˆ·æ–°æ–‡æ¡£
            self.markdown_refresh()

            # æ›´æ–°æ–‡æ¡£ç‰ˆæœ¬
            self.meta_info.document_version = (
                self.change_detector.repo.head.commit.hexsha
            )
            self.meta_info.in_generation_process = False
            self.meta_info.checkpoint(
                target_dir_path=self.absolute_project_hierarchy_path
            )
            logger.info(
                f"Successfully generated {before_task_len - len(task_manager.task_dict)} documents."
            )

        except BaseException as e:
            logger.error(
                f"An error occurred: {e}. {before_task_len - len(task_manager.task_dict)} docs are generated at this time"
            )

    def markdown_refresh(self):
        """åˆ·æ–°æœ€æ–°çš„æ–‡æ¡£ä¿¡æ¯åˆ°markdownæ ¼å¼æ–‡ä»¶å¤¹ä¸­"""
        with self.runner_lock:
            # å®šä¹‰ markdown æ–‡ä»¶å¤¹è·¯å¾„
            markdown_folder = (
                Path(self.setting.project.target_repo)
                / self.setting.project.markdown_docs_name
            )

            # åˆ é™¤å¹¶é‡æ–°åˆ›å»ºç›®å½•
            if markdown_folder.exists():
                logger.debug(f"Deleting existing contents of {markdown_folder}")
                shutil.rmtree(markdown_folder)
            markdown_folder.mkdir(parents=True, exist_ok=True)
            logger.debug(f"Created markdown folder at {markdown_folder}")

        # éå†æ–‡ä»¶åˆ—è¡¨ç”Ÿæˆ markdown
        file_item_list = self.meta_info.get_all_files()
        logger.debug(f"Found {len(file_item_list)} files to process.")

        for file_item in tqdm(file_item_list):
            # æ£€æŸ¥æ–‡æ¡£å†…å®¹
            def recursive_check(doc_item) -> bool:
                if doc_item.md_content:
                    return True
                for child in doc_item.children.values():
                    if recursive_check(child):
                        return True
                return False

            if not recursive_check(file_item):
                logger.debug(
                    f"No documentation content for: {file_item.get_full_name()}, skipping."
                )
                continue

            # ç”Ÿæˆ markdown å†…å®¹
            markdown = ""
            for child in file_item.children.values():
                markdown += self.to_markdown(child, 2)

            if not markdown:
                logger.warning(
                    f"No markdown content generated for: {file_item.get_full_name()}"
                )
                continue

            # ç¡®å®šå¹¶åˆ›å»ºæ–‡ä»¶è·¯å¾„
            file_path = Path(
                self.setting.project.markdown_docs_name
            ) / file_item.get_file_name().replace(".py", ".md")
            abs_file_path = self.setting.project.target_repo / file_path
            logger.debug(f"Writing markdown to: {abs_file_path}")

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            abs_file_path.parent.mkdir(parents=True, exist_ok=True)
            logger.debug(f"Ensured directory exists: {abs_file_path.parent}")

            # ä½¿ç”¨é”ä¿æŠ¤æ–‡ä»¶å†™å…¥æ“ä½œ
            with self.runner_lock:
                for attempt in range(3):  # æœ€å¤šé‡è¯•3æ¬¡
                    try:
                        with open(abs_file_path, "w", encoding="utf-8") as file:
                            file.write(markdown)
                        logger.debug(f"Successfully wrote to {abs_file_path}")
                        break
                    except IOError as e:
                        logger.error(
                            f"Failed to write {abs_file_path} on attempt {attempt + 1}: {e}"
                        )
                        time.sleep(1)  # å»¶è¿Ÿå†è¯•

        logger.info(
            f"Markdown documents have been refreshed at {self.setting.project.markdown_docs_name}"
        )

    def to_markdown(self, item, now_level: int) -> str:
        """å°†æ–‡ä»¶å†…å®¹è½¬åŒ–ä¸º markdown æ ¼å¼çš„æ–‡æœ¬"""
        markdown_content = (
            "#" * now_level + f" {item.item_type.to_str()} {item.obj_name}"
        )
        if "params" in item.content.keys() and item.content["params"]:
            markdown_content += f"({', '.join(item.content['params'])})"
        markdown_content += "\n"
        if item.md_content:
            markdown_content += f"{item.md_content[-1]}\n"
        else:
            markdown_content += "Doc is waiting to be generated...\n"
        for child in item.children.values():
            markdown_content += self.to_markdown(child, now_level + 1)
            markdown_content += "***\n"
        return markdown_content

    def git_commit(self, commit_message):
        try:
            subprocess.check_call(
                ["git", "commit", "--no-verify", "-m", commit_message],
                shell=True,
            )
        except subprocess.CalledProcessError as e:
            print(f"An error occurred while trying to commit {str(e)}")

    def run(self):
        """
        Runs the document update process.

        This method detects the changed Python files, processes each file, and updates the documents accordingly.

        Returns:
            None
        """

        if self.meta_info.document_version == "":
            # æ ¹æ®document versionè‡ªåŠ¨æ£€æµ‹æ˜¯å¦ä»åœ¨æœ€åˆç”Ÿæˆçš„processé‡Œ(æ˜¯å¦ä¸ºç¬¬ä¸€æ¬¡ç”Ÿæˆ)
            self.first_generate()  # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡åšæ–‡æ¡£ç”Ÿæˆä»»åŠ¡ï¼Œå°±é€šè¿‡first_generateç”Ÿæˆæ‰€æœ‰æ–‡æ¡£
            self.meta_info.checkpoint(
                target_dir_path=self.absolute_project_hierarchy_path,
                flash_reference_relation=True,
            )  # è¿™ä¸€æ­¥å°†ç”Ÿæˆåçš„metaä¿¡æ¯ï¼ˆåŒ…å«å¼•ç”¨å…³ç³»ï¼‰å†™å…¥åˆ°.project_doc_recordæ–‡ä»¶å¤¹ä¸­
            return

        if (
            not self.meta_info.in_generation_process
        ):  # å¦‚æœä¸æ˜¯åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œå°±å¼€å§‹æ£€æµ‹å˜æ›´
            logger.info("Starting to detect changes.")

            """é‡‡ç”¨æ–°çš„åŠæ³•
            1.æ–°å»ºä¸€ä¸ªproject-hierachy
            2.å’Œè€çš„hierarchyåšmerge,å¤„ç†ä»¥ä¸‹æƒ…å†µï¼š
            - åˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶ï¼šéœ€è¦ç”Ÿæˆå¯¹åº”çš„doc
            - æ–‡ä»¶ã€å¯¹è±¡è¢«åˆ é™¤ï¼šå¯¹åº”çš„docä¹Ÿåˆ é™¤(æŒ‰ç…§ç›®å‰çš„å®ç°ï¼Œæ–‡ä»¶é‡å‘½åç®—æ˜¯åˆ é™¤å†æ·»åŠ )
            - å¼•ç”¨å…³ç³»å˜äº†ï¼šå¯¹åº”çš„obj-docéœ€è¦é‡æ–°ç”Ÿæˆ
            
            mergeåçš„new_meta_infoä¸­ï¼š
            1.æ–°å»ºçš„æ–‡ä»¶æ²¡æœ‰æ–‡æ¡£ï¼Œå› æ­¤metainfo mergeåè¿˜æ˜¯æ²¡æœ‰æ–‡æ¡£
            2.è¢«åˆ é™¤çš„æ–‡ä»¶å’Œobjï¼Œæœ¬æ¥å°±ä¸åœ¨æ–°çš„metaé‡Œé¢ï¼Œç›¸å½“äºæ–‡æ¡£è¢«è‡ªåŠ¨åˆ é™¤äº†
            3.åªéœ€è¦è§‚å¯Ÿè¢«ä¿®æ”¹çš„æ–‡ä»¶ï¼Œä»¥åŠå¼•ç”¨å…³ç³»éœ€è¦è¢«é€šçŸ¥çš„æ–‡ä»¶å»é‡æ–°ç”Ÿæˆæ–‡æ¡£"""
            file_path_reflections, jump_files = make_fake_files()
            new_meta_info = MetaInfo.init_meta_info(file_path_reflections, jump_files)
            new_meta_info.load_doc_from_older_meta(self.meta_info)

            self.meta_info = new_meta_info  # æ›´æ–°è‡ªèº«çš„meta_infoä¿¡æ¯ä¸ºnewçš„ä¿¡æ¯
            self.meta_info.in_generation_process = True  # å°†in_generation_processè®¾ç½®ä¸ºTrueï¼Œè¡¨ç¤ºæ£€æµ‹åˆ°å˜æ›´åGenerating document çš„è¿‡ç¨‹ä¸­

        # å¤„ç†ä»»åŠ¡é˜Ÿåˆ—
        check_task_available_func = partial(
            need_to_generate, ignore_list=self.setting.project.ignore_list
        )

        task_manager = self.meta_info.get_task_manager(
            self.meta_info.target_repo_hierarchical_tree,
            task_available_func=check_task_available_func,
        )

        for item_name, item_type in self.meta_info.deleted_items_from_older_meta:
            print(
                f"{Fore.LIGHTMAGENTA_EX}[Dir/File/Obj Delete Dected]: {Style.RESET_ALL} {item_type} {item_name}"
            )
        self.meta_info.print_task_list(task_manager.task_dict)
        if task_manager.all_success:
            logger.info(
                "No tasks in the queue, all documents are completed and up to date."
            )

        threads = [
            threading.Thread(
                target=worker,
                args=(task_manager, process_id, self.generate_doc_for_a_single_item),
            )
            for process_id in range(self.setting.project.max_thread_count)
        ]
        for thread in threads:
            thread.start()
        for thread in threads:
            thread.join()

        self.meta_info.in_generation_process = False
        self.meta_info.document_version = self.change_detector.repo.head.commit.hexsha

        self.meta_info.checkpoint(
            target_dir_path=self.absolute_project_hierarchy_path,
            flash_reference_relation=True,
        )
        logger.info(f"Doc has been forwarded to the latest version")

        self.markdown_refresh()
        delete_fake_files()

        logger.info(f"Starting to git-add DocMetaInfo and newly generated Docs")
        time.sleep(1)

        # å°†runè¿‡ç¨‹ä¸­æ›´æ–°çš„Markdownæ–‡ä»¶ï¼ˆæœªæš‚å­˜ï¼‰æ·»åŠ åˆ°æš‚å­˜åŒº
        git_add_result = self.change_detector.add_unstaged_files()

        if len(git_add_result) > 0:
            logger.info(
                f"Added {[file for file in git_add_result]} to the staging area."
            )

        # self.git_commit(f"Update documentation for {file_handler.file_path}") # æäº¤å˜æ›´

    def add_new_item(self, file_handler, json_data):
        """
        Add new projects to the JSON file and generate corresponding documentation.

        Args:
            file_handler (FileHandler): The file handler object for reading and writing files.
            json_data (dict): The JSON data storing the project structure information.

        Returns:
            None
        """
        file_dict = {}
        # å› ä¸ºæ˜¯æ–°å¢çš„é¡¹ç›®ï¼Œæ‰€ä»¥è¿™ä¸ªæ–‡ä»¶é‡Œçš„æ‰€æœ‰å¯¹è±¡éƒ½è¦å†™ä¸€ä¸ªæ–‡æ¡£
        for (
            structure_type,
            name,
            start_line,
            end_line,
            parent,
            params,
        ) in file_handler.get_functions_and_classes(file_handler.read_file()):
            code_info = file_handler.get_obj_code_info(
                structure_type, name, start_line, end_line, parent, params
            )
            response_message = self.chat_engine.generate_doc(code_info, file_handler)
            md_content = response_message.content
            code_info["md_content"] = md_content
            # æ–‡ä»¶å¯¹è±¡file_dictä¸­æ·»åŠ ä¸€ä¸ªæ–°çš„å¯¹è±¡
            file_dict[name] = code_info

        json_data[file_handler.file_path] = file_dict
        # å°†æ–°çš„é¡¹å†™å…¥jsonæ–‡ä»¶
        with open(self.project_manager.project_hierarchy, "w", encoding="utf-8") as f:
            json.dump(json_data, f, indent=4, ensure_ascii=False)
        logger.info(
            f"The structural information of the newly added file {file_handler.file_path} has been written into a JSON file."
        )
        # å°†å˜æ›´éƒ¨åˆ†çš„jsonæ–‡ä»¶å†…å®¹è½¬æ¢æˆmarkdownå†…å®¹
        markdown = file_handler.convert_to_markdown_file(
            file_path=file_handler.file_path
        )
        # å°†markdownå†…å®¹å†™å…¥.mdæ–‡ä»¶
        file_handler.write_file(
            os.path.join(
                self.project_manager.repo_path,
                self.setting.project.markdown_docs_name,
                file_handler.file_path.replace(".py", ".md"),
            ),
            markdown,
        )
        logger.info(f"å·²ç”Ÿæˆæ–°å¢æ–‡ä»¶ {file_handler.file_path} çš„Markdownæ–‡æ¡£ã€‚")

    def process_file_changes(self, repo_path, file_path, is_new_file):
        """
        This function is called in the loop of detected changed files. Its purpose is to process changed files according to the absolute file path, including new files and existing files.
        Among them, changes_in_pyfile is a dictionary that contains information about the changed structures. An example format is: {'added': {'add_context_stack', '__init__'}, 'removed': set()}

        Args:
            repo_path (str): The path to the repository.
            file_path (str): The relative path to the file.
            is_new_file (bool): Indicates whether the file is new or not.

        Returns:
            None
        """

        file_handler = FileHandler(
            repo_path=repo_path, file_path=file_path
        )  # å˜æ›´æ–‡ä»¶çš„æ“ä½œå™¨
        # è·å–æ•´ä¸ªpyæ–‡ä»¶çš„ä»£ç 
        source_code = file_handler.read_file()
        changed_lines = self.change_detector.parse_diffs(
            self.change_detector.get_file_diff(file_path, is_new_file)
        )
        changes_in_pyfile = self.change_detector.identify_changes_in_structure(
            changed_lines, file_handler.get_functions_and_classes(source_code)
        )
        logger.info(f"æ£€æµ‹åˆ°å˜æ›´å¯¹è±¡ï¼š\n{changes_in_pyfile}")

        # åˆ¤æ–­project_hierarchy.jsonæ–‡ä»¶ä¸­èƒ½å¦æ‰¾åˆ°å¯¹åº”.pyæ–‡ä»¶è·¯å¾„çš„é¡¹
        with open(self.project_manager.project_hierarchy, "r", encoding="utf-8") as f:
            json_data = json.load(f)

        # å¦‚æœæ‰¾åˆ°äº†å¯¹åº”æ–‡ä»¶
        if file_handler.file_path in json_data:
            # æ›´æ–°jsonæ–‡ä»¶ä¸­çš„å†…å®¹
            json_data[file_handler.file_path] = self.update_existing_item(
                json_data[file_handler.file_path], file_handler, changes_in_pyfile
            )
            # å°†æ›´æ–°åçš„fileå†™å›åˆ°jsonæ–‡ä»¶ä¸­
            with open(
                self.project_manager.project_hierarchy, "w", encoding="utf-8"
            ) as f:
                json.dump(json_data, f, indent=4, ensure_ascii=False)

            logger.info(f"å·²æ›´æ–°{file_handler.file_path}æ–‡ä»¶çš„jsonç»“æ„ä¿¡æ¯ã€‚")

            # å°†å˜æ›´éƒ¨åˆ†çš„jsonæ–‡ä»¶å†…å®¹è½¬æ¢æˆmarkdownå†…å®¹
            markdown = file_handler.convert_to_markdown_file(
                file_path=file_handler.file_path
            )
            # å°†markdownå†…å®¹å†™å…¥.mdæ–‡ä»¶
            file_handler.write_file(
                os.path.join(
                    self.setting.project.markdown_docs_name,
                    file_handler.file_path.replace(".py", ".md"),
                ),
                markdown,
            )
            logger.info(f"å·²æ›´æ–°{file_handler.file_path}æ–‡ä»¶çš„Markdownæ–‡æ¡£ã€‚")

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„æ–‡ä»¶ï¼Œå°±æ·»åŠ ä¸€ä¸ªæ–°çš„é¡¹
        else:
            self.add_new_item(file_handler, json_data)

        # å°†runè¿‡ç¨‹ä¸­æ›´æ–°çš„Markdownæ–‡ä»¶ï¼ˆæœªæš‚å­˜ï¼‰æ·»åŠ åˆ°æš‚å­˜åŒº
        git_add_result = self.change_detector.add_unstaged_files()

        if len(git_add_result) > 0:
            logger.info(f"å·²æ·»åŠ  {[file for file in git_add_result]} åˆ°æš‚å­˜åŒº")

        # self.git_commit(f"Update documentation for {file_handler.file_path}") # æäº¤å˜æ›´

    def update_existing_item(self, file_dict, file_handler, changes_in_pyfile):
        """
        Update existing projects.

        Args:
            file_dict (dict): A dictionary containing file structure information.
            file_handler (FileHandler): The file handler object.
            changes_in_pyfile (dict): A dictionary containing information about the objects that have changed in the file.

        Returns:
            dict: The updated file structure information dictionary.
        """
        new_obj, del_obj = self.get_new_objects(file_handler)

        # å¤„ç†è¢«åˆ é™¤çš„å¯¹è±¡
        for obj_name in del_obj:  # çœŸæ­£è¢«åˆ é™¤çš„å¯¹è±¡
            if obj_name in file_dict:
                del file_dict[obj_name]
                logger.info(f"å·²åˆ é™¤ {obj_name} å¯¹è±¡ã€‚")

        referencer_list = []

        # ç”Ÿæˆæ–‡ä»¶çš„ç»“æ„ä¿¡æ¯ï¼Œè·å¾—å½“å‰æ–‡ä»¶ä¸­çš„æ‰€æœ‰å¯¹è±¡ï¼Œ è¿™é‡Œå…¶å®å°±æ˜¯æ–‡ä»¶æ›´æ–°ä¹‹åçš„ç»“æ„äº†
        current_objects = file_handler.generate_file_structure(file_handler.file_path)

        current_info_dict = {obj["name"]: obj for obj in current_objects.values()}

        # æ›´æ–°å…¨å±€æ–‡ä»¶ç»“æ„ä¿¡æ¯ï¼Œæ¯”å¦‚ä»£ç èµ·å§‹è¡Œ\ç»ˆæ­¢è¡Œç­‰
        for current_obj_name, current_obj_info in current_info_dict.items():
            if current_obj_name in file_dict:
                # å¦‚æœå½“å‰å¯¹è±¡åœ¨æ—§å¯¹è±¡åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œæ›´æ–°æ—§å¯¹è±¡çš„ä¿¡æ¯
                file_dict[current_obj_name]["type"] = current_obj_info["type"]
                file_dict[current_obj_name]["code_start_line"] = current_obj_info[
                    "code_start_line"
                ]
                file_dict[current_obj_name]["code_end_line"] = current_obj_info[
                    "code_end_line"
                ]
                file_dict[current_obj_name]["parent"] = current_obj_info["parent"]
                file_dict[current_obj_name]["name_column"] = current_obj_info[
                    "name_column"
                ]
            else:
                # å¦‚æœå½“å‰å¯¹è±¡åœ¨æ—§å¯¹è±¡åˆ—è¡¨ä¸­ä¸å­˜åœ¨ï¼Œå°†æ–°å¯¹è±¡æ·»åŠ åˆ°æ—§å¯¹è±¡åˆ—è¡¨ä¸­
                file_dict[current_obj_name] = current_obj_info

        # å¯¹äºæ¯ä¸€ä¸ªå¯¹è±¡ï¼šè·å–å…¶å¼•ç”¨è€…åˆ—è¡¨
        for obj_name, _ in changes_in_pyfile["added"]:
            for current_object in current_objects.values():  # å¼•å…¥new_objectsçš„ç›®çš„æ˜¯è·å–åˆ°find_all_referencerä¸­å¿…è¦çš„å‚æ•°ä¿¡æ¯ã€‚åœ¨changes_in_pyfile['added']ä¸­åªæœ‰å¯¹è±¡å’Œå…¶çˆ¶çº§ç»“æ„çš„åç§°ï¼Œç¼ºå°‘å…¶ä»–å‚æ•°
                if (
                    obj_name == current_object["name"]
                ):  # ç¡®ä¿åªæœ‰å½“addedä¸­çš„å¯¹è±¡åç§°åŒ¹é…new_objectsæ—¶æ‰æ·»åŠ å¼•ç”¨è€…
                    # è·å–æ¯ä¸ªéœ€è¦ç”Ÿæˆæ–‡æ¡£çš„å¯¹è±¡çš„å¼•ç”¨è€…
                    referencer_obj = {
                        "obj_name": obj_name,
                        "obj_referencer_list": self.project_manager.find_all_referencer(
                            variable_name=current_object["name"],
                            file_path=file_handler.file_path,
                            line_number=current_object["code_start_line"],
                            column_number=current_object["name_column"],
                        ),
                    }
                    referencer_list.append(
                        referencer_obj
                    )  # å¯¹äºæ¯ä¸€ä¸ªæ­£åœ¨å¤„ç†çš„å¯¹è±¡ï¼Œæ·»åŠ ä»–çš„å¼•ç”¨è€…å­—å…¸åˆ°å…¨éƒ¨å¯¹è±¡çš„åº”ç”¨è€…åˆ—è¡¨ä¸­

        with ThreadPoolExecutor(max_workers=5) as executor:
            # é€šè¿‡çº¿ç¨‹æ± å¹¶å‘æ‰§è¡Œ
            futures = []
            for changed_obj in changes_in_pyfile["added"]:  # å¯¹äºæ¯ä¸€ä¸ªå¾…å¤„ç†çš„å¯¹è±¡
                for ref_obj in referencer_list:
                    if (
                        changed_obj[0] == ref_obj["obj_name"]
                    ):  # åœ¨referencer_listä¸­æ‰¾åˆ°å®ƒçš„å¼•ç”¨è€…å­—å…¸ï¼
                        future = executor.submit(
                            self.update_object,
                            file_dict,
                            file_handler,
                            changed_obj[0],
                            ref_obj["obj_referencer_list"],
                        )
                        print(
                            f"æ­£åœ¨ç”Ÿæˆ {Fore.CYAN}{file_handler.file_path}{Style.RESET_ALL}ä¸­çš„{Fore.CYAN}{changed_obj[0]}{Style.RESET_ALL}å¯¹è±¡æ–‡æ¡£."
                        )
                        futures.append(future)

            for future in futures:
                future.result()

        # æ›´æ–°ä¼ å…¥çš„fileå‚æ•°
        return file_dict

    def update_object(self, file_dict, file_handler, obj_name, obj_referencer_list):
        """
        Generate documentation content and update corresponding field information of the object.

        Args:
            file_dict (dict): A dictionary containing old object information.
            file_handler: The file handler.
            obj_name (str): The object name.
            obj_referencer_list (list): The list of object referencers.

        Returns:
            None
        """
        if obj_name in file_dict:
            obj = file_dict[obj_name]
            response_message = self.chat_engine.generate_doc(
                obj, file_handler, obj_referencer_list
            )
            obj["md_content"] = response_message.content

    def get_new_objects(self, file_handler):
        """
        The function gets the added and deleted objects by comparing the current version and the previous version of the .py file.

        Args:
            file_handler (FileHandler): The file handler object.

        Returns:
            tuple: A tuple containing the added and deleted objects, in the format (new_obj, del_obj)

        Output example:
            new_obj: ['add_context_stack', '__init__']
            del_obj: []
        """
        current_version, previous_version = file_handler.get_modified_file_versions()
        parse_current_py = file_handler.get_functions_and_classes(current_version)
        parse_previous_py = (
            file_handler.get_functions_and_classes(previous_version)
            if previous_version
            else []
        )

        current_obj = {f[1] for f in parse_current_py}
        previous_obj = {f[1] for f in parse_previous_py}

        new_obj = list(current_obj - previous_obj)
        del_obj = list(previous_obj - current_obj)
        return new_obj, del_obj


if __name__ == "__main__":
    runner = Runner()

    runner.run()

    logger.info("æ–‡æ¡£ä»»åŠ¡å®Œæˆã€‚")



================================================
FILE: repo_agent/settings.py
================================================
from enum import StrEnum
from typing import Optional

from iso639 import Language, LanguageNotFoundError
from pydantic import (
    DirectoryPath,
    Field,
    HttpUrl,
    PositiveFloat,
    PositiveInt,
    SecretStr,
    field_validator,
)
from pydantic_settings import BaseSettings
from pathlib import Path


class LogLevel(StrEnum):
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"


class ProjectSettings(BaseSettings):
    target_repo: DirectoryPath = ""  # type: ignore
    hierarchy_name: str = ".project_doc_record"
    markdown_docs_name: str = "markdown_docs"
    ignore_list: list[str] = []
    language: str = "English"
    max_thread_count: PositiveInt = 4
    log_level: LogLevel = LogLevel.INFO

    @field_validator("language")
    @classmethod
    def validate_language_code(cls, v: str) -> str:
        try:
            language_name = Language.match(v).name
            return language_name  # Returning the resolved language name
        except LanguageNotFoundError:
            raise ValueError(
                "Invalid language input. Please enter a valid ISO 639 code or language name."
            )

    @field_validator("log_level", mode="before")
    @classmethod
    def set_log_level(cls, v: str) -> LogLevel:
        if isinstance(v, str):
            v = v.upper()  # Convert input to uppercase
        if (
            v in LogLevel._value2member_map_
        ):  # Check if the converted value is in enum members
            return LogLevel(v)
        raise ValueError(f"Invalid log level: {v}")


class ChatCompletionSettings(BaseSettings):
    model: str = "gpt-4o-mini"  # NOTE: No model restrictions for user flexibility, but it's recommended to use models with a larger context window.
    temperature: PositiveFloat = 0.2
    request_timeout: PositiveInt = 60
    openai_base_url: str = "https://api.openai.com/v1"
    openai_api_key: SecretStr = Field(..., exclude=True)

    @field_validator("openai_base_url", mode="before")
    @classmethod
    def convert_base_url_to_str(cls, openai_base_url: HttpUrl) -> str:
        return str(openai_base_url)


class Setting(BaseSettings):
    project: ProjectSettings = {}  # type: ignore
    chat_completion: ChatCompletionSettings = {}  # type: ignore


class SettingsManager:
    _setting_instance: Optional[Setting] = (
        None  # Private class attribute, initially None
    )

    @classmethod
    def get_setting(cls):
        if cls._setting_instance is None:
            cls._setting_instance = Setting()
        return cls._setting_instance

    @classmethod
    def initialize_with_params(
        cls,
        target_repo: Path,
        markdown_docs_name: str,
        hierarchy_name: str,
        ignore_list: list[str],
        language: str,
        max_thread_count: int,
        log_level: str,
        model: str,
        temperature: float,
        request_timeout: int,
        openai_base_url: str,
    ):
        project_settings = ProjectSettings(
            target_repo=target_repo,
            hierarchy_name=hierarchy_name,
            markdown_docs_name=markdown_docs_name,
            ignore_list=ignore_list,
            language=language,
            max_thread_count=max_thread_count,
            log_level=LogLevel(log_level),
        )

        chat_completion_settings = ChatCompletionSettings(
            model=model,
            temperature=temperature,
            request_timeout=request_timeout,
            openai_base_url=openai_base_url,
        )

        cls._setting_instance = Setting(
            project=project_settings,
            chat_completion=chat_completion_settings,
        )


if __name__ == "__main__":
    setting = SettingsManager.get_setting()
    print(setting.model_dump())



================================================
FILE: repo_agent/chat_with_repo/__init__.py
================================================
# repo_agent/chat_with_repo/__init__.py

from .main import main



================================================
FILE: repo_agent/chat_with_repo/__main__.py
================================================
from .main import main

main()



================================================
FILE: repo_agent/chat_with_repo/gradio_interface.py
================================================
import gradio as gr
import markdown

from repo_agent.log import logger


class GradioInterface:
    def __init__(self, respond_function):
        self.respond = respond_function
        self.cssa = """
                <style>
                        .outer-box {
                            border: 1px solid #333; /* å¤–æ¡†çš„è¾¹æ¡†é¢œè‰²å’Œå¤§å° */
                            border-radius: 10px; /* å¤–æ¡†çš„è¾¹æ¡†åœ†è§’æ•ˆæœ */
                            padding: 10px; /* å¤–æ¡†çš„å†…è¾¹è· */
                        }

                        .title {
                            margin-bottom: 10px; /* æ ‡é¢˜å’Œå†…æ¡†ä¹‹é—´çš„è·ç¦» */
                        }

                        .inner-box {
                            border: 1px solid #555; /* å†…æ¡†çš„è¾¹æ¡†é¢œè‰²å’Œå¤§å° */
                            border-radius: 5px; /* å†…æ¡†çš„è¾¹æ¡†åœ†è§’æ•ˆæœ */
                            padding: 10px; /* å†…æ¡†çš„å†…è¾¹è· */
                        }

                        .content {
                            white-space: pre-wrap; /* ä¿ç•™ç©ºç™½ç¬¦å’Œæ¢è¡Œç¬¦ */
                            font-size: 16px; /* å†…å®¹æ–‡å­—å¤§å° */
                            height: 405px;
                            overflow: auto;
                        }
                    </style>
                    <div class="outer-box"">
        
        """
        self.cssb = """
                        </div>
                    </div>
                </div>
        """
        self.setup_gradio_interface()

    def wrapper_respond(self, msg_input, system_input):
        # è°ƒç”¨åŸæ¥çš„ respond å‡½æ•°
        msg, output1, output2, output3, code, codex = self.respond(
            msg_input, system_input
        )
        output1 = markdown.markdown(str(output1))
        output2 = markdown.markdown(str(output2))
        code = markdown.markdown(str(code))
        output1 = (
            self.cssa
            + """
                          <div class="title">Response</div>
                            <div class="inner-box">
                                <div class="content">
                """
            + str(output1)
            + """
                        </div>
                    </div>
                </div>
                """
        )
        output2 = (
            self.cssa
            + """
                          <div class="title">Embedding Recall</div>
                            <div class="inner-box">
                                <div class="content">
                """
            + str(output2)
            + self.cssb
        )
        code = (
            self.cssa
            + """
                          <div class="title">Code</div>
                            <div class="inner-box">
                                <div class="content">
                """
            + str(code)
            + self.cssb
        )

        return msg, output1, output2, output3, code, codex

    def clean(self):
        msg = ""
        output1 = gr.HTML(
            self.cssa
            + """
                                        <div class="title">Response</div>
                                            <div class="inner-box">
                                                <div class="content">
                      
                                            """
            + self.cssb
        )
        output2 = gr.HTML(
            self.cssa
            + """
                                        <div class="title">Embedding Recall</div>
                                            <div class="inner-box">
                                                <div class="content">
                                    
                                            """
            + self.cssb
        )
        output3 = ""
        code = gr.HTML(
            self.cssa
            + """
                                        <div class="title">Code</div>
                                            <div class="inner-box">
                                                <div class="content">
                                   
                                            """
            + self.cssb
        )
        codex = ""
        return msg, output1, output2, output3, code, codex

    def setup_gradio_interface(self):
        with gr.Blocks() as demo:
            gr.Markdown("""
                # RepoAgent: Chat with doc
            """)
            with gr.Tab("main chat"):
                with gr.Row():
                    with gr.Column():
                        msg = gr.Textbox(label="Question Input", lines=4)
                        system = gr.Textbox(
                            label="(Optional)insturction editing", lines=4
                        )
                        btn = gr.Button("Submit")
                        btnc = gr.ClearButton()
                        btnr = gr.Button("record")

                    output1 = gr.HTML(
                        self.cssa
                        + """
                                        <div class="title">Response</div>
                                            <div class="inner-box">
                                                <div class="content">
                      
                                            """
                        + self.cssb
                    )
                with gr.Row():
                    with gr.Column():
                        # output2 = gr.Textbox(label = "Embedding recall")
                        output2 = gr.HTML(
                            self.cssa
                            + """
                                        <div class="title">Embedding Recall</div>
                                            <div class="inner-box">
                                                <div class="content">
                                    
                                            """
                            + self.cssb
                        )
                    code = gr.HTML(
                        self.cssa
                        + """
                                        <div class="title">Code</div>
                                            <div class="inner-box">
                                                <div class="content">
                                   
                                            """
                        + self.cssb
                    )
                    with gr.Row():
                        with gr.Column():
                            output3 = gr.Textbox(label="key words", lines=2)
                            output4 = gr.Textbox(label="key words code", lines=14)

            btn.click(
                self.wrapper_respond,
                inputs=[msg, system],
                outputs=[msg, output1, output2, output3, code, output4],
            )
            btnc.click(
                self.clean, outputs=[msg, output1, output2, output3, code, output4]
            )
            msg.submit(
                self.wrapper_respond,
                inputs=[msg, system],
                outputs=[msg, output1, output2, output3, code, output4],
            )  # Press enter to submit

        gr.close_all()
        demo.queue().launch(share=False, height=800)


# ä½¿ç”¨æ–¹æ³•
if __name__ == "__main__":

    def respond_function(msg, system):
        RAG = """

        
        """
        return msg, RAG, "Embedding_recall_output", "Key_words_output", "Code_output"

    gradio_interface = GradioInterface(respond_function)



================================================
FILE: repo_agent/chat_with_repo/json_handler.py
================================================
import json
import sys

from repo_agent.log import logger


class JsonFileProcessor:
    def __init__(self, file_path):
        self.file_path = file_path

    def read_json_file(self):
        try:
            with open(self.file_path, "r", encoding="utf-8") as file:
                data = json.load(file)
            return data
        except FileNotFoundError:
            logger.exception(f"File not found: {self.file_path}")
            sys.exit(1)

    def extract_data(self):
        # Load JSON data from a file
        json_data = self.read_json_file()
        md_contents = []
        extracted_contents = []
        # Iterate through each file in the JSON data
        for file, items in json_data.items():
            # Check if the value is a list (new format)
            if isinstance(items, list):
                # Iterate through each item in the list
                for item in items:
                    # Check if 'md_content' exists and is not empty
                    if "md_content" in item and item["md_content"]:
                        # Append the first element of 'md_content' to the result list
                        md_contents.append(item["md_content"][0])
                        # Build a dictionary containing the required information
                        item_dict = {
                            "type": item.get("type", "UnknownType"),
                            "name": item.get("name", "Unnamed"),
                            "code_start_line": item.get("code_start_line", -1),
                            "code_end_line": item.get("code_end_line", -1),
                            "have_return": item.get("have_return", False),
                            "code_content": item.get("code_content", "NoContent"),
                            "name_column": item.get("name_column", 0),
                            "item_status": item.get("item_status", "UnknownStatus"),
                            # Adapt or remove fields based on new structure requirements
                        }
                        extracted_contents.append(item_dict)
        return md_contents, extracted_contents

    def recursive_search(self, data_item, search_text, code_results, md_results):
        if isinstance(data_item, dict):
            # Direct comparison is removed as there's no direct key==search_text in the new format
            for key, value in data_item.items():
                # Recursively search through dictionary values and lists
                if isinstance(value, (dict, list)):
                    self.recursive_search(value, search_text, code_results, md_results)
        elif isinstance(data_item, list):
            for item in data_item:
                # Now we check for the 'name' key in each item of the list
                if isinstance(item, dict) and item.get("name") == search_text:
                    # If 'code_content' exists, append it to results
                    if "code_content" in item:
                        code_results.append(item["code_content"])
                        md_results.append(item["md_content"])
                # Recursive call in case of nested lists or dicts
                self.recursive_search(item, search_text, code_results, md_results)

    def search_code_contents_by_name(self, file_path, search_text):
        # Attempt to retrieve code from the JSON file
        try:
            with open(file_path, "r", encoding="utf-8") as file:
                data = json.load(file)
                code_results = []
                md_results = []  # List to store matching items' code_content and md_content
                self.recursive_search(data, search_text, code_results, md_results)
                # ç¡®ä¿æ— è®ºç»“æœå¦‚ä½•éƒ½è¿”å›ä¸¤ä¸ªå€¼
                if code_results or md_results:
                    return code_results, md_results
                else:
                    return ["No matching item found."], ["No matching item found."]
        except FileNotFoundError:
            return "File not found."
        except json.JSONDecodeError:
            return "Invalid JSON file."
        except Exception as e:
            return f"An error occurred: {e}"


if __name__ == "__main__":
    processor = JsonFileProcessor("database.json")
    md_contents, extracted_contents = processor.extract_data()



================================================
FILE: repo_agent/chat_with_repo/main.py
================================================
import time

from repo_agent.chat_with_repo.gradio_interface import GradioInterface
from repo_agent.chat_with_repo.rag import RepoAssistant
from repo_agent.log import logger
from repo_agent.settings import SettingsManager


def main():
    logger.info("Initializing the RepoAgent chat with doc module.")

    # Load settings
    setting = SettingsManager.get_setting()

    api_key = setting.chat_completion.openai_api_key.get_secret_value()
    api_base = str(setting.chat_completion.openai_base_url)
    db_path = (
        setting.project.target_repo
        / setting.project.hierarchy_name
        / "project_hierarchy.json"
    )

    # Initialize RepoAssistant
    assistant = RepoAssistant(api_key, api_base, db_path)

    # Extract data
    md_contents, meta_data = assistant.json_data.extract_data()

    # Create vector store and measure runtime
    logger.info("Starting vector store creation...")
    start_time = time.time()
    assistant.vector_store_manager.create_vector_store(
        md_contents, meta_data, api_key, api_base
    )
    elapsed_time = time.time() - start_time
    logger.info(f"Vector store created successfully in {elapsed_time:.2f} seconds.")

    # Launch Gradio interface
    GradioInterface(assistant.respond)


if __name__ == "__main__":
    main()



================================================
FILE: repo_agent/chat_with_repo/prompt.py
================================================
from llama_index.core import ChatPromptTemplate, PromptTemplate
from llama_index.core.llms import ChatMessage, MessageRole

# Query Generation Prompt
query_generation_prompt_str = (
    "You are a helpful assistant that generates multiple search queries based on a "
    "single input query. Generate {num_queries} search queries, one on each line, "
    "related to the following input query:\n"
    "Query: {query}\n"
    "Queries:\n"
)
query_generation_template = PromptTemplate(query_generation_prompt_str)

# Relevance Ranking Prompt
relevance_ranking_instruction = (
    "You are an expert relevance ranker. Given a list of documents and a query, your job is to determine how relevant each document is for answering the query. "
    "Your output is JSON, which is a list of documents. Each document has two fields, content and relevance_score. relevance_score is from 0.0 to 100.0. "
    "Higher relevance means higher score."
)
relevance_ranking_guideline = "Query: {query} Docs: {docs}"

relevance_ranking_message_template = [
    ChatMessage(content=relevance_ranking_instruction, role=MessageRole.SYSTEM),
    ChatMessage(
        content=relevance_ranking_guideline,
        role=MessageRole.USER,
    ),
]
relevance_ranking_chat_template = ChatPromptTemplate(
    message_templates=relevance_ranking_message_template
)

# RAG (Retrieve and Generate) Prompt
rag_prompt_str = (
    "You are a helpful assistant in repository Q&A. Users will ask questions about something contained in a repository. "
    "You will be shown the user's question, and the relevant information from the repository. Answer the user's question only with information given.\n\n"
    "Question: {query}.\n\n"
    "Information: {information}"
)
rag_template = PromptTemplate(rag_prompt_str)

# RAG_AR (Advanced RAG) Prompt
rag_ar_prompt_str = (
    "You are a helpful Repository-Level Software Q&A assistant. Your task is to answer users' questions based on the given information about a software repository, "
    "including related code and documents.\n\n"
    "Currently, you're in the {project_name} project. The user's question is:\n"
    "{query}\n\n"
    "Now, you are given related code and documents as follows:\n\n"
    "-------------------Code-------------------\n"
    "Some most likely related code snippets recalled by the retriever are:\n"
    "{related_code}\n\n"
    "-------------------Document-------------------\n"
    "Some most relevant documents recalled by the retriever are:\n"
    "{embedding_recall}\n\n"
    "Please note:   \n"
    "1. All the provided recall results are related to the current project {project_name}. Please filter useful information according to the user's question and provide corresponding answers or solutions.\n"
    "2. Ensure that your responses are accurate and detailed. Present specific answers in a professional manner and tone.\n"
    "3. The user's question may be asked in any language. You must respond **in the same language** as the user's question, even if the input language is not English.\n"
    "4. If you find the user's question completely unrelated to the provided information or if you believe you cannot provide an accurate answer, kindly decline. Note: DO NOT fabricate any non-existent information.\n\n"
    "Now, focusing on the user's query, and incorporating the given information to offer a specific, detailed, and professional answer IN THE SAME LANGUAGE AS the user's question."
)


rag_ar_template = PromptTemplate(rag_ar_prompt_str)



================================================
FILE: repo_agent/chat_with_repo/rag.py
================================================
import json

from llama_index.llms.openai import OpenAI

from repo_agent.chat_with_repo.json_handler import JsonFileProcessor
from repo_agent.chat_with_repo.prompt import (
    query_generation_template,
    rag_ar_template,
    rag_template,
    relevance_ranking_chat_template,
)
from repo_agent.chat_with_repo.text_analysis_tool import TextAnalysisTool
from repo_agent.chat_with_repo.vector_store_manager import VectorStoreManager
from repo_agent.log import logger


class RepoAssistant:
    def __init__(self, api_key, api_base, db_path):
        self.db_path = db_path
        self.md_contents = []

        self.weak_model = OpenAI(
            api_key=api_key,
            api_base=api_base,
            model="gpt-4o-mini",
        )
        self.strong_model = OpenAI(
            api_key=api_key,
            api_base=api_base,
            model="gpt-4o",
        )
        self.textanslys = TextAnalysisTool(self.weak_model, db_path)
        self.json_data = JsonFileProcessor(db_path)
        self.vector_store_manager = VectorStoreManager(top_k=5, llm=self.weak_model)

    def generate_queries(self, query_str: str, num_queries: int = 4):
        fmt_prompt = query_generation_template.format(
            num_queries=num_queries - 1, query=query_str
        )
        response = self.weak_model.complete(fmt_prompt)
        queries = response.text.split("\n")
        return queries

    def rerank(self, query, docs):  # è¿™é‡Œè¦é˜²æ­¢è¿”å›å€¼æ ¼å¼ä¸Šå‡ºé—®é¢˜
        response = self.weak_model.chat(
            response_format={"type": "json_object"},
            temperature=0,
            messages=relevance_ranking_chat_template.format_messages(
                query=query, docs=docs
            ),
        )
        scores = json.loads(response.message.content)["documents"]  # type: ignore
        logger.debug(f"scores: {scores}")
        sorted_data = sorted(scores, key=lambda x: x["relevance_score"], reverse=True)
        top_5_contents = [doc["content"] for doc in sorted_data[:5]]
        return top_5_contents

    def rag(self, query, retrieved_documents):
        rag_prompt = rag_template.format(
            query=query, information="\n\n".join(retrieved_documents)
        )
        response = self.weak_model.complete(rag_prompt)
        return response.text

    def list_to_markdown(self, list_items):
        markdown_content = ""

        # å¯¹äºåˆ—è¡¨ä¸­çš„æ¯ä¸ªé¡¹ç›®ï¼Œæ·»åŠ ä¸€ä¸ªå¸¦æ•°å­—çš„åˆ—è¡¨é¡¹
        for index, item in enumerate(list_items, start=1):
            markdown_content += f"{index}. {item}\n"

        return markdown_content

    def rag_ar(self, query, related_code, embedding_recall, project_name):
        rag_ar_prompt = rag_ar_template.format_messages(
            query=query,
            related_code=related_code,
            embedding_recall=embedding_recall,
            project_name=project_name,
        )
        response = self.strong_model.chat(rag_ar_prompt)
        return response.message.content

    def respond(self, message, instruction):
        """
        Respond to a user query by processing input, querying the vector store,
        reranking results, and generating a final response.
        """
        logger.debug("Starting response generation.")

        # Step 1: Format the chat prompt
        prompt = self.textanslys.format_chat_prompt(message, instruction)
        logger.debug(f"Formatted prompt: {prompt}")

        questions = self.textanslys.keyword(prompt)
        logger.debug(f"Generated keywords from prompt: {questions}")

        # Step 2: Generate additional queries
        prompt_queries = self.generate_queries(prompt, 3)
        logger.debug(f"Generated queries: {prompt_queries}")

        all_results = []
        all_documents = []

        # Step 3: Query the VectorStoreManager for each query
        for query in prompt_queries:
            logger.debug(f"Querying vector store with: {query}")
            query_results = self.vector_store_manager.query_store(query)
            logger.debug(f"Results for query '{query}': {query_results}")
            all_results.extend(query_results)

        # Step 4: Deduplicate results by content
        unique_results = {result["text"]: result for result in all_results}.values()
        unique_documents = [result["text"] for result in unique_results]
        logger.debug(f"Unique documents: {unique_documents}")

        unique_code = [
            result.get("metadata", {}).get("code_content") for result in unique_results
        ]
        logger.debug(f"Unique code content: {unique_code}")

        # Step 5: Rerank documents based on relevance
        retrieved_documents = self.rerank(message, unique_documents)
        logger.debug(f"Reranked documents: {retrieved_documents}")

        # Step 6: Generate a response using RAG (Retrieve and Generate)
        response = self.rag(prompt, retrieved_documents)
        chunkrecall = self.list_to_markdown(retrieved_documents)
        logger.debug(f"RAG-generated response: {response}")
        logger.debug(f"Markdown chunk recall: {chunkrecall}")

        bot_message = str(response)
        logger.debug(f"Initial bot_message: {bot_message}")

        # Step 7: Perform NER and queryblock processing
        keyword = str(self.textanslys.nerquery(bot_message))
        keywords = str(self.textanslys.nerquery(str(prompt) + str(questions)))
        logger.debug(f"Extracted keywords: {keyword}, {keywords}")

        codez, mdz = self.textanslys.queryblock(keyword)
        codey, mdy = self.textanslys.queryblock(keywords)

        # Ensure all returned items are lists
        codez = codez if isinstance(codez, list) else [codez]
        mdz = mdz if isinstance(mdz, list) else [mdz]
        codey = codey if isinstance(codey, list) else [codey]
        mdy = mdy if isinstance(mdy, list) else [mdy]

        # Step 8: Merge and deduplicate results
        codex = list(dict.fromkeys(codez + codey))
        md = list(dict.fromkeys(mdz + mdy))
        unique_mdx = list(set([item for sublist in md for item in sublist]))
        uni_codex = list(dict.fromkeys(codex))
        uni_md = list(dict.fromkeys(unique_mdx))

        # Convert to Markdown format
        codex_md = self.textanslys.list_to_markdown(uni_codex)
        retrieved_documents = list(dict.fromkeys(retrieved_documents + uni_md))

        # Final rerank and response generation
        retrieved_documents = self.rerank(message, retrieved_documents[:6])
        logger.debug(f"Final retrieved documents after rerank: {retrieved_documents}")

        uni_code = self.rerank(
            message, list(dict.fromkeys(uni_codex + unique_code))[:6]
        )
        logger.debug(f"Final unique code after rerank: {uni_code}")

        unique_code_md = self.textanslys.list_to_markdown(unique_code)
        logger.debug(f"Unique code in Markdown: {unique_code_md}")

        # Generate final response using RAG_AR
        bot_message = self.rag_ar(prompt, uni_code, retrieved_documents, "test")
        logger.debug(f"Final bot_message after RAG_AR: {bot_message}")

        return message, bot_message, chunkrecall, questions, unique_code_md, codex_md



================================================
FILE: repo_agent/chat_with_repo/text_analysis_tool.py
================================================
from llama_index.core.llms.function_calling import FunctionCallingLLM
from llama_index.llms.openai import OpenAI

from repo_agent.chat_with_repo.json_handler import JsonFileProcessor


class TextAnalysisTool:
    def __init__(self, llm: FunctionCallingLLM, db_path):
        self.jsonsearch = JsonFileProcessor(db_path)
        self.llm = llm
        self.db_path = db_path

    def keyword(self, query):
        prompt = f"Please provide a list of Code keywords according to the following query, please output no more than 3 keywords, Input: {query}, Output:"
        response = self.llm.complete(prompt)
        return response

    def tree(self, query):
        prompt = f"Please analyze the following text and generate a tree structure based on its hierarchy:\n\n{query}"
        response = self.llm.complete(prompt)
        return response

    def format_chat_prompt(self, message, instruction):
        prompt = f"System:{instruction}\nUser: {message}\nAssistant:"
        return prompt

    def queryblock(self, message):
        search_result, md = self.jsonsearch.search_code_contents_by_name(
            self.db_path, message
        )
        return search_result, md

    def list_to_markdown(self, search_result):
        markdown_str = ""
        # éå†åˆ—è¡¨ï¼Œå°†æ¯ä¸ªå…ƒç´ è½¬æ¢ä¸ºMarkdownæ ¼å¼çš„é¡¹
        for index, content in enumerate(search_result, start=1):
            # æ·»åŠ åˆ°Markdownå­—ç¬¦ä¸²ä¸­ï¼Œæ¯ä¸ªé¡¹åè·Ÿä¸€ä¸ªæ¢è¡Œç¬¦
            markdown_str += f"{index}. {content}\n\n"

        return markdown_str

    def nerquery(self, message):
        instrcution = """
Extract the most relevant class or function base on the following instrcution:

The output must strictly be a pure function name or class name, without any additional characters.
For example:
Pure function names: calculateSum, processData
Pure class names: MyClass, DataProcessor
The output function name or class name should be only one.
        """
        query = f"{instrcution}\n\nThe input is shown as bellow:\n{message}\n\nAnd now directly give your Output:"
        response = self.llm.complete(query)
        # logger.debug(f"Input: {message}, Output: {response}")
        return response


if __name__ == "__main__":
    api_base = "https://api.openai.com/v1"
    api_key = "your_api_key"
    log_file = "your_logfile_path"
    llm = OpenAI(api_key=api_key, api_base=api_base)
    db_path = "your_database_path"
    test = TextAnalysisTool(llm, db_path)



================================================
FILE: repo_agent/chat_with_repo/vector_store_manager.py
================================================
import chromadb
from llama_index.core import (
    Document,
    StorageContext,
    VectorStoreIndex,
    get_response_synthesizer,
)
from llama_index.core.node_parser import (
    SemanticSplitterNodeParser,
    SentenceSplitter,
)
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore

from repo_agent.log import logger


class VectorStoreManager:
    def __init__(self, top_k, llm):
        """
        Initialize the VectorStoreManager.
        """
        self.query_engine = None  # Initialize as None
        self.chroma_db_path = "./chroma_db"  # Path to Chroma database
        self.collection_name = "test"  # Default collection name
        self.similarity_top_k = top_k
        self.llm = llm

    def create_vector_store(self, md_contents, meta_data, api_key, api_base):
        """
        Add markdown content and metadata to the index.
        """
        if not md_contents or not meta_data:
            logger.warning("No content or metadata provided. Skipping.")
            return

        # Ensure lengths match
        min_length = min(len(md_contents), len(meta_data))
        md_contents = md_contents[:min_length]
        meta_data = meta_data[:min_length]

        logger.debug(f"Number of markdown contents: {len(md_contents)}")
        logger.debug(f"Number of metadata entries: {len(meta_data)}")

        # Initialize Chroma client and collection
        db = chromadb.PersistentClient(path=self.chroma_db_path)
        chroma_collection = db.get_or_create_collection(self.collection_name)

        # Define embedding model
        embed_model = OpenAIEmbedding(
            model_name="text-embedding-3-large",
            api_key=api_key,
            api_base=api_base,
        )

        # Initialize semantic chunker (SimpleNodeParser)
        logger.debug("Initializing semantic chunker (SimpleNodeParser).")
        splitter = SemanticSplitterNodeParser(
            buffer_size=1, breakpoint_percentile_threshold=95, embed_model=embed_model
        )
        base_splitter = SentenceSplitter(chunk_size=1024)

        documents = [
            Document(text=content, extra_info=meta)
            for content, meta in zip(md_contents, meta_data)
        ]

        all_nodes = []
        for i, doc in enumerate(documents):
            logger.debug(
                f"Processing document {i+1}: Content length={len(doc.get_text())}"
            )

            try:
                # Try semantic splitting first
                nodes = splitter.get_nodes_from_documents([doc])
                logger.debug(f"Document {i+1} split into {len(nodes)} semantic chunks.")

            except Exception as e:
                # Fallback to baseline sentence splitting
                logger.warning(
                    f"Semantic splitting failed for document {i+1}, falling back to SentenceSplitter. Error: {e}"
                )
                nodes = base_splitter.get_nodes_from_documents([doc])
                logger.debug(f"Document {i+1} split into {len(nodes)} sentence chunks.")

            all_nodes.extend(nodes)

        if not all_nodes:
            logger.warning("No valid nodes to add to the index after chunking.")
            return

        logger.debug(f"Number of valid chunks: {len(all_nodes)}")

        # Set up ChromaVectorStore and load data
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        index = VectorStoreIndex(
            all_nodes, storage_context=storage_context, embed_model=embed_model
        )
        retriever = VectorIndexRetriever(
            index=index, similarity_top_k=self.similarity_top_k, embed_model=embed_model
        )

        response_synthesizer = get_response_synthesizer(llm=self.llm)

        # Set the query engine
        self.query_engine = RetrieverQueryEngine(
            retriever=retriever,
            response_synthesizer=response_synthesizer,
        )

        logger.info(f"Vector store created and loaded with {len(documents)} documents.")

    def query_store(self, query):
        """
        Query the vector store for relevant documents.
        """
        if not self.query_engine:
            logger.error(
                "Query engine is not initialized. Please create a vector store first."
            )
            return []

        # Query the vector store
        logger.debug(f"Querying vector store with: {query}")
        results = self.query_engine.query(query)

        # Extract relevant information from results
        return [{"text": results.response, "metadata": results.metadata}]



================================================
FILE: repo_agent/utils/gitignore_checker.py
================================================
import fnmatch
import os


class GitignoreChecker:
    def __init__(self, directory: str, gitignore_path: str):
        """
        Initialize the GitignoreChecker with a specific directory and the path to a .gitignore file.

        Args:
            directory (str): The directory to be checked.
            gitignore_path (str): The path to the .gitignore file.
        """
        self.directory = directory
        self.gitignore_path = gitignore_path
        self.folder_patterns, self.file_patterns = self._load_gitignore_patterns()

    def _load_gitignore_patterns(self) -> tuple:
        """
        Load and parse the .gitignore file, then split the patterns into folder and file patterns.

        If the specified .gitignore file is not found, fall back to the default path.

        Returns:
            tuple: A tuple containing two lists - one for folder patterns and one for file patterns.
        """
        try:
            with open(self.gitignore_path, "r", encoding="utf-8") as file:
                gitignore_content = file.read()
        except FileNotFoundError:
            # Fallback to the default .gitignore path if the specified file is not found
            default_path = os.path.join(
                os.path.dirname(__file__), "..", "..", ".gitignore"
            )
            with open(default_path, "r", encoding="utf-8") as file:
                gitignore_content = file.read()

        patterns = self._parse_gitignore(gitignore_content)
        return self._split_gitignore_patterns(patterns)

    @staticmethod
    def _parse_gitignore(gitignore_content: str) -> list:
        """
        Parse the .gitignore content and return patterns as a list.

        Args:
            gitignore_content (str): The content of the .gitignore file.

        Returns:
            list: A list of patterns extracted from the .gitignore content.
        """
        patterns = []
        for line in gitignore_content.splitlines():
            line = line.strip()
            if line and not line.startswith("#"):
                patterns.append(line)
        return patterns

    @staticmethod
    def _split_gitignore_patterns(gitignore_patterns: list) -> tuple:
        """
        Split the .gitignore patterns into folder patterns and file patterns.

        Args:
            gitignore_patterns (list): A list of patterns from the .gitignore file.

        Returns:
            tuple: Two lists, one for folder patterns and one for file patterns.
        """
        folder_patterns = []
        file_patterns = []
        for pattern in gitignore_patterns:
            if pattern.endswith("/"):
                folder_patterns.append(pattern.rstrip("/"))
            else:
                file_patterns.append(pattern)
        return folder_patterns, file_patterns

    @staticmethod
    def _is_ignored(path: str, patterns: list, is_dir: bool = False) -> bool:
        """
        Check if the given path matches any of the patterns.

        Args:
            path (str): The path to check.
            patterns (list): A list of patterns to check against.
            is_dir (bool): True if the path is a directory, False otherwise.

        Returns:
            bool: True if the path matches any pattern, False otherwise.
        """
        for pattern in patterns:
            if fnmatch.fnmatch(path, pattern):
                return True
            if is_dir and pattern.endswith("/") and fnmatch.fnmatch(path, pattern[:-1]):
                return True
        return False

    def check_files_and_folders(self) -> list:
        """
        Check all files and folders in the given directory against the split gitignore patterns.
        Return a list of files that are not ignored and have the '.py' extension.
        The returned file paths are relative to the self.directory.

        Returns:
            list: A list of paths to files that are not ignored and have the '.py' extension.
        """
        not_ignored_files = []
        for root, dirs, files in os.walk(self.directory):
            dirs[:] = [
                d
                for d in dirs
                if not self._is_ignored(d, self.folder_patterns, is_dir=True)
            ]

            for file in files:
                file_path = os.path.join(root, file)
                relative_path = os.path.relpath(file_path, self.directory)
                if not self._is_ignored(
                    file, self.file_patterns
                ) and file_path.endswith(".py"):
                    not_ignored_files.append(relative_path)

        return not_ignored_files


# Example usage:
# gitignore_checker = GitignoreChecker('path_to_directory', 'path_to_gitignore_file')
# not_ignored_files = gitignore_checker.check_files_and_folders()
# print(not_ignored_files)



================================================
FILE: repo_agent/utils/meta_info_utils.py
================================================
[Binary file]


================================================
FILE: tests/__init__.py
================================================
[Empty file]


================================================
FILE: tests/test_change_detector.py
================================================
[Binary file]


================================================
FILE: tests/test_json_handler.py
================================================
import unittest
from unittest.mock import mock_open, patch

from ..repo_agent.chat_with_repo.json_handler import (
    JsonFileProcessor,  # Adjust the import according to your project structure
)


class TestJsonFileProcessor(unittest.TestCase):

    def setUp(self):
        self.processor = JsonFileProcessor("test.json")

    @patch("builtins.open", new_callable=mock_open, read_data='{"files": [{"objects": [{"md_content": "content1"}]}]}')
    def test_read_json_file(self, mock_file):
        # Test read_json_file method
        data = self.processor.read_json_file()
        self.assertEqual(data, {"files": [{"objects": [{"md_content": "content1"}]}]})
        mock_file.assert_called_with("test.json", "r", encoding="utf-8")

    @patch.object(JsonFileProcessor, 'read_json_file')
    def test_extract_md_contents(self, mock_read_json):
        # Test extract_md_contents method
        mock_read_json.return_value = {"files": [{"objects": [{"md_content": "content1"}]}]}
        md_contents = self.processor.extract_md_contents()
        self.assertIn("content1", md_contents)

    @patch("builtins.open", new_callable=mock_open, read_data='{"name": "test", "files": [{"name": "file1"}]}')
    def test_search_in_json_nested(self, mock_file):
        # Test search_in_json_nested method
        result = self.processor.search_in_json_nested("test.json", "file1")
        self.assertEqual(result, {"name": "file1"})
        mock_file.assert_called_with("test.json", "r", encoding="utf-8")

    # Additional tests for error handling (FileNotFoundError, JSONDecodeError, etc.) can be added here

if __name__ == '__main__':
    unittest.main()



================================================
FILE: tests/test_structure_tree.py
================================================
import os
from collections import defaultdict


def build_path_tree(who_reference_me, reference_who, doc_item_path):
    def tree():
        return defaultdict(tree)
    path_tree = tree()

    for path_list in [who_reference_me, reference_who]:
        for path in path_list:
            parts = path.split(os.sep)
            node = path_tree
            for part in parts:
                node = node[part]

    # å¤„ç† doc_item_path
    parts = doc_item_path.split(os.sep)
    parts[-1] = 'âœ³ï¸' + parts[-1]  # åœ¨æœ€åä¸€ä¸ªå¯¹è±¡å‰é¢åŠ ä¸Šæ˜Ÿå·
    node = path_tree
    for part in parts:
        node = node[part]

    def tree_to_string(tree, indent=0):
        s = ''
        for key, value in sorted(tree.items()):
            s += '    ' * indent + key + '\n'
            if isinstance(value, dict):
                s += tree_to_string(value, indent + 1)
        return s

    return tree_to_string(path_tree)


if "__name__ == main":
    who_reference_me = [
        "repo_agent/file_handler.py/FileHandler/__init__",
        "repo_agent/runner.py/need_to_generate"
    ]
    reference_who = [
        "repo_agent/file_handler.py/FileHandler/__init__",
        "repo_agent/runner.py/need_to_generate",
    ]

    doc_item_path = 'tests/test_change_detector.py/TestChangeDetector'

    result = build_path_tree(who_reference_me,reference_who,doc_item_path)
    print(result)



================================================
FILE: .github/workflows/release.yml
================================================
name: Release

on:
  release:
    types: [published]

jobs:
  pypi-publish:
    name: upload release to PyPI
    runs-on: ubuntu-latest
    permissions:
      # This permission is needed for private repositories.
      contents: read
      # IMPORTANT: this permission is mandatory for trusted publishing
      id-token: write
    steps:
      - uses: actions/checkout@v3

      - uses: pdm-project/setup-pdm@v3

      - name: Publish package distributions to PyPI
        run: pdm publish


