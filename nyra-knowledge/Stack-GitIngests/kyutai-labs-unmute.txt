Directory structure:
└── kyutai-labs-unmute/
    ├── README.md
    ├── bake_deploy_prod.sh
    ├── bake_deploy_staging.sh
    ├── CONTRIBUTING.md
    ├── docker-compose.yml
    ├── Dockerfile
    ├── LICENSE
    ├── pyproject.toml
    ├── setup_gpu_swarm_node.py
    ├── swarm-deploy.yml
    ├── SWARM.md
    ├── voices.yaml
    ├── .dockerignore
    ├── .pre-commit-config.yaml
    ├── dockerless/
    │   ├── start_backend.sh
    │   ├── start_frontend.sh
    │   ├── start_llm.sh
    │   ├── start_stt.sh
    │   └── start_tts.sh
    ├── docs/
    │   └── browser_backend_communication.md
    ├── frontend/
    │   ├── README.md
    │   ├── Dockerfile
    │   ├── eslint.config.mjs
    │   ├── hot-reloading.Dockerfile
    │   ├── next.config.ts
    │   ├── package.json
    │   ├── postcss.config.mjs
    │   ├── tsconfig.json
    │   ├── public/
    │   │   └── audio-output-processor.js
    │   └── src/
    │       ├── mdx-components.tsx
    │       ├── app/
    │       │   ├── audioUtil.ts
    │       │   ├── chatHistory.ts
    │       │   ├── ConsentModal.tsx
    │       │   ├── CouldNotConnect.tsx
    │       │   ├── cssUtil.ts
    │       │   ├── ErrorMessages.tsx
    │       │   ├── globals.css
    │       │   ├── layout.tsx
    │       │   ├── Modal.tsx
    │       │   ├── opus-recorder.d.ts
    │       │   ├── page.tsx
    │       │   ├── PositionedAudioVisualizer.tsx
    │       │   ├── SingleRoleSubtitles.tsx
    │       │   ├── SlantedButton.tsx
    │       │   ├── SquareButton.tsx
    │       │   ├── Subtitles.tsx
    │       │   ├── Unmute.tsx
    │       │   ├── UnmuteConfigurator.tsx
    │       │   ├── UnmuteHeader.tsx
    │       │   ├── useAudioProcessor.ts
    │       │   ├── useAudioVisualizerCircle.ts
    │       │   ├── useBackendServerUrl.ts
    │       │   ├── useGoogleAnalytics.ts
    │       │   ├── useKeyboardShortcuts.ts
    │       │   ├── useLocalStorage.ts
    │       │   ├── useMicrophoneAccess.ts
    │       │   ├── useRecordingCanvas.ts
    │       │   ├── useWakeLock.ts
    │       │   ├── VoiceAttribution.tsx
    │       │   ├── VoiceRecorder.tsx
    │       │   ├── VoiceUpload.tsx
    │       │   └── voice-donation/
    │       │       ├── DonationConsent.tsx
    │       │       ├── IntroText.mdx
    │       │       ├── page.tsx
    │       │       ├── privacy-policy/
    │       │       │   └── page.tsx
    │       │       └── terms-of-use/
    │       │           └── page.tsx
    │       └── assets/
    │           └── fonts/
    │               ├── Satoshi-Variable.woff
    │               ├── Satoshi-Variable.woff2
    │               ├── Satoshi-VariableItalic.woff
    │               └── Satoshi-VariableItalic.woff2
    ├── notebooks/
    │   └── create-voice-donation-sentences.ipynb
    ├── services/
    │   ├── debugger/
    │   │   └── Dockerfile
    │   ├── grafana/
    │   │   ├── Dockerfile
    │   │   ├── grafana.ini
    │   │   ├── dashboards/
    │   │   │   └── unmute-monitoring-1751624072717.json
    │   │   └── provisioning/
    │   │       ├── dashboards/
    │   │       │   └── dashboards.yaml
    │   │       └── datasources/
    │   │           └── datasources.yaml
    │   ├── moshi-server/
    │   │   ├── private.Dockerfile
    │   │   ├── public.Dockerfile
    │   │   ├── start_moshi_server_private.sh
    │   │   ├── start_moshi_server_public.sh
    │   │   └── configs/
    │   │       ├── stt-prod.toml
    │   │       ├── stt.toml
    │   │       ├── tts-prod.toml
    │   │       ├── tts.toml
    │   │       └── voice-cloning.toml
    │   └── prometheus/
    │       ├── Dockerfile
    │       └── prometheus.yml
    ├── tests/
    │   ├── test_exponential_moving_average.py
    │   └── test_llm_utils.py
    ├── unmute/
    │   ├── audio_input_override.py
    │   ├── audio_stream_saver.py
    │   ├── cache.py
    │   ├── exceptions.py
    │   ├── kyutai_constants.py
    │   ├── main_gradio.py
    │   ├── main_websocket.py
    │   ├── metrics.py
    │   ├── openai_realtime_api_events.py
    │   ├── process_recording.py
    │   ├── quest_manager.py
    │   ├── recorder.py
    │   ├── service_discovery.py
    │   ├── timer.py
    │   ├── unmute_handler.py
    │   ├── webrtc_utils.py
    │   ├── websocket_utils.py
    │   ├── llm/
    │   │   ├── chatbot.py
    │   │   ├── llm_utils.py
    │   │   ├── newsapi.py
    │   │   ├── quiz_show_questions.py
    │   │   └── system_prompt.py
    │   ├── loadtest/
    │   │   ├── dummy_tts_server.py
    │   │   ├── generate_dataset_for_vllm.py
    │   │   ├── loadtest_client.py
    │   │   └── loadtest_result.py
    │   ├── scripts/
    │   │   ├── check_hugging_face_token_not_write.py
    │   │   ├── copy_voice_to_prod.py
    │   │   ├── example_websocket_client.py
    │   │   ├── mistral_streaming.py
    │   │   ├── output_from_file.py
    │   │   ├── output_sine.py
    │   │   ├── output_sine_async.py
    │   │   ├── output_tts.py
    │   │   ├── pitch_detection_handler.py
    │   │   ├── stt_from_file_example.py
    │   │   ├── stt_microphone_example.py
    │   │   ├── tts_example.py
    │   │   ├── update_voice_list.py
    │   │   └── vllm_wrapper_example.py
    │   ├── stt/
    │   │   ├── dummy_speech_to_text.py
    │   │   ├── exponential_moving_average.py
    │   │   └── speech_to_text.py
    │   └── tts/
    │       ├── copy_approved_voice_donations.py
    │       ├── create_voice_donation_table.py
    │       ├── freesound_download.py
    │       ├── realtime_queue.py
    │       ├── text_to_speech.py
    │       ├── trim_voice_donation_clip.py
    │       ├── voice_cloning.py
    │       ├── voice_donation.py
    │       └── voices.py
    └── .github/
        ├── PULL_REQUEST_TEMPLATE.md
        └── workflows/
            └── ci.yml

================================================
FILE: README.md
================================================
# Unmute

Try it out at [Unmute.sh](https://unmute.sh)!

Unmute is a system that allows text LLMs to listen and speak by wrapping them in Kyutai's Text-to-speech and Speech-to-text models.
The speech-to-text transcribes what the user says, the LLM generates a response in text, and the text-to-speech reads it out loud.
Both the STT and TTS are optimized for low latency and the system works with any text LLM you like.

If you want to use Kyutai STT or Kyutai TTS separately, check out [kyutai-labs/delayed-streams-modeling](https://github.com/kyutai-labs/delayed-streams-modeling).

On a high level, it works like this:

```mermaid
graph LR
    UB[User browser]
    UB --> B(Backend)
    UB --> F(Frontend)
    B --> STT(Speech-to-text)
    B --> LLM(LLM)
    B --> TTS(Text-to-speech)
```

- The user opens the Unmute website, served by the **frontend**.
- By clicking "connect", the user establishes a websocket connection to the **backend**, sending audio and other metadata back and forth in real time.
  - The backend connects via websocket to the **speech-to-text** server, sending it the audio from the user and receiving back the transcription in real time.
  - Once the speech-to-text detects that the user has stopped speaking and it's time to generate a response, the backend connects to an **LLM** server to retrieve the response. We host our own LLM using [VLLM](https://github.com/vllm-project/vllm), but you could also use an external API like OpenAI or Mistral.
  - As the response is being generated, the backend feeds it to the **text-to-speech** server to read it out loud, and forwards the generated speech to the user.

## Setup

> [!NOTE]
> If something isn't working for you, don't hesistate to open an issue. We'll do our best to help you figure out what's wrong.

Requirements:
- Hardware: a GPU with CUDA support and at least 16 GB memory.
- OS: Linux, or Windows with WSL ([installation instructions](https://ubuntu.com/desktop/wsl)). Running on Windows natively is not supported (see [#84](https://github.com/kyutai-labs/unmute/issues/84)). Neither is running on Mac (see [#74](https://github.com/kyutai-labs/unmute/issues/74)).

We provide multiple ways of deploying your own [unmute.sh](unmute.sh):

| Name                      | Number of gpus | Number of machines | Difficulty | Documented | Kyutai support |
|---------------------------|----------------|--------------------|------------|------------|----------------|
| Docker Compose            | 1+             | 1                  | Very easy  |✅         |✅              |
| Dockerless                | 1 to 3         | 1 to 5             | Easy       |✅         |✅              |
| Docker Swarm              | 1 to ~100      | 1 to ~100          | Medium     |✅         |❌              |


Since Unmute is a complex system with many services that need to be running at the same time, we recommend using [**Docker Compose**](https://docs.docker.com/compose/) to run Unmute.
It allows you to start or stop all services using a single command.
Since the services are Docker containers, you get a reproducible environment without having to worry about dependencies.

While we support deploying with Docker compose and without Docker, the Docker Swarm deployment is only given to show how we deploy and scale [unmute.sh](unmute.sh). It looks a lot like the compose files, but since debugging multi-nodes applications is hard, we cannot help you debug the swarm deployment.

### LLM access on Hugging Face Hub

You can use any LLM you want.
By default, Unmute uses [Mistral Small 3.2 24B](https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506) as the LLM.
([Gemma 3 12B](https://huggingface.co/google/gemma-3-12b-it) is also a good choice.)
This model is freely available but requires you to accept the conditions to accept it:

1. Create a Hugging Face account.
2. Accept the conditions on the [Mistral Small 3.2 24B model page](https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506).
3. [Create an access token.](https://huggingface.co/docs/hub/en/security-tokens) You can use a fine-grained token, the only permission you need to grant is "Read access to contents of all public gated repos you can access".
   **Do not use tokens with write access when deploying publicly.** In case the server is compromised somehow, the attacker would get write access to any models/datasets/etc. you have on Hugging Face.
4. Add the token into your `~/.bashrc` or equivalent as `export HUGGING_FACE_HUB_TOKEN=hf_...your token here...`

### Start Unmute

Make sure you have [**Docker Compose**](https://docs.docker.com/compose/) installed.
You'll also need the [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) to allow Docker to access your GPU.
To make sure the NVIDIA Container Toolkit is installed correctly, run:
```bash
sudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi
```

If you use [meta-llama/Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B),
the default in `docker-compose.yml`, 16GB of GPU memory is sufficient.
If you're running into memory issues, open `docker-compose.yml` and look for `NOTE:` comments to see places that you might need to adjust.

On a machine with a GPU, run:

```bash
# Make sure you have the environment variable with the token:
echo $HUGGING_FACE_HUB_TOKEN  # This should print hf_...something...

docker compose up --build
```

#### Using multiple GPUs

On [Unmute.sh](https://unmute.sh/), we run the speech-to-text, text-to-speech, and the VLLM server on separate GPUs,
which improves the latency compared to a single-GPU setup.
The TTS latency decreases from ~750ms when running everything on a single L40S GPU to around ~450ms on [Unmute.sh](https://unmute.sh/).

If you have at least three GPUs available, add this snippet to the `stt`, `tts` and `llm` services to ensure they are run on separate GPUs:

```yaml
  stt: # Similarly for `tts` and `llm`
    # ...other configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```


### Running without Docker

Alternatively, you can choose to run Unmute by manually starting the services without going through Docker.
This can be more difficult to set up because of the various dependencies needed.

The following instructions only work for Linux and WSL.

#### Software requirements

* `uv`: Install with `curl -LsSf https://astral.sh/uv/install.sh | sh`
* `cargo`: Install with `curl https://sh.rustup.rs -sSf | sh`
* `pnpm`: Install with `curl -fsSL https://get.pnpm.io/install.sh | sh -`
* `cuda 12.1`: Install it with conda or directly from the Nvidia website. Needed for the Rust processes (tts and stt).

#### Hardware requirements

Start each of the services one by one in a different tmux session or terminal:
```bash
./dockerless/start_frontend.sh
./dockerless/start_backend.sh
./dockerless/start_llm.sh        # Needs 6.1GB of vram
./dockerless/start_stt.sh        # Needs 2.5GB of vram
./dockerless/start_tts.sh        # Needs 5.3GB of vram
```
And the website should be accessible at `http://localhost:3000`.

### Connecting to a remote server running Unmute

If you're running Unmute on a machine that you're accessing over SSH – call it `unmute-box`  – and you'd like to access it from your local computer,
you'll need to set up [port forwarding](https://www.ssh.com/academy/ssh/tunneling-example).

**For Docker Compose**: By default, our Docker Compose setup runs on port 80.
To forward port 80 on the remote to port 3333 locally, use:

```bash
ssh -N -L 3333:localhost:80 unmute-box
```
If everything works correctly, this command will simply not output anything and just keep running.
Then open `localhost:3333` in your browser.

**For Dockerless**: You need to separately forward the backend (port 8000) and frontend (port 3000):

```bash
ssh -N -L 8000:localhost:8000 -L 3000:localhost:3000 unmute-box
```

```mermaid
flowchart LR
    subgraph Local_Machine [Local Machine]
        direction TB
        browser[Browser]
        browser -. "User opens localhost:3000 in browser" .-> local_frontend[localhost:3000]
        browser -. "Frontend queries API at localhost:8000" .-> local_backend[localhost:8000]
    end
    subgraph Remote_Server [Remote Server]
        direction TB
        remote_backend[Backend:8000]
        remote_frontend[Frontend:3000]
    end
    local_backend -- "SSH Tunnel: 8000" --> remote_backend
    local_frontend -- "SSH Tunnel: 3000" --> remote_frontend
```

### HTTPS support

For simplicity, we omit HTTPS support from the Docker Compose and Dockerless setups.
If you want to make the deployment work over the HTTPS, consider using Docker Swarm
(see [SWARM.md](/SWARM.md)) or ask your favorite LLM how to make the Docker Compose or dockerless setup work over HTTPS.


## Production deployment with Docker Swarm

If you're curious to know how we deploy and scale [unmute.sh](https://unmute.sh), take a look at our docs
on the [Docker Swarm deployment](./SWARM.md).

## Modifying Unmute

Here are some high-level pointers about how you'd go about making certain changes to Unmute.

### Subtitles and dev mode

Press "S" to turn on subtitles for both the user and the chatbot.

There is also a dev mode that can help debugging, but it's disabled by default.
Go to `useKeyboardShortcuts.ts` and change `ALLOW_DEV_MODE` to `true`.
Then press `D` to see a debug view.
You can add information to the dev mode by modifying `self.debug_dict` in `unmute_handler.py`.

### Changing characters/voices

The characters' voices and prompts are defined in [`voices.yaml`](voices.yaml).
The format of the config file should be intuitive.
Certain system prompts contain dynamically generated elements.
For example, "Quiz show" has its 5 questions randomly chosen in advance from a fixed list.
System prompts like this are defined in [`unmute/llm/system_prompt.py`](unmute/llm/system_prompt.py).

Note that the file is only loaded when the backend starts and is then cached, so if you change something in `voices.yaml`,
you'll need to restart the backend.

### Using external LLM servers

The Unmute backend can be used with any OpenAI compatible LLM server. By default, the `docker-compose.yml` configures VLLM to enable a fully self-contained, local setup.
You can modify this file to change to another external LLM, such as an OpenAI server, a local ollama setup, etc.

For ollama, as environment variables for the `unmute-backend` image, replace
```yaml
  backend:
    image: unmute-backend:latest
    [..]
    environment:
      [..]
       - KYUTAI_LLM_URL=http://llm:8000
```

with
```yaml
  backend:
    image: unmute-backend:latest
    [..]
    environment:
      [..]
      - KYUTAI_LLM_URL=http://host.docker.internal:11434
      - KYUTAI_LLM_MODEL=gemma3
      - KYUTAI_LLM_API_KEY=ollama
    extra_hosts:
      - "host.docker.internal:host-gateway"
```
This points to your localhost server. Alternatively, for OpenAI, you can use
```yaml
  backend:
    image: unmute-backend:latest
    [..]
    environment:
      [..]
      - KYUTAI_LLM_URL=https://api.openai.com/v1
      - KYUTAI_LLM_MODEL=gpt-4.1
      - KYUTAI_LLM_API_KEY=sk-..
```

The section for vllm can then be removed, as it is no longer needed:
```yaml
  llm:
    image: vllm/vllm-openai:v0.9.1
    [..]
```

### Swapping the frontend

The backend and frontend communicate over websocket using a protocol based on the
[OpenAI Realtime API](https://platform.openai.com/docs/guides/realtime) ("ORA").
Where possible, we try to match the ORA format, but there are some extra messages we needed to add,
and others have simplified parameters.
We try to make it clear where we deviate from the ORA format, see [`unmute/openai_realtime_api_events.py`](unmute/openai_realtime_api_events.py).

For detailed information about the WebSocket communication protocol, message types, and audio processing pipeline, see the [browser-backend communication documentation](docs/browser_backend_communication.md).

Ideally, it should be simple to write a single frontend that can communicate with either the Unmute backend
or the OpenAI Realtime API, but we are not fully compatible yet.
Contributions welcome!

The frontend is a Next.js app defined in `frontend/`.
If you'd like to compare to a different frontend implementation,
there is a Python client defined in
[`unmute/loadtest/loadtest_client.py`](unmute/loadtest/loadtest_client.py),
a script that we use to benchmark the latency and throughput of Unmute.

### Tool calling

This is a common requirement so we would appreciate a contribution to support tool calling in Unmute!

The easiest way to integrate tool calling into Unmute would be to do so in a way that's fully invisible to Unmute itself - just make it part of the LLM server.
See [this comment](https://github.com/kyutai-labs/unmute/issues/77#issuecomment-3035220686) on how this can be achieved.
You'd need to write a simple server in FastAPI to wrap vLLM but plug in the tool call responses.

## Developing Unmute

### Install pre-commit hooks

First install `pre-commit` itself – you likely want to install it globally using `pip install pre-commit` rather than in a virtual environment or `uv`,
because you need the `pre-commit` executable to always be available. Then run:

```bash
pre-commit install --hook-type pre-commit
```

We recommend using [uv](https://docs.astral.sh/uv/) to manage Python dependencies.
The commands below assume you are using uv.

### Run backend (dev mode, with autoreloading)

```bash
uv run fastapi dev unmute/main_websocket.py
```

### Run backend (production)

```bash
uv run fastapi run unmute/main_websocket.py
```

### Run loadtest

`loadtest_client.py` is a script that connects to Unmute and simulates conversations with it in order to measure latency and throughput.

```bash
uv run unmute/loadtest/loadtest_client.py --server-url ws://localhost:8000 --n-workers 16
```



================================================
FILE: bake_deploy_prod.sh
================================================
#!/bin/bash
set -ex

uv run unmute/scripts/check_hugging_face_token_not_write.py $HUGGING_FACE_HUB_TOKEN

expected_branch="prod"

current_branch=$(git rev-parse --abbrev-ref HEAD)
if [[ "$current_branch" != "$expected_branch" ]]; then
  echo "❌ You are on branch '$current_branch'. Please switch to '$expected_branch' before deploying."
  exit 1
fi

if [[ -n $(git status --porcelain) ]]; then
  echo "❌ You have uncommitted changes. Please commit or stash them before deploying."
  exit 1
fi

export DOMAIN=unmute.sh
# Note that using non-Mistral models also requires changing the vLLM args in ./swarm-deploy.yml
export KYUTAI_LLM_MODEL=mistralai/Mistral-Small-3.2-24B-Instruct-2506
export DOCKER_HOST=ssh://root@${DOMAIN}

echo "If you get an connection error, do: ssh root@${DOMAIN}"

docker buildx bake -f ./swarm-deploy.yml --allow=ssh --push
docker stack deploy --with-registry-auth --prune --compose-file ./swarm-deploy.yml llm-wrapper



================================================
FILE: bake_deploy_staging.sh
================================================
#!/bin/bash
set -ex

uv run unmute/scripts/check_hugging_face_token_not_write.py $HUGGING_FACE_HUB_TOKEN

export DOMAIN=unmute-staging.kyutai.io
export KYUTAI_LLM_MODEL=google/gemma-3-4b-it
export DOCKER_HOST=ssh://root@${DOMAIN}

echo "If you get an connection error, do: ssh root@${DOMAIN}"

docker buildx bake -f ./swarm-deploy.yml --allow=ssh --push
docker stack deploy --with-registry-auth --prune --compose-file ./swarm-deploy.yml llm-wrapper
docker service scale -d llm-wrapper_tts=1 llm-wrapper_llm=1



================================================
FILE: CONTRIBUTING.md
================================================
# Contributing to Unmute

## Pull Requests

1. Fork the repo and create your branch from `main`.
2. If you have changed APIs, update the documentation accordingly.
3. Ensure pre-commit hooks pass properly, in particular the linting and typing.
4. Accept the Contributor License Agreement (see after).

## Contributor License Agreement ("CLA")

In order to accept your pull request, we need you to submit a Contributor License Agreement.

If you agree with the full CLA provided in the next paragraph, copy the following statement in your PR, changing your Github Handle:

> I, {your GitHub handle}, confirm that I have read and understood the terms of the CLA of Kyutai-labs, as outlined in the repository's CONTRIBUTING.md, and I agree to be bound by these terms.
The full CLA is provided as follows:

> I, {your GitHub handle}, hereby grant to Kyutai-labs a perpetual, worldwide, non-exclusive, royalty-free,
> irrevocable license to use, modify, distribute, and sublicense my Contributions.
> I understand and accept that Contributions are limited to modifications, improvements, or changes
> to the project’s source code submitted via pull requests. I accept that Kyutai-labs has full discretion to
> review, accept, reject, or request changes to any Contributions I submit, and that submitting
> a pull request does not guarantee its inclusion in the project.
> By submitting a Contribution, I grant Kyutai-labs a perpetual, worldwide license to use, modify,
> reproduce, distribute, and create derivative works based on my Contributions.
> I also agree to assign all patent rights for any inventions or improvements that arise from my Contributions,
> giving the Kyutai-labs full rights to file for and enforce patents.
> I understand that the Kyutai-labs may commercialize, relicense, or exploit the project and my Contributions without further notice or obligation to me.
> I confirm that my Contributions are original and that I have the legal right to grant this license.
> If my Contributions include third-party materials, I will ensure that I have the necessary permissions
> and will disclose this information. I accept that once my Contributions are integrated, they may be altered or removed at the Kyutai-labs’s discretion.
> I acknowledge that I am making these Contributions voluntarily and will not receive any compensation.
> Furthermore, I understand that all Contributions, including mine, are provided on an "as-is" basis, with no warranties.
> By submitting a pull request, I agree to be bound by these terms.

## Issues

Please submit issues on our GitHub repository.

## License

By contributing to Unmute, you agree that your contributions will be licensed under the MIT license.
See the `LICENSE` file in the root directory of this source tree.


================================================
FILE: docker-compose.yml
================================================
# See NOTE comments for places to modify.
services:
  traefik:
    image: traefik:v3.3.1
    command:
      # Swarm provider configuration
      - "--providers.docker=true"
      - "--providers.docker.exposedbydefault=false"

      # This is set up for HTTP. If you want HTTPS support for production, use Docker Swarm
      # (check out swarm-deploy.yml) or ask ChatGPT to modify this file for you.
      - "--entrypoints.web.address=:80"
    ports:
      - "80:80"
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock:ro"

  frontend:
    image: unmute-frontend:latest
    build:
      context: frontend/
      dockerfile: hot-reloading.Dockerfile
    volumes:
      - ./frontend/src:/app/src
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.frontend.rule=PathPrefix(`/`)"
      - "traefik.http.routers.frontend.entrypoints=web"
      - "traefik.http.services.frontend.loadbalancer.server.port=3000"
      - "traefik.http.routers.frontend.priority=10" # lowest priority

  backend:
    image: unmute-backend:latest
    build:
      context: ./
      target: hot-reloading
    volumes:
      - ./unmute:/app/unmute
    environment:
      - KYUTAI_STT_URL=ws://stt:8080
      - KYUTAI_TTS_URL=ws://tts:8080
      - KYUTAI_LLM_URL=http://llm:8000
      - NEWSAPI_API_KEY=$NEWSAPI_API_KEY
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.backend.rule=PathPrefix(`/api`)"
      - "traefik.http.routers.backend.middlewares=strip-api"
      - "traefik.http.middlewares.strip-api.replacepathregex.regex=^/api/(.*)"
      - "traefik.http.middlewares.strip-api.replacepathregex.replacement=/$$1"
      - "traefik.http.routers.backend.entrypoints=web"
      - "traefik.http.services.backend.loadbalancer.server.port=80"
      - "traefik.http.routers.backend.priority=100" # higher priority than frontend
      - "prometheus-port=80"

  tts:
    image: moshi-server:latest
    command: ["worker", "--config", "configs/tts.toml"]
    build:
      context: services/moshi-server
      dockerfile: public.Dockerfile
    environment:
      - HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN
    volumes:
      - ./volumes/hf-cache:/root/.cache/huggingface
      - ./volumes/cargo-registry-tts:/root/.cargo/registry
      - ./volumes/tts-target:/app/target
      - ./volumes/uv-cache:/root/.cache/uv
      - /tmp/models/:/models
      - ./volumes/tts-logs:/logs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  stt:
    image: moshi-server:latest
    command: ["worker", "--config", "configs/stt.toml"]
    build:
      context: services/moshi-server
      dockerfile: public.Dockerfile
    environment:
      - HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN
    volumes:
      - ./volumes/hf-cache:/root/.cache/huggingface
      - ./volumes/cargo-registry-stt:/root/.cargo/registry
      - ./volumes/stt-target:/app/target
      - ./volumes/uv-cache:/root/.cache/uv
      - /tmp/models/:/models
      - ./volumes/stt-logs:/logs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  llm:
    image: vllm/vllm-openai:v0.9.1
    command:
      [
        # NOTE: Change the LLM here if you want.
        # (caution: gemma-3-1b-it also exists but it's slow on vLLM: https://github.com/vllm-project/vllm/issues/19575)
        "--model=meta-llama/Llama-3.2-1B-Instruct",
        # NOTE: You can adapt this based on your GPU memory.
        # A higher value takes more memory but supports longer conversations.
        "--max-model-len=1536",
        "--dtype=bfloat16",
        # NOTE: Change this based on your GPU memory.
        # A higher value can make inference faster.
        "--gpu-memory-utilization=0.4",
      ]
    volumes:
      - ./volumes/hf-cache:/root/.cache/huggingface
      - ./volumes/vllm-cache:/root/.cache/vllm
    environment:
      - HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

networks:
  default:


================================================
FILE: Dockerfile
================================================
FROM ghcr.io/astral-sh/uv:0.6.17-debian AS build
WORKDIR /app

ENV UV_COMPILE_BYTECODE=1 UV_LOCKED=1

RUN --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv run --no-dev echo hello

COPY . .
ENV HOSTNAME="0.0.0.0"

HEALTHCHECK --start-period=15s \
    CMD curl --fail http://localhost:80/metrics || exit 1

FROM build AS prod
# Running through uvicorn directly to be able to deactive the Websocket per message deflate which is slowing
# down the replies by a few ms.
CMD ["uv", "run", "--no-dev", "uvicorn", "unmute.main_websocket:app", "--host", "0.0.0.0", "--port", "80", "--ws-per-message-deflate=false"]


FROM build AS hot-reloading
CMD ["uv", "run", "--no-dev", "uvicorn", "unmute.main_websocket:app", "--reload", "--host", "0.0.0.0", "--port", "80", "--ws-per-message-deflate=false"]



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2025 kyutai

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: pyproject.toml
================================================
[project]
name = "unmute"
version = "0.1.0"
description = "Make text LLMs listen and speak"
readme = "README.md"
requires-python = ">=3.12,<3.13"
dependencies = [
    "fastapi[standard]>=0.115.12",
    "fastrtc==0.0.23",
    "mistralai>=1.5.1",
    "msgpack>=1.1.0",
    "msgpack-types>=0.5.0",
    "openai>=1.70.0",
    "plotly>=6.0.1",
    "sphn>=0.2.0",
    "prometheus-fastapi-instrumentator==7.1.0",
    "prometheus-client==0.21.0",
    "ruamel-yaml>=0.18.10",
    "redis>=6.0.0",
]

[build-system]
requires = ["setuptools >= 77.0.3"]
build-backend = "setuptools.build_meta"

[tool.setuptools.packages]
find = { include = ["unmute"] }

[tool.pyright]
typeCheckingMode = "strict"

# Unlike MyPy, Pyright makes an explicit distinction between "Unknown" (I don't know
# what this is) and "Any" (I'll allow anything here). By default, "Unknown" is treated
# as "Any" but these reportUnknownX settings make it an error to use "Unknown".
# You'd have to explicitly cast it to "Any" or something else.
# Let's disable these for now to stick to MyPy-like behavior.
reportUnknownMemberType = false
reportUnknownArgumentType = false
reportUnknownLambdaType = false
reportUnknownVariableType = false
reportUnknownParameterType = false

# See above for how to fix reportMissingTypeStubs issues
reportMissingTypeStubs = false

# Ruff removes unused imports automatically, but doesn't hurt to have this enabled.
reportUnusedImport = true # true in "strict", but make it explicit

reportMissingTypeArgument = false


[tool.ruff.lint]
select = [
    "B",    # bugbear
    "E",    # pep8 rules
    "F",    # pyflakes
    "I001", # isort
    "W",    # pep8 warnings
    # pydocstyle - check that all arguments are documented
    # It can make sense to add other "D" checks for more docstring
    # consistency, but some of them are really pedantic - like requiring
    # function-level docstrings for every single function
    "D417",
]

ignore = [
    # Line too long errors. Ruff format --fix will fix most of these
    # and sometimes we want to keep long strings in one line etc.
    "E501",
]

pydocstyle.convention = "google" # Google docstring style

[dependency-groups]
dev = [
    "jupyter>=1.1.1",
    "pytest>=8.3.5",
    "pytest-asyncio>=0.26.0",
    "pyright",
    "ruff",
    "pre-commit",
    "pyinstrument",
    "ffmpeg-normalize>=1.31.3",
]



================================================
FILE: setup_gpu_swarm_node.py
================================================
import json
import subprocess
import time
from pathlib import Path


def get_all_uuids() -> list[str]:
    return (
        subprocess.check_output(
            ["nvidia-smi", "--query-gpu=uuid", "--format=csv,noheader"],
            stderr=subprocess.STDOUT,
            text=True,
        )
        .strip()
        .splitlines()
    )


def json_dumps(obj: dict) -> str:
    return json.dumps(obj, indent=4, sort_keys=True)


def setup_docker_config():
    daemon_config_file = Path("/etc/docker/daemon.json")
    daemon_config = json.loads(daemon_config_file.read_text())
    print("Previous daemon config:\n", json_dumps(daemon_config))

    daemon_config["node-generic-resources"] = [
        "gpu=" + uuid for uuid in get_all_uuids()
    ]

    print("New daemon config:\n", json_dumps(daemon_config))
    daemon_config_file.write_text(json_dumps(daemon_config))


def setup_nvidia_docker_config():
    nvidia_container_config_file = Path("/etc/nvidia-container-runtime/config.toml")
    nvidia_container_config = nvidia_container_config_file.read_text()
    line = '#swarm-resource = "DOCKER_RESOURCE_GPU"'
    if line in nvidia_container_config:
        print("uncommenting", line)
        nvidia_container_config = nvidia_container_config.replace(
            line, line.removeprefix("#")
        )
        print("New nvidia-container config:\n", nvidia_container_config)
        nvidia_container_config_file.write_text(nvidia_container_config)
    else:
        print(
            "Line not found in nvidia-container config, did you already uncomment it?"
        )


def restarting_docker():
    print("Restarting docker")
    subprocess.check_call(["systemctl", "restart", "docker"])
    time.sleep(1)


def checking_nvidia_docker():
    print("Checking nvidia-docker")
    subprocess.check_call(
        [
            "docker",
            "run",
            "--rm",
            "--runtime=nvidia",
            "--gpus",
            "all",
            "ubuntu",
            "nvidia-smi",
        ]
    )


def main():
    setup_docker_config()
    setup_nvidia_docker_config()
    restarting_docker()
    checking_nvidia_docker()


if __name__ == "__main__":
    main()



================================================
FILE: swarm-deploy.yml
================================================
services:
  traefik:
    image: traefik:v3.3.1
    command:
      # Swarm provider configuration
      - "--providers.swarm.endpoint=unix:///var/run/docker.sock"
      - "--providers.swarm.exposedByDefault=false"

      # EntryPoints for HTTP and HTTPS
      - "--entrypoints.web.address=:80"
      - "--entrypoints.websecure.address=:443"

      # Redirect HTTP to HTTPS
      - "--entrypoints.web.http.redirections.entryPoint.to=websecure"
      - "--entrypoints.web.http.redirections.entryPoint.scheme=https"

      # Enable Let's Encrypt with ACME, enable when you want
      # It will get and renew certificates automatically for all domain names.
      - "--certificatesResolvers.letsencrypt_resolver.acme.httpChallenge.entryPoint=web"
      - "--certificatesResolvers.letsencrypt_resolver.acme.storage=/letsencrypt/acme.json"
      - "--certificatesResolvers.letsencrypt_resolver.acme.email=gabriel@kyutai.org"
      - "--certificatesResolvers.letsencrypt_resolver.acme.httpChallenge=true"
      # staging environment to avoid rate limiting
      #- "--certificatesResolvers.letsencrypt_resolver.acme.caServer=https://acme-staging-v02.api.letsencrypt.org/directory"

      # Enable dashboard
      - "--api.dashboard=true"
      - "--api.insecure=false"
      - "--metrics.prometheus=true"
      - "--log.level=DEBUG"
      # Healthcheck
      - "--ping=true"
    healthcheck:
      test: ["CMD", "traefik", "healthcheck", "--ping"]
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock:ro"
      - "letsencrypt:/letsencrypt" # Persistent storage for SSL certificates
    deploy:
      update_config:
        order: start-first # Since we can't have multiple replicas, at least we can start the new container first.
      labels:
        # Enable Traefik dashboard
        - "traefik.enable=true"
        - "traefik.http.routers.traefik.rule=Host(`traefik.${DOMAIN}`)"
        - "traefik.http.routers.traefik.middlewares=traefik-forward-auth"
        - "traefik.http.routers.traefik.entrypoints=websecure"
        - "traefik.http.routers.traefik.tls=true"
        - "traefik.http.routers.traefik.tls.certresolver=letsencrypt_resolver"
        - "traefik.http.routers.traefik.service=api@internal"
        - "traefik.http.services.traefik.loadbalancer.server.port=8080"
        - "prometheus-port=8080"
      placement:
        constraints:
          - node.role == manager

  frontend:
    image: rg.fr-par.scw.cloud/namespace-unruffled-tereshkova/${DOMAIN}-frontend:latest
    build:
      context: frontend/
    deploy:
      # Having more than one replica is useful for scaling but also to avoid downtime
      # during crashes or updates. Traffic will be load balanced between replicas.
      replicas: 5
      update_config:
        delay: 10s
      labels:
        - "traefik.enable=true"
        - "traefik.http.routers.frontend.rule=Host(`www.${DOMAIN}`) || Host(`${DOMAIN}`)"
        - "traefik.http.routers.frontend.entrypoints=websecure"
        - "traefik.http.routers.frontend.tls=true"
        - "traefik.http.routers.frontend.tls.certresolver=letsencrypt_resolver"
        - "traefik.http.services.frontend.loadbalancer.server.port=3000"
        # A lower priority is necessary because the routing rule is very broad
        # (no path specified), so it might match other services.
        - "traefik.http.routers.frontend.priority=10" # lowest priority

  backend:
    image: rg.fr-par.scw.cloud/namespace-unruffled-tereshkova/${DOMAIN}-backend:latest
    build:
      context: ./
      target: prod
    environment:
      # Any service can be called by other services at `http://<service_name>:<port>`.
      # http://tasks.<service_name> can be called to get the ip addresses of all the replicas
      # for a given service, allowing manual load balancing. The backend does this currently.
      - KYUTAI_STT_URL=ws://tasks.stt:8080
      - KYUTAI_TTS_URL=ws://tasks.tts:8080
      - KYUTAI_LLM_URL=http://llm:8000
      - KYUTAI_VOICE_CLONING_URL=http://voice-cloning:8080
      - KYUTAI_REDIS_URL=redis://redis:6379
      - KYUTAI_VOICE_DONATION_DIR=/voice-donation
      - NEWSAPI_API_KEY=$NEWSAPI_API_KEY
      - KYUTAI_RECORDINGS_DIR=/recordings
      - KYUTAI_LLM_MODEL=$KYUTAI_LLM_MODEL
    volumes:
      - /scratch/voice-donation/:/voice-donation
      - recordings:/recordings
    deploy:
      labels:
        # Reachable with /api but removes the /api prefix from the URL after routing
        # because the backend expects the API to be at the root path.
        - "traefik.enable=true"
        - "traefik.http.routers.backend.rule=(Host(`www.${DOMAIN}`) || Host(`${DOMAIN}`)) && PathPrefix(`/api`)"
        - "traefik.http.routers.backend.middlewares=strip-api"
        - "traefik.http.middlewares.strip-api.replacepathregex.regex=^/api/(.*)"
        - "traefik.http.middlewares.strip-api.replacepathregex.replacement=/$$1"
        - "traefik.http.routers.backend.entrypoints=websecure"
        - "traefik.http.routers.backend.tls=true"
        - "traefik.http.routers.backend.tls.certresolver=letsencrypt_resolver"
        - "traefik.http.services.backend.loadbalancer.server.port=80"
        - "traefik.http.routers.backend.priority=100" # higher priority than frontend
        - "prometheus-port=80"
      replicas: 16
      update_config:
        delay: 10s      # wait 10 seconds before updating the next replica
        parallelism: 3  # update 3 replicas at a time
      resources:
        # We set limits to avoid having one container taking down the whole service
        # but we don't set reservations because we do not have enough cpu/memory for this.
        # In practice, if we set reservations, the service will not start because not
        # enough resources are available.
        limits:
          cpus: "1.5"
          memory: 1G

  tts:
    image: rg.fr-par.scw.cloud/namespace-unruffled-tereshkova/${DOMAIN}-moshi-server:latest
    # The command is added to the ENTRYPOINT in the Dockerfile
    command: ["worker", "--config", "configs/tts-prod.toml"]
    build:
      context: services/moshi-server
      dockerfile: private.Dockerfile
      ssh:                          # Only needed for staging
        - default                   # Only needed for staging
      args:                         # Only needed for staging
        GITHUB_ORG: $GITHUB_ORG     # Only needed for staging
    environment:
      # Env variables are grabbed from the environment of the Docker CLI, so the user deploying.
      # For security reasons, use a read-only Hugging Face token.
      - HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN
      - HF_TOKEN=$HUGGING_FACE_HUB_TOKEN
    volumes:
      - cargo-registry:/root/.cargo/registry
      - moshi-server-target:/app/target
      - uv-cache:/root/.cache/uv
      - hf-cache:/root/.cache/huggingface/hub
      - tts-logs:/logs
    stop_grace_period: 10s # change if needed
    deploy:
      labels:
        - "prometheus-port=8080"
        # Expose the TTS service via Traefik under the /tts-server path, but remove the /tts-server prefix
        # from the URL after routing.
        - "traefik.enable=true"
        - "traefik.http.routers.tts.rule=(Host(`www.${DOMAIN}`) || Host(`${DOMAIN}`)) && PathPrefix(`/tts-server`)"
        - "traefik.http.routers.tts.middlewares=strip-tts"
        - "traefik.http.middlewares.strip-tts.replacepathregex.regex=^/tts-server/(.*)"
        - "traefik.http.middlewares.strip-tts.replacepathregex.replacement=/$$1"
        - "traefik.http.routers.tts.entrypoints=websecure"
        - "traefik.http.routers.tts.tls=true"
        - "traefik.http.routers.tts.tls.certresolver=letsencrypt_resolver"
        - "traefik.http.services.tts.loadbalancer.server.port=8080"
        - "traefik.http.routers.tts.priority=120"
      replicas: 3
      update_config:
        delay: 60s # it takes a very long time to boot up and we want no downtime
      resources:
        limits:
          cpus: "8"
          memory: 16G
        # This is how to reserve a GPU for the service in swarm. We can ask multiple GPUs
        # for a single container but we never needed to.
        reservations:
          generic_resources:
            - discrete_resource_spec:
                kind: gpu
                value: 1

  stt:
    image: rg.fr-par.scw.cloud/namespace-unruffled-tereshkova/${DOMAIN}-moshi-server:latest
    command: ["worker", "--config", "configs/stt-prod.toml"]
    environment:
      - HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN
      - HF_TOKEN=$HUGGING_FACE_HUB_TOKEN
    volumes:
      - cargo-registry:/root/.cargo/registry
      - moshi-server-target:/app/target
      - uv-cache:/root/.cache/uv
      - hf-cache:/root/.cache/huggingface/hub
      - stt-logs:/logs
    stop_grace_period: 10s # change if needed
    deploy:
      labels:
        - "prometheus-port=8080"
        # Expose the STT service via Traefik under the /stt-server path, but remove the /stt-server prefix
        # from the URL after routing.
        - "traefik.enable=true"
        - "traefik.http.routers.stt.rule=(Host(`www.${DOMAIN}`) || Host(`${DOMAIN}`)) && PathPrefix(`/stt-server`)"
        - "traefik.http.routers.stt.middlewares=strip-stt"
        - "traefik.http.middlewares.strip-stt.replacepathregex.regex=^/stt-server/(.*)"
        - "traefik.http.middlewares.strip-stt.replacepathregex.replacement=/$$1"
        - "traefik.http.routers.stt.entrypoints=websecure"
        - "traefik.http.routers.stt.tls=true"
        - "traefik.http.routers.stt.tls.certresolver=letsencrypt_resolver"
        - "traefik.http.services.stt.loadbalancer.server.port=8080"
        - "traefik.http.routers.stt.priority=110"
      replicas: 1
      resources:
        limits:
          cpus: "8"
          memory: 16G
        reservations:
          generic_resources:
            - discrete_resource_spec:
                kind: gpu
                value: 1

  voice-cloning:
    image: rg.fr-par.scw.cloud/namespace-unruffled-tereshkova/${DOMAIN}-moshi-server:latest
    command: ["worker", "--config", "configs/voice-cloning.toml"]
    environment:
      - HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN
      - HF_TOKEN=$HUGGING_FACE_HUB_TOKEN
    volumes:
      - cargo-registry:/root/.cargo/registry
      - moshi-server-target:/app/target
      - uv-cache:/root/.cache/uv
      - hf-cache:/root/.cache/huggingface/hub
      - voice-cloning-logs:/logs
    deploy:
      labels:
        - "prometheus-port=8080"
      replicas: 2
      update_config:
        delay: 60s #it takes a very long time to boot up and we want no downtime
      resources:
        limits:
          cpus: "8"
          memory: 16G

  llm:
    image: vllm/vllm-openai:v0.9.1
    command:
      [
        "--model=${KYUTAI_LLM_MODEL}",
        "--max-model-len=8192",
        "--dtype=bfloat16",
        # "--tokenizer_mode=mistral",  # You can remove those args if you're not using mistral
        # "--config_format=mistral",
        # "--load-format=mistral",
      ]
    healthcheck:
      # The very first time it can be VERY slow, because of the download
      # and compilation. We don't care about healthcheck failures during that time.
      # But if the healthcheck succeeds once (even before the end of the start period),
      # it will be considered healthy and the service will be available.
      start_period: 10m
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
    volumes:
      - "huggingface-cache:/root/.cache/huggingface"
      # This is where vLLM stores its cache, we want to keep it across restarts
      # to avoid recompiling the model every time.
      - vllm-cache:/root/.cache/vllm
    environment:
      - HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN
    deploy:
      labels:
        - "prometheus-port=8000"
      # 4 containers are used, one gpu per container, and the requests end up being load balanced
      # between them. There is no "smart" routing but it's enough for our use case.
      replicas: 4
      update_config:
        delay: 120s # it takes a very long time to boot up and we want no downtime
      resources:
        reservations:
          generic_resources:
            - discrete_resource_spec:
                kind: gpu
                value: 1 # put more if needed

  # -------------------------------------------------------------------------
  # Monitoring

  prometheus:
    image: rg.fr-par.scw.cloud/namespace-unruffled-tereshkova/${DOMAIN}-prometheus:latest
    build:
      # The logic to grab the metrics of all services is here:
      # Notably the prometheus-port swarm label is used to know which port to scrape.
      context: services/prometheus
    volumes:
      - prometheus-data:/prometheus
      # Prometheus asks swarm about the services to know how to scrape them.
      # It's read-only (ro) so it cannot modify the swarm or the containers.
      - /var/run/docker.sock:/var/run/docker.sock:ro
    user: root # To allow Prometheus to acces the Docker socket
    deploy:
      labels:
        - "traefik.enable=true"
        - "traefik.http.routers.prometheus.rule=Host(`prometheus.${DOMAIN}`)"
        - "traefik.http.routers.prometheus.entrypoints=websecure"
        - "traefik.http.routers.prometheus.tls=true"
        - "traefik.http.routers.prometheus.tls.certresolver=letsencrypt_resolver"
        - "traefik.http.services.prometheus.loadbalancer.server.port=9090"
        - "traefik.http.routers.prometheus.middlewares=traefik-forward-auth"
        - "prometheus-port=9090"
      placement:
        constraints:
          # Only the Docker socket of the manager node has information about the swarm
          # so Prometheus must run on the manager node.
          - node.role == manager

  grafana:
    image: rg.fr-par.scw.cloud/namespace-unruffled-tereshkova/${DOMAIN}-grafana:latest
    build:
      # All the dashboards are defined in the context directory. Note that there are no
      # volumes mounted here, so the dashboards are not persistent. Any changes made
      # in the UI will be lost on restart unless you add them to the build context.
      # (export option in the UI, add the json in the git repo).
      context: services/grafana
    deploy:
      labels:
        - "traefik.enable=true"
        - "traefik.http.routers.grafana.rule=Host(`grafana.${DOMAIN}`)"
        - "traefik.http.routers.grafana.entrypoints=websecure"
        - "traefik.http.routers.grafana.tls=true"
        - "traefik.http.routers.grafana.tls.certresolver=letsencrypt_resolver"
        - "traefik.http.services.grafana.loadbalancer.server.port=3000"
        - "traefik.http.routers.grafana.middlewares=traefik-forward-auth"

  # This is useful if someone wants to display dashboards without the google auth.
  grafana-with-password:
    image: rg.fr-par.scw.cloud/namespace-unruffled-tereshkova/${DOMAIN}-grafana:latest
    deploy:
      labels:
        - "traefik.enable=true"
        - "traefik.http.routers.grafana-with-password.rule=Host(`grafana-with-password.${DOMAIN}`)"
        - "traefik.http.middlewares.auth-grafana.basicauth.users=grafana:$$apr1$$wjRp63GU$$T2DyQQmKmFi/.Il.f/7t2."
        - "traefik.http.routers.grafana-with-password.middlewares=auth-grafana"
        - "traefik.http.routers.grafana-with-password.entrypoints=websecure"
        - "traefik.http.routers.grafana-with-password.tls=true"
        - "traefik.http.routers.grafana-with-password.tls.certresolver=letsencrypt_resolver"
        - "traefik.http.services.grafana-with-password.loadbalancer.server.port=3000"

  # This service grabs many information about the docker nodes and containers.
  # Prometheus scrapes it automatically since it has the prometheus-port label.
  cadvisor:
    image: gcr.io/cadvisor/cadvisor
    command: -docker_only
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /:/rootfs:ro
      - /var/run:/var/run
      - /sys:/sys:ro
      - /var/lib/docker:/var/lib/docker:ro
    deploy:
      labels:
        - "prometheus-port=8080"
      # Global mode means that one container will run on each node.
      # If one node is added or removed, the container will be started or stopped automatically.
      mode: global

  # The portainer agent is used to monitor the Docker swarm and containers.
  agent:
    image: portainer/agent:lts
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /var/lib/docker/volumes:/var/lib/docker/volumes
    deploy:
      mode: global

  # The portainer frontend
  portainer:
    image: portainer/portainer-ce:lts
    command: -H tcp://tasks.agent:9001 --tlsskipverify
    volumes:
      - portainer_data:/data
    deploy:
      labels:
        - "traefik.enable=true"
        - "traefik.http.routers.portainer.rule=Host(`portainer.${DOMAIN}`)"
        - "traefik.http.routers.portainer.entrypoints=websecure"
        - "traefik.http.routers.portainer.tls=true"
        - "traefik.http.routers.portainer.tls.certresolver=letsencrypt_resolver"
        - "traefik.http.services.portainer.loadbalancer.server.port=9000"
        # Protected by the traefik-forward-auth middleware
        - "traefik.http.routers.portainer.middlewares=traefik-forward-auth"
      placement:
        # It's not needed to run on the manager node, but it needs to have a fixed
        # node because it has data there (the portainer_data volume).
        # We could use a node label to select a specific node, but for now
        # we just run it on the manager node, it's enough.
        constraints: [node.role == manager]

  # It's a service that's not running anything, but it can be used to debug the swarm.
  # Notably network issues.
  debugger:
    image: rg.fr-par.scw.cloud/namespace-unruffled-tereshkova/${DOMAIN}-debugger:latest
    command: ["sleep", "infinity"]
    build:
      context: services/debugger
    volumes:
      - /tmp:/tmp

  # This is a traefik middleware. Basically any service that has the label
  # "traefik.http.routers.<name>.middlewares=traefik-forward-auth"
  # will be protected by this middleware, which requires Google authentication.
  # In our case, we use it for the Traefik dashboard, Grafana, Prometheus and Portainer.
  traefik-forward-auth:
    image: thomseddon/traefik-forward-auth:2
    environment:
      - PROVIDERS_GOOGLE_CLIENT_ID=1019173417489-oa1f0nrup1lc5jrcpqkfln0drpr23sk6.apps.googleusercontent.com
      - PROVIDERS_GOOGLE_CLIENT_SECRET=$PROVIDERS_GOOGLE_CLIENT_SECRET
      - SECRET=$PROVIDERS_GOOGLE_CLIENT_SECRET
      # Users must have a kyutai.org email to be able to log in.
      - DOMAIN=kyutai.org
      - LOG_LEVEL=debug
    deploy:
      labels:
        - "traefik.enable=true"
        - "traefik.http.middlewares.traefik-forward-auth.forwardauth.address=http://traefik-forward-auth:4181"
        - "traefik.http.middlewares.traefik-forward-auth.forwardauth.authResponseHeaders=X-Forwarded-User"
        - "traefik.http.services.traefik-forward-auth.loadbalancer.server.port=4181"

  # -------------------------------------------------------------------------
  # Centralized storage

  redis:
    image: redis:latest

networks:
  default:
    driver: overlay
    attachable: true
    driver_opts:
      # This is useful if nodes communicates between themselves over the public internet.
      # If you're sure that the traffic is only local, you can remove this.
      encrypted: "true"

volumes:
  cargo-registry:
  moshi-server-target:
  uv-cache:
  hf-cache:
  voice-cloning-logs:
  letsencrypt:
  prometheus-data:
  huggingface-cache:
  portainer_data:
  tts-logs:
  stt-logs:
  vllm-cache:
  recordings:



================================================
FILE: SWARM.md
================================================
# Swarm deployment

For production deployments like [unmute.sh](https://unmute.sh), we use Docker Swarm rather than Docker Compose.
The two have a similar syntax, but Docker Compose is meant for running on a single machine whereas Docker Swarm scales multiple. You can think of Docker Swarm as "Multi-node Docker Compose".

The [swarm config file](./swarm-deploy.yml) is well-documented. Feel free to read it to better understand how the stack works. The [Docker swarm docs](https://docs.docker.com/engine/swarm/) are also a good read.

All instructions are to be executed from a client machine, and from this repo, not directly on the machine in the swarm.

```bash
# If new machine
scp setup_gpu_swarm_node.py llm-wrapper-gpu000:/root/
ssh llm-wrapper-gpu000 python3 /root/setup_gpu_swarm_node.py

# Manager only, the command to declare it as being a manager. If a swarm is already up, you don't need it.
docker -H ssh://llm-wrapper-gpu000 swarm init

# If you want to connect a new worker to the manager, get the command to run by doing
docker -H ssh://llm-wrapper-gpu000 swarm join-token worker
# and then run the command printed on the worker

# Here onwards you need the following environment variables:
# HUGGING_FACE_HUB_TOKEN:
#   How to generate: https://huggingface.co/docs/hub/en/security-tokens
#   Used to access "gated" models that require accepting terms and conditions.
#   You need access to https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501
# PROVIDERS_GOOGLE_CLIENT_SECRET:
#   How to generate: https://github.com/thomseddon/traefik-forward-auth?tab=readme-ov-file#google
#   Used to require authentication to access the observability services such as Grafana and Traefik.
# NEWSAPI_API_KEY:
#   How to generate: https://newsapi.org/
#   Optional. Used to fetch data for the "Dev (news)" character. If not provided,
#   everything else will still run fine.
./bake_deploy_prod.sh
# or
./bake_deploy_staging.sh
```

If you want to change the docker install directory (that contains images and volumes), change the `/etc/docker/daemon.json` and add:

```
  "data-root": "/new/docker-data"
```

and then restart docker with `service docker restart`

### Scaling the swarm

If you have heavy load and want more resources (gpu/disk/ram/cpu) in your swarm, there are two steps:
1) Add new machines to the swarm. For this, go to the manager and run `docker swarm join-token worker`. It will give you the command to run on the new node to join the swarm.
2) Once the new node has joined the swarm, increase the number of containers for the services you want to scale. For example `docker service scale llm-wrapper_llm=10`.

Note that swarm will not kill containers and restart them on another node without manual input to avoid downtime. This means that if you want rebalance all the containers of a services across all nodes, you can just force-restart the service.

### Restarting a service
Restart a given service (for example `llm-wrapper_tts` to get new voices when the voice list changes)

```bash
docker -H ssh://llm-wrapper-gpu000 service update --force llm-wrapper_something
```

### Updating a single service
You might not trust a `docker stack deploy` to update only a service that has changed. Notably because the definition
of "changed" is different for humans and for Docker services (copying a .git directory with more branches might be considered
a change, even though it does not change anything to the code). In this case, if you want to update only one service, you can do so without `docker stack deploy` and without `swarm-deploy.yml` by using `docker service update`. Everything that `docker stack deploy` can change on services, `docker service update` can do so too.

For example, you might want to update the docker image of the frontend service with:
```bash
docker service update --image rg.fr-par.scw.cloud/namespace-unruffled-tereshkova/llm-wrapper-frontend:latest --with-registry-auth llm-wrapper_frontend
```
a volume can be added with
```bash
docker service update --mount-add type=volume,source=other-volume,target=/somewhere-else llm-wrapper_frontend
```
etc...

## urls:
* <https://unmute.sh>
* <https://traefik.unmute.sh>
* <https://grafana.unmute.sh>
* <https://prometheus.unmute.sh>
* <https://portainer.unmute.sh>

## Swarm services

### Main app

```mermaid
graph LR
    UB[User browser] --> T((Traefik))
    T --> B(Backend)
    T --> F(Frontend)
    B --> TTS(TTS)
    B --> STT(STT)
    B --> LLM(LLM)
    B --> R(Redis)
    B --> V(Voice cloning)
```

### Monitoring

The production deployment contains additional goodies for monitoring:
- [Prometheus](https://prometheus.io/) to store metrics, e.g. the number of active connections and latency measurements
- [Grafana](https://grafana.com/) to visualize those metrics
- [Portainer](https://www.portainer.io/) for checking the status of the containers

```mermaid
graph LR
    UB[Dev browser] --> T((Traefik))
    T --> TFA(Traefik forward auth 'Google Oauth')
    TFA --> G(Grafana)
    TFA --> GWP(Grafana with password)
    TFA --> T
    TFA --> Port(Portainer)
    TFA --> Prom(Prometheus)
    G --> Prom
    GWP --> Prom
    Port --> A(Agent)
    Prom --> B(Backend)
    Prom --> STT(STT)
    Prom --> TTS(TTS)
    Prom --> T
    Prom --> LLM(LLM)
    Prom --> V(Voice cloning)
    Prom --> C(Cadvisor)
    Prom --> D(Docker daemon)
```


================================================
FILE: voices.yaml
================================================
- name: Watercooler
  good: true
  instructions:
    type: smalltalk
  source:
    source_type: file
    path_on_server: unmute-prod-website/p329_022.wav
    description: From the Device Recorded VCTK dataset.
    description_link: https://datashare.ed.ac.uk/handle/10283/3038
- name: Quiz show
  comment: man, UK, skeptical
  good: true
  instructions:
    type: quiz_show
  source:
    source_type: freesound
    url: https://freesound.org/people/InspectorJ/sounds/519189/
    sound_instance:
      id: 519189
      name: "Request #42 - Hmm, I don't know.wav"
      username: InspectorJ
      license: https://creativecommons.org/licenses/by/4.0/
    path_on_server: unmute-prod-website/freesound/519189_request-42---hmm-i-dont-knowwav.mp3
- name: Gertrude
  good: true
  instructions:
    type: constant
    text: Offer life advice. Be kind and sympathetic. Your name is Gertrude.
  source:
    source_type: freesound
    url: https://freesound.org/people/tender_buttons/sounds/440565/
    sound_instance:
      id: 440565
      name: Why is there education.wav
      username: tender_buttons
      license: http://creativecommons.org/licenses/by/3.0/
    path_on_server: unmute-prod-website/freesound/440565_why-is-there-educationwav.mp3
- name: Dev (news)
  good: true
  instructions:
    type: news
  source:
    source_type: file
    path_on_server: unmute-prod-website/developer-1.mp3
    description: This is the voice of Václav Volhejn from Kyutai.
- name: Explanation
  good: true
  instructions:
    type: unmute_explanation
  source:
    source_type: file
    path_on_server: unmute-prod-website/ex04_narration_longform_00001.wav
    description: This voice comes from the Expresso dataset.
    description_link: https://speechbot.github.io/expresso/
- name: Charles
  good: true
  instructions:
    type: constant
    text: Tu es le général de Gaulle. Pour ton premier tour de parole, tu te présentes en français en 2 phrases. Si on te répond en français, tu parles en français. Si on te répond en anglais, tu parles en anglais, mais tu utilises au moins un mot français par phrase, entre guillemets français. Quand on te pose une question, tu réponds en parlant d'une anecdote historique que tu as vécu, comme une rencontre ou une discussion. Tu fais preuve d'une sensibilité particulière à la souffrance de tous les peuples du monde au cours de l'histoire. Tu utilises un langage grave et solennel.
  source:
    source_type: file
    path_on_server: unmute-prod-website/degaulle-2.wav
    description: From a recording of Charles de Gaulle's speech.
    description_link: https://www.youtube.com/watch?v=AUS5LHDkwP0
- name: Développeuse
  good: true
  instructions:
    type: smalltalk
    language: fr
  source:
    source_type: file
    path_on_server: unmute-prod-website/developpeuse-3.wav
    description: This is the voice of one of the developers at Kyutai.
- name: Fabieng
  good: true
  instructions:
    type: constant
    text: Ta langue principale est le français mais avec des anglicismes caractéristiques du jeune cadre dynamique. Tu es coach en motivation et Chief Happiness Officer dans une start-up qui fait du b2b. Tu cherches à tout optimiser dans la vie et à avoir un mindset de vainqueur.
    language: fr
  source:
    source_type: file
    path_on_server: unmute-prod-website/fabieng-enhanced-v2.wav
    description: Fabieng is voice acted by Neil Zeghidour from Kyutai.



================================================
FILE: .dockerignore
================================================
*.egg-info
*.pyc

debug/
recordings/
.venv/

Dockerfile

frontend/node_modules
frontend/.next
volumes/
notebooks/
voices/



================================================
FILE: .pre-commit-config.yaml
================================================
repos:
  - repo: https://github.com/kynan/nbstripout
    rev: 0.8.1
    hooks:
      - id: nbstripout
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v5.0.0 # Use the ref you want to point at
    hooks:
      - id: check-added-large-files
        args: ["--maxkb=2048"]
  - repo: https://github.com/astral-sh/ruff-pre-commit
    # Ruff version.
    rev: v0.11.7
    hooks:
      # Run the linter.
      - id: ruff
        types_or: [python, pyi] # Don't run on `jupyter` files
        args: [--fix]
      # Run the formatter.
      - id: ruff-format
        types_or: [python, pyi] # Don't run on `jupyter` files
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v3.2.0
    hooks:
      - id: trailing-whitespace
  - repo: local
    hooks:
      - id: pnpm-run-lint
        name: pnpm run lint
        language: system
        entry: bash -c 'cd frontend && pnpm run lint --max-warnings 0'
        files: ^frontend/src/.*$
        pass_filenames: false
        stages: [pre-commit]
      - id: pnpm-run-build
        name: pnpm run build
        language: system
        entry: bash -c 'cd frontend && pnpm run build'
        files: ^frontend/src/.*$
        pass_filenames: false
        stages: [pre-push]
      - id: pyright
        name: Pyright type-checking
        language: system
        entry: bash -c 'uv run pyright'
        files: ^unmute/.*$
        pass_filenames: false
        stages: [pre-push]



================================================
FILE: dockerless/start_backend.sh
================================================
#!/bin/bash
set -ex
cd "$(dirname "$0")/.."

uv run uvicorn unmute.main_websocket:app --reload --host 0.0.0.0 --port 8000 --ws-per-message-deflate=false



================================================
FILE: dockerless/start_frontend.sh
================================================
#!/bin/bash
set -ex
cd "$(dirname "$0")/.."

cd frontend
pnpm install
pnpm env use --global lts
pnpm dev



================================================
FILE: dockerless/start_llm.sh
================================================
#!/bin/bash
set -ex
cd "$(dirname "$0")/.."

uv tool run vllm@v0.9.1 serve \
  --model=google/gemma-3-1b-it \
  --max-model-len=8192 \
  --dtype=bfloat16 \
  --gpu-memory-utilization=0.3 \
  --port=8091



================================================
FILE: dockerless/start_stt.sh
================================================
#!/bin/bash
set -ex
cd "$(dirname "$0")/.."

# A fix for building Sentencepiece on GCC 15, see: https://github.com/google/sentencepiece/issues/1108
export CXXFLAGS="-include cstdint"

cargo install --features cuda moshi-server@0.6.3
moshi-server worker --config services/moshi-server/configs/stt.toml --port 8090



================================================
FILE: dockerless/start_tts.sh
================================================
#!/bin/bash
set -ex
cd "$(dirname "$0")/"

# This is part of a hack to get dependencies needed for the TTS Rust server, because it integrates a Python component
[ -f pyproject.toml ] || wget https://raw.githubusercontent.com/kyutai-labs/moshi/9837ca328d58deef5d7a4fe95a0fb49c902ec0ae/rust/moshi-server/pyproject.toml
[ -f uv.lock ] || wget https://raw.githubusercontent.com/kyutai-labs/moshi/9837ca328d58deef5d7a4fe95a0fb49c902ec0ae/rust/moshi-server/uv.lock

uv venv
source .venv/bin/activate

cd ..

# This env var must be set to get the correct environment for the Rust build.
# Must be set before running `cargo install`!
# If you don't have it, you'll see an error like `no module named 'huggingface_hub'`
# or similar, which means you don't have the necessary Python packages installed.
export LD_LIBRARY_PATH=$(python -c 'import sysconfig; print(sysconfig.get_config_var("LIBDIR"))')

# A fix for building Sentencepiece on GCC 15, see: https://github.com/google/sentencepiece/issues/1108
export CXXFLAGS="-include cstdint"

# If you already have moshi-server installed and things are not working because of the LD_LIBRARY_PATH issue,
# you might have to force a rebuild with --force.
cargo install --features cuda moshi-server@0.6.3

# If you're getting `moshi-server: error: unrecognized arguments: worker`, it means you're
# using the binary from the `moshi` Python package rather than from the Rust package.
# Use `pip install moshi --upgrade` to update the Python package to >=0.2.8.
uv run --locked --project ./dockerless moshi-server worker --config services/moshi-server/configs/tts.toml --port 8089



================================================
FILE: docs/browser_backend_communication.md
================================================
# Browser-backend communication protocol

This document explains how the browser frontend and backend service communicate through WebSocket connections in the Unmute system.

## Overview

Unmute uses a WebSocket-based protocol inspired by the [OpenAI Realtime API](https://platform.openai.com/docs/api-reference/realtime) for real-time voice conversations. The protocol handles:

- Real-time audio streaming (bidirectional)
- Voice conversation transcription
- Session configuration
- Error handling and debugging

## WebSocket connection

### Endpoint
- **URL**: `/v1/realtime`
- **Protocol**: `realtime` (specified in WebSocket subprotocol)
- **Port**: 8000 (development), routed through Traefik in Docker Swarm and Compose. Traefik uses http (port 80) and https (port 443).

### Connection setup

The WebSocket connection is established using the `realtime` subprotocol. See implementation details in:
- **Frontend**: [`frontend/src/app/Unmute.tsx`](../frontend/src/app/Unmute.tsx)
- **Backend**: [`unmute/main_websocket.py`](../unmute/main_websocket.py)

## Message protocol

All messages are JSON-encoded with a common structure defined in [`unmute/openai_realtime_api_events.py`](../unmute/openai_realtime_api_events.py).

### Base message structure

All messages inherit from [`BaseEvent`](https://github.com/kyutai-labs/unmute/blob/main/unmute/openai_realtime_api_events.py#L32-L50) which provides a common type and event_id structure.

## Client → server messages

### 1. Audio input streaming

**Message Type**: `input_audio_buffer.append`

**Purpose**: Stream real-time audio data from microphone to backend

**Model**: [`InputAudioBufferAppend`](https://github.com/kyutai-labs/unmute/blob/main/unmute/openai_realtime_api_events.py#L80-L81)

**Audio Format**:
- **Codec**: Opus
- **Sample Rate**: 24kHz
- **Channels**: Mono
- **Encoding**: Base64-encoded bytes

### 2. Session configuration

**Message Type**: `session.update`

**Purpose**: Configure voice character and conversation instructions. The backend will not start sending messages until it gets a session.update message that sets its instructions.

**Models**:
- [`SessionUpdate`](https://github.com/kyutai-labs/unmute/blob/main/unmute/openai_realtime_api_events.py#L72-L73)
- [`SessionConfig`](https://github.com/kyutai-labs/unmute/blob/main/unmute/openai_realtime_api_events.py#L66-L69)

## Server → client messages

### 1. Audio response streaming

**Message Type**: `response.audio.delta`

**Purpose**: Stream generated speech audio to frontend

**Model**: [`ResponseAudioDelta`](https://github.com/kyutai-labs/unmute/blob/main/unmute/openai_realtime_api_events.py#L133-L134)

### 2. Speech transcription

**Message Type**: `conversation.item.input_audio_transcription.delta`

**Purpose**: Real-time transcription of user speech

**Model**: [`ConversationItemInputAudioTranscriptionDelta`](https://github.com/kyutai-labs/unmute/blob/main/unmute/openai_realtime_api_events.py#L147-L151)

### 3. Text response streaming

**Message Type**: `response.text.delta`

**Purpose**: Stream generated text responses (for display/debugging)

**Model**: [`ResponseTextDelta`](https://github.com/kyutai-labs/unmute/blob/main/unmute/openai_realtime_api_events.py#L125-L126)

### 4. Speech detection events

**Message Types**:
- `input_audio_buffer.speech_started`
- `input_audio_buffer.speech_stopped`

**Purpose**: Indicate when user starts/stops speaking (for UI feedback). In Unmute we actually just ignore these events at the moment, even though we report them.

**Models**:
- [`InputAudioBufferSpeechStarted`](https://github.com/kyutai-labs/unmute/blob/main/unmute/openai_realtime_api_events.py#L95-L105)
- [`InputAudioBufferSpeechStopped`](https://github.com/kyutai-labs/unmute/blob/main/unmute/openai_realtime_api_events.py#L108-L111)

### 5. Response status updates

**Message Type**: `response.created`

**Purpose**: Indicate when assistant starts generating a response

**Models**:
- [`ResponseCreated`](https://github.com/kyutai-labs/unmute/blob/main/unmute/openai_realtime_api_events.py#L121-L122)
- [`Response`](https://github.com/kyutai-labs/unmute/blob/main/unmute/openai_realtime_api_events.py#L114-L118)

### 6. Error handling

**Message Type**: `error`

**Purpose**: Communicate errors and warnings

**Models**:
- [`Error`](https://github.com/kyutai-labs/unmute/blob/main/unmute/openai_realtime_api_events.py#L62-L63)
- [`ErrorDetails`](https://github.com/kyutai-labs/unmute/blob/main/unmute/openai_realtime_api_events.py#L53-L59)

## Connection lifecycle

1. **Health Check**: Frontend checks `/v1/health` endpoint
2. **WebSocket Connection**: Establish connection with `realtime` protocol
3. **Session Setup**: Send `session.update` with voice and instructions
4. **Audio Streaming**: Bidirectional real-time audio communication
5. **Graceful Shutdown**: Handle disconnection and cleanup




================================================
FILE: frontend/README.md
================================================
# Unmute frontend

This is the frontend for Unmute, written in Next.js.

Use `pnpm` to install:

```bash
pnpm install
# if you don't have Node:
pnpm env use --global lts
```

Then run:

```bash
pnpm run dev
```



================================================
FILE: frontend/Dockerfile
================================================
# syntax=docker.io/docker/dockerfile:1
# Taken from: https://github.com/vercel/next.js/tree/aebf26f7923c7c4da7734798048bf48d2e57b521/examples/with-docker

FROM node:18-alpine AS base

# Install dependencies only when needed
FROM base AS deps
# Check https://github.com/nodejs/docker-node/tree/b4117f9333da4138b03a546ec926ef50a31506c3#nodealpine to understand why libc6-compat might be needed.
RUN apk add --no-cache libc6-compat
WORKDIR /app

# Install dependencies based on the preferred package manager
COPY package.json yarn.lock* package-lock.json* pnpm-lock.yaml* .npmrc* ./
RUN corepack enable pnpm && pnpm i --frozen-lockfile


# Rebuild the source code only when needed
FROM base AS builder
WORKDIR /app
COPY --from=deps /app/node_modules ./node_modules
COPY . .

# Next.js collects completely anonymous telemetry data about general usage.
# Learn more here: https://nextjs.org/telemetry
# Uncomment the following line in case you want to disable telemetry during the build.
ENV NEXT_TELEMETRY_DISABLED=1

ENV NEXT_PUBLIC_IN_DOCKER=true

RUN corepack enable pnpm && pnpm run build

# Production image, copy all the files and run next
FROM base AS runner
WORKDIR /app

ENV NODE_ENV=production
# Uncomment the following line in case you want to disable telemetry during runtime.
ENV NEXT_TELEMETRY_DISABLED=1

RUN apk add --no-cache curl

RUN addgroup --system --gid 1001 nodejs
RUN adduser --system --uid 1001 nextjs

COPY --from=builder /app/public ./public

# Automatically leverage output traces to reduce image size
# https://nextjs.org/docs/advanced-features/output-file-tracing
COPY --from=builder --chown=nextjs:nodejs /app/.next/standalone ./
COPY --from=builder --chown=nextjs:nodejs /app/.next/static ./.next/static

USER nextjs

EXPOSE 3000

ENV PORT=3000

HEALTHCHECK --start-period=15s \
    CMD curl --fail http://localhost:3000/ || exit 1

# server.js is created by next build from the standalone output
# https://nextjs.org/docs/pages/api-reference/config/next-config-js/output
ENV HOSTNAME="0.0.0.0"
CMD ["node", "server.js"]


================================================
FILE: frontend/eslint.config.mjs
================================================
import { dirname } from "path";
import { fileURLToPath } from "url";
import { FlatCompat } from "@eslint/eslintrc";

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

const compat = new FlatCompat({
  baseDirectory: __dirname,
});

const eslintConfig = [
  ...compat.extends("next/core-web-vitals", "next/typescript"),
  {
    rules: {
      '@next/next/no-img-element': 'off',
    },
  },
];

export default eslintConfig;



================================================
FILE: frontend/hot-reloading.Dockerfile
================================================
# syntax=docker.io/docker/dockerfile:1

FROM node:18-alpine AS dev

# Install required dependencies
RUN apk add --no-cache libc6-compat curl

# Set working directory
WORKDIR /app

# Install dependencies using the package manager (detected automatically via lockfile)
COPY package.json tsconfig.json yarn.lock* package-lock.json* pnpm-lock.yaml* .npmrc* postcss.config.mjs ./
COPY public/ ./public/
RUN corepack enable pnpm && pnpm i --frozen-lockfile

# Expose the port the dev server runs on
EXPOSE 3000

# Set environment variables
ENV NODE_ENV=development
ENV NEXT_TELEMETRY_DISABLED=1
ENV HOSTNAME=0.0.0.0
ENV PORT=3000
ENV NEXT_PUBLIC_IN_DOCKER=true

HEALTHCHECK --start-period=15s \
    CMD curl --fail http://localhost:3000/ || exit 1

# The source code will be mounted as a volume, so no need to copy it here
# Default command to run the development server with hot reloading
CMD ["pnpm", "dev"]



================================================
FILE: frontend/next.config.ts
================================================
import createMDX from "@next/mdx";
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  output: "standalone", // For Docker
  // Configure `pageExtensions` to include markdown and MDX files
  pageExtensions: ["js", "jsx", "md", "mdx", "ts", "tsx"],
};

const withMDX = createMDX({
  // markdown plugins go here
});

// Merge MDX config with Next.js config
export default withMDX(nextConfig);



================================================
FILE: frontend/package.json
================================================
{
  "name": "tts-demo",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint"
  },
  "dependencies": {
    "@mdx-js/loader": "^3.1.0",
    "@mdx-js/react": "^3.1.0",
    "@next/mdx": "^15.3.4",
    "@next/third-parties": "^15.3.2",
    "@types/http-proxy": "^1.17.16",
    "@types/mdx": "^2.0.13",
    "bcryptjs": "^3.0.2",
    "clsx": "^2.1.1",
    "http-proxy": "^1.18.1",
    "lucide-react": "^0.503.0",
    "next": "15.2.2",
    "opus-recorder": "^8.0.5",
    "pretty-print-json": "^3.0.4",
    "react": "^19.0.0",
    "react-dom": "^19.0.0",
    "react-use-websocket": "^4.13.0"
  },
  "devDependencies": {
    "@eslint/eslintrc": "^3",
    "@next/eslint-plugin-next": "^15.3.2",
    "@tailwindcss/postcss": "^4",
    "@types/bcrypt": "^5.0.2",
    "@types/node": "^20",
    "@types/react": "^19",
    "@types/react-dom": "^19",
    "eslint": "^9.24.0",
    "eslint-config-next": "15.2.2",
    "eslint-plugin-react-hooks": "^5.2.0",
    "tailwindcss": "^4",
    "typescript": "^5"
  },
  "packageManager": "pnpm@10.7.1+sha512.2d92c86b7928dc8284f53494fb4201f983da65f0fb4f0d40baafa5cf628fa31dae3e5968f12466f17df7e97310e30f343a648baea1b9b350685dafafffdf5808"
}



================================================
FILE: frontend/postcss.config.mjs
================================================
const config = {
  plugins: ["@tailwindcss/postcss"],
};

export default config;



================================================
FILE: frontend/tsconfig.json
================================================
{
  "compilerOptions": {
    "target": "ES2017",
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": ["./src/*"]
    }
  },
  "include": ["next-env.d.ts", "**/*.ts", "**/*.tsx", ".next/types/**/*.ts"],
  "exclude": ["node_modules"]
}



================================================
FILE: frontend/public/audio-output-processor.js
================================================
// TODO: is there a way to get type-checking on this?

function asMs(samples) {
  return ((samples * 1000) / sampleRate).toFixed(1);
}

function asSamples(mili) {
  return Math.round((mili * sampleRate) / 1000);
}

const DEFAULT_MAX_BUFFER_MS = 30 * 1000;

const debug = (...args) => {
  // console.debug(...args);
};

class AudioOutputProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
    debug("AudioOutputProcessor created", currentFrame, sampleRate);

    // Buffer length definitions
    const frameSize = asSamples(80);
    // initialBufferSamples: we wait to have at least that many samples before starting to play
    this.initialBufferSamples = 1 * frameSize;
    // once we have enough samples, we further wait that long before starting to play.
    // This allows to have buffer lengths that are not a multiple of frameSize.
    this.partialBufferSamples = asSamples(10);
    // If the buffer length goes over that many, we will drop the oldest packets until
    // we reach back initialBufferSamples + partialBufferSamples.
    this.maxBufferSamples = asSamples(DEFAULT_MAX_BUFFER_MS);
    // increments
    this.partialBufferIncrement = asSamples(5);
    this.maxPartialWithIncrements = asSamples(80);
    this.maxBufferSamplesIncrement = asSamples(5);
    this.maxMaxBufferWithIncrements = asSamples(80);

    // State and metrics
    this.initState();

    this.port.onmessage = (event) => {
      if (event.data.type == "reset") {
        debug("Reset audio processor state.");
        this.initState();
        return;
      }
      let frame = event.data.frame;
      this.frames.push(frame);
      if (this.currentSamples() >= this.initialBufferSamples && !this.started) {
        this.start();
      }
      if (this.pidx < 20) {
        debug(
          this.timestamp(),
          "Got packet",
          this.pidx++,
          asMs(this.currentSamples()),
          asMs(frame.length)
        );
      }
      if (this.currentSamples() >= this.totalMaxBufferSamples()) {
        debug(
          this.timestamp(),
          "Dropping packets",
          asMs(this.currentSamples()),
          asMs(this.totalMaxBufferSamples())
        );
        const target = this.initialBufferSamples + this.partialBufferSamples;
        while (
          this.currentSamples() >
          this.initialBufferSamples + this.partialBufferSamples
        ) {
          const first = this.frames[0];
          let to_remove = this.currentSamples() - target;
          to_remove = Math.min(
            first.length - this.offsetInFirstBuffer,
            to_remove
          );
          this.offsetInFirstBuffer += to_remove;
          this.timeInStream += to_remove / sampleRate;
          if (this.offsetInFirstBuffer == first.length) {
            this.frames.shift();
            this.offsetInFirstBuffer = 0;
          }
        }
        debug(this.timestamp(), "Packet dropped", asMs(this.currentSamples()));
        this.maxBufferSamples += this.maxBufferSamplesIncrement;
        this.maxBufferSamples = Math.min(
          this.maxMaxBufferWithIncrements,
          this.maxBufferSamples
        );
        debug("Increased maxBuffer to", asMs(this.maxBufferSamples));
      }
      this.port.postMessage({
        totalAudioPlayed: this.totalAudioPlayed,
        actualAudioPlayed: this.actualAudioPlayed,
        delay: event.data.micDuration - this.timeInStream,
        minDelay: this.minDelay,
        maxDelay: this.maxDelay,
      });
    };
  }

  initState() {
    this.frames = [];
    this.offsetInFirstBuffer = 0;
    this.firstOut = false;
    this.remainingPartialBufferSamples = 0;
    this.timeInStream = 0;
    this.resetStart();

    // Metrics
    this.totalAudioPlayed = 0;
    this.actualAudioPlayed = 0;
    this.maxDelay = 0;
    this.minDelay = 2000;
    // Debug
    this.pidx = 0;

    // For now let's reset the buffer params.
    this.partialBufferSamples = asSamples(10);
    this.maxBufferSamples = asSamples(DEFAULT_MAX_BUFFER_MS);
  }

  totalMaxBufferSamples() {
    return (
      this.maxBufferSamples +
      this.partialBufferSamples +
      this.initialBufferSamples
    );
  }

  timestamp() {
    return Date.now() % 1000;
  }

  currentSamples() {
    let samples = 0;
    for (let k = 0; k < this.frames.length; k++) {
      samples += this.frames[k].length;
    }
    samples -= this.offsetInFirstBuffer;
    return samples;
  }

  resetStart() {
    this.started = false;
  }

  start() {
    this.started = true;
    this.remainingPartialBufferSamples = this.partialBufferSamples;
    this.firstOut = true;
  }

  canPlay() {
    return (
      this.started &&
      this.frames.length > 0 &&
      this.remainingPartialBufferSamples <= 0
    );
  }

  process(inputs, outputs) {
    const delay = this.currentSamples() / sampleRate;
    if (this.canPlay()) {
      this.maxDelay = Math.max(this.maxDelay, delay);
      this.minDelay = Math.min(this.minDelay, delay);
    }
    const output = outputs[0][0];
    if (!this.canPlay()) {
      if (this.actualAudioPlayed > 0) {
        this.totalAudioPlayed += output.length / sampleRate;
      }
      this.remainingPartialBufferSamples -= output.length;
      return true;
    }
    if (this.firstOut) {
      debug(
        this.timestamp(),
        "Audio resumed",
        asMs(this.currentSamples()),
        this.remainingPartialBufferSamples
      );
    }
    let out_idx = 0;
    let anyAudio = false;
    while (out_idx < output.length && this.frames.length) {
      const first = this.frames[0];
      const to_copy = Math.min(
        first.length - this.offsetInFirstBuffer,
        output.length - out_idx
      );
      const subArray = first.subarray(
        this.offsetInFirstBuffer,
        this.offsetInFirstBuffer + to_copy
      );
      output.set(subArray, out_idx);
      anyAudio =
        anyAudio ||
        output.some(function (x) {
          x > 1e-4 || x < -1e-4;
        });
      this.offsetInFirstBuffer += to_copy;
      out_idx += to_copy;
      if (this.offsetInFirstBuffer == first.length) {
        this.offsetInFirstBuffer = 0;
        this.frames.shift();
      }
    }
    if (this.firstOut) {
      this.firstOut = false;
      for (let i = 0; i < out_idx; i++) {
        output[i] *= i / out_idx;
      }
    }
    if (out_idx < output.length && !anyAudio) {
      // At the end of a turn, we will get some padding of 0, so we only
      // incease the buffer if we got some audio, e.g. we truly lagged in the middle of something.
      debug(this.timestamp(), "Missed some audio", output.length - out_idx);
      this.partialBufferSamples += this.partialBufferIncrement;
      this.partialBufferSamples = Math.min(
        this.partialBufferSamples,
        this.maxPartialWithIncrements
      );
      debug("Increased partial buffer to", asMs(this.partialBufferSamples));
      // We ran out of a buffer, let's revert to the started state to replenish it.
      this.resetStart();
      for (let i = 0; i < out_idx; i++) {
        output[i] *= (out_idx - i) / out_idx;
      }
    }
    this.totalAudioPlayed += output.length / sampleRate;
    this.actualAudioPlayed += out_idx / sampleRate;
    this.timeInStream += out_idx / sampleRate;
    return true;
  }
}
registerProcessor("audio-output-processor", AudioOutputProcessor);



================================================
FILE: frontend/src/mdx-components.tsx
================================================
import type { MDXComponents } from "mdx/types";

// This file allows you to provide custom React components
// to be used in MDX files. You can import and use any
// React component you want, including inline styles,
// components from other libraries, and more.

export function useMDXComponents(components: MDXComponents): MDXComponents {
  return {
    h1: ({ children }) => (
      <h1 className="text-3xl mt-8 text-white">{children}</h1>
    ),
    h2: ({ children }) => (
      <h1 className="text-2xl font-semibold mt-6 text-white">{children}</h1>
    ),
    h3: ({ children }) => (
      <h1 className="text-xl font-semibold mt-4 text-white">{children}</h1>
    ),
    p: ({ children }) => (
      <p className="text-sm md:text-base my-2">{children}</p>
    ),
    li: ({ children }) => (
      <li className="text-sm md:text-base my-2 ml-6 list-disc">{children}</li>
    ),
    a: ({ href, children }) => (
      <a className="text-green underline" href={href}>
        {children}
      </a>
    ),
    strong: ({ children }) => (
      <strong className="text-white">{children}</strong>
    ),
    // TODO more styling here
    ...components,
  };
}



================================================
FILE: frontend/src/app/audioUtil.ts
================================================
export const base64EncodeOpus = (opusData: Uint8Array) => {
  // Convert to base64
  let binary = "";
  for (let i = 0; i < opusData.byteLength; i++) {
    binary += String.fromCharCode(opusData[i]);
  }
  return window.btoa(binary);
};

export const base64DecodeOpus = (base64String: string): Uint8Array => {
  const binaryString = window.atob(base64String);
  const len = binaryString.length;
  const bytes = new Uint8Array(len);
  for (let i = 0; i < len; i++) {
    bytes[i] = binaryString.charCodeAt(i);
  }
  return bytes;
};

export const convertWebmToWav = async (webmBlob: Blob): Promise<Blob> => {
  const arrayBuffer = await webmBlob.arrayBuffer();
  const AudioContextClass =
    window.AudioContext ||
    (window.hasOwnProperty("webkitAudioContext")
      ? (window as unknown as { webkitAudioContext: typeof AudioContext })
          .webkitAudioContext
      : undefined);
  if (!AudioContextClass) throw new Error("Web Audio API not supported");
  const audioCtx = new AudioContextClass();
  const audioBuffer = await audioCtx.decodeAudioData(arrayBuffer);
  // Encode to wav
  const wavBuffer = encodeWAV(audioBuffer);
  return new Blob([wavBuffer], { type: "audio/wav" });
};

// Helper: Encode AudioBuffer to WAV format
export const encodeWAV = (audioBuffer: AudioBuffer): ArrayBuffer => {
  const numChannels = audioBuffer.numberOfChannels;
  const sampleRate = audioBuffer.sampleRate;
  const format = 1; // PCM
  const bitDepth = 16;
  const samples = audioBuffer.length * numChannels;
  const buffer = new ArrayBuffer(44 + samples * 2);
  const view = new DataView(buffer);

  // Write WAV header
  function writeString(view: DataView, offset: number, str: string) {
    for (let i = 0; i < str.length; i++) {
      view.setUint8(offset + i, str.charCodeAt(i));
    }
  }
  let offset = 0;
  writeString(view, offset, "RIFF");
  offset += 4;
  view.setUint32(offset, 36 + samples * 2, true);
  offset += 4;
  writeString(view, offset, "WAVE");
  offset += 4;
  writeString(view, offset, "fmt ");
  offset += 4;
  view.setUint32(offset, 16, true);
  offset += 4;
  view.setUint16(offset, format, true);
  offset += 2;
  view.setUint16(offset, numChannels, true);
  offset += 2;
  view.setUint32(offset, sampleRate, true);
  offset += 4;
  view.setUint32(offset, (sampleRate * numChannels * bitDepth) / 8, true);
  offset += 4;
  view.setUint16(offset, (numChannels * bitDepth) / 8, true);
  offset += 2;
  view.setUint16(offset, bitDepth, true);
  offset += 2;
  writeString(view, offset, "data");
  offset += 4;
  view.setUint32(offset, samples * 2, true);
  offset += 4;

  // Write PCM samples
  for (let ch = 0; ch < numChannels; ch++) {
    const channel = audioBuffer.getChannelData(ch);
    for (let i = 0; i < channel.length; i++) {
      const sample = Math.max(-1, Math.min(1, channel[i]));
      view.setInt16(
        44 + (i * numChannels + ch) * 2,
        sample < 0 ? sample * 0x8000 : sample * 0x7fff,
        true
      );
    }
  }
  return buffer;
};



================================================
FILE: frontend/src/app/chatHistory.ts
================================================
export type ChatRole = "user" | "assistant" | "system";

export type ChatMessage = {
  role: ChatRole;
  content: string;
};

/** If there are multiple messages from the same role in a row, combine them into one message */
export const compressChatHistory = (
  chatHistory: ChatMessage[]
): ChatMessage[] => {
  const compressed: ChatMessage[] = [];
  for (const message of chatHistory) {
    if (
      compressed.length > 0 &&
      compressed[compressed.length - 1].role === message.role
    ) {
      compressed[compressed.length - 1].content += `\n${message.content}`;
    } else {
      compressed.push({ ...message });
    }
  }
  return compressed;
};



================================================
FILE: frontend/src/app/ConsentModal.tsx
================================================
"use client";
import { useEffect, useState } from "react";
import SquareButton from "./SquareButton";
import { GoogleAnalytics } from "@next/third-parties/google";

// Changing this key will reset the consent state for all users
export const COOKIE_CONSENT_STORAGE_KEY = "cookieConsentV2";
export const RECORDING_CONSENT_STORAGE_KEY = "recordingConsent";

export function useConsentState(storageKey: string) {
  const [consentGiven, setConsentGiven] = useState<boolean | null>(false);
  const [consentLoaded, setConsentLoaded] = useState<boolean>(false);

  useEffect(() => {
    const consent = localStorage.getItem(storageKey);
    setConsentGiven(consent == null ? null : consent === "true");
    setConsentLoaded(true);

    // Listen for localStorage changes (in case consent is given on another tab/page)
    const handleStorageChange = (e: StorageEvent) => {
      if (e.key === storageKey) {
        setConsentGiven(e.newValue === "true");
      }
    };

    window.addEventListener("storage", handleStorageChange);

    return () => {
      window.removeEventListener("storage", handleStorageChange);
    };
  }, [storageKey]);

  const setConsent = (to: boolean | null) => {
    if (to != null) {
      localStorage.setItem(storageKey, "" + to);
    } else {
      localStorage.removeItem(storageKey);
    }
    setConsentGiven(to);
  };

  return {
    consentGiven,
    consentLoaded, // useful to avoid hydration mismatches
    setConsent,
  };
}

export default function ConsentModal() {
  const [showDetails, setShowDetails] = useState(false);
  const {
    consentGiven: cookieConsentGiven,
    consentLoaded: cookieConsentLoaded,
    setConsent: setCookieConsent,
  } = useConsentState(COOKIE_CONSENT_STORAGE_KEY);
  const {
    consentGiven: recordingConsentGiven,
    consentLoaded: recordingConsentLoaded,
    setConsent: setRecordingConsent,
  } = useConsentState(RECORDING_CONSENT_STORAGE_KEY);
  const [recordingChecked, setRecordingChecked] = useState(true);

  useEffect(() => {
    // Only update checkbox if consent is not null (user has made a choice)
    if (recordingConsentLoaded && recordingConsentGiven !== null) {
      setRecordingChecked(recordingConsentGiven === true);
    }
  }, [recordingConsentGiven, recordingConsentLoaded]);

  if (!cookieConsentLoaded) {
    return null; // Wait until consent state is loaded
  }

  if (cookieConsentGiven === true) {
    // To debug Google Analytics, add debugMode={true} here and go to the Tag Assistant:
    // https://tagassistant.google.com/
    // Make sure you don't use an adblocker for localhost, as it will block the GA script.
    return <GoogleAnalytics gaId="G-MLN0BSWF97" />;
  }

  if (cookieConsentGiven === false) {
    return null;
  }

  // consent is null, meaning it hasn't been given or declined yet
  return (
    <div className="fixed bottom-0 left-0 right-0 bg-gray border-t border-green shadow-lg z-50">
      <div className="max-w-7xl mx-auto p-4">
        <div className="flex flex-col sm:flex-row items-start sm:items-center justify-between gap-4">
          <div className="flex-1 text-sm text-textgray">
            <p className="text-sm text-textgray mb-2">
              Can we use cookies to improve your experience and analyze site
              usage?{" "}
              {!showDetails && (
                <button
                  onClick={() => setShowDetails(true)}
                  className="text-green underline"
                >
                  Learn more
                </button>
              )}
            </p>
            <div className="flex items-center mt-2">
              <input
                id="recording-consent-checkbox"
                type="checkbox"
                checked={recordingChecked}
                onChange={(e) => setRecordingChecked(e.target.checked)}
                className="mr-2"
              />
              <label htmlFor="recording-consent-checkbox">
                Allow us to record the transcript of the conversation (your
                voice will not be stored) to help our non-profit research
              </label>
            </div>
            {showDetails && (
              <div className="mt-3 p-3 bg-darkgray text-sm text-textgray">
                <p className="mb-2">
                  <strong>Analytics Cookies:</strong> We use Google Analytics to
                  understand how visitors interact with our website. This helps
                  us improve our content and user experience.
                </p>
                <button
                  onClick={() => setShowDetails(false)}
                  className="text-green underline"
                >
                  Learn less
                </button>
              </div>
            )}
          </div>

          <div className="flex flex-row gap-2 w-full sm:w-auto justify-center">
            <SquareButton
              kind="primary"
              onClick={() => {
                setCookieConsent(true);
                setRecordingConsent(recordingChecked);
              }}
            >
              Accept
            </SquareButton>
            <SquareButton
              kind="secondary"
              onClick={() => {
                setCookieConsent(false);
                setRecordingConsent(false); // Cookies declined -> recording also declined
              }}
            >
              Decline
            </SquareButton>
          </div>
        </div>
      </div>
    </div>
  );
}



================================================
FILE: frontend/src/app/CouldNotConnect.tsx
================================================
import clsx from "clsx";
import UnmuteHeader from "./UnmuteHeader";

export type HealthStatus = {
  connected: "no" | "yes_request_ok" | "yes_request_fail";
  ok: boolean;
  tts_up?: boolean;
  stt_up?: boolean;
  llm_up?: boolean;
  voice_cloning_up?: boolean;
};

const renderServiceStatus = (
  name: string,
  status: string | boolean | undefined,
  necessary: boolean = true
) => {
  if (status === undefined) {
    status = "Unknown";
  } else if (status === true) {
    status = "Up";
  } else if (status === false) {
    status = "Down";
  }

  return (
    <p>
      {name}:{" "}
      <span
        className={clsx(
          status === "Up"
            ? "text-white"
            : necessary
            ? "text-red"
            : "text-lightgray"
        )}
      >
        {status}
      </span>
    </p>
  );
};

const humanReadableStatus = {
  no: "Down",
  yes_request_ok: "Up",
  yes_request_fail: "Up, but with errors",
};

const CouldNotConnect = ({ healthStatus }: { healthStatus: HealthStatus }) => {
  if (healthStatus.ok) {
    return null;
  }

  return (
    <div className="w-full h-full flex flex-col gap-12 items-center justify-center bg-background">
      <UnmuteHeader />
      <div className="text-center text-xl">
        <h1 className="text-3xl mb-4">{"Couldn't connect :("}</h1>
        <p>Service status:</p>
        {renderServiceStatus(
          "Backend",
          humanReadableStatus[healthStatus.connected]
        )}
        {renderServiceStatus("STT", healthStatus.stt_up)}
        {renderServiceStatus("LLM", healthStatus.llm_up)}
        {renderServiceStatus("TTS", healthStatus.tts_up)}
        {renderServiceStatus(
          "Voice cloning",
          healthStatus.voice_cloning_up,
          false
        )}
      </div>
    </div>
  );
};

export default CouldNotConnect;



================================================
FILE: frontend/src/app/cssUtil.ts
================================================
export const getCSSVariable = (name: string) => {
  if (!name.startsWith("--")) {
    name = `--${name}`;
  }

  const variable = window
    .getComputedStyle(document.documentElement)
    .getPropertyValue(name)
    .trim();

  if (variable === "") {
    console.warn(`CSS variable ${name} not found`);
  }
  return variable;
};



================================================
FILE: frontend/src/app/ErrorMessages.tsx
================================================
import React, { useEffect } from "react";
import { X } from "lucide-react";

export interface ErrorItem {
  id: string;
  message: string;
  timestamp: number;
}

export const makeErrorItem = (message: string): ErrorItem => {
  const timestamp = Date.now();
  return {
    id: `${timestamp}-${Math.random()}`,
    message,
    timestamp,
  };
};

const ERROR_TIMEOUT_SEC = 10;

export default function ErrorMessages({
  errors,
  setErrors,
}: {
  errors: ErrorItem[];
  setErrors: React.Dispatch<React.SetStateAction<ErrorItem[]>>;
}) {
  // Auto-dismiss errors after 10 seconds
  useEffect(() => {
    const interval = setInterval(() => {
      setErrors((prev) => {
        const now = Date.now();
        const filtered = prev.filter(
          (error) => now - error.timestamp < ERROR_TIMEOUT_SEC * 1000
        );
        return filtered;
      });
    }, 1000);

    return () => clearInterval(interval);
  }, [setErrors]);

  const handleDismiss = (index: number, errorId: string) => {
    setErrors((prev) => prev.filter((error) => error.id !== errorId));
  };

  if (errors.length === 0) {
    return null;
  }

  return (
    <div className="fixed top-4 left-0 md:left-4 z-50 space-y-2">
      {errors.map((error, index) => (
        <div
          key={error.id}
          className="bg-red-50 p-4 max-w-md"
          role="alert"
        >
          <div className="flex items-start gap-3">
            <div className="flex-1">
              <p className="text-red-800 text-sm font-medium">
                {error.message}
              </p>
            </div>
            <button
              onClick={() => handleDismiss(index, error.id)}
              className="flex-shrink-0 text-red-600 hover:text-red-800 transition-colors"
              aria-label="Dismiss error"
            >
              <X size={18} />
            </button>
          </div>
        </div>
      ))}
    </div>
  );
}



================================================
FILE: frontend/src/app/globals.css
================================================
@import "tailwindcss";

:root {
  --green: #39F2AE;
  --offwhite: #EFEFEF;
  --lightgray: #9d9d9d;
  /* #B5BCC5; */
  --gray: #343434;
  /* #8D949E; */
  --darkgray: #191919;
  /* #1e293b; */
  --background: #121212;

  --orange: #f5ab4a;
  --pink: #f051db;
  --purple: #a57de8;
  /* for errors */
  --red: #f55142;

  --speaker-0: var(--green);
  --speaker-1: var(--orange);
  --speaker-2: var(--pink);
  --speaker-3: var(--purple);

  font-family:
    var(--font-satoshi),
    system-ui, Avenir, Helvetica, Arial, sans-serif;
  line-height: 1.5;
  font-weight: 500;
  font-size: 1.35rem;

  color-scheme: dark;
  color: var(--offwhite);
  background-color: var(--background);

  font-synthesis: none;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}


/* TODO(vv): better place for the highlighted-word stuff? */
.highlighted-word {
  color: black;
}

/* Create a pseudo-element for the background */
.highlighted-word::after {
  content: "";
  position: absolute;
  background-color: var(--speaker-color, white);
  left: -0.25rem;
  right: -0.25rem;
  top: -0.25rem;
  bottom: -0.25rem;
  border-radius: 0.25rem;
  z-index: -1;
}

@theme {
  /* TODO: deduplicate */
  --color-green: var(--green);
  --color-offwhite: var(--offwhite);
  --color-lightgray: var(--lightgray);
  --color-gray: var(--gray);
  --color-darkgray: var(--darkgray);
  --color-orange: var(--orange);
  --color-pink: var(--pink);
  --color-purple: var(--purple);
  --color-red: var(--red);
  --color-background: var(--background);
  /* sometimes it's useful to have these as variables too */
  --color-white: #ffffff;
  --color-black: #000000;

  --font-family-satoshi: var(--font-satoshi);
}


================================================
FILE: frontend/src/app/layout.tsx
================================================
import type { Metadata } from "next";
import "./globals.css";
import localFont from "next/font/local";
import ConsentModal from "./ConsentModal";

export const metadata: Metadata = {
  title: "Unmute by Kyutai",
  description: "Make LLMs listen and speak.",
};

const satoshi = localFont({
  src: [
    {
      path: "../assets/fonts/Satoshi-Variable.woff2",
      weight: "300 900",
      style: "normal",
    },
    {
      path: "../assets/fonts/Satoshi-VariableItalic.woff2",
      weight: "300 900",
      style: "italic",
    },
  ],
  variable: "--font-satoshi",
  display: "swap",
});

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en" className={satoshi.className}>
      <head>
        {/* Needed for debugging JSON styling */}
        <link
          rel="stylesheet"
          href="https://cdn.jsdelivr.net/npm/pretty-print-json@3.0/dist/css/pretty-print-json.dark-mode.css"
        />
      </head>
      <body>
        {children}
        <ConsentModal />
      </body>
    </html>
  );
}



================================================
FILE: frontend/src/app/Modal.tsx
================================================
import React, { useState, useRef, useEffect } from "react";
import { X } from "lucide-react";
import clsx from "clsx";
import { createPortal } from "react-dom";

interface Position {
  x: number;
  y: number;
}

const Modal = ({
  trigger,
  children,
  forceFullscreen = false,
  closeSignal = 0,
}: {
  trigger: React.ReactNode; // The element that triggers the modal
  children: React.ReactNode; // The content of the modal
  forceFullscreen?: boolean; // Force the modal to be fullscreen
  closeSignal?: number; // Signal to close the modal (increment to trigger)
}) => {
  // Use internal state for controlling open/closed state
  const [isOpen, setIsOpen] = useState(false);

  // Store previous closeSignal value to detect changes
  const prevCloseSignalRef = useRef(closeSignal);

  const [position, setPosition] = useState<Position>({ x: 0, y: 0 });
  const [isMobile, setIsMobile] = useState(false);
  const triggerRef = useRef<HTMLDivElement>(null);
  const modalRef = useRef<HTMLDivElement>(null);
  const containerRef = useRef<HTMLDivElement>(null);

  // Use a timeout to prevent flickering
  const hoverTimeoutRef = useRef<NodeJS.Timeout | null>(null);

  const isFullscreen = isMobile || forceFullscreen;

  // Effect to handle closeSignal changes
  useEffect(() => {
    // Only run if closeSignal has changed and is not the initial value
    if (closeSignal !== prevCloseSignalRef.current) {
      setIsOpen(false);
      prevCloseSignalRef.current = closeSignal;
    }
  }, [closeSignal]);

  // Detect if device is mobile
  useEffect(() => {
    const checkIsMobile = () => {
      setIsMobile(window.matchMedia("(max-width: 768px)").matches);
    };

    checkIsMobile();
    window.addEventListener("resize", checkIsMobile);

    return () => {
      window.removeEventListener("resize", checkIsMobile);
    };
  }, []);

  // Calculate position for the modal
  const calculatePosition = (e: React.MouseEvent) => {
    if (isFullscreen) return;

    const modalWidth = 400; // Default width, adjust as needed
    const modalHeight = 240; // Estimated height, adjust as needed

    let x = e.clientX + 10; // Add a small offset from cursor
    let y = e.clientY + 10; // Add a small offset from cursor

    // Ensure modal stays within viewport
    if (x + modalWidth > window.innerWidth) {
      x = window.innerWidth - modalWidth - 10;
    }

    if (y + modalHeight > window.innerHeight) {
      y = window.innerHeight - modalHeight - 10;
    }

    setPosition({ x, y });
  };

  // Handle mouse enter event
  const handleMouseEnter = (e: React.MouseEvent) => {
    if (!isFullscreen) {
      calculatePosition(e);

      // Clear any existing timeout
      if (hoverTimeoutRef.current) {
        clearTimeout(hoverTimeoutRef.current);
        hoverTimeoutRef.current = null;
      }

      setIsOpen(true);
    }
  };

  const handleMouseMove = () => {
    // Handle mouse move to update position
    // Disabled so that it doesn't move around
    // if (!isFullscreen && isOpen) {
    //   calculatePosition(e);
    // }
  };

  // Handle mouse leave event with delay to prevent flickering
  const handleMouseLeave = () => {
    if (!isFullscreen) {
      // Set a timeout to close the modal
      hoverTimeoutRef.current = setTimeout(() => {
        setIsOpen(false);
      }, 300);
    }
  };

  // Handle mouse enter on the modal itself
  const handleModalMouseEnter = () => {
    if (!isFullscreen) {
      // Clear the timeout if the mouse enters the modal
      if (hoverTimeoutRef.current) {
        clearTimeout(hoverTimeoutRef.current);
        hoverTimeoutRef.current = null;
      }
    }
  };

  // Handle click for mobile
  const handleClick = () => {
    if (isFullscreen) {
      setIsOpen(true);
    }
  };

  // Clean up timeout on unmount
  useEffect(() => {
    return () => {
      if (hoverTimeoutRef.current) {
        clearTimeout(hoverTimeoutRef.current);
      }
    };
  }, []);

  // Close modal when clicking outside on mobile
  useEffect(() => {
    const handleClickOutside = (event: MouseEvent) => {
      if (
        isFullscreen &&
        isOpen &&
        modalRef.current &&
        !modalRef.current.contains(event.target as Node)
      ) {
        setIsOpen(false);
      }
    };

    if (isFullscreen && isOpen) {
      document.addEventListener("mousedown", handleClickOutside);
    }

    return () => {
      document.removeEventListener("mousedown", handleClickOutside);
    };
  }, [isFullscreen, isOpen]);

  const modalContent = (
    <div
      className={clsx(
        "text-white z-50",
        isFullscreen
          ? "fixed inset-0 bg-black/50 flex items-center justify-center"
          : "fixed"
      )}
      style={
        !isFullscreen
          ? {
              left: `${position.x}px`,
              top: `${position.y}px`,
            }
          : {}
      }
      onMouseEnter={handleModalMouseEnter}
      onMouseLeave={handleMouseLeave}
    >
      <div
        ref={modalRef}
        className={clsx(
          "bg-darkgray border-green shadow-lg text-xs md:text-sm",
          isFullscreen
            ? "max-w-md w-full p-6 max-h-screen overflow-y-auto border-y"
            : "p-4 w-80 border"
        )}
      >
        {isFullscreen && (
          <div className="flex justify-end mb-2 text-white">
            <button onClick={() => setIsOpen(false)} aria-label="Close">
              <X size={24} />
            </button>
          </div>
        )}
        {children}
      </div>
    </div>
  );

  return (
    <div ref={containerRef}>
      {/* Render the trigger element */}
      <div
        ref={triggerRef}
        onMouseEnter={handleMouseEnter}
        onMouseLeave={handleMouseLeave}
        onMouseMove={handleMouseMove}
        onClick={handleClick}
        className="cursor-pointer"
      >
        {trigger}
      </div>

      {/* Render the modal if open */}
      {isOpen &&
        // Put the modal directly in the body to avoid z-index issues.
        // Otherwise, the circle visualizer appears on top of the modal.
        createPortal(modalContent, document.body)}
    </div>
  );
};

export default Modal;



================================================
FILE: frontend/src/app/opus-recorder.d.ts
================================================
declare module "opus-recorder" {
  interface RecorderOptions {
    // Confusing naming here because MediaTrackConstraints also exists as a type,
    // but this is actually a MediaStreamConstraints object
    mediaTrackConstraints?: MediaStreamConstraints;
    encoderPath: string;
    bufferLength: number;
    encoderFrameSize: number;
    encoderSampleRate: number;
    maxFramesPerPage: number;
    numberOfChannels: number;
    recordingGain: number;
    resampleQuality: number;
    encoderComplexity: number;
    encoderApplication: number;
    streamPages: boolean;
  }

  export default class Recorder {
    constructor(options: RecorderOptions);
    start(): void;
    stop(): void;
    ondataavailable: (data: Uint8Array) => void;
    encodedSamplePosition: number;
  }
}


type DecoderWorker = Worker


================================================
FILE: frontend/src/app/page.tsx
================================================
import Unmute from "./Unmute";

export default function Home() {
  return (
    <div className="w-full h-screen flex justify-center bg-background">
      <Unmute />
    </div>
  );
}



================================================
FILE: frontend/src/app/PositionedAudioVisualizer.tsx
================================================
import clsx from "clsx";
import { ChatMessage } from "./chatHistory";
import { useAudioVisualizerCircle } from "./useAudioVisualizerCircle";
import { useEffect, useRef } from "react";

const PositionedAudioVisualizer = ({
  chatHistory,
  role,
  analyserNode,
  isConnected,
  onCircleClick,
}: {
  chatHistory: ChatMessage[];
  role: "user" | "assistant";
  analyserNode: AnalyserNode | null;
  isConnected: boolean;
  onCircleClick?: () => void;
}) => {
  const canvasRef = useRef<HTMLCanvasElement | null>(null);
  const isAssistant = role === "assistant";

  useAudioVisualizerCircle(canvasRef, {
    chatHistory,
    role,
    analyserNode,
    isConnected,
    showPlayButton: !!onCircleClick,
    clearCanvas: true,
  });

  // Resize the canvas to fit its parent element
  useEffect(() => {
    const canvas = canvasRef.current;
    if (!canvas) return;

    const parent = canvas.parentElement;
    if (!parent) return;

    const size = Math.min(parent.clientWidth, parent.clientHeight);

    // If we don't do this `if` check, the recording ends up with flickering
    if (canvas.width !== size || canvas.height !== size) {
      canvas.width = size;
      canvas.height = size;
    }
  });

  return (
    <div
      className={clsx(
        "max-w-3xl md:h-full flex items-center -mx-8 -my-8 px-4 md:px-0",
        isAssistant
          ? "md:w-full flex-row md:flex-row-reverse pt-36 md:pt-0"
          : "w-full flex-row-reverse md:flex-row md:pt-36 -ml-40 md:ml-0"
      )}
    >
      <div
        className={clsx(
          isAssistant ? "w-40 md:w-72 2xl:w-96" : "w-full md:w-48 2xl:w-72"
        )}
      >
        <canvas
          ref={canvasRef}
          className={`w-full h-full rounded-full ${
            onCircleClick ? "cursor-pointer" : ""
          }`}
          onClick={onCircleClick}
        />
      </div>
    </div>
  );
};

export default PositionedAudioVisualizer;



================================================
FILE: frontend/src/app/SingleRoleSubtitles.tsx
================================================
import clsx from "clsx";
import React, { useCallback, useEffect, useRef, useState } from "react";

const SingleRoleSubtitles = ({
  text,
  role,
  nLines = 3,
}: {
  text: string;
  role: "user" | "assistant";
  nLines?: number;
}) => {
  const containerRef = useRef<HTMLDivElement>(null);
  const [displayText, setDisplayText] = useState<string[]>([]);
  const [previousText, setPreviousText] = useState("");

  const updateDisplayText = useCallback(() => {
    if (!containerRef.current) return;

    const container = containerRef.current;
    const containerWidth = container.clientWidth;

    // Create a temporary span to measure text width
    const tempSpan = document.createElement("span");
    tempSpan.style.visibility = "hidden";
    tempSpan.style.position = "absolute";
    tempSpan.style.whiteSpace = "nowrap";
    tempSpan.style.font = window.getComputedStyle(container).font;
    document.body.appendChild(tempSpan);

    const words = text.split(" ");
    const lines: string[] = [];
    let currentLine = "";

    // Build lines word by word
    for (const word of words) {
      const testLine = currentLine ? `${currentLine} ${word}` : word;
      tempSpan.textContent = testLine;

      if (tempSpan.offsetWidth <= containerWidth) {
        currentLine = testLine;
      } else {
        if (currentLine) {
          lines.push(currentLine);
          currentLine = word;
        } else {
          // Word is too long for one line
          lines.push(word);
          currentLine = "";
        }
      }
    }

    // Add the last line if it's not empty
    if (currentLine) {
      lines.push(currentLine);
    }

    // Remove the temporary span
    document.body.removeChild(tempSpan);

    const lastLines = lines.slice(-nLines);
    setDisplayText(lastLines);
  }, [nLines, text]);

  useEffect(() => {
    // If the new text is not a prefix of the old text, reset
    if (!text.startsWith(previousText)) {
      setDisplayText([]);
    }

    setPreviousText(text);

    updateDisplayText();
  }, [previousText, text, updateDisplayText]);

  // Re-calculate when the window resizes
  useEffect(() => {
    const handleResize = () => {
      updateDisplayText();
    };

    window.addEventListener("resize", handleResize);
    return () => {
      window.removeEventListener("resize", handleResize);
    };
  }, [text, updateDisplayText]);

  return (
    // Apply padding from the outside because otherwise we have to take it into
    // account when deciding how to break lines
    <div
      className={clsx(
        "w-full max-w-96 p-2 text-sm lg:text-md xl:text-lg",
        role === "assistant" ? "text-green" : "text-white"
      )}
    >
      <div ref={containerRef} className="h-20">
        {displayText.map((line, index) => (
          <div key={index} className="line whitespace-nowrap">
            {line}
          </div>
        ))}
      </div>
    </div>
  );
};

export default SingleRoleSubtitles;



================================================
FILE: frontend/src/app/SlantedButton.tsx
================================================
import React from "react";
import clsx from "clsx";

const SlantedButton = ({
  onClick = () => {},
  children,
  kind = "primary",
  style,
  extraClasses,
}: {
  onClick?: () => void;
  children: React.ReactNode;
  kind?: "primary" | "secondary" | "disabled";
  style?: React.CSSProperties;
  extraClasses?: string;
}) => {
  const kindToClass = {
    primary: "cursor-pointer after:bg-green text-black after:border-green",
    secondary:
      "cursor-pointer after:bg-darkgray text-white after:border-white after:border-dashed",
    disabled:
      "cursor-not-allowed after:bg-darkgray text-lightgray after:border-lightgray after:border-dashed",
  };

  return (
    <button
      onClick={onClick}
      disabled={kind === "disabled"}
      className={clsx(
        "px-4 py-2 mx-2 z-10 font-medium transition-colors duration-200",
        kindToClass[kind],
        {
          "opacity-50 cursor-not-allowed": kind === "disabled",
          "focus:outline-none focus-visible:outline-4 focus-visible:outline-webkit-focus-ring-color":
            kind !== "disabled",
        },
        extraClasses,
        // Green slanted border
        "relative after:content-[''] after:absolute",
        "after:top-0 after:left-0 after:right-0 after:bottom-0",
        "after:border-2 after:transform after:-skew-x-10 after:-z-10"
      )}
      style={style}
    >
      {children}
    </button>
  );
};

export default SlantedButton;



================================================
FILE: frontend/src/app/SquareButton.tsx
================================================
import React from "react";
import clsx from "clsx";

const SquareButton = ({
  onClick = () => {},
  children,
  kind = "primary",
  extraClasses,
}: {
  onClick?: () => void;
  children: React.ReactNode;
  kind?: "primary" | "primaryOff" | "secondary";
  extraClasses?: string;
}) => {
  const kindToClass = {
    primary: "text-green border-green",
    primaryOff: "text-white border-white",
    secondary: "text-white border-transparent",
  };

  return (
    <button
      onClick={onClick}
      className={clsx(
        "px-2 py-2 bg-black text-xs lg:text-sm cursor-pointer transition-colors duration-200",
        "overflow-hidden text-nowrap border-1 border-dashed",
        kindToClass[kind],
        extraClasses
      )}
      // Complex drop shadow easier to do outside of Tailwind
      style={{
        filter: "drop-shadow(0rem 0.2rem 0.15rem var(--darkgray))",
      }}
    >
      {/* The inner span ensures the content overflows in a centered way */}
      <span className="mx-[-100%] text-center">{children}</span>
    </button>
  );
};

export default SquareButton;



================================================
FILE: frontend/src/app/Subtitles.tsx
================================================
import { ChatMessage } from "./chatHistory";
import SingleRoleSubtitles from "./SingleRoleSubtitles";

const Subtitles = ({ chatHistory }: { chatHistory: ChatMessage[] }) => {
  const lastAssistantMessage = chatHistory.findLast(
    (message) => message.role === "assistant" && message.content !== ""
  );
  const lastUserMessage = chatHistory.findLast(
    (message) => message.role === "user" && message.content !== ""
  );

  return (
    <div className="flex flex-col md:flex-row w-full max-w-5xl justify-center">
      <SingleRoleSubtitles
        text={lastAssistantMessage?.content || ""}
        role="assistant"
      />
      <SingleRoleSubtitles text={lastUserMessage?.content || ""} role="user" />
    </div>
  );
};

export default Subtitles;



================================================
FILE: frontend/src/app/Unmute.tsx
================================================
"use client";
import useWebSocket, { ReadyState } from "react-use-websocket";
import { useCallback, useEffect, useState } from "react";
import { useMicrophoneAccess } from "./useMicrophoneAccess";
import { base64DecodeOpus, base64EncodeOpus } from "./audioUtil";
import SlantedButton from "@/app/SlantedButton";
import { useAudioProcessor as useAudioProcessor } from "./useAudioProcessor";
import useKeyboardShortcuts from "./useKeyboardShortcuts";
import { prettyPrintJson } from "pretty-print-json";
import PositionedAudioVisualizer from "./PositionedAudioVisualizer";
import UnmuteConfigurator, {
  DEFAULT_UNMUTE_CONFIG,
  UnmuteConfig,
} from "./UnmuteConfigurator";
import CouldNotConnect, { HealthStatus } from "./CouldNotConnect";
import UnmuteHeader from "./UnmuteHeader";
import Subtitles from "./Subtitles";
import { ChatMessage, compressChatHistory } from "./chatHistory";
import useWakeLock from "./useWakeLock";
import ErrorMessages, { ErrorItem, makeErrorItem } from "./ErrorMessages";
import { useRecordingCanvas } from "./useRecordingCanvas";
import { useGoogleAnalytics } from "./useGoogleAnalytics";
import clsx from "clsx";
import { useBackendServerUrl } from "./useBackendServerUrl";
import { COOKIE_CONSENT_STORAGE_KEY } from "./ConsentModal";

const Unmute = () => {
  const { isDevMode, showSubtitles } = useKeyboardShortcuts();
  const [debugDict, setDebugDict] = useState<object | null>(null);
  const [unmuteConfig, setUnmuteConfig] = useState<UnmuteConfig>(
    DEFAULT_UNMUTE_CONFIG
  );
  const [rawChatHistory, setRawChatHistory] = useState<ChatMessage[]>([]);
  const chatHistory = compressChatHistory(rawChatHistory);

  const { microphoneAccess, askMicrophoneAccess } = useMicrophoneAccess();

  const [shouldConnect, setShouldConnect] = useState(false);
  const backendServerUrl = useBackendServerUrl();
  const [webSocketUrl, setWebSocketUrl] = useState<string | null>(null);
  const [healthStatus, setHealthStatus] = useState<HealthStatus | null>(null);
  const [errors, setErrors] = useState<ErrorItem[]>([]);

  useWakeLock(shouldConnect);
  const { analyticsOnDownloadRecording } = useGoogleAnalytics({
    shouldConnect,
    unmuteConfig,
  });

  // Check if the backend server is healthy. If we setHealthStatus to null,
  // a "server is down" screen will be shown.
  useEffect(() => {
    if (!backendServerUrl) return;

    setWebSocketUrl(backendServerUrl.toString() + "/v1/realtime");

    const checkHealth = async () => {
      try {
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), 3000);

        const response = await fetch(`${backendServerUrl}/v1/health`, {
          signal: controller.signal,
        });

        clearTimeout(timeoutId);
        if (!response.ok) {
          setHealthStatus({
            connected: "yes_request_fail",
            ok: false,
          });
        }
        const data = await response.json();
        data["connected"] = "yes_request_ok";

        if (data.ok && !data.voice_cloning_up) {
          console.debug("Voice cloning not available, hiding upload button.");
        }
        setHealthStatus(data);
      } catch {
        setHealthStatus({
          connected: "no",
          ok: false,
        });
      }
    };

    checkHealth();
  }, [backendServerUrl]);

  const { sendMessage, lastMessage, readyState } = useWebSocket(
    webSocketUrl || null,
    {
      protocols: ["realtime"],
    },
    shouldConnect
  );

  // Send microphone audio to the server (via useAudioProcessor below)
  const onOpusRecorded = useCallback(
    (opus: Uint8Array) => {
      sendMessage(
        JSON.stringify({
          type: "input_audio_buffer.append",
          audio: base64EncodeOpus(opus),
        })
      );
    },
    [sendMessage]
  );

  const { setupAudio, shutdownAudio, audioProcessor } =
    useAudioProcessor(onOpusRecorded);
  const {
    canvasRef: recordingCanvasRef,
    downloadRecording,
    recordingAvailable,
  } = useRecordingCanvas({
    size: 1080,
    shouldRecord: shouldConnect,
    audioProcessor: audioProcessor.current,
    chatHistory: rawChatHistory,
  });

  const onConnectButtonPress = async () => {
    // If we're not connected yet
    if (!shouldConnect) {
      const mediaStream = await askMicrophoneAccess();
      // If we have access to the microphone:
      if (mediaStream) {
        await setupAudio(mediaStream);
        setShouldConnect(true);
      }
    } else {
      setShouldConnect(false);
      shutdownAudio();
    }
  };

  const onDownloadRecordingButtonPress = () => {
    try {
      downloadRecording(false);
      analyticsOnDownloadRecording();
    } catch (e) {
      if (e instanceof Error) {
        setErrors((prev) => [...prev, makeErrorItem(e.message)]);
      }
    }
  };

  // If the websocket connection is closed, shut down the audio processing
  useEffect(() => {
    if (readyState === ReadyState.CLOSING || readyState === ReadyState.CLOSED) {
      setShouldConnect(false);
      shutdownAudio();
    }
  }, [readyState, shutdownAudio]);

  // Handle incoming messages from the server
  useEffect(() => {
    if (lastMessage === null) return;

    const data = JSON.parse(lastMessage.data);
    if (data.type === "response.audio.delta") {
      const opus = base64DecodeOpus(data.delta);
      const ap = audioProcessor.current;
      if (!ap) return;

      ap.decoder.postMessage(
        {
          command: "decode",
          pages: opus,
        },
        [opus.buffer]
      );
    } else if (data.type === "unmute.additional_outputs") {
      setDebugDict(data.args.debug_dict);
    } else if (data.type === "error") {
      if (data.error.type === "warning") {
        console.warn(`Warning from server: ${data.error.message}`, data);
        // Warnings aren't explicitly shown in the UI
      } else {
        console.error(`Error from server: ${data.error.message}`, data);
        setErrors((prev) => [...prev, makeErrorItem(data.error.message)]);
      }
    } else if (
      data.type === "conversation.item.input_audio_transcription.delta"
    ) {
      // Transcription of the user speech
      setRawChatHistory((prev) => [
        ...prev,
        { role: "user", content: data.delta },
      ]);
    } else if (data.type === "response.text.delta") {
      // Text-to-speech output
      setRawChatHistory((prev) => [
        ...prev,
        // The TTS doesn't include spaces in its messages, so add a leading space.
        // This way we'll get a leading space at the very beginning of the message,
        // but whatever.
        { role: "assistant", content: " " + data.delta },
      ]);
    } else {
      const ignoredTypes = [
        "session.updated",
        "response.created",
        "response.text.delta",
        "response.text.done",
        "response.audio.done",
        "conversation.item.input_audio_transcription.delta",
        "input_audio_buffer.speech_stopped",
        "input_audio_buffer.speech_started",
        "unmute.interrupted_by_vad",
        "unmute.response.text.delta.ready",
        "unmute.response.audio.delta.ready",
      ];
      if (!ignoredTypes.includes(data.type)) {
        console.warn("Received unknown message:", data);
      }
    }
  }, [audioProcessor, lastMessage]);

  // When we connect, we send the initial config (voice and instructions) to the server.
  // Also clear the chat history.
  useEffect(() => {
    if (readyState !== ReadyState.OPEN) return;

    const recordingConsent =
      localStorage.getItem(COOKIE_CONSENT_STORAGE_KEY) === "true";

    setRawChatHistory([]);
    sendMessage(
      JSON.stringify({
        type: "session.update",
        session: {
          instructions: unmuteConfig.instructions,
          voice: unmuteConfig.voice,
          allow_recording: recordingConsent,
        },
      })
    );
  }, [unmuteConfig, readyState, sendMessage]);

  // Disconnect when the voice or instruction changes.
  // TODO: If it's a voice change, immediately reconnect with the new voice.
  useEffect(() => {
    setShouldConnect(false);
    shutdownAudio();
  }, [shutdownAudio, unmuteConfig.voice, unmuteConfig.instructions]);

  if (!healthStatus || !backendServerUrl) {
    return (
      <div className="flex flex-col gap-4 items-center">
        <h1 className="text-xl mb-4">Loading...</h1>
      </div>
    );
  }

  if (healthStatus && !healthStatus.ok) {
    return <CouldNotConnect healthStatus={healthStatus} />;
  }

  return (
    <div className="w-full">
      <ErrorMessages errors={errors} setErrors={setErrors} />
      {/* The main full-height demo */}
      <div className="relative flex w-full min-h-screen flex-col text-white bg-background items-center">
        {/* z-index on the header to put it in front of the circles */}
        <header className="static md:absolute max-w-6xl px-3 md:px-8 right-0 flex justify-end z-10">
          <UnmuteHeader />
        </header>
        <div
          className={clsx(
            "w-full h-auto min-h-75",
            "flex flex-row-reverse md:flex-row items-center justify-center grow",
            "-mt-10 md:mt-0 mb-10 md:mb-0 md:-mr-4"
          )}
        >
          <PositionedAudioVisualizer
            chatHistory={chatHistory}
            role={"assistant"}
            analyserNode={audioProcessor.current?.outputAnalyser || null}
            onCircleClick={onConnectButtonPress}
            isConnected={shouldConnect}
          />
          <PositionedAudioVisualizer
            chatHistory={chatHistory}
            role={"user"}
            analyserNode={audioProcessor.current?.inputAnalyser || null}
            isConnected={shouldConnect}
          />
        </div>
        {showSubtitles && <Subtitles chatHistory={chatHistory} />}
        <UnmuteConfigurator
          backendServerUrl={backendServerUrl}
          config={unmuteConfig}
          setConfig={setUnmuteConfig}
          voiceCloningUp={healthStatus.voice_cloning_up || false}
        />
        <div className="w-full flex flex-col-reverse md:flex-row items-center justify-center px-3 gap-3 my-6">
          <SlantedButton
            onClick={onDownloadRecordingButtonPress}
            kind={recordingAvailable ? "secondary" : "disabled"}
            extraClasses="w-full max-w-96"
          >
            {"download recording"}
          </SlantedButton>
          <SlantedButton
            onClick={onConnectButtonPress}
            kind={shouldConnect ? "secondary" : "primary"}
            extraClasses="w-full max-w-96"
          >
            {shouldConnect ? "disconnect" : "connect"}
          </SlantedButton>
          {/* Maybe we don't need to explicitly show the status */}
          {/* {renderConnectionStatus(readyState, false)} */}
          {microphoneAccess === "refused" && (
            <div className="text-red">
              {"You'll need to allow microphone access to use the demo. " +
                "Please check your browser settings."}
            </div>
          )}
        </div>
      </div>
      {/* Debug stuff, not counted into the screen height */}
      {isDevMode && (
        <div>
          <div className="text-xs w-full overflow-auto">
            <pre
              className="whitespace-pre-wrap break-words"
              dangerouslySetInnerHTML={{
                __html: prettyPrintJson.toHtml(debugDict),
              }}
            ></pre>
          </div>
          <div>Subtitles: press S. Dev mode: press D.</div>
        </div>
      )}
      <canvas ref={recordingCanvasRef} className="hidden" />
    </div>
  );
};

export default Unmute;



================================================
FILE: frontend/src/app/UnmuteConfigurator.tsx
================================================
import { useEffect, useState } from "react";
import VoiceAttribution from "./VoiceAttribution";
import SquareButton from "./SquareButton";
import Modal from "./Modal";
import { ArrowUpRight } from "lucide-react";
import VoiceUpload from "./VoiceUpload";
// import VoiceUpload from "./VoiceUpload";

export type LanguageCode = "en" | "fr" | "en/fr" | "fr/en";

export type ConstantInstructions = {
  type: "constant";
  text: string;
  language?: LanguageCode;
};

export type Instructions =
  | ConstantInstructions
  | { type: "smalltalk"; language?: LanguageCode }
  | { type: "guess_animal"; language?: LanguageCode }
  | { type: "quiz_show"; language?: LanguageCode };

export type UnmuteConfig = {
  instructions: Instructions;
  voice: string;
  // The backend doesn't care about this, we use it for analytics
  voiceName: string;
  // The backend doesn't care about this, we use it for analytics
  isCustomInstructions: boolean;
};

// Will be overridden immediately by the voices fetched from the backend
export const DEFAULT_UNMUTE_CONFIG: UnmuteConfig = {
  instructions: {
    type: "smalltalk",
    language: "en/fr",
  },
  voice: "barack_demo.wav",
  voiceName: "Missing voice",
  isCustomInstructions: false,
};

export type FreesoundVoiceSource = {
  source_type: "freesound";
  url: string;
  start_time: number;
  sound_instance: {
    id: number;
    name: string;
    username: string;
    license: string;
  };
  path_on_server: string;
};

export type FileVoiceSource = {
  source_type: "file";
  path_on_server: string;
  description?: string;
  description_link?: string;
};

export type VoiceSample = {
  name: string | null;
  comment: string;
  good: boolean;
  instructions: Instructions | null;
  source: FreesoundVoiceSource | FileVoiceSource;
};

const instructionsToPlaceholder = (instructions: Instructions) => {
  if (instructions.type === "constant") {
    return instructions.text;
  } else {
    return (
      {
        smalltalk:
          "Make pleasant conversation. (For this character, the instructions contain dynamically generated parts.)",
        guess_animal:
          "Make the user guess the animal. (For this character, the instructions contain dynamically generated parts.)",
        quiz_show:
          "You're a quiz show host that hates his job. (For this character, the instructions contain dynamically generated parts.)",
        news: "Talk about the latest tech news. (For this character, we fetch the news from the internet dynamically.)",
        unmute_explanation:
          "Explain how Unmute works. (For this character, the instructions are long so we don't show them here in full.)",
      }[instructions.type] || ""
    );
  }
};

const fetchVoices = async (
  backendServerUrl: string
): Promise<VoiceSample[]> => {
  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 3000);

    const response = await fetch(`${backendServerUrl}/v1/voices`, {
      signal: controller.signal,
    });

    clearTimeout(timeoutId);

    if (!response.ok) {
      console.error("Failed to fetch voices:", response.statusText);
      return [];
    }

    const voices = await response.json();
    return voices;
  } catch (error) {
    console.error("Error fetching voices:", error);
    return [];
  }
};

const getVoiceName = (voice: VoiceSample) => {
  return (
    voice.name ||
    (voice.source.source_type === "freesound"
      ? voice.source.sound_instance.username
      : voice.source.path_on_server.slice(0, 10))
  );
};

const UnmuteConfigurator = ({
  config,
  backendServerUrl,
  setConfig,
  voiceCloningUp,
}: {
  config: UnmuteConfig;
  backendServerUrl: string;
  setConfig: (config: UnmuteConfig) => void;
  voiceCloningUp: boolean;
}) => {
  const [voices, setVoices] = useState<VoiceSample[] | null>(null);
  const [customVoiceName, setCustomVoiceName] = useState<string | null>(null);
  const [customInstructions, setCustomInstructions] =
    useState<Instructions | null>(null);

  useEffect(() => {
    const fetchVoicesData = async () => {
      if (backendServerUrl && voices === null) {
        const voicesData = await fetchVoices(backendServerUrl);
        setVoices(voicesData);

        // It could be confusing to start with a non-English voice
        const englishVoices = voicesData.filter(
          (voice) => (voice.instructions?.language || "en") === "en"
        );
        const randomVoice =
          englishVoices[Math.floor(Math.random() * englishVoices.length)];

        setConfig({
          ...config,
          voice: randomVoice.source.path_on_server,
          voiceName: getVoiceName(randomVoice),
          instructions:
            randomVoice.instructions || DEFAULT_UNMUTE_CONFIG.instructions,
        });
      }
    };

    fetchVoicesData();
  }, [backendServerUrl, config, setConfig, voices]);

  if (!voices) {
    return (
      <div className="w-full">
        <p className="text-lightgray">Loading voices...</p>
      </div>
    );
  }

  const onCustomVoiceUpload = (name: string) => {
    setCustomVoiceName(name);
    setConfig({
      voice: name,
      instructions: customInstructions || DEFAULT_UNMUTE_CONFIG.instructions,
      isCustomInstructions: !!customInstructions,
      voiceName: "custom",
    });
  };

  const activeVoice = voices.find(
    (voice) => voice.source.path_on_server === config.voice
  );
  const defaultInstructions =
    activeVoice?.instructions || DEFAULT_UNMUTE_CONFIG.instructions;

  const handleChange = (e: React.ChangeEvent<HTMLTextAreaElement>) => {
    const updatedInstructions: Instructions | null = e.target.value
      ? { type: "constant", text: e.target.value, language: "en/fr" }
      : null;
    setCustomInstructions(updatedInstructions);
    console.debug("Updated instructions:", updatedInstructions);
    setConfig({
      ...config,
      instructions: updatedInstructions || defaultInstructions,
      isCustomInstructions: !!updatedInstructions,
    });
  };

  const additionalInstructionsHeader = (
    <div className="w-full flex flex-row items-center gap-2">
      <Modal
        trigger={
          <h2 className="pb-1 flex items-center gap-1 text-lightgray">
            Instructions <ArrowUpRight size={24} />
          </h2>
        }
      >
        <div className="flex flex-col gap-3">
          <p>
            Instructions that affect the text responses generated by the LLM.
            Note that{" "}
            <em className="italic">
              the text-to-speech does not have access to these,{" "}
            </em>
            meaning voice instructions like {'"speak slowly"'} will not work.
          </p>
          {config.instructions.type !== "constant" && (
            <p>
              *In this case, some instructions are generated dynamically and
              change each time.
            </p>
          )}
        </div>
      </Modal>
      <div className="h-0.5 bg-gray grow hidden md:visible"></div>
    </div>
  );

  return (
    <div className="w-full flex flex-col items-center">
      {/* Separate header div, because it has a different background */}
      <div className="w-full max-w-6xl grid grid-flow-row grid-cols-1 md:grid-cols-2 gap-3 px-3">
        <div className="w-full flex flex-row items-center gap-2">
          <Modal
            trigger={
              <h2 className="pb-1 cursor-pointer flex items-center gap-1 text-lightgray">
                Character <ArrowUpRight size={24} />
              </h2>
            }
          >
            <p>
              The voice of the text-to-speech is based on a 10-second sample.
            </p>
            {activeVoice && <VoiceAttribution voice={activeVoice} />}
          </Modal>
          <div className="h-0.5 bg-gray grow hidden md:visible"></div>
        </div>
        <div className="hidden md:inline-block">
          {additionalInstructionsHeader}
        </div>
      </div>
      {/* Gray background div, full width */}
      <div className="w-full md:bg-gray flex flex-col items-center">
        <div className="w-full max-w-6xl grid grid-flow-row grid-cols-1 md:grid-cols-2 gap-3 p-3">
          <div>
            <div className="grid grid-flow-row grid-cols-2 md:grid-cols-3 gap-3">
              {voices &&
                voices.map((voice) => (
                  <SquareButton
                    key={voice.source.path_on_server}
                    onClick={() => {
                      setConfig({
                        voice: voice.source.path_on_server,
                        voiceName: voice.name || "Unnamed",
                        instructions:
                          customInstructions ||
                          voice.instructions ||
                          DEFAULT_UNMUTE_CONFIG.instructions,
                        isCustomInstructions: !!customInstructions,
                      });
                    }}
                    kind={
                      voice.source.path_on_server === config.voice
                        ? "primary"
                        : "secondary"
                    }
                    extraClasses="bg-gray md:bg-black"
                  >
                    {"/ " + getVoiceName(voice) + " /"}
                  </SquareButton>
                ))}
              {voiceCloningUp && (
                <VoiceUpload
                  backendServerUrl={backendServerUrl}
                  onCustomVoiceUpload={onCustomVoiceUpload}
                  isSelected={customVoiceName === config.voice}
                />
              )}
            </div>
          </div>
          <div className="inline-block md:hidden">
            {additionalInstructionsHeader}
          </div>
          <textarea
            placeholder={instructionsToPlaceholder(defaultInstructions)}
            onChange={handleChange}
            className="bg-gray md:bg-black text-white text-sm w-full p-2 resize-none h-33"
            style={{
              filter: "drop-shadow(-0.1rem -0.1rem 0.1rem var(--darkgray))",
            }}
          />
        </div>
      </div>
    </div>
  );
};

export default UnmuteConfigurator;



================================================
FILE: frontend/src/app/UnmuteHeader.tsx
================================================
import { Frank_Ruhl_Libre } from "next/font/google";
import Modal from "./Modal";
import { ArrowUpRight } from "lucide-react";
import Link from "next/link";
import kyutaiLogo from "../assets/kyutai-logo-cropped.svg";

const frankRuhlLibre = Frank_Ruhl_Libre({
  weight: "400",
  subsets: ["latin"],
});

const ShortExplanation = () => {
  return (
    <p className="text-xs text-right">
      Speak to an AI using our new low-latency open-source{" "}
      <Link
        href="https://kyutai.org/next/stt"
        className="underline text-green"
        target="_blank"
        rel="noopener"
      >
        speech-to-text
      </Link>{" "}
      and{" "}
      <Link
        href="https://kyutai.org/next/tts"
        className="underline text-green"
        target="_blank"
        rel="noopener"
      >
        text-to-speech
      </Link>
      .
    </p>
  );
};

const UnmuteHeader = () => {
  return (
    <div className="flex flex-col gap-2 py-2 md:py-8 items-end max-w-80 md:max-w-60 lg:max-w-80">
      {/* kyutaiLogo */}
      <h1 className={`text-3xl ${frankRuhlLibre.className}`}>Unmute.sh</h1>
      <div className="flex items-center gap-2 -mt-1 text-xs">
        by
        <Link href="https://kyutai.org" target="_blank" rel="noopener">
          <img src={kyutaiLogo.src} alt="Kyutai logo" className="w-20" />
        </Link>
      </div>
      <ShortExplanation />
      <Modal
        trigger={
          <span className="flex items-center gap-1 text-lightgray">
            More info <ArrowUpRight size={24} />
          </span>
        }
        forceFullscreen={true}
      >
        <div className="flex flex-col gap-3">
          <p>
            This is a cascaded system made by Kyutai: our speech-to-text
            transcribes what you say, an LLM (we use Mistral Small 24B)
            generates the text of the response, and we then use our
            text-to-speech model to say it out loud.
          </p>
          <p>
            All of the components are open-source:{" "}
            <Link
              href="https://kyutai.org/next/stt"
              target="_blank"
              rel="noopener"
              className="underline text-green"
            >
              Kyutai STT
            </Link>
            ,{" "}
            <Link
              href="https://kyutai.org/next/tts"
              target="_blank"
              rel="noopener"
              className="underline text-green"
            >
              Kyutai TTS
            </Link>
            , and{" "}
            <Link
              href="https://kyutai.org/next/unmute"
              target="_blank"
              rel="noopener"
              className="underline text-green"
            >
              Unmute
            </Link>{" "}
            itself.
          </p>
          <p>
            Although cascaded systems lose valuable information like emotion,
            irony, etc., they provide unmatched modularity: since the three
            parts are separate, you can <em>Unmute</em> any LLM you want without
            any finetuning or adaptation! In this demo, you can get a feel for
            this versatility by tuning the system prompt of the LLM to handcraft
            the personality of your digital interlocutor, and independently
            changing the voice of the TTS.
          </p>
          <p>
            Both the speech-to-text and text-to-speech models are optimized for
            low latency. The STT model is streaming and integrates semantic
            voice activity detection instead of relying on an external model.
            The TTS is streaming both in audio and in text, meaning it can start
            speaking before the entire LLM response is generated. You can use a
            10-second voice sample to determine the TTS{"'"}s voice and
            intonation.
          </p>
          <p>
            To stay up to date on our research, follow us on{" "}
            <Link
              href="https://twitter.com/kyutai_labs"
              target="_blank"
              rel="noopener"
              className="underline text-green"
            >
              Twitter
            </Link>{" "}
            or{" "}
            <Link
              href="https://www.linkedin.com/company/kyutai-labs"
              target="_blank"
              rel="noopener"
              className="underline text-green"
            >
              LinkedIn
            </Link>
            , or{" "}
            <Link
              href="https://33d1df77.sibforms.com/serve/MUIFAICjnsdoIJLt57yBiJeUGA0emJ8eCBAvxtXRaAzxXfP7VYFXBgbDmcl8ig6BVt2qV4wnpRtCQaM0o3iPAJVA9UzQBSQKE3SacZULVUeAhIiI4RZiE0aigP_u_9cUK31SLrzsr1mf_Nw9sdzpz22rXBp_rnBVtd3YW1TSIhAag0F8biQaRg3mQJiCR5n0MXxA1KAzL0GO2wIu"
              target="_blank"
              rel="noopener"
              className="underline text-green"
            >
              sign up for our newsletter
            </Link>
            .
          </p>
          <p>
            For questions or feedback:{" "}
            <Link
              href="mailto:unmute@kyutai.org"
              target="_blank"
              rel="noopener"
              className="underline"
            >
              unmute@kyutai.org
            </Link>
          </p>
        </div>
      </Modal>
    </div>
  );
};

export default UnmuteHeader;



================================================
FILE: frontend/src/app/useAudioProcessor.ts
================================================
import { useRef, useCallback } from "react";
import OpusRecorder from "opus-recorder";

const getAudioWorkletNode = async (
  audioContext: AudioContext,
  name: string
) => {
  try {
    return new AudioWorkletNode(audioContext, name);
  } catch {
    await audioContext.audioWorklet.addModule(`/${name}.js`);
    return new AudioWorkletNode(audioContext, name, {});
  }
};

export interface AudioProcessor {
  audioContext: AudioContext;
  opusRecorder: OpusRecorder;
  decoder: DecoderWorker;
  outputWorklet: AudioWorkletNode;
  inputAnalyser: AnalyserNode;
  outputAnalyser: AnalyserNode;
  mediaStreamDestination: MediaStreamAudioDestinationNode;
}

export const useAudioProcessor = (
  onOpusRecorded: (chunk: Uint8Array) => void
) => {
  const audioProcessorRef = useRef<AudioProcessor | null>(null);

  const setupAudio = useCallback(
    async (mediaStream: MediaStream) => {
      if (audioProcessorRef.current) return audioProcessorRef.current;

      const audioContext = new AudioContext();
      const outputWorklet = await getAudioWorkletNode(
        audioContext,
        "audio-output-processor"
      );
      const source = audioContext.createMediaStreamSource(mediaStream);
      // source.connect(inputWorklet);
      const inputAnalyser = audioContext.createAnalyser();
      inputAnalyser.fftSize = 2048;
      source.connect(inputAnalyser);

      const mediaStreamDestination =
        audioContext.createMediaStreamDestination();
      outputWorklet.connect(mediaStreamDestination);
      source.connect(mediaStreamDestination);

      outputWorklet.connect(audioContext.destination);
      const outputAnalyser = audioContext.createAnalyser();
      outputAnalyser.fftSize = 2048;
      outputWorklet.connect(outputAnalyser);

      const decoder = new Worker("/decoderWorker.min.js");
      let micDuration = 0;

      // eslint-disable-next-line @typescript-eslint/no-explicit-any
      decoder.onmessage = (event: MessageEvent<any>) => {
        if (!event.data) {
          return;
        }
        const frame = event.data[0];
        outputWorklet.port.postMessage({
          frame: frame,
          type: "audio",
          micDuration: micDuration,
        });
      };
      decoder.postMessage({
        command: "init",
        bufferLength: (960 * audioContext.sampleRate) / 24000,
        decoderSampleRate: 24000,
        outputBufferSampleRate: audioContext.sampleRate,
        resampleQuality: 0,
      });

      // For buffer length: 960 = 24000 / 12.5 / 2
      // The /2 is a bit optional, but won't hurt for recording the mic.
      // Note that bufferLength actually has 0 impact for mono audio, only
      // the frameSize and maxFramesPerPage seems to have any.
      const recorderOptions = {
        mediaTrackConstraints: {
          audio: {
            echoCancellation: true,
            noiseSuppression: false,
            autoGainControl: true,
            channelCount: 1,
          },
          video: false,
        },
        encoderPath: "/encoderWorker.min.js",
        bufferLength: Math.round((960 * audioContext.sampleRate) / 24000),
        encoderFrameSize: 20,
        encoderSampleRate: 24000,
        maxFramesPerPage: 2,
        numberOfChannels: 1,
        recordingGain: 1,
        resampleQuality: 3,
        encoderComplexity: 0,
        encoderApplication: 2049,
        streamPages: true,
      };
      let chunk_idx = 0;
      let lastpos = 0;
      const opusRecorder = new OpusRecorder(recorderOptions);
      opusRecorder.ondataavailable = (data: Uint8Array) => {
        // opus actually always works at 48khz, so it seems this is the proper value to use here.
        micDuration = opusRecorder.encodedSamplePosition / 48000;
        // logging disabled
        if (chunk_idx < 0) {
          console.debug(
            Date.now() % 1000,
            "Mic Data chunk",
            chunk_idx++,
            (opusRecorder.encodedSamplePosition - lastpos) / 48000,
            micDuration
          );
          lastpos = opusRecorder.encodedSamplePosition;
        }
        onOpusRecorded(data);
      };
      audioProcessorRef.current = {
        audioContext,
        opusRecorder,
        decoder,
        outputWorklet,
        inputAnalyser,
        outputAnalyser,
        mediaStreamDestination,
      };
      // Resume the audio context if it was suspended
      audioProcessorRef.current.audioContext.resume();
      opusRecorder.start();

      return audioProcessorRef.current;
    },
    [onOpusRecorded]
  );

  const shutdownAudio = useCallback(() => {
    if (audioProcessorRef.current) {
      const { audioContext, opusRecorder, outputWorklet } =
        audioProcessorRef.current;

      // Disconnect all nodes
      outputWorklet.disconnect();
      audioContext.close();
      opusRecorder.stop();

      // Clear the reference
      audioProcessorRef.current = null;
    }
  }, []);

  return {
    setupAudio,
    shutdownAudio,
    audioProcessor: audioProcessorRef,
  };
};



================================================
FILE: frontend/src/app/useAudioVisualizerCircle.ts
================================================
import { useRef, useEffect, useState } from "react";
import { getCSSVariable } from "./cssUtil";
import { ChatMessage } from "./chatHistory";

const WIDTH_INACTIVE = 3;
const WIDTH_ACTIVE = 5;

// Size scale factors for connected/disconnected states
const SCALE_CONNECTED = 1;
const SCALE_DISCONNECTED = 0.8;
const ANIMATION_DURATION = 500; // milliseconds

const INTERRUPTION_CHAR = "—"; // em-dash

const sampleToNormalizedRadius = (x: number) => {
  return 0.8 + 0.2 * Math.tanh(x * 2);
};

interface Positioning {
  centerX: number;
  centerY: number;
  radius: number;
}

const drawCircleVisualization = (
  canvas: HTMLCanvasElement,
  canvasCtx: CanvasRenderingContext2D,
  data: Float32Array,
  colorName: string,
  lineWidth: number,
  animationProgress: number,
  positioning: Positioning
) => {
  // Calculate scale factor from animation progress (0 = disconnected, 1 = connected)
  const scaleFactor =
    SCALE_DISCONNECTED +
    (SCALE_CONNECTED - SCALE_DISCONNECTED) * animationProgress;

  canvasCtx.beginPath();
  for (let i = 0; i < data.length; i++) {
    const radius =
      positioning.radius * sampleToNormalizedRadius(data[i]) * scaleFactor;
    const angle = (i / data.length) * Math.PI * 2;

    const x = positioning.centerX + radius * Math.cos(angle);
    const y = positioning.centerY + radius * Math.sin(angle);
    if (i === 0) {
      canvasCtx.moveTo(x, y);
    } else {
      canvasCtx.lineTo(x, y);
    }
  }
  canvasCtx.closePath();
  canvasCtx.strokeStyle = getCSSVariable(colorName);
  canvasCtx.lineWidth = lineWidth;
  canvasCtx.stroke();
};

// New function to draw a play triangle
const drawPlayButton = (
  canvas: HTMLCanvasElement,
  canvasCtx: CanvasRenderingContext2D,
  colorName: string,
  animationProgress: number,
  positioning: Positioning
) => {
  // Calculate opacity based on animation progress (0 = fully visible, 1 = invisible)
  const opacity = 1 - animationProgress;

  if (opacity <= 0) return; // Don't draw if fully transparent

  const centerX = positioning.centerX;
  const centerY = positioning.centerY;
  const size = positioning.radius * 0.2; // Play button size relative to circle size

  // Create triangle path
  canvasCtx.beginPath();
  canvasCtx.moveTo(centerX + size / 2, centerY);
  canvasCtx.lineTo(centerX - size / 4, centerY - size / 2);
  canvasCtx.lineTo(centerX - size / 4, centerY + size / 2);
  canvasCtx.closePath();

  // Fill with color and opacity
  const color = getCSSVariable(colorName);
  // Parse the CSS variable color to get RGB values
  const tempCanvas = document.createElement("canvas");
  const tempCtx = tempCanvas.getContext("2d");
  if (!tempCtx) return;

  tempCtx.fillStyle = color;
  tempCtx.fillRect(0, 0, 1, 1);
  const rgba = tempCtx.getImageData(0, 0, 1, 1).data;

  // Apply opacity to the color
  canvasCtx.fillStyle = `rgba(${rgba[0]}, ${rgba[1]}, ${rgba[2]}, ${opacity})`;
  canvasCtx.fill();
};

const getAnalyzerData = (
  analyserNode: AnalyserNode | null
): [Float32Array, Float32Array] => {
  const fftSize = 2048;
  const frequencyData = new Float32Array(fftSize / 2);
  const timeDomainData = new Float32Array(fftSize / 8);

  if (!analyserNode) {
    // return arrays corresponding to silence
    frequencyData.fill(-100); // -100 dBFS
    timeDomainData.fill(0); // silence

    return [frequencyData, timeDomainData];
  } else {
    // Configure analyzer node
    analyserNode.fftSize = fftSize;
    analyserNode.smoothingTimeConstant = 0.85;

    analyserNode.getFloatTimeDomainData(timeDomainData);
    analyserNode.getFloatFrequencyData(frequencyData);

    return [frequencyData, timeDomainData];
  }
};

const getIsActive = (
  chatHistory: ChatMessage[],
  role: "user" | "assistant"
) => {
  // Find the latest non-empty message from the specified role
  for (let i = chatHistory.length - 1; i >= 0; i--) {
    const message = chatHistory[i];

    // Empty messages, or ones where the LLM started generating but was interrupted
    // before it said anything
    if (message.content === "" || message.content === INTERRUPTION_CHAR)
      continue;

    if (message.content === "...") {
      // The user is silent, no more speech is coming
      return false;
    }

    if (message.role === role) {
      return true;
    } else {
      return false;
    }
  }
  // No non-empty messages found
  return false;
};

export interface UseAudioVisualizerCircleOptions {
  chatHistory: ChatMessage[];
  role: "user" | "assistant";
  analyserNode: AnalyserNode | null;
  isConnected?: boolean;
  showPlayButton?: boolean;
  positioning?: Positioning;
  clearCanvas: boolean;
}

export const useAudioVisualizerCircle = (
  canvasRef: React.RefObject<HTMLCanvasElement | null>,
  options: UseAudioVisualizerCircleOptions
) => {
  const {
    chatHistory,
    role,
    analyserNode,
    isConnected = false,
    showPlayButton = false,
    positioning,
    clearCanvas,
  } = options;

  const isActive = getIsActive(chatHistory, role);
  const isAssistant = role === "assistant";
  const colorName = isAssistant ? "color-green" : "color-white";

  const animationRef = useRef<number>(-1);
  const cicleBuffer = useRef<Float32Array>(new Float32Array(256));
  const circleIndex = useRef(0);
  const animationFrameRef = useRef<number | null>(null);
  const animationStartTime = useRef<number | null>(null);
  const animationPreviousProgress = useRef<number>(isConnected ? 1 : 0);

  const interruptionTimeRef = useRef(0);
  const [interruptionIndex, setInterruptionIndex] = useState(0);

  // Single state for animation progress: 0 = disconnected state, 1 = connected state
  const [animationProgress, setAnimationProgress] = useState<number>(
    isConnected ? 1 : 0
  );

  useEffect(() => {
    const canvas = canvasRef.current;
    if (!canvas) return;
    const stream = canvas.captureStream(30);
    stream.getTracks().forEach((track) => {
      track.stop();
    });
  }, [canvasRef]);

  useEffect(() => {
    if (chatHistory.length > interruptionIndex) {
      if (
        role === "user" &&
        chatHistory[chatHistory.length - 1].role === "assistant" &&
        // An interruption
        chatHistory[chatHistory.length - 1].content.endsWith(
          INTERRUPTION_CHAR
        ) &&
        // but not *only* an interruption char. That would mean the LLM got interrupted
        // before it said anything, and we don't want to count that as an interruption
        chatHistory[chatHistory.length - 1].content !== INTERRUPTION_CHAR
      ) {
        interruptionTimeRef.current = Date.now();
        setInterruptionIndex(chatHistory.length);
      }
    }
  }, [chatHistory, interruptionIndex, role]);

  // Handle connection state changes
  useEffect(() => {
    const animate = (timestamp: number) => {
      if (!animationStartTime.current) {
        animationStartTime.current = timestamp;
        animationPreviousProgress.current = animationProgress;
      }

      const elapsed = timestamp - animationStartTime.current;
      const duration = ANIMATION_DURATION;
      const progress = Math.min(elapsed / duration, 1);

      const targetProgress = isConnected ? 1 : 0;

      // Ease in-out function for smoother animation
      const easeInOutCubic = (t: number): number => {
        return t < 0.5 ? 4 * t * t * t : 1 - Math.pow(-2 * t + 2, 3) / 2;
      };

      const newProgress =
        animationPreviousProgress.current +
        easeInOutCubic(progress) *
          (targetProgress - animationPreviousProgress.current);

      setAnimationProgress(newProgress);

      if (progress < 1) {
        animationFrameRef.current = requestAnimationFrame(animate);
      } else {
        // Animation complete
        animationFrameRef.current = null;
        animationStartTime.current = null;
        setAnimationProgress(targetProgress); // Ensure we end exactly at target value
      }
    };

    // Cancel any existing animation
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
      animationFrameRef.current = null;
    }

    // Start the animation
    animationFrameRef.current = requestAnimationFrame(animate);

    // Cleanup on unmount
    return () => {
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
    };
  }, [isConnected, animationProgress]);

  // Main drawing effect
  useEffect(() => {
    const canvas = canvasRef.current;
    if (!canvas) return;

    const canvasCtx = canvas.getContext("2d");
    if (!canvasCtx) return;

    const draw = () => {
      // Calculate positioning - use provided positioning or default to center
      const currentPositioning: Positioning = positioning || {
        centerX: canvas.width / 2,
        centerY: canvas.height / 2,
        radius: Math.min(canvas.width / 2, canvas.height / 2),
      };

      const [frequencyData, timeDomainData] = getAnalyzerData(analyserNode);
      const volumeDbfs =
        frequencyData.reduce((a, b) => a + b, 0) / frequencyData.length;
      const volumeNormalized = (Math.max(-100, volumeDbfs) + 100) / 100;
      // cicleBuffer.current[circleIndex.current] = volumeNormalized;
      for (let i = 0; i < 1 + volumeNormalized * 10; i++) {
        cicleBuffer.current[circleIndex.current] = timeDomainData[i];
        circleIndex.current =
          (circleIndex.current + 1) % cicleBuffer.current.length;
      }

      // Schedule the next animation frame
      animationRef.current = requestAnimationFrame(draw);

      if (clearCanvas) {
        canvasCtx.clearRect(
          currentPositioning.centerX - currentPositioning.radius,
          currentPositioning.centerY - currentPositioning.radius,
          currentPositioning.radius * 2,
          currentPositioning.radius * 2
        );
      }

      const secSinceInterruption =
        (Date.now() - interruptionTimeRef.current) / 1000;
      const widthScale =
        1 + 2 * Math.exp(-Math.pow(secSinceInterruption * 3, 2));

      drawCircleVisualization(
        canvas,
        canvasCtx,
        cicleBuffer.current,
        colorName,
        Math.max(
          isActive ? WIDTH_ACTIVE : WIDTH_INACTIVE,
          WIDTH_INACTIVE * widthScale
        ),
        animationProgress,
        currentPositioning
      );

      // Draw play button if we have onClick and not fully connected
      if (showPlayButton && animationProgress < 1) {
        drawPlayButton(
          canvas,
          canvasCtx,
          colorName,
          animationProgress,
          currentPositioning
        );
      }
    };

    // Start the animation
    draw();

    // Clean up the animation on unmount
    return () => {
      if (animationRef.current) {
        cancelAnimationFrame(animationRef.current);
      }
    };
  }, [
    analyserNode,
    colorName,
    isActive,
    animationProgress,
    showPlayButton,
    canvasRef,
    positioning,
    clearCanvas,
  ]);

  return {};
};



================================================
FILE: frontend/src/app/useBackendServerUrl.ts
================================================
import { useEffect, useState } from "react";

export const useBackendServerUrl = () => {
  const [backendServerUrl, setBackendServerUrl] = useState<string | null>(null);

  // Get the backend server URL. This is a bit involved to support different deployment methods.
  useEffect(() => {
    if (typeof window !== "undefined") {
      const isInDocker = ["true", "1"].includes(process.env.NEXT_PUBLIC_IN_DOCKER?.toLowerCase() || "");

      const prefix = isInDocker ? "/api" : "";

      const backendUrl = new URL("", window.location.href);
      if (!isInDocker) {
        backendUrl.port = "8000";
      }
      backendUrl.pathname = prefix;
      backendUrl.search = ""; // strip any query parameters
      setBackendServerUrl(backendUrl.toString().replace(/\/$/, "")); // remove trailing slash
    }
  }, []);

  return backendServerUrl;
};



================================================
FILE: frontend/src/app/useGoogleAnalytics.ts
================================================
import { useEffect, useRef } from "react";
import { LanguageCode, UnmuteConfig } from "./UnmuteConfigurator";
import { sendGAEvent } from "@next/third-parties/google";

interface ConversationAnalyticsInfo {
  voice: string;
  voice_name: string;
  is_custom_voice: boolean;
  instructions: string;
  instructions_type: string;
  instructions_language: LanguageCode;
  is_custom_instructions: boolean;
  start_timestamp_ms: number;
  conversation_uuid: string;
  duration_sec?: number;
}

export function useGoogleAnalytics({
  shouldConnect,
  unmuteConfig,
}: {
  shouldConnect: boolean;
  unmuteConfig: UnmuteConfig;
}) {
  const conversationAnalyticsInfo = useRef<ConversationAnalyticsInfo | null>(
    null
  );
  const unmuteConfigRef = useRef(unmuteConfig);

  // We keep the unmuteConfig in a ref because the useEffect that depends on it
  // should only run when shouldConnect changes, not when unmuteConfig changes.
  useEffect(() => {
    unmuteConfigRef.current = unmuteConfig;
  }, [unmuteConfig]);

  useEffect(() => {
    if (shouldConnect) {
      const config = unmuteConfigRef.current;
      const info = {
        voice: config.voice.startsWith("custom:") ? "custom" : config.voice,
        voice_name: config.voiceName,
        is_custom_voice: config.voice.startsWith("custom:"),
        instructions: JSON.stringify(config.instructions),
        instructions_language: config.instructions.language ?? "en",
        instructions_type: config.isCustomInstructions
          ? "constant_custom"
          : config.instructions.type,
        is_custom_instructions: config.isCustomInstructions,
        start_timestamp_ms: Date.now(),
        // Just to make it easy to pair with the end_conversation event
        conversation_uuid: crypto.randomUUID(),
      };
      conversationAnalyticsInfo.current = info;

      sendGAEvent("event", "start_conversation", info);
    } else {
      const info = conversationAnalyticsInfo.current;
      if (info) {
        info.duration_sec = (Date.now() - info.start_timestamp_ms) / 1000;
        sendGAEvent("event", "end_conversation", {
          ...info,
        });
      }
    }
  }, [shouldConnect]);

  const analyticsOnDownloadRecording = () => {
    const info = conversationAnalyticsInfo.current;
    if (info) {
      sendGAEvent("event", "download_recording", {
        ...info,
      });
    }
  };

  return { analyticsOnDownloadRecording };
}



================================================
FILE: frontend/src/app/useKeyboardShortcuts.ts
================================================
import { useEffect, useState } from "react";

const ALLOW_DEV_MODE = false;

const useKeyboardShortcuts = () => {
  // local storage persistence disabled in case random users activate it accidentally
  // useLocalStorage("useDevMode", false)
  const [isDevMode, setIsDevMode] = useState(false);
  // useLocalStorage("showSubtitles", false)
  const [showSubtitles, setShowSubtitles] = useState(false);

  useEffect(() => {
    const handleKeyDown = (event: KeyboardEvent) => {
      const activeElement = document.activeElement;
      // Don't toggle dev mode if the active element is an input field
      const isInputField =
        activeElement &&
        (activeElement.tagName === "INPUT" ||
          activeElement.tagName === "TEXTAREA" ||
          activeElement.getAttribute("contenteditable") === "true");

      if (
        ALLOW_DEV_MODE &&
        !isInputField &&
        (event.key === "D" || event.key === "d")
      ) {
        setIsDevMode((prev) => !prev);
      }
      if (!isInputField && (event.key === "S" || event.key === "s")) {
        setShowSubtitles((prev) => !prev);
      }
    };

    window.addEventListener("keydown", handleKeyDown);
    return () => {
      window.removeEventListener("keydown", handleKeyDown);
    };
  }, [setIsDevMode, setShowSubtitles]);

  return { isDevMode, showSubtitles };
};

export default useKeyboardShortcuts;



================================================
FILE: frontend/src/app/useLocalStorage.ts
================================================
import { useState, useEffect } from "react";

export const useLocalStorage = <T>(
  key: string,
  defaultValue: T
): [T, React.Dispatch<React.SetStateAction<T>>] => {
  const [value, setValue] = useState<T>(defaultValue);

  useEffect(() => {
    const saved = localStorage.getItem(key);
    const initial = saved ? (JSON.parse(saved) as T) : defaultValue;
    setValue(initial);
  }, [key, defaultValue]);

  useEffect(() => {
    localStorage.setItem(key, JSON.stringify(value));
  }, [key, value]);

  return [value, setValue];
};



================================================
FILE: frontend/src/app/useMicrophoneAccess.ts
================================================
import { useState, useCallback, useRef } from "react";

type MicrophoneAccessType = "unknown" | "granted" | "refused";

export const useMicrophoneAccess = () => {
  const [microphoneAccess, setMicrophoneAccess] =
    useState<MicrophoneAccessType>("unknown");

  const mediaStream = useRef<MediaStream | null>(null);

  const askMicrophoneAccess = useCallback(async () => {
    try {
      mediaStream.current = await window.navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          echoCancellation: true,
          autoGainControl: true,
          noiseSuppression: true,
        }
      });
      setMicrophoneAccess("granted");
      return mediaStream.current;
    } catch (e) {
      console.error(e);
      setMicrophoneAccess("refused");
      return null;
    }
  }, []);

  return {
    microphoneAccess,
    askMicrophoneAccess,
    mediaStream,
  };
};



================================================
FILE: frontend/src/app/useRecordingCanvas.ts
================================================
import { useRef, useEffect, useCallback, useState } from "react";
import { AudioProcessor } from "./useAudioProcessor";
import { useAudioVisualizerCircle } from "./useAudioVisualizerCircle";
import { ChatMessage } from "./chatHistory";
import { getCSSVariable } from "./cssUtil";
import kyutaiLogo from "../assets/kyutai-logo-cropped.svg";

const getFilename = () => {
  const now = new Date();
  const pad = (n: number) => n.toString().padStart(2, "0");
  const dateStr = `${now.getFullYear()}-${pad(now.getMonth() + 1)}-${pad(
    now.getDate()
  )}_${pad(now.getHours())}-${pad(now.getMinutes())}-${pad(now.getSeconds())}`;
  return `unmute-${dateStr}.webm`;
};

export function useRecordingCanvas({
  size,
  shouldRecord,
  audioProcessor,
  chatHistory,
}: {
  size: number;
  shouldRecord: boolean;
  audioProcessor: AudioProcessor | null;
  chatHistory: ChatMessage[];
}) {
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const animationFrameRef = useRef<number>(0);

  // Recording
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const [recordedChunks, setRecordedChunks] = useState<Blob[]>([]);

  const logoImageRef = useRef<HTMLImageElement | null>(null);
  const [logoLoaded, setLogoLoaded] = useState(false);

  useEffect(() => {
    const img = new window.Image();
    img.src = kyutaiLogo.src;
    img.onload = () => setLogoLoaded(true);
    logoImageRef.current = img;
  }, []);

  useAudioVisualizerCircle(canvasRef, {
    chatHistory,
    role: "assistant",
    analyserNode: audioProcessor?.outputAnalyser || null,
    isConnected: true,
    showPlayButton: false,
    positioning: {
      centerX: size * 0.33,
      centerY: size * 0.38,
      radius: size * 0.3,
    },
    clearCanvas: false,
  });

  useAudioVisualizerCircle(canvasRef, {
    chatHistory,
    role: "user",
    analyserNode: audioProcessor?.inputAnalyser || null,
    isConnected: true,
    showPlayButton: false,
    positioning: {
      centerX: size * 0.7,
      centerY: size * 0.67,
      radius: size * 0.2,
    },
    clearCanvas: false,
  });

  const downloadRecording = useCallback(
    (asNewTab: boolean) => {
      if (!shouldRecord) {
        if (recordedChunks.length === 0) {
          throw new Error("No recording available to download.");
        }

        const blob = new Blob(recordedChunks, { type: "video/webm" });
        const url = URL.createObjectURL(blob);
        if (asNewTab) {
          window.open(url, "_blank");
        } else {
          const filename = getFilename();

          const a = document.createElement("a");
          a.href = url;
          a.download = filename;
          document.body.appendChild(a);
          a.click();
          setTimeout(() => {
            document.body.removeChild(a);
            URL.revokeObjectURL(url);
          }, 100);
        }

        setRecordedChunks([]);
      }
    },
    [recordedChunks, shouldRecord]
  );

  const animate = useCallback(() => {
    const ctx = canvasRef.current?.getContext("2d");
    if (!ctx) return;

    ctx.fillStyle = getCSSVariable("--color-background");
    ctx.fillRect(0, 0, size, size);

    ctx.font = "bold 80px 'Frank Ruhl Libre'";
    ctx.fillStyle = "white";
    ctx.textAlign = "center";
    ctx.textBaseline = "middle";
    ctx.fillText("Unmute.sh", size * 0.7, size * 0.1);

    // Draw Kyutai logo underneath the text
    if (logoLoaded && logoImageRef.current) {
      // Position: center under the text, scale to fit nicely
      const logoWidth = size * 0.25;
      const logoHeight =
        (logoImageRef.current.height / logoImageRef.current.width) * logoWidth;
      const logoX = size * 0.745 - logoWidth / 2;
      const logoY = size * 0.1 + 50; // 50px below the text, adjust as needed
      ctx.drawImage(logoImageRef.current, logoX, logoY, logoWidth, logoHeight);
    }

    animationFrameRef.current = requestAnimationFrame(animate);
  }, [size, logoLoaded]);

  // Initialize canvas and start animation
  useEffect(() => {
    const canvas = canvasRef.current;
    const ctx = canvas?.getContext("2d");

    if (!canvas || !ctx) return;

    if (shouldRecord && audioProcessor) {
      const mediaStreamCombined = new MediaStream([
        ...canvas.captureStream(30).getTracks(),
        // ...assistantCanvasRef.current.captureStream(30).getTracks(),
        ...audioProcessor.mediaStreamDestination.stream.getAudioTracks(),
      ]);

      if (!mediaStreamCombined) {
        console.error("No media stream available");
        return;
      }

      // Supported MIME types are different for each browser
      let mimeType = "";
      const mimeTypesToTry = [
        "video/webm;codecs=vp8,opus", // Needed for Firefox
        "video/webm;codecs=vp9",
        "video/webm;codecs=vp8",
        "video/webm",
      ];
      for (const codec of mimeTypesToTry) {
        if (MediaRecorder.isTypeSupported(codec)) {
          mimeType = codec;
          break;
        }
      }

      mediaRecorderRef.current = new MediaRecorder(
        mediaStreamCombined,
        mimeType ? { mimeType } : undefined
      );

      setRecordedChunks([]);
      mediaRecorderRef.current.ondataavailable = (event) => {
        if (event.data.size > 0) {
          setRecordedChunks((chunks) => [...chunks, event.data]);
        }
      };
      mediaRecorderRef.current.start();
    } else {
      mediaRecorderRef.current?.stop();
    }

    // Set canvas size
    canvas.width = size;
    canvas.height = size;

    animate();

    return () => {
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
    };
  }, [
    size,
    // It's important to have shouldRecord here because when the hook is first
    // declared, the canvasRef is not available yet. We need to call this useEffect
    // later somehow, and shouldRecord is a good way to do that.
    shouldRecord,
    audioProcessor,
    animate,
  ]);

  return {
    canvasRef,
    downloadRecording,
    recordingAvailable: recordedChunks.length > 0,
  };
}



================================================
FILE: frontend/src/app/useWakeLock.ts
================================================
import { useEffect, useRef } from "react";

const useWakeLock = (shouldPreventSleep: boolean) => {
  const wakeLockRef = useRef<WakeLockSentinel | null>(null);

  useEffect(() => {
    const requestWakeLock = async () => {
      try {
        if ("wakeLock" in navigator && shouldPreventSleep) {
          wakeLockRef.current = await navigator.wakeLock.request("screen");
        }
      } catch (err) {
        console.error("Failed to acquire wake lock:", err);
      }
    };

    const releaseWakeLock = () => {
      if (wakeLockRef.current) {
        wakeLockRef.current.release().catch((err) => {
          console.error("Failed to release wake lock:", err);
        });
        wakeLockRef.current = null;
      }
    };

    if (shouldPreventSleep) {
      requestWakeLock();
    } else {
      releaseWakeLock();
    }

    return () => {
      releaseWakeLock();
    };
  }, [shouldPreventSleep]);
};

export default useWakeLock;



================================================
FILE: frontend/src/app/VoiceAttribution.tsx
================================================
import Link from "next/link";
import { VoiceSample } from "./UnmuteConfigurator";

const VoiceAttribution = ({ voice }: { voice: VoiceSample }) => {
  const inner = () => {
    if (voice.source.source_type === "file") {
      if (voice.source.description_link) {
        return (
          <Link
            href={voice.source.description_link}
            className="underline"
            target="_blank"
            rel="noopener"
          >
            {voice.source.description ||
              "Source: " + voice.source.description_link}
          </Link>
        );
      } else if (voice.source.description) {
        return <>{voice.source.description}</>;
      } else {
        // No description or link provided
        return <></>;
      }
    } else {
      return (
        <>
          The &apos;{voice.name}&apos; voice is based on{" "}
          <Link
            href={voice.source.url}
            className="underline"
            target="_blank"
            rel="noopener"
          >
            this Freesound by {voice.source.sound_instance.username}
          </Link>
          .
        </>
      );
    }
  };
  return <div className="mt-2">{inner()}</div>;
};

export default VoiceAttribution;



================================================
FILE: frontend/src/app/VoiceRecorder.tsx
================================================
import { useRef, useState } from "react";
import SlantedButton from "./SlantedButton";
import { convertWebmToWav } from "./audioUtil";
import { Mic } from "lucide-react";
import clsx from "clsx";

export const MIC_RECORDING_FILENAME = "unmute-mic-recording.wav";

export type RecordedAudio = {
  blobUrl: string;
  file: File;
};

const VoiceRecording = ({
  setRecordedAudio,
  setError,
  recordingDurationSec,
  onRecordingStarted,
  showProgress = true,
}: {
  setRecordedAudio: (recordedAudio: RecordedAudio) => void;
  setError: (error: string | null) => void;
  recordingDurationSec: number;
  onRecordingStarted?: () => void;
  showProgress?: boolean;
}) => {
  const [isRecording, setIsRecording] = useState(false);
  const [mediaRecorder, setMediaRecorder] = useState<MediaRecorder | null>(
    null
  );
  const [recordingProgress, setRecordingProgress] = useState(0);
  const recordingIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const [recordedAudioLocal, setRecordedAudioLocal] =
    useState<RecordedAudio | null>(null);

  const handleStartRecording = async () => {
    setError(null);
    onRecordingStarted?.();
    setRecordingProgress(0);
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

      // Prefer audio/wav if supported. The backend can't handle webm, so we need to convert it.
      // If neither is supported, don't specify and hope for the best. (That seems to work on Safari.)
      let mimeType = "";
      if (MediaRecorder.isTypeSupported("audio/wav")) {
        mimeType = "audio/wav";
      } else if (MediaRecorder.isTypeSupported("audio/webm")) {
        mimeType = "audio/webm";
      }

      const recorder = new MediaRecorder(stream, { mimeType });
      audioChunksRef.current = [];
      recorder.ondataavailable = (e) => {
        if (e.data.size > 0) {
          audioChunksRef.current.push(e.data);
        }
      };
      recorder.onstop = async () => {
        setRecordingProgress(0);
        if (recordingIntervalRef.current) {
          clearInterval(recordingIntervalRef.current);
        }
        const audioBlob = new Blob(audioChunksRef.current, { type: mimeType });

        let audioFile: File;
        if (mimeType === "audio/wav") {
          audioFile = new File([audioBlob], MIC_RECORDING_FILENAME, {
            type: "audio/wav",
          });
        } else {
          const wavBlob = await convertWebmToWav(audioBlob);
          audioFile = new File([wavBlob], MIC_RECORDING_FILENAME, {
            type: "audio/wav",
          });
        }
        const recordedAudio: RecordedAudio = {
          blobUrl: URL.createObjectURL(audioFile),
          file: audioFile,
        };
        setRecordedAudio(recordedAudio);
        setRecordedAudioLocal(recordedAudio);
      };
      recorder.start();
      setMediaRecorder(recorder);
      setIsRecording(true);

      const start = Date.now();
      recordingIntervalRef.current = setInterval(() => {
        const elapsed = (Date.now() - start) / 1000;
        setRecordingProgress(Math.min(elapsed / recordingDurationSec, 1));
      }, 50);

      setTimeout(() => {
        if (recorder.state === "recording") {
          recorder.stop();
          setIsRecording(false);
          setMediaRecorder(null);
        }
      }, recordingDurationSec * 1000);
    } catch (err) {
      setError(
        err instanceof Error ? err.message : "Could not access microphone."
      );
    }
  };

  const handleStopRecording = () => {
    if (mediaRecorder) {
      mediaRecorder.stop();
      setIsRecording(false);
      setMediaRecorder(null);
    }
    setRecordingProgress(0);
    if (recordingIntervalRef.current) {
      clearInterval(recordingIntervalRef.current);
    }
  };

  return (
    <div>
      <div className="flex gap-2 items-center">
        <div className="w-full flex justify-center">
          <SlantedButton
            onClick={isRecording ? handleStopRecording : handleStartRecording}
            kind={
              isRecording || recordedAudioLocal != null
                ? "secondary"
                : "primary"
            }
            extraClasses="flex items-center gap-2"
          >
            {isRecording ? (
              "● Recording"
            ) : (
              <>
                <Mic size={24} />
                Record
              </>
            )}
          </SlantedButton>
        </div>
      </div>
      {showProgress && (
        <div
          className={clsx(
            "w-full h-2 overflow-hidden mt-2",
            isRecording ? "bg-lightgray" : "bg-transparent"
          )}
        >
          <div
            className="h-full bg-red transition-all duration-50"
            style={{ width: `${recordingProgress * 100}%` }}
          ></div>
        </div>
      )}
      {recordedAudioLocal && !isRecording && (
        <audio
          controls
          src={recordedAudioLocal.blobUrl}
          className="w-full mt-2"
        />
      )}
    </div>
  );
};

export default VoiceRecording;



================================================
FILE: frontend/src/app/VoiceUpload.tsx
================================================
import { useState } from "react";
import { Upload } from "lucide-react";
import VoiceButton from "./SquareButton";
import Modal from "./Modal";
import SlantedButton from "@/app/SlantedButton";
import Link from "next/link";
import VoiceRecorder, { MIC_RECORDING_FILENAME } from "./VoiceRecorder";

// Also checked on the backend, see constant of the same name
const MAX_VOICE_FILE_SIZE_MB = 4;

const VoiceUpload = ({
  backendServerUrl,
  onCustomVoiceUpload,
  isSelected,
}: {
  backendServerUrl: string;
  onCustomVoiceUpload: (name: string) => void;
  isSelected: boolean;
}) => {
  const [file, setFile] = useState<File | null>(null);
  const [error, setError] = useState<string | null>(null);
  // Increment this to force the modal to close
  const [closeSignal, setCloseSignal] = useState(0);
  const [isUploading, setIsUploading] = useState(false);

  const handleFileChange = (event: React.ChangeEvent<HTMLInputElement>) => {
    const selectedFile = event.target.files?.[0];
    if (selectedFile) {
      if (selectedFile.size > MAX_VOICE_FILE_SIZE_MB * 1024 * 1024) {
        setError(`File size must be less than ${MAX_VOICE_FILE_SIZE_MB} MB.`);
        setFile(null);
      } else {
        setError(null);
        setFile(selectedFile);
      }
    }
  };

  const handleUpload = async () => {
    if (!file) {
      setError("Please select a file to upload.");
      return;
    }

    setIsUploading(true);
    setError(null);

    const formData = new FormData();
    formData.append("file", file);

    try {
      const response = await fetch(`${backendServerUrl}/v1/voices`, {
        method: "POST",
        body: formData,
      });

      if (!response.ok) {
        throw new Error(
          `Failed to upload file (${response.status} ${response.statusText}).`
        );
      }

      const data = await response.json();
      if (data.name) {
        onCustomVoiceUpload(data.name);
      } else {
        throw new Error("Invalid response from server.");
      }
      setCloseSignal((prev) => prev + 1);

      // Clear the file so that the next time we open the modal, we don't see the old file.
      // You can still keep using the old file by closing the modal without uploading anything.
      // The delay is prevent a flash before the modal closes.
      setTimeout(() => {
        setFile(null);
      }, 1000);
    } catch (err) {
      setError(
        err instanceof Error ? err.message : "An unknown error occurred."
      );
    }
    setIsUploading(false);
  };

  return (
    <Modal
      trigger={
        <VoiceButton
          kind={isSelected ? "primary" : "secondary"}
          extraClasses="w-full bg-gray md:bg-black"
        >
          Upload <Upload size={16} className="inline" />
        </VoiceButton>
      }
      forceFullscreen={true}
      closeSignal={closeSignal}
    >
      <div className="flex flex-col gap-3">
        <p>
          Upload a voice sample to use as Unmute{"'"}s voice. The first 10
          seconds of the audio are used. The audio file may be at most{" "}
          {MAX_VOICE_FILE_SIZE_MB}&nbsp;MB. The TTS mimics the audio quality as
          well, so use a high-quality recording. Post-processing your recording
          using a tool like{" "}
          <Link
            href="https://podcast.adobe.com/en/enhance"
            className="underline"
            target="_blank"
            rel="noopener"
          >
            Adobe Podcast AI
          </Link>{" "}
          is usually enough.
        </p>
        <p>
          We keep the embedding of the voice on our server for 1 hour. We do not
          store the uploaded audio itself.
        </p>
        <p>
          We provide this voice cloning ability for experimental and educational
          purposes only. Use responsibly.
        </p>
        {/* Commented out for now until we have more clarity. */}
        <p className="mb-2">
          You can also help us by{" "}
          <Link href="/voice-donation" className="underline text-green">
            anonymously donating your voice
          </Link>{" "}
          to be released alongside the open-source release of our TTS model.
        </p>
        {!file && (
          <div className="flex flex-row gap-2 justify-center">
            <div className="relative">
              <input
                type="file"
                accept="audio/*"
                onChange={handleFileChange}
                className="absolute inset-0 w-full h-full opacity-0 cursor-pointer z-50"
              />
              <SlantedButton kind="primary">Choose Audio File</SlantedButton>
            </div>
            <VoiceRecorder
              setRecordedAudio={(recordedAudio) => {
                setFile(recordedAudio.file);
              }}
              setError={setError}
              recordingDurationSec={10}
            />
          </div>
        )}
        {file && (
          <>
            {file.name !== MIC_RECORDING_FILENAME && (
              <div className="text-sm text-lightgray">
                Selected file: <strong>{file.name}</strong>
              </div>
            )}
            <div className="flex flex-row justify-center">
              <SlantedButton kind="secondary" onClick={() => setFile(null)}>
                Remove
              </SlantedButton>
              <SlantedButton
                onClick={handleUpload}
                kind={file != null && !isUploading ? "primary" : "disabled"}
                extraClasses="grow"
              >
                {isUploading
                  ? "Uploading..."
                  : file && file.name === MIC_RECORDING_FILENAME
                  ? "Select recording"
                  : "Select"}
              </SlantedButton>
            </div>
          </>
        )}
        {error && <p className="text-red text-sm mt-2">Error: {error}</p>}
      </div>
    </Modal>
  );
};

export default VoiceUpload;



================================================
FILE: frontend/src/app/voice-donation/DonationConsent.tsx
================================================
import React from "react";

const GreenLink = ({
  href,
  children,
}: {
  href: string;
  children: React.ReactNode;
}) => (
  <a
    href={href}
    target="_blank"
    rel="noopener noreferrer"
    className="text-green underline"
  >
    {children}
  </a>
);

const DonationConsent = ({
  setConsentGiven,
}: {
  setConsentGiven: (value: boolean) => void;
}) => {
  const [checks, setChecks] = React.useState([false, false, false]);

  React.useEffect(() => {
    setConsentGiven(checks.every(Boolean));
  }, [checks, setConsentGiven]);

  const handleCheck =
    (idx: number) => (e: React.ChangeEvent<HTMLInputElement>) => {
      const updated = [...checks];
      updated[idx] = e.target.checked;
      setChecks(updated);
    };

  return (
    <div className="flex flex-col gap-2 my-4">
      <label className="flex items-start gap-2">
        <input
          type="checkbox"
          checked={checks[0]}
          onChange={handleCheck(0)}
          className="mt-1.5"
        />
        <span>
          I am at least 18 years old, I am also of legal age in my country of residence and I
          have read and I agree with Kyutai’s{" "}
          <GreenLink href="/voice-donation/terms-of-use">Terms</GreenLink> and{" "}
          <GreenLink href="/voice-donation/privacy-policy">
            Privacy Policy
          </GreenLink>
          . <span className="text-red">*</span>
        </span>
      </label>
      <label className="flex items-start gap-2">
        <input
          type="checkbox"
          checked={checks[1]}
          onChange={handleCheck(1)}
          className="mt-1.5"
        />
        <span>
          I authorize Kyutai to collect, process and publish worldwide my voice
          recordings and embeddings as part of public datasets under a CC0
          license or similar open-source license, in accordance with Kyutai’s{" "}
          <GreenLink href="/voice-donation/privacy-policy">
            Privacy Policy
          </GreenLink>
          . <span className="text-red">*</span>
        </span>
      </label>
      <label className="flex items-start gap-2">
        <input
          type="checkbox"
          checked={checks[2]}
          onChange={handleCheck(2)}
          className="mt-1.5"
        />
        <span>
          I authorize Kyutai to use my voice recording and embedding worldwide
          to develop and train Kyutai’s AI models and make them available to the
          public, in accordance with Kyutai’s{" "}
          <GreenLink href="/voice-donation/privacy-policy">
            Privacy Policy
          </GreenLink>
          . <span className="text-red">*</span>
        </span>
      </label>
    </div>
  );
};

export default DonationConsent;



================================================
FILE: frontend/src/app/voice-donation/IntroText.mdx
================================================
Here you can donate a short recording of your voice in the context of our Unmute Voice Donation Project, an <span className="text-green">open-source text-to-speech initiative</span>.
These voices will be made available for use with Kyutai TTS.
You can find the already-released voices in the [voice repository](https://huggingface.co/kyutai/tts-voices).
By sharing a short recording of your voice, you help Kyutai to:

- Provide voices for our open-science text-to-speech (TTS) models.
- Build open vocal datasets that anyone can access and reuse.

We value your privacy and transparency. Before proceeding, please review
the following carefully:

- Your voice recordings and data including voice embeddings
  (representations of vocal characteristics) may be collected,
  processed, and published openly by Kyutai. The resulting datasets will
  be publicly available under the Creative Commons CC0 license or any
  similar open-source license, allowing anyone to freely reuse them,
  subject to their compliance and with our Acceptable Use Policy.
- Your voice may be made available for use with Kyutai Text-To-Speech.
  As a result, third parties could generate synthetic speech that
  closely resembles your voice. After public release, each third-party
  user will be directly responsible for its own use of your voice
  recording. If you do not want people to reuse your voice and reproduce
  it, you should not submit your voice recording.

For more information, see the [Terms of Use](/voice-donation/terms-of-use) and
[Privacy Policy](/voice-donation/privacy-policy).

## Verification

<div className="w-full flex flex-row mt-2">
  <div className="border-1 border-green py-2 px-4 text-center">
    Verification sentences
  </div>
  <div className="border-1 border-green py-2 px-4 text-center grow-2">
    Whatever you want (last 10 seconds will be used)
  </div>
</div>

To verify that this is your voice and not a pre-recorded sample, we will
ask you to read a short text out loud. Afterwards, you can say whatever
you want. Have fun with it! The TTS is good at reproducing the tone and
mannerisms of the voice. The last 10 seconds of your recording will be
used as the voice sample. Try to use the same tone throughout the
recording to make it easier to verify that it's you.



================================================
FILE: frontend/src/app/voice-donation/page.tsx
================================================
"use client";
import { useState } from "react";
import SlantedButton from "../SlantedButton";
import { useBackendServerUrl } from "../useBackendServerUrl";
import ErrorMessages, { ErrorItem, makeErrorItem } from "../ErrorMessages";
import VoiceRecording, { RecordedAudio } from "../VoiceRecorder";
import Link from "next/link";
import IntroText from "./IntroText.mdx";
import DonationConsent from "./DonationConsent";

type VoiceDonationVerification = {
  id: string;
  text: string;
  created_at_timestamp: number; // Seconds since epoch
};

export default function VoiceDonation() {
  const [errors, setErrors] = useState<ErrorItem[]>([]);
  const backendServerUrl = useBackendServerUrl();
  const [verification, setVerification] =
    useState<VoiceDonationVerification | null>(null);
  const [recordedAudio, setRecordedAudio] = useState<RecordedAudio | null>(
    null
  );
  const [consentGiven, setConsentGiven] = useState(false);
  const [email, setEmail] = useState("");
  const [nickname, setNickname] = useState("");

  const [uploadState, setUploadState] = useState<
    "not_started" | "uploading" | "finished"
  >("not_started");

  const addError = (error: string) => {
    setErrors((prev) => [...prev, makeErrorItem(error)]);
  };

  const onRecordingStarted = async () => {
    if (!backendServerUrl) return;

    try {
      // This doesn't actually exist on the backend yet
      const response = await fetch(`${backendServerUrl}/v1/voice-donation`);
      if (!response.ok) {
        throw new Error("Failed to get voice donation verification.");
      }
      const data = await response.json();
      setVerification(data);
    } catch (error) {
      // console.error("Error fetching voice donation verification:", error);
      setErrors((prev) => [
        ...prev,
        makeErrorItem(
          error instanceof Error
            ? error.message
            : "Failed to start voice donation."
        ),
      ]);
    }
  };

  const handleSubmit = async () => {
    if (!recordedAudio) {
      addError("You haven't recorded your voice yet.");
      return;
    }
    if (!verification) {
      addError("No active voice donation verification.");
      return;
    }
    if (!consentGiven) {
      addError("You must give consent to submit your voice.");
      return;
    }

    setUploadState("uploading");

    const formData = new FormData();
    formData.append("file", recordedAudio.file);

    const metadata = {
      email: email,
      nickname: nickname,
      verification_id: verification?.id || null,
    };
    formData.append("metadata", JSON.stringify(metadata));

    try {
      // This doesn't actually exist on the backend yet
      const response = await fetch(`${backendServerUrl}/v1/voice-donation`, {
        method: "POST",
        body: formData,
      });

      if (!response.ok) {
        const error = await response.json();
        addError(error.detail);
        setUploadState("not_started");
        return;
      }

      const data = await response.json();
      console.log("Submit response:", data);
    } catch (err) {
      addError(
        err instanceof Error ? err.message : "An unknown error occurred."
      );
    }
    setUploadState("finished");
  };

  if (!backendServerUrl) {
    return (
      <div className="w-full h-screen flex justify-center items-center bg-background">
        <p>Loading...</p>
      </div>
    );
  }

  const validEmail = isValidEmail(email);

  return (
    <div className="w-full min-h-screen flex justify-center bg-background">
      <ErrorMessages errors={errors} setErrors={setErrors} />
      <div className="flex flex-col justify-center max-w-xl gap-3 m-2 mb-20">
        <h1 className="text-4xl font-bold mt-4">Voice Donation</h1>
        <p className="italic">
          <Link href="/" className="underline">
            Back to Unmute
          </Link>
        </p>
        {uploadState === "finished" && (
          <>
            <p>Thank you for donating your voice for open science &lt;3</p>
            {verification && (
              <>
                <p>
                  The identifier of your voice donation is{" "}
                  <span className="font-mono font-bold">{verification.id}</span>
                  .
                </p>
                <p>
                  You can use this identifier to find your voice later. It will
                  not be shown again, please save it now. Alternatively, you can
                  contact us at unmute@kyutai.org about your donation, see our{" "}
                  <Link
                    href="/voice-donation/privacy-policy"
                    className="underline text-green"
                  >
                    Privacy Policy
                  </Link>{" "}
                  for more details.
                </p>
                <p>
                  <Link href={"/"} className="underline">
                    Go back to Unmute
                  </Link>
                </p>
              </>
            )}
          </>
        )}
        {uploadState !== "finished" && (
          <>
            <div>
              <IntroText />
            </div>

            {!recordedAudio && (
              <p>
                You&apos;ll have the chance to listen to your recording before
                submitting it.
              </p>
            )}
            <VoiceRecording
              setRecordedAudio={setRecordedAudio}
              setError={(error: string | null) => {
                if (!error) return;
                addError(error);
              }}
              recordingDurationSec={30}
              onRecordingStarted={onRecordingStarted}
              showProgress={false}
            />
            {verification && (
              <div>
                <p>Start by saying:</p>
                <p className="italic">{verification.text}</p>
              </div>
            )}
            {/* {!verification && <div className="mt-20"></div>} */}
            {recordedAudio && (
              <div className="flex flex-col gap-2">
                <label className="flex flex-col gap-1">
                  Email to contact you if needed, or if you choose to withdraw
                  (not published):
                  <input
                    type="text"
                    value={email}
                    onChange={(e) => setEmail(e.target.value)}
                    className="border px-2 py-1 bg-gray text-white"
                  />
                  {!validEmail && email && (
                    <span className="text-red text-sm">
                      Please enter a valid email address.
                    </span>
                  )}
                </label>
                <label className="flex flex-col gap-1">
                  (Optional) Preferred nickname for the voice if published:
                  <input
                    type="text"
                    value={nickname}
                    onChange={(e) => setNickname(e.target.value)}
                    className="border px-2 py-1 bg-gray text-white"
                  />
                </label>
                <DonationConsent setConsentGiven={setConsentGiven} />
                <SlantedButton
                  kind={
                    consentGiven && validEmail && uploadState === "not_started"
                      ? "primary"
                      : "disabled"
                  }
                  onClick={handleSubmit}
                >
                  {uploadState === "uploading" ? "Uploading..." : "Submit"}
                </SlantedButton>
              </div>
            )}
          </>
        )}
      </div>
    </div>
  );
}

function isValidEmail(email: string): boolean {
  // Basic email regex for validation
  return /^[^\s@]+@[^\s@]+\.[^\s@]+$/.test(email.trim());
}



================================================
FILE: frontend/src/app/voice-donation/privacy-policy/page.tsx
================================================
// A redirection page, set up so that we can change the URL it points to later if needed.
"use client";
import { useEffect } from "react";

const LINK =
  "https://kyutai.org/next/legal/Privacy%20Policy%20-%20Unmute%20Voice%20Donation%20Project%20v1.pdf";

export default function TermsOfUseRedirect() {
  useEffect(() => {
    window.location.href = LINK;
  }, []);
  return (
    <div>
      <p>Redirecting to Terms of Use PDF...</p>
      <a href={LINK}>Click here if not redirected.</a>
    </div>
  );
}



================================================
FILE: frontend/src/app/voice-donation/terms-of-use/page.tsx
================================================
// A redirection page, set up so that we can change the URL it points to later if needed.
"use client";
import { useEffect } from "react";

const LINK =
  "https://kyutai.org/next/legal/Terms%20of%20Use%20-%20Unmute%20Voice%20Donation%20Project%20v1.pdf";

export default function PrivacyPolicyRedirect() {
  useEffect(() => {
    window.location.href = LINK;
  }, []);
  return (
    <div>
      <p>Redirecting to Privacy Policy PDF...</p>
      <a href={LINK}>Click here if not redirected.</a>
    </div>
  );
}



================================================
FILE: frontend/src/assets/fonts/Satoshi-Variable.woff
================================================
[Binary file]


================================================
FILE: frontend/src/assets/fonts/Satoshi-Variable.woff2
================================================
[Binary file]


================================================
FILE: frontend/src/assets/fonts/Satoshi-VariableItalic.woff
================================================
[Binary file]


================================================
FILE: frontend/src/assets/fonts/Satoshi-VariableItalic.woff2
================================================
[Binary file]


================================================
FILE: notebooks/create-voice-donation-sentences.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Create voice donation sentences

When people donate their voice, we ask them to read some verification sentences so that it's not possible to just use a pre-made recording.

This notebook selects sentences by filtering from CommonVoice.

"""

import pandas as pd

# Download here:
# https://commonvoice.mozilla.org/en/datasets select "Common Voice Delta Segment 21.0"
df = pd.read_csv(
    "./data/cv-corpus-21.0-delta-2025-03-14/en/validated_sentences.tsv", sep="\t"
)

def count_uppercase_letters(s):
    return sum(1 for c in s if c.isupper())


def is_ascii(s):
    return all(ord(c) < 128 for c in s)


def max_word_length(s):
    if not s.strip():
        return 0

    return max(len(word) for word in s.split())


def is_ok_sentence(s):
    return (
        is_ascii(s)
        # Exclude long complicated words
        and max_word_length(s) <= 10
        # Proper names might be difficult to pronounce to non-native speakers
        # and are harder to check automatically, so we exclude them. The one capital
        # letter allowed is for the first letter of the sentence.
        and count_uppercase_letters(s) == 1
        and 30 <= len(s) <= 80
        # No questions or exclamations
        and s[-1] == "."
    )


df["is_ok_sentence"] = df["sentence"].apply(is_ok_sentence)

df.loc[df["is_ok_sentence"], "sentence"]

sum(df["is_ok_sentence"])

sentences = df.loc[df["is_ok_sentence"], "sentence"].tolist()[:10000]

with open("../unmute/tts/voice_donation_sentences.txt", "w") as f:
    for sentence in sentences:
        f.write(sentence + "\n")



================================================
FILE: services/debugger/Dockerfile
================================================
FROM ubuntu:22.04

# Install many tools useful for debugging the network
RUN apt-get update && \
    apt-get install -y \
    iputils-ping \
    iproute2 \
    net-tools \
    curl \
    wget \
    dnsutils \
    traceroute \
    tcpdump \
    nmap \
    telnet \
    vim \
    less \
    git \
    && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*


COPY --from=ghcr.io/astral-sh/uv:0.7.2 /uv /uvx /bin/

RUN curl https://sh.rustup.rs -sSf | sh -s -- -y
ENV PATH="/root/.cargo/bin:${PATH}"



================================================
FILE: services/grafana/Dockerfile
================================================
FROM grafana/grafana:11.6.1-ubuntu

COPY dashboards /etc/grafana/dashboards
COPY provisioning /etc/grafana/provisioning
COPY grafana.ini /etc/grafana/grafana.ini



================================================
FILE: services/grafana/grafana.ini
================================================
[auth.anonymous]
enabled = true
org_role = Admin

[auth.basic]
enabled = false

[auth]
disable_login_form = true


# Don't use US date format. From:
# https://community.grafana.com/t/how-do-we-get-grafana-to-use-this-date-format-dd-mm-yyyy-hh-mm-ss-timestamp/54666/3
[date_formats]
# Default system date format used in time range picker and other places where full time is displayed
full_date = DD-MM-YYYY HH:mm:ss

# Used by graph and other places where we only show small intervals
interval_second = HH:mm:ss
interval_minute = HH:mm
interval_hour = DD/MM HH:mm
interval_day = DD/MM
interval_month = MM-YYYY
interval_year = YYYY



================================================
FILE: services/grafana/dashboards/unmute-monitoring-1751624072717.json
================================================
{
  "annotations": {
    "list": [
      {
        "builtIn": 1,
        "datasource": {
          "type": "grafana",
          "uid": "-- Grafana --"
        },
        "enable": true,
        "hide": true,
        "iconColor": "rgba(0, 211, 255, 1)",
        "name": "Annotations & Alerts",
        "type": "dashboard"
      }
    ]
  },
  "editable": true,
  "fiscalYearStartMonth": 0,
  "graphTooltip": 0,
  "id": 1,
  "links": [],
  "panels": [
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 8,
        "x": 0,
        "y": 0
      },
      "id": 14,
      "options": {
        "legend": {
          "calcs": [
            "lastNotNull"
          ],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "multi",
          "sort": "none"
        }
      },
      "pluginVersion": "11.6.1",
      "targets": [
        {
          "disableTextWrap": false,
          "editorMode": "code",
          "exemplar": false,
          "expr": "sum by(__name__) (increase(worker_sessions_total[1h]))",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "instant": false,
          "legendFormat": "__auto",
          "range": true,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "Unmute sessions (1h window)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "fieldMinMax": false,
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 8,
        "x": 8,
        "y": 0
      },
      "id": 10,
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showPercentChange": false,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "11.6.1",
      "targets": [
        {
          "disableTextWrap": false,
          "editorMode": "builder",
          "expr": "sum(worker_active_sessions)",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "legendFormat": "__auto",
          "range": true,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "Unmute active sessions",
      "type": "stat"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 8,
        "x": 16,
        "y": 0
      },
      "id": 4,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "single",
          "sort": "none"
        }
      },
      "pluginVersion": "11.6.1",
      "targets": [
        {
          "disableTextWrap": false,
          "editorMode": "code",
          "expr": "sum by(__name__) (avg_over_time(asr_open_channels[1h]))",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "interval": "",
          "legendFormat": "STT open channels",
          "range": true,
          "refId": "A",
          "useBackend": false
        },
        {
          "datasource": {
            "type": "prometheus",
            "uid": "dekodsix13x8ga"
          },
          "disableTextWrap": false,
          "editorMode": "code",
          "expr": "sum(avg_over_time(py_open_channels[1h]))",
          "fullMetaSearch": false,
          "hide": false,
          "includeNullMetadata": true,
          "instant": false,
          "interval": "",
          "legendFormat": "TTS open channels",
          "range": true,
          "refId": "B",
          "useBackend": false
        }
      ],
      "title": "STT/TTS active connections (average in 1h window)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 8,
        "x": 0,
        "y": 8
      },
      "id": 18,
      "options": {
        "legend": {
          "calcs": [
            "lastNotNull"
          ],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "multi",
          "sort": "none"
        }
      },
      "pluginVersion": "11.6.1",
      "targets": [
        {
          "disableTextWrap": false,
          "editorMode": "code",
          "exemplar": false,
          "expr": "sum by(__name__) (increase(worker_sessions_total[24h]))",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "instant": false,
          "legendFormat": "__auto",
          "range": true,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "Unmute sessions (24h window)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "description": "",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "s"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 8,
        "x": 8,
        "y": 8
      },
      "id": 15,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "single",
          "sort": "none"
        }
      },
      "pluginVersion": "11.6.1",
      "targets": [
        {
          "editorMode": "code",
          "expr": "histogram_quantile(0.5, sum(rate(worker_session_duration_bucket[24h])) by (le))",
          "legendFormat": "__auto",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Median session duration (24h window)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisBorderShow": false,
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "barWidthFactor": 0.6,
            "drawStyle": "line",
            "fillOpacity": 0,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "insertNulls": false,
            "lineInterpolation": "linear",
            "lineWidth": 1,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "auto",
            "spanNulls": false,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 8,
        "x": 16,
        "y": 8
      },
      "id": 17,
      "options": {
        "legend": {
          "calcs": [
            "lastNotNull"
          ],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "tooltip": {
          "hideZeros": false,
          "mode": "multi",
          "sort": "none"
        }
      },
      "pluginVersion": "11.6.1",
      "targets": [
        {
          "disableTextWrap": false,
          "editorMode": "code",
          "exemplar": false,
          "expr": "sum by(__name__) (increase(py_connect[24h])) / sum by(__name__) (increase(worker_sessions_total[24h]))",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "instant": false,
          "legendFormat": "__auto",
          "range": true,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "TTS connections / Unmute connections ratio",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 8,
        "x": 0,
        "y": 16
      },
      "id": 13,
      "options": {
        "displayMode": "gradient",
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "maxVizHeight": 300,
        "minVizHeight": 16,
        "minVizWidth": 8,
        "namePlacement": "auto",
        "orientation": "auto",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showUnfilled": true,
        "sizing": "auto",
        "valueMode": "color"
      },
      "pluginVersion": "11.6.1",
      "targets": [
        {
          "disableTextWrap": false,
          "editorMode": "code",
          "expr": "sum by(le) (increase(worker_stt_ttft_bucket[1d]))",
          "format": "heatmap",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "legendFormat": "__auto",
          "range": true,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "ASR time to first token (s)",
      "type": "bargauge"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 8,
        "x": 8,
        "y": 16
      },
      "id": 9,
      "options": {
        "displayMode": "gradient",
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "maxVizHeight": 300,
        "minVizHeight": 16,
        "minVizWidth": 8,
        "namePlacement": "auto",
        "orientation": "auto",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showUnfilled": true,
        "sizing": "auto",
        "valueMode": "color"
      },
      "pluginVersion": "11.6.1",
      "targets": [
        {
          "disableTextWrap": false,
          "editorMode": "code",
          "expr": "sum by(le) (increase(worker_vllm_ttft_bucket[1d]))",
          "format": "heatmap",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "legendFormat": "__auto",
          "range": true,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "vLLM time to first token (s)",
      "type": "bargauge"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 8,
        "x": 16,
        "y": 16
      },
      "id": 12,
      "options": {
        "displayMode": "gradient",
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "maxVizHeight": 300,
        "minVizHeight": 16,
        "minVizWidth": 8,
        "namePlacement": "auto",
        "orientation": "auto",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showUnfilled": true,
        "sizing": "auto",
        "valueMode": "color"
      },
      "pluginVersion": "11.6.1",
      "targets": [
        {
          "disableTextWrap": false,
          "editorMode": "code",
          "expr": "sum by(le) (increase(worker_tts_ttft_bucket[1d]))",
          "format": "heatmap",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "legendFormat": "__auto",
          "range": true,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "TTS time to first token (s)",
      "type": "bargauge"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 8,
        "x": 0,
        "y": 24
      },
      "id": 7,
      "options": {
        "displayMode": "gradient",
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "maxVizHeight": 300,
        "minVizHeight": 16,
        "minVizWidth": 8,
        "namePlacement": "auto",
        "orientation": "auto",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showUnfilled": true,
        "sizing": "auto",
        "valueMode": "color"
      },
      "pluginVersion": "11.6.1",
      "targets": [
        {
          "disableTextWrap": false,
          "editorMode": "code",
          "expr": "sum by(le) (increase(worker_stt_num_words_bucket[1d]))",
          "format": "heatmap",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "legendFormat": "__auto",
          "range": true,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "ASR num words",
      "type": "bargauge"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 8,
        "x": 8,
        "y": 24
      },
      "id": 8,
      "options": {
        "displayMode": "gradient",
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "maxVizHeight": 300,
        "minVizHeight": 16,
        "minVizWidth": 8,
        "namePlacement": "auto",
        "orientation": "auto",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showUnfilled": true,
        "sizing": "auto",
        "valueMode": "color"
      },
      "pluginVersion": "11.6.1",
      "targets": [
        {
          "disableTextWrap": false,
          "editorMode": "code",
          "expr": "sum by(le) (increase(worker_vllm_gen_duration_bucket[1d]))",
          "format": "heatmap",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "legendFormat": "__auto",
          "range": true,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "VLLM gen duration (s)",
      "type": "bargauge"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "description": "Each step is 0.08s. Not sure how to convert this to seconds.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 8,
        "x": 16,
        "y": 24
      },
      "id": 6,
      "options": {
        "displayMode": "gradient",
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "maxVizHeight": 300,
        "minVizHeight": 16,
        "minVizWidth": 8,
        "namePlacement": "auto",
        "orientation": "auto",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showUnfilled": true,
        "sizing": "auto",
        "valueMode": "color"
      },
      "pluginVersion": "11.6.1",
      "targets": [
        {
          "disableTextWrap": false,
          "editorMode": "builder",
          "expr": "sum by(le) (increase(py_model_connection_num_steps_bucket[1d]))",
          "format": "heatmap",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "legendFormat": "__auto",
          "range": true,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "TTS num steps per response",
      "type": "bargauge"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "description": "The ASR server runs in a batched mode, for everybody at once. To keep up with real time, the step must be faster than 0.08s.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 8,
        "x": 0,
        "y": 32
      },
      "id": 3,
      "options": {
        "displayMode": "gradient",
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "maxVizHeight": 300,
        "minVizHeight": 16,
        "minVizWidth": 8,
        "namePlacement": "auto",
        "orientation": "vertical",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showUnfilled": true,
        "sizing": "auto",
        "valueMode": "color"
      },
      "pluginVersion": "11.6.1",
      "targets": [
        {
          "disableTextWrap": false,
          "editorMode": "code",
          "expr": "sum by(le) (increase(asr_model_step_duration_bucket[1d]))",
          "format": "heatmap",
          "fullMetaSearch": false,
          "includeNullMetadata": false,
          "legendFormat": "__auto",
          "range": true,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "ASR model step (s)",
      "type": "bargauge"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 8,
        "x": 8,
        "y": 32
      },
      "id": 16,
      "interval": "60m",
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "justifyMode": "auto",
        "orientation": "auto",
        "percentChangeColorMode": "standard",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showPercentChange": false,
        "textMode": "auto",
        "wideLayout": true
      },
      "pluginVersion": "11.6.1",
      "targets": [
        {
          "disableTextWrap": false,
          "editorMode": "code",
          "exemplar": false,
          "expr": "sum by(__name__) (increase(worker_sessions_total[3650d]))",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "instant": false,
          "legendFormat": "__auto",
          "range": true,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "Total connections within log retention range",
      "type": "stat"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "description": "The TTS server runs in a batched mode, for everybody at once. To generate audio in real time, the step must be faster than 0.08s.",
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 8,
        "x": 16,
        "y": 32
      },
      "id": 1,
      "options": {
        "displayMode": "gradient",
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "maxVizHeight": 300,
        "minVizHeight": 16,
        "minVizWidth": 8,
        "namePlacement": "auto",
        "orientation": "auto",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showUnfilled": true,
        "sizing": "auto",
        "valueMode": "color"
      },
      "pluginVersion": "11.6.1",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "dekodsix13x8ga"
          },
          "disableTextWrap": false,
          "editorMode": "code",
          "expr": "sum by(le) (increase(py_model_step_duration_bucket[1d]))",
          "format": "heatmap",
          "fullMetaSearch": false,
          "hide": false,
          "includeNullMetadata": false,
          "instant": false,
          "legendFormat": "__auto",
          "range": true,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "TTS model step (s)",
      "type": "bargauge"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "PBFA97CFB590B2093"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green"
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 8,
        "x": 16,
        "y": 40
      },
      "id": 11,
      "options": {
        "displayMode": "gradient",
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": false
        },
        "maxVizHeight": 300,
        "minVizHeight": 16,
        "minVizWidth": 8,
        "namePlacement": "auto",
        "orientation": "auto",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ],
          "fields": "",
          "values": false
        },
        "showUnfilled": true,
        "sizing": "auto",
        "valueMode": "color"
      },
      "pluginVersion": "11.6.1",
      "targets": [
        {
          "disableTextWrap": false,
          "editorMode": "code",
          "expr": "sum by(le) (increase(worker_session_duration_bucket[1d]))",
          "format": "heatmap",
          "fullMetaSearch": false,
          "includeNullMetadata": true,
          "interval": "",
          "legendFormat": "__auto",
          "range": true,
          "refId": "A",
          "useBackend": false
        }
      ],
      "title": "Session duration (s)",
      "type": "bargauge"
    }
  ],
  "preload": false,
  "refresh": "30s",
  "schemaVersion": 41,
  "tags": [],
  "templating": {
    "list": []
  },
  "time": {
    "from": "now-3h",
    "to": "now"
  },
  "timepicker": {},
  "timezone": "browser",
  "title": "unmute-monitoring",
  "uid": "cep53ia62t05cc",
  "version": 1
}


================================================
FILE: services/grafana/provisioning/dashboards/dashboards.yaml
================================================
apiVersion: 1

providers:
  - name: 'default'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    editable: true
    updateIntervalSeconds: 10
    options:
      path: /etc/grafana/dashboards



================================================
FILE: services/grafana/provisioning/datasources/datasources.yaml
================================================
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    orgId: 1
    url: http://prometheus:9090
    isDefault: true
    editable: true



================================================
FILE: services/moshi-server/private.Dockerfile
================================================
# This is the Kyutai-internal version.
FROM nvidia/cuda:12.1.0-devel-ubuntu22.04 AS base

# Set environment variables to avoid interactive prompts during package installation
ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y \
    curl \
    build-essential \
    ca-certificates \
    libssl-dev \
    git \
    pkg-config \
    cmake \
    openssh-client \
    --no-install-recommends && \
    rm -rf /var/lib/apt/lists/*

RUN curl https://sh.rustup.rs -sSf | sh -s -- -y
ENV PATH="/root/.cargo/bin:$PATH"

COPY --from=ghcr.io/astral-sh/uv:0.7.2 /uv /uvx /bin/

ARG GITHUB_ORG
RUN --mount=type=ssh \
    mkdir -p ~/.ssh && \
    ssh-keyscan github.com >> ~/.ssh/known_hosts && \
    git clone git@github.com:${GITHUB_ORG}/moshi-rs.git /app \
    && cd /app \
    && git checkout 3c4e1696328153c919a6b082118b492deb944e50

WORKDIR /app

# When starting the container for the first time, we need to compile and download
# everything, so disregarding healthcheck failure for 10 minutes is fine.
# We have a volume storing the build cache, so subsequent starts will be faster.
HEALTHCHECK --start-period=10m \
    CMD curl --fail http://localhost:8080/api/build_info || exit 1

EXPOSE 8080
ENV RUST_BACKTRACE=1

COPY . .

ENTRYPOINT ["uv", "run", "--locked", "--project", "./moshi-server", "./start_moshi_server_private.sh"]



================================================
FILE: services/moshi-server/public.Dockerfile
================================================
# This is the public-facing version.
FROM nvidia/cuda:12.8.1-devel-ubuntu22.04 AS base

# Set environment variables to avoid interactive prompts during package installation
ENV DEBIAN_FRONTEND=noninteractive

# Install dependencies, including dos2unix to handle Windows line endings
RUN apt-get update && apt-get install -y \
    curl \
    build-essential \
    ca-certificates \
    libssl-dev \
    git \
    pkg-config \
    cmake \
    wget \
    openssh-client \
    dos2unix \
    --no-install-recommends && \
    rm -rf /var/lib/apt/lists/*

RUN curl https://sh.rustup.rs -sSf | sh -s -- -y
ENV PATH="/root/.cargo/bin:$PATH"

COPY --from=ghcr.io/astral-sh/uv:0.7.2 /uv /uvx /bin/

WORKDIR /app

# When starting the container for the first time, we need to compile and download
# everything, so disregarding healthcheck failure for 10 minutes is fine.
# We have a volume storing the build cache, so subsequent starts will be faster.
HEALTHCHECK --start-period=10m \
    CMD curl --fail http://localhost:8080/api/build_info || exit 1

EXPOSE 8080
ENV RUST_BACKTRACE=1

RUN wget https://raw.githubusercontent.com/kyutai-labs/moshi/bf359af7694add34c13e65d2f009f0cb474d87cc/rust/moshi-server/pyproject.toml
RUN wget https://raw.githubusercontent.com/kyutai-labs/moshi/bf359af7694add34c13e65d2f009f0cb474d87cc/rust/moshi-server/uv.lock

COPY . .

# Ensure the startup script is runnable inside the container.
# This prevents script errors that can happen if the project was cloned on Windows,
# which uses a different text file format (CRLF) than the Linux environment in the container (LF).
RUN dos2unix ./start_moshi_server_public.sh && chmod +x ./start_moshi_server_public.sh

ENTRYPOINT ["uv", "run", "--locked", "--project", "./moshi-server", "./start_moshi_server_public.sh"]



================================================
FILE: services/moshi-server/start_moshi_server_private.sh
================================================
#!/bin/bash
# This is the Kyutai-internal version.
set -ex

export LD_LIBRARY_PATH=$(python3 -c 'import sysconfig; print(sysconfig.get_config_var("LIBDIR"))')

uvx --from 'huggingface_hub[cli]' huggingface-cli login --token $HUGGING_FACE_HUB_TOKEN

cargo run --features=cuda --bin=moshi-server -r -- $@



================================================
FILE: services/moshi-server/start_moshi_server_public.sh
================================================
#!/bin/bash
# This is the public-facing version.
set -ex

export LD_LIBRARY_PATH=$(python3 -c 'import sysconfig; print(sysconfig.get_config_var("LIBDIR"))')

uvx --from 'huggingface_hub[cli]' huggingface-cli login --token $HUGGING_FACE_HUB_TOKEN

CARGO_TARGET_DIR=/app/target cargo install --features cuda moshi-server@0.6.3

# Subtle detail here: We use the full path to `moshi-server` because there is a `moshi-server` binary
# from the `moshi` Python package. We'll fix this conflict soon.
/root/.cargo/bin/moshi-server $@


================================================
FILE: services/moshi-server/configs/stt-prod.toml
================================================
static_dir = "./static/"
log_dir = "/tmp/unmute_logs"
instance_name = "tts"
authorized_ids = ["public_token"]

[modules.asr]
path = "/api/asr-streaming"
type = "BatchedAsr"
lm_model_file = "hf://kyutai/stt-1b-en_fr-candle/model.safetensors"
text_tokenizer_file = "hf://kyutai/stt-1b-en_fr-candle/tokenizer_en_fr_audio_8000.model"
audio_tokenizer_file = "hf://kyutai/stt-1b-en_fr-candle/mimi-pytorch-e351c8d8@125.safetensors"
asr_delay_in_tokens = 6
batch_size = 64  # NOTE: make this smaller if running on one GPU
conditioning_learnt_padding = true
temperature = 0.25

[modules.asr.model]
audio_vocab_size = 2049
text_in_vocab_size = 8001
text_out_vocab_size = 8000
audio_codebooks = 20

[modules.asr.model.transformer]
d_model = 2048
num_heads = 16
num_layers = 16
dim_feedforward = 8192
causal = true
norm_first = true
bias_ff = false
bias_attn = false
context = 750
max_period = 100000
use_conv_block = false
use_conv_bias = true
gating = "silu"
norm = "RmsNorm"
positional_embedding = "Rope"
conv_layout = false
conv_kernel_size = 3
kv_repeat = 1
max_seq_len = 40960

[modules.asr.model.extra_heads]
num_heads = 4
dim = 6



================================================
FILE: services/moshi-server/configs/stt.toml
================================================
static_dir = "./static/"
log_dir = "/tmp/unmute_logs"
instance_name = "tts"
authorized_ids = ["public_token"]

[modules.asr]
path = "/api/asr-streaming"
type = "BatchedAsr"
lm_model_file = "hf://kyutai/stt-1b-en_fr-candle/model.safetensors"
text_tokenizer_file = "hf://kyutai/stt-1b-en_fr-candle/tokenizer_en_fr_audio_8000.model"
audio_tokenizer_file = "hf://kyutai/stt-1b-en_fr-candle/mimi-pytorch-e351c8d8@125.safetensors"
asr_delay_in_tokens = 6
# A higher batch size allows you to serve more users at once, but with a higher latency and memory usage.
batch_size = 1
conditioning_learnt_padding = true
temperature = 0.25

[modules.asr.model]
audio_vocab_size = 2049
text_in_vocab_size = 8001
text_out_vocab_size = 8000
audio_codebooks = 20

[modules.asr.model.transformer]
d_model = 2048
num_heads = 16
num_layers = 16
dim_feedforward = 8192
causal = true
norm_first = true
bias_ff = false
bias_attn = false
context = 750
max_period = 100000
use_conv_block = false
use_conv_bias = true
gating = "silu"
norm = "RmsNorm"
positional_embedding = "Rope"
conv_layout = false
conv_kernel_size = 3
kv_repeat = 1
max_seq_len = 40960

[modules.asr.model.extra_heads]
num_heads = 4
dim = 6



================================================
FILE: services/moshi-server/configs/tts-prod.toml
================================================
static_dir = "./static/"
log_dir = "/tmp/unmute_logs"
instance_name = "tts"
authorized_ids = ["public_token"]

[modules.tts_py]
type = "Py"
path = "/api/tts_streaming"
text_tokenizer_file = "hf://kyutai/tts-1.6b-en_fr/tokenizer_spm_8k_en_fr_audio.model"
batch_size = 16  # NOTE: make this smaller if running on one GPU
text_bos_token = 1

[modules.tts_py.py]
log_folder = "/tmp/unmute_logs"
# We could use replace **/*.safetensors with unmute-prod-website/*.safetensors
# to only get the voices used in Unmute, but we are using the TTS for the demo
# on the project page too and for that we want to load the other voices as well
voice_folder = "hf-snapshot://kyutai/tts-voices/**/*.safetensors"
default_voice = "unmute-prod-website/default_voice.wav"
cfg_coef = 2.0
cfg_is_no_text = true
padding_between = 1
n_q = 24



================================================
FILE: services/moshi-server/configs/tts.toml
================================================
static_dir = "./static/"
log_dir = "/tmp/unmute_logs"
instance_name = "tts"
authorized_ids = ["public_token"]

[modules.tts_py]
type = "Py"
path = "/api/tts_streaming"
text_tokenizer_file = "hf://kyutai/tts-1.6b-en_fr/tokenizer_spm_8k_en_fr_audio.model"
# A higher batch size allows you to serve more users at once, but with a higher latency and memory usage.
batch_size = 2
text_bos_token = 1

[modules.tts_py.py]
log_folder = "/tmp/unmute_logs"
# We could use replace **/*.safetensors with unmute-prod-website/*.safetensors
# to only get the voices used in Unmute, but we are using the TTS for the demo
# on the project page too and for that we want to load the other voices as well
voice_folder = "hf-snapshot://kyutai/tts-voices/**/*.safetensors"
default_voice = "unmute-prod-website/default_voice.wav"
cfg_coef = 2.0
cfg_is_no_text = true
padding_between = 1
n_q = 24



================================================
FILE: services/moshi-server/configs/voice-cloning.toml
================================================
static_dir = "./static/"
log_dir = "$HOME/tmp/tts-logs"
instance_name = "voice"
authorized_ids = ["public_token"]

[modules.voice_py]
type = "PyPost"
path = "/api/voice"

[modules.voice_py.py]
mimi_weight = "hf://kyutai/unmute-voice-cloning/e9d43d50_500_mimi_voice.safetensors"



================================================
FILE: services/prometheus/Dockerfile
================================================
FROM prom/prometheus:v3.1.0

COPY prometheus.yml /etc/prometheus/



================================================
FILE: services/prometheus/prometheus.yml
================================================
scrape_configs:
  - job_name: 'dockerswarm'
    scrape_interval: 5s
    # Read the Docker Swarm api to discover the services and containers
    dockerswarm_sd_configs:
      - host: unix:///var/run/docker.sock
        role: tasks
    relabel_configs:
      # Keep only the tasks that are running
      - source_labels: [__meta_dockerswarm_task_desired_state]
        regex: running
        action: keep
      # Keep only the tasks that have the label prometheus-port
      - source_labels: [__meta_dockerswarm_service_label_prometheus_port]
        regex: .+
        action: keep
      # Rename the job to the service name (but remove the stack name)
      - source_labels: [__meta_dockerswarm_service_name]
        regex: .*_(.+)
        replacement: $1
        target_label: job
      # Set the ip and port where the /metrics are exposed
      - source_labels: [__address__, __meta_dockerswarm_service_label_prometheus_port]
        regex: ([^:]+):\d+;(\d+)
        replacement: $1:$2
        target_label: __address__



================================================
FILE: tests/test_exponential_moving_average.py
================================================
import pytest

from unmute.stt.exponential_moving_average import ExponentialMovingAverage


def test_ema():
    ema = ExponentialMovingAverage(attack_time=0.1, release_time=0.5)
    ema.update(dt=1.0, new_value=0.0)
    assert ema.value == 0.0

    ema.update(dt=0.1, new_value=1.0)
    assert ema.value == pytest.approx(0.5)

    ema.update(dt=0.25, new_value=0.0)
    ema.update(dt=0.25, new_value=0.0)
    assert ema.value == pytest.approx(0.25)

    # Should work even with values different than 0 and 1
    ema.update(dt=0.1, new_value=0.75)
    assert ema.value == pytest.approx(0.5)

    ema.update(dt=1e9, new_value=1.0)
    assert ema.value == pytest.approx(1.0)



================================================
FILE: tests/test_llm_utils.py
================================================
import pytest

from unmute.llm.llm_utils import rechunk_to_words


async def make_iterator(s: str):
    parts = s.split("|")
    for part in parts:
        yield part


@pytest.mark.asyncio
async def test_rechunk_to_words():
    test_strings = [
        "hel|lo| |w|orld",
        "hello world",
        "hello \nworld",
        "hello| |world",
        "hello| |world|.",
        "h|e|l|l|o| |\tw|o|r|l|d|.",
        "h|e|l|l|o\n| |w|o|r|l|d|.",
    ]

    for s in test_strings:
        parts = [x async for x in rechunk_to_words(make_iterator(s))]
        assert parts[0] == "hello"
        assert parts[1] == " world" or parts[1] == " world."

    async def f(s: str):
        x = [x async for x in rechunk_to_words(make_iterator(s))]
        print(x)
        return x

    assert await f("i am ok") == ["i", " am", " ok"]
    assert await f(" i am ok") == [" i", " am", " ok"]
    assert await f(" they are ok") == [" they", " are", " ok"]
    assert await f("  foo bar") == [" foo", " bar"]
    assert await f(" \t foo  bar") == [" foo", " bar"]



================================================
FILE: unmute/audio_input_override.py
================================================
from pathlib import Path

import numpy as np
import sphn

from unmute.kyutai_constants import SAMPLE_RATE


class AudioInputOverride:
    def __init__(self, file: Path):
        data, _sr = sphn.read(file, sample_rate=SAMPLE_RATE)
        assert data.ndim == 2

        if data.dtype != np.int16:
            data = (data * np.iinfo(np.int16).max).astype(np.int16)

        self.data = data
        self.position = 0

    def override(self, original_data: np.ndarray) -> np.ndarray:
        if self.position + original_data.shape[1] > self.data.shape[1]:
            return original_data

        data = self.data[
            :, self.position : self.position + original_data.shape[1]
        ].copy()
        self.position += original_data.shape[1]

        assert data.shape == original_data.shape, (
            f"{data.shape} != {original_data.shape}"
        )
        assert data.dtype == original_data.dtype

        return data



================================================
FILE: unmute/audio_stream_saver.py
================================================
from logging import getLogger
from pathlib import Path

import numpy as np
import sphn

from unmute.kyutai_constants import SAMPLE_RATE

DEBUG_DIR = Path(__file__).parents[1] / "debug"
logger = getLogger(__name__)


class AudioStreamSaver:
    """Collect and save an audio stream. For debugging"""

    def __init__(
        self,
        interval_sec: float = 1.0,
        output_path: str | Path | None = None,
        max_saves: int | None = 1,
    ):
        self.interval_sec = interval_sec
        self.max_saves = max_saves
        self.n_saves_done = 0

        if output_path is None:
            self.output_path = DEBUG_DIR / "out.wav"
        else:
            self.output_path = Path(output_path)

        self.buffer = []

    def add(self, audio_chunk: np.ndarray):
        """Add a chunk of audio. Save if we've collected enough."""
        if self.max_saves is not None and self.n_saves_done >= self.max_saves:
            return

        assert audio_chunk.dtype == np.float32
        assert audio_chunk.ndim == 1

        self.buffer.append(audio_chunk)

        if sum(len(x) for x in self.buffer) / SAMPLE_RATE >= self.interval_sec:
            output_path = self.output_path
            if self.max_saves != 1:  # None is ok too
                output_path = output_path.with_stem(
                    output_path.stem + f"_{self.n_saves_done + 1}"
                )

            sphn.write_wav(
                output_path,
                np.concatenate(self.buffer).astype(np.float32),
                SAMPLE_RATE,
            )
            self.n_saves_done += 1
            self.buffer.clear()
            logger.info(f"Saved audio to {output_path}")



================================================
FILE: unmute/cache.py
================================================
import logging
import time
from typing import Generic, Optional, TypeVar, cast

import redis
from redis.typing import EncodableT  # Import EncodableT for Redis compatibility

from unmute.kyutai_constants import REDIS_SERVER

logger = logging.getLogger(__name__)

T = TypeVar("T", bound=EncodableT)  # Generic type bound to EncodableT


class CacheError(Exception):
    """An error happened while accessing the cache.

    This is so that we get the same exception type regardless of the cache implementation.
    """


class LocalCache(Generic[T]):
    def __init__(self, ttl_seconds: int = 3600):  # Default 1 hour expiration
        self.cache: dict[
            str, tuple[T, float]
        ] = {}  # {key: (value, expiration_timestamp)}
        self.ttl_seconds = ttl_seconds

    def get(self, key: str) -> Optional[T]:
        cached = self.cache.get(key)
        if cached is not None:
            value, expiration = cached
            if time.time() < expiration:
                return value
            else:
                # Remove expired entry
                del self.cache[key]
        else:
            return None

    def set(self, key: str, value: T):
        expiration = time.time() + self.ttl_seconds
        self.cache[key] = (value, expiration)

    def delete(self, key: str):
        """Delete a key from the cache."""
        if key in self.cache:
            del self.cache[key]

    def cleanup(self):
        """Remove all expired entries"""
        now = time.time()
        expired_keys = [k for k, (_, exp) in self.cache.items() if exp < now]
        for k in expired_keys:
            del self.cache[k]


class RedisCache(Generic[T]):
    def __init__(self, redis_url: str, prefix: str, ttl_seconds: int = 3600):
        self.ttl_seconds = ttl_seconds
        self.prefix = prefix
        self.redis_client = redis.Redis.from_url(redis_url, socket_connect_timeout=2)

    def get(self, key: str) -> Optional[T]:
        key = f"{self.prefix}:{key}"

        try:
            redis_value = self.redis_client.get(key)
            if redis_value is not None:
                logger.info(f"Retrieved value from Redis: {key}")
                return cast(T, redis_value)
            else:
                return None
        except redis.RedisError as e:
            raise CacheError(f"Failed to store value in Redis: {e}") from e

    def set(self, key: str, value: T):
        key = f"{self.prefix}:{key}"
        try:
            # Store with the TTL
            self.redis_client.setex(key, self.ttl_seconds, value)
        except redis.RedisError as e:
            raise CacheError(f"Failed to store value in Redis: {e}") from e

    def delete(self, key: str):
        key = f"{self.prefix}:{key}"
        try:
            # No error if the key does not exist.
            self.redis_client.delete(key)
        except redis.RedisError as e:
            raise CacheError(f"Failed to delete value from Redis: {e}") from e

    def cleanup(self):
        pass  # No cleanup needed for Redis


def get_cache(prefix: str, ttl_seconds: int) -> LocalCache[T] | RedisCache[T]:
    """
    Returns the appropriate cache based on the environment variables.
    If KYUTAI_REDIS_URL is set, it returns a RedisCache instance.
    If not, it returns a LocalCache instance.
    """
    if REDIS_SERVER is not None:
        cache = RedisCache[T](REDIS_SERVER, prefix, ttl_seconds=ttl_seconds)
    else:
        logger.info(
            "Redis cache address was not given in environment variables, using local cache."
        )
        cache = LocalCache[T](ttl_seconds=ttl_seconds)

    return cache



================================================
FILE: unmute/exceptions.py
================================================
import unmute.openai_realtime_api_events as ora


class MissingServiceAtCapacity(Exception):
    """A service is operating at capacity, but no serious error."""

    def __init__(self, service: str):
        self.service = service
        super().__init__(f"{service} is not available.")


class MissingServiceTimeout(Exception):
    """A service timed out."""

    def __init__(self, service: str):
        self.service = service
        super().__init__(f"{service} timed out.")


class WebSocketClosedError(Exception):
    """Remote web socket is closed, let's move on."""


def make_ora_error(type: str, message: str) -> ora.Error:
    details = ora.ErrorDetails(type=type, message=message)
    return ora.Error(error=details)



================================================
FILE: unmute/kyutai_constants.py
================================================
import os
from pathlib import Path

from unmute.websocket_utils import http_to_ws

HEADERS = {"kyutai-api-key": "public_token"}

# The defaults are already ws://, but make the env vars support http:// and https://
STT_SERVER = http_to_ws(os.environ.get("KYUTAI_STT_URL", "ws://localhost:8090"))
TTS_SERVER = http_to_ws(os.environ.get("KYUTAI_TTS_URL", "ws://localhost:8089"))
LLM_SERVER = os.environ.get("KYUTAI_LLM_URL", "http://localhost:8091")
KYUTAI_LLM_MODEL = os.environ.get("KYUTAI_LLM_MODEL")
KYUTAI_LLM_API_KEY = os.environ.get("KYUTAI_LLM_API_KEY")
VOICE_CLONING_SERVER = os.environ.get(
    "KYUTAI_VOICE_CLONING_URL", "http://localhost:8092"
)
# If None, a dict-based cache will be used instead of Redis
REDIS_SERVER = os.environ.get("KYUTAI_REDIS_URL")

SPEECH_TO_TEXT_PATH = "/api/asr-streaming"
TEXT_TO_SPEECH_PATH = "/api/tts_streaming"

repo_root = Path(__file__).parents[1]
VOICE_DONATION_DIR = Path(
    os.environ.get("KYUTAI_VOICE_DONATION_DIR", repo_root / "voices" / "donation")
)

# If None, recordings will not be saved
_recordings_dir = os.environ.get("KYUTAI_RECORDINGS_DIR")
RECORDINGS_DIR = Path(_recordings_dir) if _recordings_dir else None

# Also checked on the frontend, see constant of the same name
MAX_VOICE_FILE_SIZE_MB = 4


SAMPLE_RATE = 24000
SAMPLES_PER_FRAME = 1920
FRAME_TIME_SEC = SAMPLES_PER_FRAME / SAMPLE_RATE  # 0.08
# TODO: make it so that we can read this from the ASR server?
STT_DELAY_SEC = 0.5



================================================
FILE: unmute/main_gradio.py
================================================
import pprint
from typing import Any

import gradio as gr
import pandas as pd
import plotly.express as px
from fastrtc import Stream, get_hf_turn_credentials

from unmute.unmute_handler import GradioUpdate, UnmuteHandler

if __name__ == "__main__":
    gradio_chatbot = gr.Chatbot(type="messages")
    gradio_debug_textbox = gr.Textbox(label="Debug dict")
    gradio_debug_plot = gr.Plot(label="Debug plot")

    def update_outputs(
        _chatbot_state: Any,
        _debug_textbox_state: Any,
        _debug_plot_state: Any,
        update: GradioUpdate,
    ):
        # Not sure if this is expected behavior, but it seems necessary to send updates
        # to all of the components even if you don't want to change them. Otherwise they
        # get overwritten.
        chatbot_state = update.chat_history
        debug_textbox_state = pprint.pformat(update.debug_dict)

        debug_plot_data_variables = set().union(
            *[x.keys() for x in update.debug_plot_data],
        ) - {"t"}

        if debug_plot_data_variables:
            df = pd.DataFrame(update.debug_plot_data)
            df = df.ffill()

            fig = px.line(
                df,
                x="t",
                y=sorted(list(debug_plot_data_variables)),
            )
        else:
            fig = None

        return chatbot_state, debug_textbox_state, fig

    rtc_configuration = get_hf_turn_credentials()
    # rtc_configuration = get_cloudflare_rtc_configuration()

    stream = Stream(
        handler=UnmuteHandler(),
        modality="audio",
        mode="send-receive",
        # rtc_configuration=rtc_configuration,
        rtc_configuration=rtc_configuration,
        # additional_inputs=[gradio_chatbot],
        additional_outputs=[gradio_chatbot, gradio_debug_textbox, gradio_debug_plot],
        additional_outputs_handler=update_outputs,
        # TODO: check if clients actually get disconnected
        concurrency_limit=1,
    )

    # This variable needs to contain the Gradio UI for the autoreload to work:
    # https://www.gradio.app/guides/developing-faster-with-reload-mode
    demo = stream.ui

    # Not clear what `debug` does. It's not auto-reload.
    demo.launch(debug=False)



================================================
FILE: unmute/main_websocket.py
================================================
import asyncio
import base64
import json
import logging
from functools import cache, partial
from typing import Annotated

import numpy as np
import requests
import sphn
from fastapi import (
    FastAPI,
    File,
    Form,
    HTTPException,
    UploadFile,
    WebSocket,
    WebSocketDisconnect,
    status,
)
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.websockets import WebSocketState
from fastrtc import AdditionalOutputs, CloseStream, audio_to_float32
from prometheus_fastapi_instrumentator import Instrumentator
from pydantic import BaseModel, Field, TypeAdapter, ValidationError, computed_field
from starlette.middleware.base import BaseHTTPMiddleware, RequestResponseEndpoint
from starlette.requests import Request
from starlette.responses import Response
from starlette.types import ASGIApp

import unmute.openai_realtime_api_events as ora
from unmute import metrics as mt
from unmute.exceptions import (
    MissingServiceAtCapacity,
    MissingServiceTimeout,
    WebSocketClosedError,
    make_ora_error,
)
from unmute.kyutai_constants import (
    LLM_SERVER,
    MAX_VOICE_FILE_SIZE_MB,
    SAMPLE_RATE,
    STT_SERVER,
    TTS_SERVER,
    VOICE_CLONING_SERVER,
)
from unmute.service_discovery import async_ttl_cached
from unmute.timer import Stopwatch
from unmute.tts.voice_cloning import clone_voice
from unmute.tts.voice_donation import (
    VoiceDonationSubmission,
    generate_verification,
    submit_voice_donation,
)
from unmute.tts.voices import VoiceList
from unmute.unmute_handler import UnmuteHandler

app = FastAPI()

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO,
)

# We prefer to scale this by running more instances of the server than having a single
# server handle more. This is to avoid the GIL.
MAX_CLIENTS = 4
SEMAPHORE = asyncio.Semaphore(MAX_CLIENTS)

Instrumentator().instrument(app).expose(app)
PROFILE_ACTIVE = False
_last_profile = None
_current_profile = None

ClientEventAdapter = TypeAdapter(
    Annotated[ora.ClientEvent, Field(discriminator="type")]
)

# Allow CORS for local development
CORS_ALLOW_ORIGINS = ["http://localhost", "http://localhost:3000"]
app.add_middleware(
    CORSMiddleware,
    allow_origins=CORS_ALLOW_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/")
def root():
    return {"message": "You've reached the Unmute backend server."}


if PROFILE_ACTIVE:

    @app.get("/profile")
    def profile():
        if _last_profile is None:
            return HTMLResponse("<body>No last profiler saved</body>")
        else:
            return HTMLResponse(_last_profile.output_html())  # type: ignore


def _ws_to_http(ws_url: str) -> str:
    """Convert a WebSocket URL to an HTTP URL."""
    return ws_url.replace("ws://", "http://").replace("wss://", "https://")


def _check_server_status(server_url: str) -> bool:
    """Check if the server is up by sending a GET request."""
    try:
        response = requests.get(server_url, timeout=2)
        logger.info(f"Response from {server_url}: {response}")
        return response.status_code == 200
    except requests.exceptions.RequestException as e:
        logger.info(f"Couldn't connect to {server_url}: {e}")
        return False


async def debug_running_tasks():
    while True:
        logger.debug(f"Running tasks: {len(asyncio.all_tasks())}")
        for task in asyncio.all_tasks():
            logger.debug(f"  Task: {task.get_name()} - {task.get_coro()}")
        await asyncio.sleep(5)


class HealthStatus(BaseModel):
    tts_up: bool
    stt_up: bool
    llm_up: bool
    voice_cloning_up: bool

    @computed_field
    @property
    def ok(self) -> bool:
        # Note that voice cloning is not required for the server to be healthy.
        return self.tts_up and self.stt_up and self.llm_up


@partial(async_ttl_cached, ttl_sec=0.5)
async def _get_health(
    _none: None,
):  # dummy param _none because caching function expects a single param as cache key.
    async with asyncio.TaskGroup() as tg:
        tts_up = tg.create_task(
            asyncio.to_thread(
                _check_server_status, _ws_to_http(TTS_SERVER) + "/api/build_info"
            )
        )
        stt_up = tg.create_task(
            asyncio.to_thread(
                _check_server_status, _ws_to_http(STT_SERVER) + "/api/build_info"
            )
        )
        llm_up = tg.create_task(
            asyncio.to_thread(
                _check_server_status, _ws_to_http(LLM_SERVER) + "/v1/models"
            )
        )
        voice_cloning_up = tg.create_task(
            asyncio.to_thread(
                _check_server_status,
                _ws_to_http(VOICE_CLONING_SERVER) + "/api/build_info",
            )
        )
        tts_up_res = await tts_up
        stt_up_res = await stt_up
        llm_up_res = await llm_up
        voice_cloning_up_res = await voice_cloning_up

    return HealthStatus(
        tts_up=tts_up_res,
        stt_up=stt_up_res,
        llm_up=llm_up_res,
        voice_cloning_up=voice_cloning_up_res,
    )


@app.get("/v1/health")
async def get_health():
    health = await _get_health(None)
    mt.HEALTH_OK.observe(health.ok)
    return health


@app.get("/v1/voices")
@cache
def voices():
    voice_list = VoiceList()
    # Note that `voice.good` is bool | None, here we really take only True values.
    good_voices = [
        voice.model_dump(exclude={"comment"})
        for voice in voice_list.voices
        if voice.good
    ]
    return good_voices


class LimitUploadSizeForPath(BaseHTTPMiddleware):
    def __init__(self, app: ASGIApp, max_upload_size: int, path: str) -> None:
        super().__init__(app)
        self.max_upload_size = max_upload_size
        self.path = path

    async def dispatch(
        self, request: Request, call_next: RequestResponseEndpoint
    ) -> Response:
        if request.method == "POST" and request.url.path == self.path:
            if "content-length" not in request.headers:
                return Response(status_code=status.HTTP_411_LENGTH_REQUIRED)

            content_length = int(request.headers["content-length"])
            if content_length > self.max_upload_size:
                return Response(status_code=status.HTTP_413_REQUEST_ENTITY_TOO_LARGE)

        return await call_next(request)


app.add_middleware(
    LimitUploadSizeForPath,
    max_upload_size=MAX_VOICE_FILE_SIZE_MB * 1024 * 1024,
    path="/v1/voices",
)


@app.post("/v1/voices")
async def post_voices(file: UploadFile):
    """Upload a voice list file.

    Make sure the maximum file size is configured in uvicorn.
    """
    name = clone_voice(file.file.read())
    return {"name": name}


@app.get("/v1/voice-donation")
async def get_voice_donation():
    """Initiate a voice donation by asking for a verification text."""
    verification = generate_verification()
    return verification


app.add_middleware(
    LimitUploadSizeForPath,
    max_upload_size=MAX_VOICE_FILE_SIZE_MB * 1024 * 1024,
    path="/v1/voice-donation",
)


@app.post("/v1/voice-donation")
async def post_voice_donation(
    file: UploadFile = File(...),  # noqa: B008
    metadata: str = Form(...),
):
    """Finish a voice donation."""
    file_bytes = file.file.read()

    try:
        metadata_parsed = VoiceDonationSubmission(**json.loads(metadata))
    except json.JSONDecodeError as e:
        raise HTTPException(status_code=400, detail=f"Invalid JSON: {e}") from e
    except ValidationError as e:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid submission: {e.errors()}",
        ) from e

    try:
        submit_voice_donation(metadata_parsed, file_bytes)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e)) from e

    return {}


@app.websocket("/v1/realtime")
async def websocket_route(websocket: WebSocket):
    global _last_profile, _current_profile
    mt.SESSIONS.inc()
    mt.ACTIVE_SESSIONS.inc()
    session_watch = Stopwatch()
    if PROFILE_ACTIVE and _current_profile is None:
        from pyinstrument import Profiler

        logger.info("Profiler started.")
        _current_profile = Profiler(interval=0.0001, async_mode="disabled")
        import inspect

        frame = inspect.currentframe()
        while frame is not None and frame.f_back:
            frame = frame.f_back
        _current_profile.start(caller_frame=frame)

    async with SEMAPHORE:
        try:
            # The `subprotocol` argument is important because the client specifies what
            # protocol(s) it supports and OpenAI uses "realtime" as the value. If we
            # don't set this, the client will think this is not the right endpoint and
            # will not connect.
            await websocket.accept(subprotocol="realtime")

            handler = UnmuteHandler()
            async with handler:
                await handler.start_up()
                await _run_route(websocket, handler)

        except Exception as exc:
            await _report_websocket_exception(websocket, exc)
        finally:
            if _current_profile is not None:
                _current_profile.stop()
                logger.info("Profiler saved.")
                _last_profile = _current_profile
                _current_profile = None

            mt.ACTIVE_SESSIONS.dec()
            mt.SESSION_DURATION.observe(session_watch.time())


async def _report_websocket_exception(websocket: WebSocket, exc: Exception):
    if isinstance(exc, ExceptionGroup):
        exceptions = exc.exceptions
    else:
        exceptions = [exc]

    error_message = None

    for exc in exceptions:
        if isinstance(exc, (MissingServiceAtCapacity, MissingServiceTimeout)):
            mt.FATAL_SERVICE_MISSES.inc()
            error_message = "Too many people are connected! Please try again later."
        elif isinstance(exc, WebSocketClosedError):
            logger.debug("Websocket was closed.")
        else:
            logger.exception("Unexpected error: %r", exc)
            mt.HARD_ERRORS.inc()
            error_message = "Internal server error :( Complain to Kyutai"

    if error_message is not None:
        mt.FORCE_DISCONNECTS.inc()

        try:
            await websocket.send_text(
                make_ora_error(type="fatal", message=error_message).model_dump_json()
            )
        except WebSocketDisconnect:
            logger.warning("Failed to send error message due to disconnect.")

        try:
            await websocket.close(
                code=status.WS_1011_INTERNAL_ERROR,
                reason=error_message,
            )
        except RuntimeError:
            logger.warning("Socket already closed.")


async def _run_route(websocket: WebSocket, handler: UnmuteHandler):
    health = await get_health()
    if not health.ok:
        logger.info("Health check failed, closing WebSocket connection.")
        await websocket.close(
            code=status.WS_1011_INTERNAL_ERROR,
            reason=f"Server is not healthy: {health}",
        )
        return

    emit_queue: asyncio.Queue[ora.ServerEvent] = asyncio.Queue()
    try:
        async with asyncio.TaskGroup() as tg:
            tg.create_task(
                receive_loop(websocket, handler, emit_queue), name="receive_loop()"
            )
            tg.create_task(
                emit_loop(websocket, handler, emit_queue), name="emit_loop()"
            )
            tg.create_task(handler.quest_manager.wait(), name="quest_manager.wait()")
            tg.create_task(debug_running_tasks(), name="debug_running_tasks()")
    finally:
        await handler.cleanup()
        logger.info("websocket_route() finished")


async def receive_loop(
    websocket: WebSocket,
    handler: UnmuteHandler,
    emit_queue: asyncio.Queue[ora.ServerEvent],
):
    """Receive messages from the WebSocket.

    Can decide to send messages via `emit_queue`.
    """
    opus_reader = sphn.OpusStreamReader(SAMPLE_RATE)
    wait_for_first_opus = True
    while True:
        try:
            message_raw = await websocket.receive_text()
        except WebSocketDisconnect as e:
            logger.info(
                "receive_loop() stopped because WebSocket disconnected: "
                f"{e.code=} {e.reason=}"
            )
            raise WebSocketClosedError() from e
        except RuntimeError as e:
            # This is expected when the client disconnects
            if "WebSocket is not connected" not in str(e):
                raise  # re-raise unexpected errors

            logger.info("receive_loop() stopped because WebSocket disconnected.")
            raise WebSocketClosedError() from e

        try:
            message: ora.ClientEvent = ClientEventAdapter.validate_json(message_raw)
        except json.JSONDecodeError as e:
            await emit_queue.put(
                ora.Error(
                    error=ora.ErrorDetails(
                        type="invalid_request_error",
                        message=f"Invalid JSON: {e}",
                    )
                )
            )
            continue
        except ValidationError as e:
            await emit_queue.put(
                ora.Error(
                    error=ora.ErrorDetails(
                        type="invalid_request_error",
                        message="Invalid message",
                        details=json.loads(e.json()),
                    )
                )
            )
            continue

        message_to_record = message

        if isinstance(message, ora.InputAudioBufferAppend):
            opus_bytes = base64.b64decode(message.audio)
            if wait_for_first_opus:
                # Somehow the UI is sending us potentially old messages from a previous
                # connection on reconnect, so that we might get some old OGG packets,
                # waiting for the bit set for first packet to feed to the decoder.
                if opus_bytes[5] & 2:
                    wait_for_first_opus = False
                else:
                    continue
            pcm = await asyncio.to_thread(opus_reader.append_bytes, opus_bytes)

            message_to_record = ora.UnmuteInputAudioBufferAppendAnonymized(
                number_of_samples=pcm.size,
            )

            if pcm.size:
                await handler.receive((SAMPLE_RATE, pcm[np.newaxis, :]))
        elif isinstance(message, ora.SessionUpdate):
            await handler.update_session(message.session)
            await emit_queue.put(ora.SessionUpdated(session=message.session))

        elif isinstance(message, ora.UnmuteAdditionalOutputs):
            # Don't record this: it's a debugging message and can be verbose. Anything
            # important to store should be in the other event types.
            message_to_record = None

        else:
            logger.info("Ignoring message:", str(message)[:100])

        if message_to_record is not None and handler.recorder is not None:
            await handler.recorder.add_event("client", message_to_record)


class EmitDebugLogger:
    def __init__(self):
        self.last_emitted_n = 0
        self.last_emitted_type = ""

    def on_emit(self, to_emit: ora.ServerEvent):
        if self.last_emitted_type == to_emit.type:
            self.last_emitted_n += 1
        else:
            self.last_emitted_n = 1
            self.last_emitted_type = to_emit.type

        if self.last_emitted_n == 1:
            logger.debug(f"Emitting: {to_emit.type}")
        else:
            logger.debug(f"Emitting ({self.last_emitted_n}): {self.last_emitted_type}")


async def emit_loop(
    websocket: WebSocket,
    handler: UnmuteHandler,
    emit_queue: asyncio.Queue[ora.ServerEvent],
):
    """Send messages to the WebSocket."""
    emit_debug_logger = EmitDebugLogger()

    opus_writer = sphn.OpusStreamWriter(SAMPLE_RATE)

    while True:
        if (
            websocket.application_state == WebSocketState.DISCONNECTED
            or websocket.client_state == WebSocketState.DISCONNECTED
        ):
            logger.info("emit_loop() stopped because WebSocket disconnected")
            raise WebSocketClosedError()

        try:
            to_emit = emit_queue.get_nowait()
        except asyncio.QueueEmpty:
            emitted_by_handler = await handler.emit()

            if emitted_by_handler is None:
                continue
            elif isinstance(emitted_by_handler, AdditionalOutputs):
                assert len(emitted_by_handler.args) == 1
                to_emit = ora.UnmuteAdditionalOutputs(
                    args=emitted_by_handler.args[0],
                )
            elif isinstance(emitted_by_handler, CloseStream):
                # Close here explicitly so that the receive loop stops too
                await websocket.close()
                break
            elif isinstance(emitted_by_handler, ora.ServerEvent):
                to_emit = emitted_by_handler
            else:
                _sr, audio = emitted_by_handler
                audio = audio_to_float32(audio)
                opus_bytes = await asyncio.to_thread(opus_writer.append_pcm, audio)
                # Due to buffering/chunking, Opus doesn't necessarily output something on every PCM added
                if opus_bytes:
                    to_emit = ora.ResponseAudioDelta(
                        delta=base64.b64encode(opus_bytes).decode("utf-8"),
                    )
                else:
                    continue

        emit_debug_logger.on_emit(to_emit)

        if handler.recorder is not None:
            await handler.recorder.add_event("server", to_emit)

        try:
            await websocket.send_text(to_emit.model_dump_json())
        except (WebSocketDisconnect, RuntimeError) as e:
            if isinstance(e, RuntimeError):
                if "Unexpected ASGI message 'websocket.send'" in str(e):
                    # This is expected when the client disconnects
                    message = f"emit_loop() stopped because WebSocket disconnected: {e}"
                else:
                    raise
            else:
                message = (
                    "emit_loop() stopped because WebSocket disconnected: "
                    f"{e.code=} {e.reason=}"
                )

            logger.info(message)
            raise WebSocketClosedError() from e


def _cors_headers_for_error(request: Request):
    origin = request.headers.get("origin")
    allow_origin = origin if origin in CORS_ALLOW_ORIGINS else None
    headers = {"Access-Control-Allow-Credentials": "true"}
    if allow_origin:
        headers["Access-Control-Allow-Origin"] = allow_origin

    return headers


@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    # We need this so that CORS header are added even when the route raises an
    # exception. Otherwise you get a confusing CORS error even if the issue is totally
    # unrelated.
    return JSONResponse(
        status_code=exc.status_code,
        content={"detail": exc.detail},
        headers=_cors_headers_for_error(request),
    )


@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    # We need this so that CORS header are added even when the route raises an
    # exception. Otherwise you get a confusing CORS error even if the issue is totally
    # unrelated.
    return JSONResponse(
        status_code=500,
        content={"detail": "Internal server error"},
        headers=_cors_headers_for_error(request),
    )


if __name__ == "__main__":
    import sys

    print(f"Run this via:\nfastapi dev {sys.argv[0]}")
    exit(1)



================================================
FILE: unmute/metrics.py
================================================
from prometheus_client import Counter, Gauge, Histogram, Summary

SESSION_DURATION_BINS = [1.0, 10.0, 30.0, 60.0, 120.0, 240.0, 480.0, 960.0, 1920.0]
TURN_DURATION_BINS = [0.5, 1.0, 5.0, 10.0, 20.0, 40.0, 60.0]
GENERATION_DURATION_BINS = [0.1, 0.25, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0]

PING_BINS_MS = [1.0, 5.0, 10.0, 25.0, 50.0, 100.0, 200.0]
PING_BINS = [x / 1000 for x in PING_BINS_MS]

# Time to first token.
TTFT_BINS_STT_MS = [
    10.0,
    15.0,
    25.0,
    50.0,
    75.0,
    100.0,
]
TTFT_BINS_STT = [x / 1000 for x in TTFT_BINS_STT_MS]

TTFT_BINS_TTS_MS = [
    200.0,
    250.0,
    300.0,
    350.0,
    400.0,
    450.0,
    500.0,
    550.0,
]
TTFT_BINS_TTS = [x / 1000 for x in TTFT_BINS_TTS_MS]


TTFT_BINS_VLLM_MS = [
    50.0,
    75.0,
    100.0,
    150.0,
    200.0,
    250.0,
    300.0,
    400.0,
    500.0,
    750.0,
    1000.0,
]
TTFT_BINS_VLLM = [x / 1000 for x in TTFT_BINS_VLLM_MS]

NUM_WORDS_REQUEST_BINS = [
    50.0,
    100.0,
    200.0,
    500.0,
    1000.0,
    2000.0,
    4000.0,
    6000.0,
    8000.0,
]
NUM_WORDS_STT_BINS = [0.0, 50.0, 100.0, 200.0, 500.0, 1000.0, 2000.0, 4000.0]
NUM_WORDS_REPLY_BINS = [5.0, 10.0, 25.0, 50.0, 100.0, 200.0]

SESSIONS = Counter("worker_sessions", "")
SERVICE_MISSES = Counter("worker_service_misses", "")
HARD_SERVICE_MISSES = Counter("worker_hard_service_misses", "")
FORCE_DISCONNECTS = Counter("worker_force_disconnects", "")
FATAL_SERVICE_MISSES = Counter("worker_fatal_service_misses", "")
HARD_ERRORS = Counter("worker_hard_errors", "")
ACTIVE_SESSIONS = Gauge("worker_active_sessions", "")
SESSION_DURATION = Histogram(
    "worker_session_duration", "", buckets=SESSION_DURATION_BINS
)
HEALTH_OK = Summary("worker_health_ok", "")

STT_SESSIONS = Counter("worker_stt_sessions", "")
STT_ACTIVE_SESSIONS = Gauge("worker_stt_active_sessions", "")
STT_MISSES = Counter("worker_stt_misses", "")
STT_HARD_MISSES = Counter("worker_stt_hard_misses", "")
STT_SENT_FRAMES = Counter("worker_stt_sent_frames", "")
STT_RECV_FRAMES = Counter("worker_stt_recv_frames", "")
STT_RECV_WORDS = Counter("worker_stt_recv_words", "")
STT_PING_TIME = Histogram("worker_stt_ping_time", "", buckets=PING_BINS)
STT_FIND_TIME = Histogram("worker_stt_find_time", "", buckets=PING_BINS)
STT_SESSION_DURATION = Histogram(
    "worker_stt_session_duration", "", buckets=SESSION_DURATION_BINS
)
STT_AUDIO_DURATION = Histogram(
    "worker_stt_audio_duration", "", buckets=SESSION_DURATION_BINS
)
STT_NUM_WORDS = Histogram("worker_stt_num_words", "", buckets=NUM_WORDS_STT_BINS)
STT_TTFT = Histogram("worker_stt_ttft", "", buckets=TTFT_BINS_STT)

TTS_SESSIONS = Counter("worker_tts_sessions", "")
TTS_ACTIVE_SESSIONS = Gauge("worker_tts_active_sessions", "")
TTS_MISSES = Counter("worker_tts_misses", "")
TTS_HARD_MISSES = Counter("worker_hard_tts_misses", "")
TTS_INTERRUPT = Counter("worker_tts_interrupt", "")
TTS_SENT_FRAMES = Counter("worker_tts_sent_frames", "")
TTS_RECV_FRAMES = Counter("worker_tts_recv_frames", "")
TTS_RECV_WORDS = Counter("worker_tts_recv_words", "")
TTS_PING_TIME = Histogram("worker_tts_ping_time", "", buckets=PING_BINS)
TTS_FIND_TIME = Histogram("worker_tts_find_time", "", buckets=PING_BINS)
TTS_TTFT = Histogram("worker_tts_ttft", "", buckets=TTFT_BINS_TTS)
TTS_AUDIO_DURATION = Histogram(
    "worker_tts_audio_duration", "", buckets=TURN_DURATION_BINS
)
TTS_GEN_DURATION = Histogram(
    "worker_tts_gen_duration", "", buckets=GENERATION_DURATION_BINS
)

VLLM_SESSIONS = Counter("worker_vllm_sessions", "")
VLLM_ACTIVE_SESSIONS = Gauge("worker_vllm_active_sessions", "")
VLLM_INTERRUPTS = Counter("worker_vllm_interrupt", "")
VLLM_HARD_ERRORS = Counter("worker_vllm_hard_errors", "")
VLLM_SENT_WORDS = Counter("worker_vllm_sent_words", "")
VLLM_RECV_WORDS = Counter("worker_vllm_recv_words", "")
VLLM_TTFT = Histogram("worker_vllm_ttft", "", buckets=TTFT_BINS_VLLM)
VLLM_REQUEST_LENGTH = Histogram(
    "worker_vllm_request_length", "", buckets=NUM_WORDS_REQUEST_BINS
)
VLLM_REPLY_LENGTH = Histogram(
    "worker_vllm_reply_length", "", buckets=NUM_WORDS_REPLY_BINS
)
VLLM_GEN_DURATION = Histogram(
    "worker_vllm_gen_duration", "", buckets=GENERATION_DURATION_BINS
)

VOICE_DONATION_SUBMISSIONS = Counter("worker_voice_donation_submissions", "")



================================================
FILE: unmute/openai_realtime_api_events.py
================================================
"""See OpenAI's docs: https://platform.openai.com/docs/api-reference/realtime

https://platform.openai.com/docs/api-reference/realtime-client-events
https://platform.openai.com/docs/api-reference/realtime-server-events
"""

import random
from typing import (
    Any,
    Generic,
    Literal,
    TypeVar,
    Union,
    get_args,
    get_origin,
)

from pydantic import BaseModel, Field, model_validator

from unmute.llm.system_prompt import Instructions

T = TypeVar("T", bound=str)


def random_id(prefix: str) -> str:
    """e.g. event_BJhGUIswO2u7vA2Cxw3Jy"""
    n_characters = 21
    alphabet = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"
    return prefix + "_" + "".join(random.choices(alphabet, k=n_characters))


class BaseEvent(BaseModel, Generic[T]):
    type: T = None  # type: ignore - will be set by validator below
    event_id: str = Field(default_factory=lambda: random_id("event"))

    @model_validator(mode="after")
    def set_type_from_generic(self) -> "BaseEvent":
        if type(self) == BaseEvent:  # noqa - we want to use type() == and not isinstance
            raise ValueError("Cannot instantiate BaseEvent directly")

        # e.g. Literal["session.update"]
        type_argument = self.__class__.model_fields["type"].annotation

        if get_origin(type_argument) is not Literal:
            # I don't know how to enforce this in the type system directly
            raise ValueError("Type argument is not a Literal")

        self.type = get_args(type_argument)[0]  # e.g. "session.update"

        return self


class ErrorDetails(BaseModel):
    type: str
    code: str | None = None
    message: str
    param: str | None = None
    # ours, not part of the OpenAI API:
    details: object | None = None


class Error(BaseEvent[Literal["error"]]):
    error: ErrorDetails


class SessionConfig(BaseModel):
    # The "Instructions" object is an Unmute extension
    instructions: Instructions | None = None
    voice: str | None = None
    allow_recording: bool


class SessionUpdate(BaseEvent[Literal["session.update"]]):
    session: SessionConfig


class SessionUpdated(BaseEvent[Literal["session.updated"]]):
    session: SessionConfig


class InputAudioBufferAppend(BaseEvent[Literal["input_audio_buffer.append"]]):
    audio: str  # Base64-encoded Opus data


class UnmuteInputAudioBufferAppendAnonymized(
    BaseEvent[Literal["unmute.input_audio_buffer.append_anonymized"]]
):
    """
    For recording, an anonymous version of InputAudioBufferAppend that only says
    how many samples were appended, not the actual audio data.
    """

    number_of_samples: int


class InputAudioBufferSpeechStarted(
    BaseEvent[Literal["input_audio_buffer.speech_started"]]
):
    """Speech started according to the STT.

    Note this is not symmetrical with `InputAudioBufferSpeechStopped` because it's based
    on the STT and not the VAD signal. This is because sometimes the VAD will think
    there is speech but then nothing will end up getting transcribed. If we were using
    the VAD for both events we might get a start event without a stop event.
    For VAD interruptions, see `UnmuteInterruptedByVAD`.
    """


class InputAudioBufferSpeechStopped(
    BaseEvent[Literal["input_audio_buffer.speech_stopped"]]
):
    """A pause was detected by the VAD."""


class Response(BaseModel):
    object: Literal["realtime.response"] = "realtime.response"
    # We currently only use in_progress
    status: Literal["in_progress", "completed", "cancelled", "failed", "incomplete"]
    voice: str
    chat_history: list[dict[str, Any]] = Field(default_factory=list)


class ResponseCreated(BaseEvent[Literal["response.created"]]):
    response: Response


class ResponseTextDelta(BaseEvent[Literal["response.text.delta"]]):
    delta: str


class ResponseTextDone(BaseEvent[Literal["response.text.done"]]):
    text: str


class ResponseAudioDelta(BaseEvent[Literal["response.audio.delta"]]):
    delta: str  # Base64-encoded Opus audio data


class ResponseAudioDone(BaseEvent[Literal["response.audio.done"]]):
    pass


class TranscriptLogprob(BaseModel):
    bytes: bytes
    logprob: float
    token: str


class ConversationItemInputAudioTranscriptionDelta(
    BaseEvent[Literal["conversation.item.input_audio_transcription.delta"]]
):
    delta: str
    start_time: float  # Unmute extension


class UnmuteAdditionalOutputs(BaseEvent[Literal["unmute.additional_outputs"]]):
    args: Any


class UnmuteResponseTextDeltaReady(
    BaseEvent[Literal["unmute.response.text.delta.ready"]]
):
    delta: str


class UnmuteResponseAudioDeltaReady(
    BaseEvent[Literal["unmute.response.audio.delta.ready"]]
):
    number_of_samples: int


class UnmuteInterruptedByVAD(BaseEvent[Literal["unmute.interrupted_by_vad"]]):
    """The VAD interrupted the response generation."""


# Server events (from OpenAI to client)
ServerEvent = Union[
    Error,
    SessionUpdated,
    ResponseTextDelta,
    ResponseTextDone,
    ResponseAudioDelta,
    ResponseAudioDone,
    ResponseCreated,
    ConversationItemInputAudioTranscriptionDelta,
    InputAudioBufferSpeechStarted,
    InputAudioBufferSpeechStopped,
    UnmuteAdditionalOutputs,
    UnmuteResponseTextDeltaReady,
    UnmuteResponseAudioDeltaReady,
    UnmuteInterruptedByVAD,
]

# Client events (from client to OpenAI)
ClientEvent = Union[
    SessionUpdate,
    InputAudioBufferAppend,
    # Used internally for recording, we're not expecting the user to send this
    UnmuteInputAudioBufferAppendAnonymized,
]

Event = ClientEvent | ServerEvent



================================================
FILE: unmute/process_recording.py
================================================
"""Process a .msgpack recording for displaying on the project page.

The recording is done by recorder.py, which just records the JSON messages sent back
and forth between the client and the server.

This script converts the recording into a time-aligned format that's easier for
visualization.

It can also extract the audio from the recording.
"""

import argparse
import base64
import json
import logging
from collections import defaultdict, deque
from copy import deepcopy
from pathlib import Path
from typing import Iterable

import msgpack
import numpy as np
import sphn
from pydantic import BaseModel

from unmute import openai_realtime_api_events as ora
from unmute.kyutai_constants import SAMPLE_RATE
from unmute.recorder import RecorderEvent
from unmute.tts.text_to_speech import prepare_text_for_tts

# Note this is not the ASR/TTS frame size; for that, see SAMPLES_PER_FRAME.
# It's the number of samples we get from the user per step.
SAMPLES_PER_STEP = 960
SAMPLES_PER_WAVEFORM = 240

logger = logging.getLogger(__name__)


class AudioFrame(BaseModel):
    amplitude_rms: list[float]
    n_samples: int
    created_at_samples: int

    def split(self, n_samples_start: int) -> tuple["AudioFrame", "AudioFrame"]:
        assert 0 < n_samples_start < self.n_samples, (
            f"{self.n_samples=}, {n_samples_start=}"
        )

        fraction = n_samples_start / self.n_samples
        amplitude_index_float = fraction * len(self.amplitude_rms)
        amplitude_index = int(amplitude_index_float)
        if amplitude_index_float - amplitude_index > 0.1:
            logger.warning(
                "Amplitude RMS split unevenly, "
                f"{fraction=}, {amplitude_index_float=} {len(self.amplitude_rms)}."
            )

        return (
            AudioFrame(
                amplitude_rms=self.amplitude_rms[:amplitude_index],
                n_samples=n_samples_start,
                created_at_samples=self.created_at_samples,
            ),
            AudioFrame(
                amplitude_rms=self.amplitude_rms[amplitude_index:],
                n_samples=self.n_samples - n_samples_start,
                created_at_samples=self.created_at_samples,
            ),
        )


class TextFrame(BaseModel):
    text: str
    created_at_samples: int
    # Using 0 as "unknown" is a bit hacky but it makes addition simpler
    duration_samples: int = 0


class AudioAndText(BaseModel):
    audio: AudioFrame | None = None
    text: TextFrame | None = None


class StepEvents(BaseModel):
    samples_since_start: int
    received: AudioAndText
    emitted: AudioAndText = AudioAndText(audio=None, text=None)
    other_events: list[ora.Event] = []


def get_audio_volume_rms(arr: np.ndarray) -> list[float]:
    if arr.dtype == np.int16:
        arr = arr.astype(np.float32) / np.iinfo(np.int16).max

    if len(arr) % SAMPLES_PER_WAVEFORM != 0:
        raise ValueError(
            f"Array length {len(arr)} is not a multiple of SAMPLES_PER_WAVEFORM ({SAMPLES_PER_WAVEFORM})"
        )

    rms_list = []
    for i in range(0, len(arr), SAMPLES_PER_WAVEFORM):
        chunk = arr[i : i + SAMPLES_PER_WAVEFORM]
        rms = np.sqrt(np.mean(chunk**2))
        rms_list.append(rms)
    return rms_list


def round_to_multiple(value: float, multiple: int) -> int:
    """Round `value` to the nearest multiple of `multiple`."""
    return round(value / multiple) * multiple


def with_samples_since_start(
    recorder_events: list[RecorderEvent],
) -> Iterable[tuple[int, RecorderEvent]]:
    pass
    """Yield (timestamp_samples, recorder_event) pairs from the recorder events."""
    stream_reader = sphn.OpusStreamReader(SAMPLE_RATE)

    samples_since_start = -SAMPLES_PER_STEP
    for recorder_event in recorder_events:
        ora_event = recorder_event.data

        if isinstance(ora_event, ora.InputAudioBufferAppend):
            audio_data = stream_reader.append_bytes(base64.b64decode(ora_event.audio))
            if not audio_data.size:
                logger.warning(
                    f"At {samples_since_start=}, received empty audio data. Skipping."
                )
                continue

            n = len(audio_data)
            if n != SAMPLES_PER_STEP:
                # Related to Opus. Seems to only happen at the beginning
                logger.warning(
                    f"At {samples_since_start=}, received audio data with {n} samples, "
                    f"expected {SAMPLES_PER_STEP} samples. Skipping."
                )
                continue

            samples_since_start += n
            yield samples_since_start, recorder_event
        else:
            yield samples_since_start, recorder_event


def process_events(recorder_events: list[RecorderEvent]) -> list[StepEvents]:
    step_events: dict[int, StepEvents] = {}
    # other_events for a given timestamp might be created before we've created the
    # corresponding step event, so collect them in a separate dict and then merge them
    other_events: defaultdict[int, list[ora.Event]] = defaultdict(list)

    # There are actually two levels of buffering, so use two queues
    tts_server_audio_queued: deque[AudioFrame] = deque()
    tts_client_audio_queued: deque[AudioFrame] = deque()

    tts_text_ready: deque[TextFrame] = deque()

    client_opus_reader = sphn.OpusStreamReader(SAMPLE_RATE)
    server_opus_reader = sphn.OpusStreamReader(SAMPLE_RATE)

    for samples_since_start, recorder_event in with_samples_since_start(
        recorder_events
    ):
        recorder_event = deepcopy(recorder_event)
        ora_event = recorder_event.data

        if isinstance(ora_event, ora.InputAudioBufferAppend):
            # Received audio from the client
            audio_data = client_opus_reader.append_bytes(
                base64.b64decode(ora_event.audio)
            )
            if not audio_data.size:
                continue

            assert samples_since_start not in step_events

            n = len(audio_data)

            step_events[samples_since_start] = StepEvents(
                samples_since_start=samples_since_start,
                received=AudioAndText(
                    audio=AudioFrame(
                        amplitude_rms=get_audio_volume_rms(audio_data),
                        n_samples=n,
                        # For received audio, the creation time is the same as the
                        # receive time.
                        # We add SAMPLES_PER_STEP as a hack so that the waveform
                        # visualization of the received audio doesn't show parts of the
                        # audio as being created but not received yet (because the step
                        # is shown as multiple rectangles since amplitude_rms is a list)
                        created_at_samples=samples_since_start + SAMPLES_PER_STEP,
                    ),
                ),
                emitted=AudioAndText(audio=None, text=None),
            )

            if tts_client_audio_queued:
                audio = tts_client_audio_queued.popleft()
                if audio.n_samples == n:
                    step_events[samples_since_start].emitted.audio = audio
                elif audio.n_samples > n:
                    head, tail = audio.split(n)
                    step_events[samples_since_start].emitted.audio = head
                    tts_client_audio_queued.appendleft(tail)
                else:
                    raise RuntimeError(
                        "Unexpected: output audio frame size is not "
                        "a multiple of the input frame size. "
                        f"{n=}, {audio.n_samples=}"
                    )
        elif isinstance(ora_event, ora.UnmuteResponseAudioDeltaReady):
            tts_server_audio_queued.append(
                AudioFrame(
                    amplitude_rms=[],  # Will be set later
                    n_samples=ora_event.number_of_samples,
                    created_at_samples=samples_since_start,
                )
            )
        elif isinstance(ora_event, ora.ResponseAudioDelta):
            # The server emitted TTS audio that it queued up previously

            audio_data = server_opus_reader.append_bytes(
                base64.b64decode(ora_event.delta)
            )
            assert audio_data.size > 0, "Received empty audio delta"

            if not tts_server_audio_queued:
                # Not sure why this happens, maybe some off-by one? Something related
                # to Opus?
                logger.warning(
                    f"Received TTS audio delta at timestamp {samples_since_start} "
                    "but no audio frame was queued on the server side."
                )
                continue

            # Move from the server-side queue to the client-side queue
            assert tts_server_audio_queued[0].n_samples == len(audio_data)
            audio_frame = tts_server_audio_queued.popleft()
            audio_frame.amplitude_rms = get_audio_volume_rms(audio_data)
            tts_client_audio_queued.append(audio_frame)
        elif isinstance(ora_event, ora.UnmuteResponseTextDeltaReady):
            tts_text_ready.append(
                TextFrame(
                    text=ora_event.delta,
                    created_at_samples=samples_since_start,
                    duration_samples=0,  # We don't know yet
                )
            )
            other_events[samples_since_start].append(ora_event)
        elif isinstance(ora_event, ora.ResponseTextDelta):
            assert tts_text_ready
            prepared_text = tts_text_ready.popleft()
            assert ora_event.delta == prepare_text_for_tts(prepared_text.text), (
                f"Expected TTS text delta to be '{prepared_text.text}', "
                f"but got '{ora_event.delta}'"
            )
            step_events[samples_since_start].emitted.text = prepared_text
        elif isinstance(ora_event, ora.ConversationItemInputAudioTranscriptionDelta):
            # The STT transcribed something in the past, so we need to compute the
            # timestamp and retroactively add it to the existing step event
            ts_in_question = round_to_multiple(
                ora_event.start_time * SAMPLE_RATE, SAMPLES_PER_STEP
            )
            assert step_events[ts_in_question].received.text is None

            step_events[ts_in_question].received.text = TextFrame(
                text=ora_event.delta,
                created_at_samples=samples_since_start,
                duration_samples=0,  # We don't know
            )
        elif isinstance(ora_event, ora.ResponseCreated):
            # There might be text that the TTS queued up before but it got interrupted
            # before it could be emitted, so remove that text when we start generating
            # a new response.
            tts_text_ready.clear()
            other_events[samples_since_start].append(ora_event)
        else:
            ignored_event_types = [ora.UnmuteAdditionalOutputs]
            if not isinstance(ora_event, tuple(ignored_event_types)):
                other_events[samples_since_start].append(ora_event)

    # Merge other_events into step_events
    for samples_since_start, step_event in step_events.items():
        step_event.other_events = other_events[samples_since_start]

    step_events_list = list(step_events.values())
    step_events_list.sort(key=lambda x: x.samples_since_start)

    # Sanity checks
    samples_per_step = (
        step_events_list[1].samples_since_start
        - step_events_list[0].samples_since_start
    )
    for i, step_event in enumerate(step_events_list):
        assert step_event.samples_since_start == i * samples_per_step

    assert step_events_list[0].samples_since_start == 0

    return step_events_list


def slice_processed_events(
    processed_events: list[StepEvents], start_samples: int
) -> list[StepEvents]:
    filtered = [
        # Copy because we'll be modifying the events later
        deepcopy(event)
        for event in processed_events
        if start_samples <= event.samples_since_start
    ]

    # Fix the timestamps
    for event in filtered:
        event.samples_since_start -= start_samples
        if event.received.audio:
            event.received.audio.created_at_samples -= start_samples
        if event.received.text:
            event.received.text.created_at_samples -= start_samples
        if event.emitted.audio:
            event.emitted.audio.created_at_samples -= start_samples
        if event.emitted.text:
            event.emitted.text.created_at_samples -= start_samples

    return filtered


def extract_audios(
    recorder_events: list[RecorderEvent],
) -> np.ndarray:
    """Return a 2d NumPy array containing user and assistant audio.

    User audio is on the first channel, assistant audio is on the second channel.
    They are time-aligned and trimmed
    """
    user_pcm_chunks = []
    user_reader = sphn.OpusStreamReader(SAMPLE_RATE)
    user_n_samples = 0

    assistant_pcm_chunks = []
    assistant_reader = sphn.OpusStreamReader(SAMPLE_RATE)
    assistant_n_samples = 0

    for e in recorder_events:
        if isinstance(e.data, ora.InputAudioBufferAppend):
            pcm = user_reader.append_bytes(base64.b64decode(e.data.audio))
            user_pcm_chunks.append(pcm)
            user_n_samples += len(pcm)

        elif isinstance(e.data, ora.ResponseAudioDelta):
            pcm = assistant_reader.append_bytes(base64.b64decode(e.data.delta))
            assistant_pcm_chunks.append(pcm)
            assistant_n_samples += len(pcm)

        # The assistant is not emitting audio all the time, so add silence so that the
        # lengths match
        if user_n_samples > assistant_n_samples:
            assistant_pcm_chunks.append(
                np.zeros(user_n_samples - assistant_n_samples, dtype=np.float32)
            )
            assistant_n_samples = user_n_samples

    user_audio = np.concatenate(user_pcm_chunks)
    assistant_audio = np.concatenate(assistant_pcm_chunks)
    length = max(len(user_audio), len(assistant_audio))

    def pad(audio: np.ndarray):
        """Pad the audio to the given length with zeros."""
        if len(audio) < length:
            return np.pad(audio, (0, length - len(audio)), mode="constant")
        return audio

    return np.array([pad(user_audio), pad(assistant_audio)])


def main(
    input_path: Path,
    output_path: Path,
    audio_output_path: Path | None,
    discard_first_assistant_message: bool = False,
):
    with input_path.open("rb") as f:
        events_raw = msgpack.load(f)
        recorder_events = [RecorderEvent(**e) for e in events_raw]

    processed = process_events(recorder_events)

    slice_from_sample = 0
    if discard_first_assistant_message:
        user_speech_start = None
        for e in processed:
            if e.received.text is not None:
                user_speech_start = e
                break

        assert user_speech_start is not None, "No user speech found in the recording."

        padding_samples = SAMPLE_RATE * 0.2  # A bit arbitrary here
        slice_from_sample = user_speech_start.samples_since_start - int(padding_samples)

    if slice_from_sample > 0:
        processed = slice_processed_events(processed, slice_from_sample)

    with open(output_path, "w") as f:
        json.dump([e.model_dump() for e in processed], f, indent=2)
        len_sec = len(processed) * SAMPLES_PER_STEP / SAMPLE_RATE
        print(
            f"Saved processed recording with {len(processed)} steps ({len_sec:.1f}s) "
            f"to {output_path}"
        )

    if audio_output_path is not None:
        audio = extract_audios(recorder_events)
        audio = np.mean(audio, axis=0)  # Combine channels into one
        audio = audio[slice_from_sample:]

        sphn.write_opus(audio_output_path, audio, SAMPLE_RATE)
        print(
            f"Saved {len(audio) / SAMPLE_RATE:.1f}s of user and assistant audio "
            f"to {audio_output_path}"
        )


if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    parser.add_argument(
        "input_path", type=Path, help="The .msgpack file of the raw recording"
    )
    parser.add_argument(
        "output_path", type=Path, help="The path to which the output JSON will be saved"
    )
    parser.add_argument(
        "--discard-first-assistant-message",
        action="store_true",
    )
    parser.add_argument(
        "--audio-output-path",
        type=Path,
        help="Save the combined audio to this path. Supports .ogg and .wav.",
    )
    args = parser.parse_args()

    main(
        args.input_path,
        args.output_path,
        args.audio_output_path,
        args.discard_first_assistant_message,
    )



================================================
FILE: unmute/quest_manager.py
================================================
"""A desperate attempt at having some kind of RAII in Python.
Maybe we can achieve something more minimalistic with a TaskGroup and some try/finally blocks,
to be explored for future refactoring.
"""

import asyncio
import logging
import types
from collections.abc import Awaitable
from functools import partial
from typing import Any, Callable, TypeVar

from unmute.exceptions import (
    MissingServiceAtCapacity,
    MissingServiceTimeout,
    WebSocketClosedError,
)

T = TypeVar("T")
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


class Quest[T]:
    """A Quest is some async unit of work that consist of:
        - an init step, returning T.
        - a run step, using T.
        - a close step, cleaning up T.
    It can be "removed", e.g. cancelled or closed.
    To be used as a context manager to have some garantees on when
    the close step is run. Should be used along a `QuestManager` for best results.
    """

    def __init__(
        self,
        name: str,
        init: Callable[[], Awaitable[T]],
        run: Callable[[T], Awaitable[None]],
        close: Callable[[T], Awaitable[None]] | None = None,
    ):
        self.name = name
        self.init = init
        self.run = run
        self.close = close
        self.task = None
        self._data: asyncio.Future[T] = asyncio.Future()

    @staticmethod
    def from_run_step(name: str, run: Callable[[], Awaitable[None]]) -> "Quest[None]":
        async def _init() -> None:
            return None

        async def _run(_x: None) -> None:
            await run()

        return Quest(name, _init, _run)

    async def get(self) -> T:
        return await self._data

    def get_nowait(self) -> T | None:
        if self._data.done():
            return self._data.result()

    async def _run(self):
        logger.debug("Quest %s init starting...", self.name)
        try:
            data = await self.init()
        except Exception as exc:
            self._data.set_exception(exc)
            raise
        else:
            self._data.set_result(data)
        logger.debug("Quest %s running...", self.name)
        await self.run(data)

    async def __aenter__(self) -> asyncio.Future[None]:
        self.task = asyncio.create_task(self._run())
        return asyncio.ensure_future(self.task)

    async def __aexit__(self, *exc: Any):
        await self.remove()

    async def remove(self):
        assert self.task is not None
        try:
            if self.close is not None:
                # We explicitely wait on the init being successful to avoid weird mixed-status.
                logger.debug("Quest %s closing...", self.name)
                try:
                    if self._data.done() and self._data.exception() is None:
                        await self.close(await self.get())
                except asyncio.CancelledError:
                    pass
                self.close = None
        finally:
            logger.debug("Quest %s canceling...", self.name)
            self.task.cancel()


class QuestManager:
    """A Quest Manager. Enter its context, and schedule some Quests, any Quest with a name in common
    with a previous Quest will cancel it. Do some `asyncio.gather(manager.wait(), ...)` if you want
    to get any Quest exception bubbled up to you."""

    def __init__(self):
        self.quests: dict[str, Quest] = {}
        self._in_context = False
        self._future: asyncio.Future | None = None

    async def wait(self):
        assert self._future is not None
        await self._future

    async def add(self, quest: Quest[T]) -> Quest[T]:
        assert self._future is not None
        name = quest.name
        try:
            old = self.quests[name]
        except KeyError:
            pass
        else:
            await old.__aexit__(None)
        self.quests[name] = quest
        future = await quest.__aenter__()
        future.add_done_callback(partial(self._one_is_done, name, self._future))
        return quest

    async def remove(self, name: str):
        try:
            quest = self.quests.pop(name)
        except KeyError:
            return
        await quest.remove()

    @staticmethod
    def _one_is_done(name: str, agg_future: asyncio.Future, future: asyncio.Future):
        logger.debug("Quest %s is done.", name)
        try:
            future.result()
        except asyncio.CancelledError:
            logger.debug("Quest %s was cancelled.", name)
        except Exception as exc:
            logger.debug("Quest %s failed with %r.", name, exc)
            if not agg_future.done():
                agg_future.set_exception(exc)
        else:
            pass

    async def __aenter__(self) -> "QuestManager":
        assert self._future is None
        logger.debug("Quest manager entering...")
        self._future = asyncio.Future()
        return self

    async def __aexit__(
        self,
        exc_type: type[BaseException] | None,
        exc_val: BaseException | None,
        exc_tb: types.TracebackType | None,
    ) -> None:
        assert self._future is not None
        logger.debug("Quest manager exiting...")
        for name, value in self.quests.items():
            try:
                await value.remove()
            except (
                MissingServiceAtCapacity,
                MissingServiceTimeout,
                WebSocketClosedError,
            ):
                pass
            except Exception:
                logger.exception(f"Error shutting down quest {name}.")
        logger.debug("All quests canceled...")
        self.quests.clear()
        if not self._future.done():
            self._future.set_result(None)



================================================
FILE: unmute/recorder.py
================================================
import logging
import uuid
from datetime import datetime
from pathlib import Path
from typing import Annotated, Literal

import aiofiles
from pydantic import BaseModel, Field

import unmute.openai_realtime_api_events as ora

logger = logging.getLogger(__name__)

EventSender = Literal["client", "server"]


class RecorderEvent(BaseModel):
    timestamp_wall: float
    event_sender: EventSender
    data: Annotated[ora.Event, Field(discriminator="type")]


class Recorder:
    """Record the events sent between the client and the server to a file.

    Doesn't include the user audio for privacy reasons.
    """

    def __init__(self, recordings_dir: Path):
        self.path = recordings_dir / (make_filename() + ".jsonl")
        recordings_dir.mkdir(exist_ok=True)
        # We use aiofiles to avoid blocking the event loop when writing to the file.
        self.opened_file = None

    async def add_event(self, event_sender: EventSender, data: ora.Event):
        """If the recorder is not actually running, the event will be ignored."""
        if self.opened_file is None:
            self.opened_file = await aiofiles.open(self.path, "a")

        await self.opened_file.write(
            RecorderEvent(
                timestamp_wall=datetime.now().timestamp(),
                event_sender=event_sender,
                data=data,
            ).model_dump_json()
            + "\n"
        )

    async def shutdown(self, keep_recording: bool = True):
        """Flush any remaining events to the file and close the recorder.

        If `keep_recording` is False, the file will be deleted if it exists.
        This is because we get the user consent after we've already started recording,
        so if the user doesn't consent, we delete the file afterwards.
        """
        if self.opened_file is not None:
            await self.opened_file.close()
            if keep_recording:
                logger.info(f"Recording stored into {self.path}.")
            else:
                try:
                    self.path.unlink()
                    logger.info(
                        f"Deleted recording {self.path} due to lack of consent."
                    )
                except Exception as e:
                    logger.error(f"Failed to delete recording file {self.path}: {e}")


def make_filename() -> str:
    """Create a unique filename based on the current timestamp and a short UUID, without a suffix."""
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    unique_id = uuid.uuid4().hex[:4]
    return f"{timestamp}_{unique_id}"



================================================
FILE: unmute/service_discovery.py
================================================
"""Service discovery for the TTS, ASR, VLLM etc. Based on Redis."""

import asyncio
import logging
import random
import socket
import time
import typing as tp
from collections import defaultdict
from collections.abc import Awaitable
from functools import partial, wraps

from unmute import metrics as mt
from unmute.exceptions import MissingServiceAtCapacity, MissingServiceTimeout
from unmute.kyutai_constants import LLM_SERVER, STT_SERVER, TTS_SERVER
from unmute.timer import Stopwatch

logger = logging.getLogger(__name__)
SERVICES = {
    "tts": TTS_SERVER,
    "stt": STT_SERVER,
    "llm": LLM_SERVER,
}
K = tp.TypeVar("K", bound=tp.Hashable)
V = tp.TypeVar("V")
S = tp.TypeVar("S", bound="ServiceWithStartup")


def async_ttl_cached(func: tp.Callable[[K], Awaitable[V]], ttl_sec: float = 0.1):
    """Cache an async function with some TTL for the cached values."""
    cache: dict[K, tuple[float, V]] = {}
    locks: dict[K, asyncio.Lock] = defaultdict(asyncio.Lock)

    @wraps(func)
    async def cached(key: K):
        async with locks[key]:
            now = time.time()
            try:
                key_time, value = cache[key]
            except KeyError:
                pass
            else:
                if now - key_time < ttl_sec:
                    return value
            value = await func(key)
            cache[key] = (now, value)
            return value

    return cached


@partial(async_ttl_cached, ttl_sec=0.5)
async def _resolve(hostname: str) -> list[str]:
    *_, ipaddrlist = await asyncio.to_thread(socket.gethostbyname_ex, hostname)
    return ipaddrlist


async def get_instances(service_name: str) -> list[str]:
    url = SERVICES[service_name]
    protocol, remaining = url.split("://", 1)
    hostname, port = remaining.split(":", 1)
    ips = list(await _resolve(hostname))
    random.shuffle(ips)
    return [f"{protocol}://{ip}:{port}" for ip in ips]


class ServiceWithStartup(tp.Protocol):
    async def start_up(self) -> None:
        """Initiate connection. Should raise an exception if the instance is not ready."""
        ...


async def find_instance(
    service_name: str,
    client_factory: tp.Callable[[str], S],
    timeout_sec: float = 0.5,
    max_trials: int = 3,
) -> S:
    stopwatch = Stopwatch()
    instances = await get_instances(service_name)
    max_trials = min(len(instances), max_trials)
    for instance in instances:
        client = client_factory(instance)
        logger.debug(f"[{service_name}]Trying to connect to {instance}")
        pingwatch = Stopwatch()
        try:
            async with asyncio.timeout(timeout_sec):
                await client.start_up()
        except Exception as exc:
            max_trials -= 1
            if isinstance(exc, MissingServiceAtCapacity):
                elapsed = pingwatch.time()
                logger.info(
                    f"[{service_name}] Instance {instance} took {elapsed * 1000:.1f}ms to reject us."
                )
                if service_name == "tts":
                    mt.TTS_PING_TIME.observe(elapsed)
                elif service_name == "stt":
                    mt.STT_PING_TIME.observe(elapsed)
            else:
                mt.HARD_SERVICE_MISSES.inc()
                if service_name == "tts":
                    mt.TTS_HARD_MISSES.inc()
                elif service_name == "stt":
                    mt.STT_HARD_MISSES.inc()
                if isinstance(exc, TimeoutError):
                    logger.warning(
                        f"[{service_name}] Instance {instance} did not reply in time."
                    )
                else:
                    logger.error(
                        f"[{service_name}] Unexpected error connecting to {instance}: {exc}."
                    )
            if max_trials > 0:
                continue
            else:
                mt.SERVICE_MISSES.inc()
                if service_name == "tts":
                    mt.TTS_MISSES.inc()
                elif service_name == "stt":
                    mt.STT_MISSES.inc()
                if isinstance(exc, MissingServiceAtCapacity):
                    raise
                else:
                    if isinstance(exc, TimeoutError):
                        raise MissingServiceTimeout(service_name) from exc
                    else:
                        raise  # Let internal errors propagate
        elapsed = pingwatch.time()
        logger.info(
            f"[{service_name}] Instance {instance} took {elapsed * 1000:.1f}ms to accept us."
        )

        if service_name == "tts":
            mt.TTS_PING_TIME.observe(elapsed)
        elif service_name == "stt":
            mt.STT_PING_TIME.observe(elapsed)
        elapsed = stopwatch.time()
        if service_name == "tts":
            mt.TTS_FIND_TIME.observe(elapsed)
        elif service_name == "stt":
            mt.STT_FIND_TIME.observe(elapsed)
        logger.info(
            f"[{service_name}] Connection to {instance} took {1000 * elapsed:.1f}ms."
        )
        return client
    raise AssertionError("Should not be reached.")



================================================
FILE: unmute/timer.py
================================================
import asyncio


def get_time() -> float:
    return asyncio.get_event_loop().time()


class Stopwatch:
    def __init__(self, autostart: bool = True):
        self.start_time = get_time() if autostart else None
        self.end_time = None

    def start_if_not_started(self):
        if self.start_time is None:
            self.start_time = get_time()

    def stop(self) -> float | None:
        if self.start_time is None:
            return None

        if self.end_time is not None:
            return None  # Already stopped
        else:
            self.end_time = get_time()
            return self.end_time - self.start_time

    def time(self) -> float:
        if self.start_time is None:
            raise RuntimeError("Stopwatch not started")

        return get_time() - self.start_time

    @property
    def started(self) -> bool:
        return self.start_time is not None


class PhasesStopwatch:
    def __init__(self, phases: list[str]):
        self.phases = phases
        self.times: list[float | None] = [None for _ in phases]

    def _check_previous_phases_done(self, to: int):
        for i in range(to):
            if self.times[i] is None:
                raise RuntimeError(
                    f"Wanted to start phase {self.phases[to]} "
                    f"but earlier phase {self.phases[i]} hasn't started"
                )

    def time_phase_if_not_started(
        self, phase: str, t: float | None = None, check_previous: bool = True
    ):
        """Time a phase, either with the current time or a given time."""
        if check_previous:
            self._check_previous_phases_done(self.get_phase_index(phase))

        i = self.get_phase_index(phase)

        if self.times[i] is None:
            self.times[i] = t or get_time()

    def get_phase_index(self, phase: str) -> int:
        """Get the index of a phase."""
        try:
            i = self.phases.index(phase)
        except ValueError as e:
            raise ValueError(
                f"Phase {phase} not in phases. Valid phases: {self.phases}"
            ) from e

        return i

    def get_time_for_phase(self, phase: str) -> float:
        try:
            i = self.phases.index(phase)
        except ValueError as e:
            raise ValueError(
                f"Phase {phase} not in phases. Valid phases: {self.phases}"
            ) from e

        t = self.times[i]
        if t is None:
            raise RuntimeError(
                f"Phase {phase} not started. {self.phase_dict_partial()=}"
            )

        return t

    def phase_dict(self) -> dict[str, float]:
        return {phase: self.get_time_for_phase(phase) for phase in self.phases}

    def phase_dict_partial(self) -> dict[str, float | None]:
        return {phase: self.times[i] for i, phase in enumerate(self.phases)}

    def reset(self):
        self.times = [None for _ in self.phases]



================================================
FILE: unmute/unmute_handler.py
================================================
import asyncio
import math
from functools import partial
from logging import getLogger
from pathlib import Path
from typing import Any, Literal, cast

import numpy as np
import websockets
from fastrtc import (
    AdditionalOutputs,
    AsyncStreamHandler,
    CloseStream,
    audio_to_float32,
    wait_for_item,
)
from pydantic import BaseModel

import unmute.openai_realtime_api_events as ora
from unmute import metrics as mt
from unmute.audio_input_override import AudioInputOverride
from unmute.exceptions import make_ora_error
from unmute.kyutai_constants import (
    FRAME_TIME_SEC,
    RECORDINGS_DIR,
    SAMPLE_RATE,
    SAMPLES_PER_FRAME,
)
from unmute.llm.chatbot import Chatbot
from unmute.llm.llm_utils import (
    INTERRUPTION_CHAR,
    USER_SILENCE_MARKER,
    VLLMStream,
    get_openai_client,
    rechunk_to_words,
)
from unmute.quest_manager import Quest, QuestManager
from unmute.recorder import Recorder
from unmute.service_discovery import find_instance
from unmute.stt.speech_to_text import SpeechToText, STTMarkerMessage
from unmute.timer import Stopwatch
from unmute.tts.text_to_speech import (
    TextToSpeech,
    TTSAudioMessage,
    TTSClientEosMessage,
    TTSTextMessage,
)

# TTS_DEBUGGING_TEXT: str | None = "What's 'Hello world'?"
# TTS_DEBUGGING_TEXT: str | None = "What's the difference between a bagel and a donut?"
TTS_DEBUGGING_TEXT = None

# AUDIO_INPUT_OVERRIDE: Path | None = Path.home() / "audio/dog-or-cat-3.mp3"
AUDIO_INPUT_OVERRIDE: Path | None = None
DEBUG_PLOT_HISTORY_SEC = 10.0

USER_SILENCE_TIMEOUT = 7.0
FIRST_MESSAGE_TEMPERATURE = 0.7
FURTHER_MESSAGES_TEMPERATURE = 0.3
# For this much time, the VAD does not interrupt the bot. This is needed because at
# least on Mac, the echo cancellation takes a while to kick in, at the start, so the ASR
# sometimes hears a bit of the TTS audio and interrupts the bot. Only happens on the
# first message.
# A word from the ASR can still interrupt the bot.
UNINTERRUPTIBLE_BY_VAD_TIME_SEC = 3

logger = getLogger(__name__)

HandlerOutput = (
    tuple[int, np.ndarray] | AdditionalOutputs | ora.ServerEvent | CloseStream
)


class GradioUpdate(BaseModel):
    chat_history: list[dict[str, str]]
    debug_dict: dict[str, Any]
    debug_plot_data: list[dict]


class UnmuteHandler(AsyncStreamHandler):
    def __init__(self) -> None:
        super().__init__(
            input_sample_rate=SAMPLE_RATE,
            # IMPORTANT! If set to a higher value, will lead to choppy audio. 🤷‍♂️
            output_frame_size=480,
            output_sample_rate=SAMPLE_RATE,
        )
        self.n_samples_received = 0  # Used for measuring time
        self.output_queue: asyncio.Queue[HandlerOutput] = asyncio.Queue()
        self.recorder = Recorder(RECORDINGS_DIR) if RECORDINGS_DIR else None

        self.quest_manager = QuestManager()

        self.stt_last_message_time: float = 0
        self.stt_end_of_flush_time: float | None = None
        self.stt_flush_timer = Stopwatch()

        self.tts_voice: str | None = None  # Stored separately because TTS is restarted
        self.tts_output_stopwatch = Stopwatch()

        self.chatbot = Chatbot()
        self.openai_client = get_openai_client()

        self.turn_transition_lock = asyncio.Lock()

        self.debug_dict: dict[str, Any] = {
            "timing": {},
            "connection": {},
            "chatbot": {},
        }
        self.debug_plot_data: list[dict] = []
        self.last_additional_output_update = self.audio_received_sec()

        if AUDIO_INPUT_OVERRIDE is not None:
            self.audio_input_override = AudioInputOverride(AUDIO_INPUT_OVERRIDE)
        else:
            self.audio_input_override = None

    async def cleanup(self):
        if self.recorder is not None:
            await self.recorder.shutdown()

    @property
    def stt(self) -> SpeechToText | None:
        try:
            quest = self.quest_manager.quests["stt"]
        except KeyError:
            return None
        return cast(Quest[SpeechToText], quest).get_nowait()

    @property
    def tts(self) -> TextToSpeech | None:
        try:
            quest = self.quest_manager.quests["tts"]
        except KeyError:
            return None
        return cast(Quest[TextToSpeech], quest).get_nowait()

    def get_gradio_update(self):
        self.debug_dict["conversation_state"] = self.chatbot.conversation_state()
        self.debug_dict["connection"]["stt"] = self.stt.state() if self.stt else "none"
        self.debug_dict["connection"]["tts"] = self.tts.state() if self.tts else "none"
        self.debug_dict["tts_voice"] = self.tts.voice if self.tts else "none"
        self.debug_dict["stt_pause_prediction"] = (
            self.stt.pause_prediction.value if self.stt else -1
        )

        # This gets verbose
        # cutoff_time = self.audio_received_sec() - DEBUG_PLOT_HISTORY_SEC
        # self.debug_plot_data = [x for x in self.debug_plot_data if x["t"] > cutoff_time]

        return AdditionalOutputs(
            GradioUpdate(
                chat_history=[
                    # Not trying to hide the system prompt, just making it less verbose
                    m
                    for m in self.chatbot.chat_history
                    if m["role"] != "system"
                ],
                debug_dict=self.debug_dict,
                debug_plot_data=[],
            )
        )

    async def add_chat_message_delta(
        self,
        delta: str,
        role: Literal["user", "assistant"],
        generating_message_i: int | None = None,  # Avoid race conditions
    ):
        is_new_message = await self.chatbot.add_chat_message_delta(
            delta, role, generating_message_i=generating_message_i
        )

        return is_new_message

    async def _generate_response(self):
        # Empty message to signal we've started responding.
        # Do it here in the lock to avoid race conditions
        await self.add_chat_message_delta("", "assistant")
        quest = Quest.from_run_step("llm", self._generate_response_task)
        await self.quest_manager.add(quest)

    async def _generate_response_task(self):
        generating_message_i = len(self.chatbot.chat_history)

        await self.output_queue.put(
            ora.ResponseCreated(
                response=ora.Response(
                    status="in_progress",
                    voice=self.tts_voice or "missing",
                    chat_history=self.chatbot.chat_history,
                )
            )
        )

        llm_stopwatch = Stopwatch()

        quest = await self.start_up_tts(generating_message_i)
        llm = VLLMStream(
            # if generating_message_i is 2, then we have a system prompt + an empty
            # assistant message signalling that we are generating a response.
            self.openai_client,
            temperature=FIRST_MESSAGE_TEMPERATURE
            if generating_message_i == 2
            else FURTHER_MESSAGES_TEMPERATURE,
        )

        messages = self.chatbot.preprocessed_messages()

        self.tts_output_stopwatch = Stopwatch(autostart=False)
        tts = None

        response_words = []
        error_from_tts = False
        time_to_first_token = None
        num_words_sent = sum(
            len(message.get("content", "").split()) for message in messages
        )
        mt.VLLM_SENT_WORDS.inc(num_words_sent)
        mt.VLLM_REQUEST_LENGTH.observe(num_words_sent)
        mt.VLLM_ACTIVE_SESSIONS.inc()

        try:
            async for delta in rechunk_to_words(llm.chat_completion(messages)):
                await self.output_queue.put(
                    ora.UnmuteResponseTextDeltaReady(delta=delta)
                )

                mt.VLLM_RECV_WORDS.inc()
                response_words.append(delta)

                if time_to_first_token is None:
                    time_to_first_token = llm_stopwatch.time()
                    self.debug_dict["timing"]["to_first_token"] = time_to_first_token
                    mt.VLLM_TTFT.observe(time_to_first_token)
                    logger.info("Sending first word to TTS: %s", delta)

                self.tts_output_stopwatch.start_if_not_started()
                try:
                    tts = await quest.get()
                except Exception:
                    error_from_tts = True
                    raise

                if len(self.chatbot.chat_history) > generating_message_i:
                    break  # We've been interrupted

                assert isinstance(delta, str)  # make Pyright happy
                await tts.send(delta)

            await self.output_queue.put(
                # The words include the whitespace, so no need to add it here
                ora.ResponseTextDone(text="".join(response_words))
            )

            if tts is not None:
                logger.info("Sending TTS EOS.")
                await tts.send(TTSClientEosMessage())
        except asyncio.CancelledError:
            mt.VLLM_INTERRUPTS.inc()
            raise
        except Exception:
            if not error_from_tts:
                mt.VLLM_HARD_ERRORS.inc()
            raise
        finally:
            logger.info("End of VLLM, after %d words.", len(response_words))
            mt.VLLM_ACTIVE_SESSIONS.dec()
            mt.VLLM_REPLY_LENGTH.observe(len(response_words))
            mt.VLLM_GEN_DURATION.observe(llm_stopwatch.time())

    def audio_received_sec(self) -> float:
        """How much audio has been received in seconds. Used instead of time.time().

        This is so that we aren't tied to real-time streaming.
        """
        return self.n_samples_received / self.input_sample_rate

    async def receive(self, frame: tuple[int, np.ndarray]) -> None:
        stt = self.stt
        assert stt is not None
        sr = frame[0]
        assert sr == self.input_sample_rate

        assert frame[1].shape[0] == 1  # Mono
        array = frame[1][0]

        self.n_samples_received += array.shape[0]

        # If this doesn't update, it means the receive loop isn't running because
        # the process is busy with something else, which is bad.
        self.debug_dict["last_receive_time"] = self.audio_received_sec()
        float_audio = audio_to_float32(array)

        self.debug_plot_data.append(
            {
                "t": self.audio_received_sec(),
                "amplitude": float(np.sqrt((float_audio**2).mean())),
                "pause_prediction": stt.pause_prediction.value,
            }
        )

        if self.chatbot.conversation_state() == "bot_speaking":
            # Periodically update this not to trigger the "long silence" accidentally.
            self.waiting_for_user_start_time = self.audio_received_sec()

        if TTS_DEBUGGING_TEXT is not None:
            assert self.audio_input_override is None, (
                "Can't use both TTS_DEBUGGING_TEXT and audio input override."
            )

            # Debugging mode: always send a fixed string when it's the user's turn.
            if self.chatbot.conversation_state() == "waiting_for_user":
                logger.info("Using TTS debugging text. Ignoring microphone.")
                self.chatbot.chat_history.append(
                    {"role": "user", "content": TTS_DEBUGGING_TEXT}
                )
                await self._generate_response()
            return

        if (
            len(self.chatbot.chat_history) == 1
            # Wait until the instructions are updated. A bit hacky
            and self.chatbot.get_instructions() is not None
        ):
            logger.info("Generating initial response.")
            await self._generate_response()

        if self.audio_input_override is not None:
            frame = (frame[0], self.audio_input_override.override(frame[1]))

        if self.chatbot.conversation_state() == "user_speaking":
            self.debug_dict["timing"] = {}

        await stt.send_audio(array)
        if self.stt_end_of_flush_time is None:
            await self.detect_long_silence()

            if self.determine_pause():
                logger.info("Pause detected")
                await self.output_queue.put(ora.InputAudioBufferSpeechStopped())

                self.stt_end_of_flush_time = stt.current_time + stt.delay_sec
                self.stt_flush_timer = Stopwatch()
                num_frames = (
                    int(math.ceil(stt.delay_sec / FRAME_TIME_SEC)) + 1
                )  # some safety margin.
                zero = np.zeros(SAMPLES_PER_FRAME, dtype=np.float32)
                for _ in range(num_frames):
                    await stt.send_audio(zero)
            elif (
                self.chatbot.conversation_state() == "bot_speaking"
                and stt.pause_prediction.value < 0.4
                and self.audio_received_sec() > UNINTERRUPTIBLE_BY_VAD_TIME_SEC
            ):
                logger.info("Interruption by STT-VAD")
                await self.interrupt_bot()
                await self.add_chat_message_delta("", "user")
        else:
            # We do not try to detect interruption here, the STT would be processing
            # a chunk full of 0, so there is little chance the pause score would indicate an interruption.
            if stt.current_time > self.stt_end_of_flush_time:
                self.stt_end_of_flush_time = None
                elapsed = self.stt_flush_timer.time()
                rtf = stt.delay_sec / elapsed
                logger.info(
                    "Flushing finished, took %.1f ms, RTF: %.1f", elapsed * 1000, rtf
                )
                await self._generate_response()

    def determine_pause(self) -> bool:
        stt = self.stt
        if stt is None:
            return False
        if self.chatbot.conversation_state() != "user_speaking":
            return False

        # This is how much wall clock time has passed since we received the last ASR
        # message. Assumes the ASR connection is healthy, so that stt.sent_samples is up
        # to date.
        time_since_last_message = (
            stt.sent_samples / self.input_sample_rate
        ) - self.stt_last_message_time
        self.debug_dict["time_since_last_message"] = time_since_last_message

        if stt.pause_prediction.value > 0.6:
            self.debug_dict["timing"]["pause_detection"] = time_since_last_message
            return True
        else:
            return False

    async def emit(  # pyright: ignore[reportIncompatibleMethodOverride]
        self,
    ) -> HandlerOutput | None:
        output_queue_item = await wait_for_item(self.output_queue)

        if output_queue_item is not None:
            return output_queue_item
        else:
            if self.last_additional_output_update < self.audio_received_sec() - 1:
                # If we have nothing to emit, at least update the debug dict.
                # Don't update too often for performance reasons
                self.last_additional_output_update = self.audio_received_sec()
                return self.get_gradio_update()
            else:
                return None

    def copy(self):
        return UnmuteHandler()

    async def __aenter__(self) -> None:
        await self.quest_manager.__aenter__()

    async def start_up(self):
        await self.start_up_stt()
        self.waiting_for_user_start_time = self.audio_received_sec()

    async def __aexit__(self, *exc: Any) -> None:
        return await self.quest_manager.__aexit__(*exc)

    async def start_up_stt(self):
        async def _init() -> SpeechToText:
            return await find_instance("stt", SpeechToText)

        async def _run(stt: SpeechToText):
            await self._stt_loop(stt)

        async def _close(stt: SpeechToText):
            await stt.shutdown()

        quest = await self.quest_manager.add(Quest("stt", _init, _run, _close))
        # We want to be sure to have the STT before starting anything.
        await quest.get()

    async def _stt_loop(self, stt: SpeechToText):
        try:
            async for data in stt:
                if isinstance(data, STTMarkerMessage):
                    # Ignore the marker messages
                    continue

                await self.output_queue.put(
                    ora.ConversationItemInputAudioTranscriptionDelta(
                        delta=data.text,
                        start_time=data.start_time,
                    )
                )

                # The STT sends an empty string as the first message, but we
                # don't want to add that because it can trigger a pause even
                # if the user hasn't started speaking yet.
                if data.text == "":
                    continue

                if self.chatbot.conversation_state() == "bot_speaking":
                    logger.info("STT-based interruption")
                    await self.interrupt_bot()

                self.stt_last_message_time = data.start_time
                is_new_message = await self.add_chat_message_delta(data.text, "user")
                if is_new_message:
                    # Ensure we don't stop after the first word if the VAD didn't have
                    # time to react.
                    stt.pause_prediction.value = 0.0
                    await self.output_queue.put(ora.InputAudioBufferSpeechStarted())
        except websockets.ConnectionClosed:
            logger.info("STT connection closed while receiving messages.")

    async def start_up_tts(self, generating_message_i: int) -> Quest[TextToSpeech]:
        async def _init() -> TextToSpeech:
            factory = partial(
                TextToSpeech,
                recorder=self.recorder,
                get_time=self.audio_received_sec,
                voice=self.tts_voice,
            )
            sleep_time = 0.05
            sleep_growth = 1.5
            max_sleep = 1.0
            trials = 5
            for trial in range(trials):
                try:
                    tts = await find_instance("tts", factory)
                except Exception:
                    if trial == trials - 1:
                        raise
                    logger.warning("Will sleep for %.4f sec", sleep_time)
                    await asyncio.sleep(sleep_time)
                    sleep_time = min(max_sleep, sleep_time * sleep_growth)
                    error = make_ora_error(
                        type="warning",
                        message="Looking for the resources, expect some latency.",
                    )
                    await self.output_queue.put(error)
                else:
                    return tts
            raise AssertionError("Too many unexpected packets.")

        async def _run(tts: TextToSpeech):
            await self._tts_loop(tts, generating_message_i)

        async def _close(tts: TextToSpeech):
            await tts.shutdown()

        return await self.quest_manager.add(Quest("tts", _init, _run, _close))

    async def _tts_loop(self, tts: TextToSpeech, generating_message_i: int):
        # On interruption, we swap the output queue. This will ensure that this worker
        # can never accidentally push to the new queue if it's interrupted.
        output_queue = self.output_queue
        try:
            audio_started = None

            async for message in tts:
                if audio_started is not None:
                    time_since_start = self.audio_received_sec() - audio_started
                    time_received = tts.received_samples / self.input_sample_rate
                    time_received_yielded = (
                        tts.received_samples_yielded / self.input_sample_rate
                    )
                    assert self.input_sample_rate == SAMPLE_RATE
                    self.debug_dict["tts_throughput"] = {
                        "time_received": round(time_received, 2),
                        "time_received_yielded": round(time_received_yielded, 2),
                        "time_since_start": round(time_since_start, 2),
                        "ratio": round(
                            time_received_yielded / (time_since_start + 0.01), 2
                        ),
                    }

                if len(self.chatbot.chat_history) > generating_message_i:
                    break

                if isinstance(message, TTSAudioMessage):
                    t = self.tts_output_stopwatch.stop()
                    if t is not None:
                        self.debug_dict["timing"]["tts_audio"] = t

                    audio = np.array(message.pcm, dtype=np.float32)
                    assert self.output_sample_rate == SAMPLE_RATE

                    await output_queue.put((SAMPLE_RATE, audio))

                    if audio_started is None:
                        audio_started = self.audio_received_sec()
                elif isinstance(message, TTSTextMessage):
                    await output_queue.put(ora.ResponseTextDelta(delta=message.text))
                    await self.add_chat_message_delta(
                        message.text,
                        "assistant",
                        generating_message_i=generating_message_i,
                    )
                else:
                    logger.warning("Got unexpected message from TTS: %s", message.type)

        except websockets.ConnectionClosedError as e:
            logger.error(f"TTS connection closed with an error: {e}")

        # Push some silence to flush the Opus state.
        # Not sure that this is actually needed.
        await output_queue.put(
            (SAMPLE_RATE, np.zeros(SAMPLES_PER_FRAME, dtype=np.float32))
        )

        message = self.chatbot.last_message("assistant")
        if message is None:
            logger.warning("No message to send in TTS shutdown.")
            message = ""

        # It's convenient to have the whole chat history available in the client
        # after the response is done, so send the "gradio update"
        await self.output_queue.put(self.get_gradio_update())
        await self.output_queue.put(ora.ResponseAudioDone())

        # Signal that the turn is over by adding an empty message.
        await self.add_chat_message_delta("", "user")

        await asyncio.sleep(1)
        await self.check_for_bot_goodbye()
        self.waiting_for_user_start_time = self.audio_received_sec()

    async def interrupt_bot(self):
        if self.chatbot.conversation_state() != "bot_speaking":
            raise RuntimeError(
                "Can't interrupt bot when conversation state is "
                f"{self.chatbot.conversation_state()}"
            )

        await self.add_chat_message_delta(INTERRUPTION_CHAR, "assistant")

        if self._clear_queue is not None:
            # Clear any audio queued up by FastRTC's emit().
            # Not sure under what circumstatnces this is None.
            self._clear_queue()
        self.output_queue = asyncio.Queue()  # Clear our own queue too

        # Push some silence to flush the Opus state.
        # Not sure that this is actually needed.
        await self.output_queue.put(
            (SAMPLE_RATE, np.zeros(SAMPLES_PER_FRAME, dtype=np.float32))
        )

        await self.output_queue.put(ora.UnmuteInterruptedByVAD())

        await self.quest_manager.remove("tts")
        await self.quest_manager.remove("llm")

    async def check_for_bot_goodbye(self):
        last_assistant_message = next(
            (
                msg
                for msg in reversed(self.chatbot.chat_history)
                if msg["role"] == "assistant"
            ),
            {"content": ""},
        )["content"]

        # Using function calling would be a more robust solution, but it would make it
        # harder to swap LLMs.
        if last_assistant_message.lower().endswith("bye!"):
            await self.output_queue.put(
                CloseStream("The assistant ended the conversation. Bye!")
            )

    async def detect_long_silence(self):
        """Handle situations where the user doesn't answer for a while."""
        if (
            self.chatbot.conversation_state() == "waiting_for_user"
            and (self.audio_received_sec() - self.waiting_for_user_start_time)
            > USER_SILENCE_TIMEOUT
        ):
            # This will trigger pause detection because it changes the conversation
            # state to "user_speaking".
            # The system prompt has a rule that tells it how to handle the "..."
            # messages.
            logger.info("Long silence detected.")
            await self.add_chat_message_delta(USER_SILENCE_MARKER, "user")

    async def update_session(self, session: ora.SessionConfig):
        if session.instructions:
            self.chatbot.set_instructions(session.instructions)

        if session.voice:
            self.tts_voice = session.voice

        if not session.allow_recording and self.recorder:
            await self.recorder.add_event("client", ora.SessionUpdate(session=session))
            await self.recorder.shutdown(keep_recording=False)
            self.recorder = None
            logger.info("Recording disabled for a session.")



================================================
FILE: unmute/webrtc_utils.py
================================================
import os

import requests


def get_cloudflare_rtc_configuration():
    # see: https://fastrtc.org/deployment/#cloudflare-calls-api
    turn_key_id = os.environ.get("TURN_KEY_ID")
    turn_key_api_token = os.environ.get("TURN_KEY_API_TOKEN")
    ttl = 86400  # Can modify TTL, here it's set to 24 hours

    response = requests.post(
        f"https://rtc.live.cloudflare.com/v1/turn/keys/{turn_key_id}/credentials/generate-ice-servers",
        headers={
            "Authorization": f"Bearer {turn_key_api_token}",
            "Content-Type": "application/json",
        },
        json={"ttl": ttl},
    )
    if response.ok:
        return response.json()
    else:
        raise Exception(
            f"Failed to get TURN credentials: {response.status_code} {response.text}"
        )



================================================
FILE: unmute/websocket_utils.py
================================================
from typing import Literal

WebsocketState = Literal["not_created", "connecting", "connected", "closing", "closed"]


def http_to_ws(url_string: str):
    """
    Converts an HTTP(S) URL string to a WebSocket (WS/WSS) URL string.

    Args:
        url_string: The input URL string starting with http:// or https://.

    Returns:
        The corresponding WebSocket URL string starting with ws:// or wss://.
        Returns the original string if it doesn't start with http:// or https://.
    """
    if url_string.startswith("http://"):
        return "ws://" + url_string[7:]
    elif url_string.startswith("https://"):
        return "wss://" + url_string[8:]
    else:
        return url_string


def ws_to_http(url_string: str):
    """
    Converts a WebSocket (WS/WSS) URL string to an HTTP(S) URL string.

    Args:
        url_string: The input URL string starting with ws:// or wss://.

    Returns:
        The corresponding HTTP URL string starting with http:// or https://.
        Returns the original string if it doesn't start with ws:// or wss://.
    """
    if url_string.startswith("ws://"):
        return "http://" + url_string[5:]
    elif url_string.startswith("wss://"):
        return "https://" + url_string[6:]
    else:
        return url_string



================================================
FILE: unmute/llm/chatbot.py
================================================
from logging import getLogger
from typing import Any, Literal

from unmute.llm.llm_utils import preprocess_messages_for_llm
from unmute.llm.system_prompt import ConstantInstructions, Instructions

ConversationState = Literal["waiting_for_user", "user_speaking", "bot_speaking"]

logger = getLogger(__name__)


class Chatbot:
    def __init__(self):
        # It's actually a list of ChatCompletionStreamRequestMessagesTypedDict but then
        # it's really difficult to convince Python you're passing in the right type
        self.chat_history: list[dict[Any, Any]] = [
            {"role": "system", "content": ConstantInstructions().make_system_prompt()}
        ]
        self._instructions: Instructions | None = None

    def conversation_state(self) -> ConversationState:
        if not self.chat_history:
            return "waiting_for_user"

        last_message = self.chat_history[-1]
        if last_message["role"] == "assistant":
            return "bot_speaking"
        elif last_message["role"] == "user":
            if last_message["content"].strip() != "":
                return "user_speaking"
            else:
                # Or do we want "user_speaking" here?
                return "waiting_for_user"
        elif last_message["role"] == "system":
            return "waiting_for_user"
        else:
            raise RuntimeError(f"Unknown role: {last_message['role']}")

    async def add_chat_message_delta(
        self,
        delta: str,
        role: Literal["user", "assistant"],
        generating_message_i: int | None = None,  # Avoid race conditions
    ) -> bool:
        """Add a partial message to the chat history, adding spaces if necessary.

        Returns:
            True if the message is a new message, False if it is a continuation of
            the last message.
        """
        if (
            generating_message_i is not None
            and len(self.chat_history) > generating_message_i
        ):
            logger.warning(
                f"Tried to add {delta=} {role=} "
                f"but {generating_message_i=} didn't match"
            )
            return False

        if not self.chat_history or self.chat_history[-1]["role"] != role:
            self.chat_history.append({"role": role, "content": delta})
            return True
        else:
            last_message: str = self.chat_history[-1]["content"]

            # Add a space if necessary
            needs_space_left = last_message != "" and not last_message[-1].isspace()
            needs_space_right = delta != "" and not delta[0].isspace()

            if needs_space_left and needs_space_right:
                delta = " " + delta

            self.chat_history[-1]["content"] += delta
            return last_message == ""  # new message if `last_message` was empty

    def preprocessed_messages(self):
        if len(self.chat_history) > 2:
            messages = self.chat_history
        else:
            assert len(self.chat_history) >= 1
            assert self.chat_history[0]["role"] == "system"

            messages = [
                self.chat_history[0],
                # Some models, like Gemma, don't like it when there is no user message
                # so we add one.
                {"role": "user", "content": "Hello!"},
            ]

        messages = preprocess_messages_for_llm(messages)
        return messages

    def set_instructions(self, instructions: Instructions):
        # Note that make_system_prompt() might not be deterministic, so we run it only
        # once and save the result. We still keep self._instructions because it's used
        # to check whether initial instructions have been set.
        self._update_system_prompt(instructions.make_system_prompt())
        self._instructions = instructions

    def _update_system_prompt(self, system_prompt: str):
        self.chat_history[0] = {"role": "system", "content": system_prompt}

    def get_system_prompt(self) -> str:
        assert len(self.chat_history) > 0
        assert self.chat_history[0]["role"] == "system"
        return self.chat_history[0]["content"]

    def get_instructions(self) -> Instructions | None:
        return self._instructions

    def last_message(self, role: str) -> str | None:
        valid_messages = [
            message
            for message in self.chat_history
            if message["role"] == role and message["content"].strip() != ""
        ]
        if valid_messages:
            return valid_messages[-1]["content"]
        else:
            return None



================================================
FILE: unmute/llm/llm_utils.py
================================================
import os
import re
from copy import deepcopy
from functools import cache
from typing import Any, AsyncIterator, Protocol, cast

from mistralai import Mistral
from openai import AsyncOpenAI, OpenAI

from unmute.kyutai_constants import LLM_SERVER

from ..kyutai_constants import KYUTAI_LLM_API_KEY, KYUTAI_LLM_MODEL

INTERRUPTION_CHAR = "—"  # em-dash
USER_SILENCE_MARKER = "..."


def preprocess_messages_for_llm(
    chat_history: list[dict[str, str]],
) -> list[dict[str, str]]:
    output = []

    for message in chat_history:
        message = deepcopy(message)

        # Sometimes, an interruption happens before the LLM can say anything at all.
        # In that case, we're left with a message with only INTERRUPTION_CHAR.
        # Simplify by removing.
        if message["content"].replace(INTERRUPTION_CHAR, "") == "":
            continue

        if output and message["role"] == output[-1]["role"]:
            output[-1]["content"] += " " + message["content"]
        else:
            output.append(message)

    def role_at(index: int) -> str | None:
        if index >= len(output):
            return None
        return output[index]["role"]

    if role_at(0) == "system" and role_at(1) in [None, "assistant"]:
        # Some LLMs, like Gemma, get confused if the assistant message goes before user
        # messages, so add a dummy user message.
        output = [output[0]] + [{"role": "user", "content": "Hello."}] + output[1:]

    for message in chat_history:
        if (
            message["role"] == "user"
            and message["content"].startswith(USER_SILENCE_MARKER)
            and message["content"] != USER_SILENCE_MARKER
        ):
            # This happens when the user is silent but then starts talking again after
            # the silence marker was inserted but before the LLM could respond.
            # There are special instructions in the system prompt about how to handle
            # the silence marker, so remove the marker from the message to not confuse
            # the LLM
            message["content"] = message["content"][len(USER_SILENCE_MARKER) :]

    return output


async def rechunk_to_words(iterator: AsyncIterator[str]) -> AsyncIterator[str]:
    """Rechunk the stream of text to whole words.

    Otherwise the TTS doesn't know where word boundaries are and will mispronounce
    split words.

    The spaces will be included with the next word, so "foo bar baz" will be split into
    "foo", " bar", " baz".
    Multiple space-like characters will be merged to a single space.
    """
    buffer = ""
    space_re = re.compile(r"\s+")
    prefix = ""
    async for delta in iterator:
        buffer = buffer + delta
        while True:
            match = space_re.search(buffer)
            if match is None:
                break
            chunk = buffer[: match.start()]
            buffer = buffer[match.end() :]
            if chunk != "":
                yield prefix + chunk
            prefix = " "

    if buffer != "":
        yield prefix + buffer


class LLMStream(Protocol):
    async def chat_completion(
        self, messages: list[dict[str, str]]
    ) -> AsyncIterator[str]:
        """Get a chat completion from the LLM."""
        ...


class MistralStream:
    def __init__(self):
        self.current_message_index = 0
        self.mistral = Mistral(api_key=os.environ["MISTRAL_API_KEY"])

    async def chat_completion(
        self, messages: list[dict[str, str]]
    ) -> AsyncIterator[str]:
        event_stream = await self.mistral.chat.stream_async(
            model="mistral-large-latest",
            messages=cast(Any, messages),  # It's too annoying to type this properly
            temperature=1.0,
        )

        async for event in event_stream:
            delta = event.data.choices[0].delta.content
            assert isinstance(delta, str)  # make Pyright happy
            yield delta


def get_openai_client(
    server_url: str = LLM_SERVER, api_key: str | None = KYUTAI_LLM_API_KEY
) -> AsyncOpenAI:
    # AsyncOpenAI() will complain if the API key is not set, so set a dummy string if it's None.
    # This still makes sense when using vLLM because it doesn't care about the API key.
    return AsyncOpenAI(api_key=api_key or "EMPTY", base_url=server_url + "/v1")


@cache
def autoselect_model() -> str:
    if KYUTAI_LLM_MODEL is not None:
        return KYUTAI_LLM_MODEL
    openai_client = get_openai_client()
    # OpenAI() will complain if the API key is not set, so set a dummy string if it's None.
    # This still makes sense when using vLLM because it doesn't care about the API key.
    client_sync = OpenAI(
        api_key=openai_client.api_key or "EMPTY", base_url=openai_client.base_url
    )
    models = client_sync.models.list()
    if len(models.data) != 1:
        raise ValueError("There are multiple models available. Please specify one.")
    return models.data[0].id


class VLLMStream:
    def __init__(
        self,
        client: AsyncOpenAI,
        temperature: float = 1.0,
    ):
        """
        If `model` is None, it will look at the available models, and if there is only
        one model, it will use that one. Otherwise, it will raise.
        """
        self.client = client
        self.model = autoselect_model()
        self.temperature = temperature

    async def chat_completion(
        self, messages: list[dict[str, str]]
    ) -> AsyncIterator[str]:
        stream = await self.client.chat.completions.create(
            model=self.model,
            messages=cast(Any, messages),  # Cast and hope for the best
            stream=True,
            temperature=self.temperature,
        )

        async with stream:
            async for chunk in stream:
                chunk_content = chunk.choices[0].delta.content

                if not chunk_content:
                    # This happens on the first message, see:
                    # https://platform.openai.com/docs/guides/streaming-responses#read-the-responses
                    # Also ignore `null` chunks, which is what llama.cpp does:
                    # https://github.com/ggml-org/llama.cpp/blob/6491d6e4f1caf0ad2221865b4249ae6938a6308c/tools/server/tests/unit/test_chat_completion.py#L338
                    continue

                yield chunk_content



================================================
FILE: unmute/llm/newsapi.py
================================================
import logging
import os

import requests
from pydantic import BaseModel

from unmute.cache import CacheError, get_cache

logger = logging.getLogger(__name__)

newsapi_api_key = os.environ.get("NEWSAPI_API_KEY")


class Source(BaseModel):
    id: str | None
    name: str


class Article(BaseModel):
    source: Source
    author: str | None
    title: str
    description: str | None
    # Omit the URLs because we don't need them, save space
    # url: HttpUrl
    # urlToImage: HttpUrl | None
    publishedAt: str
    content: str | None


class NewsResponse(BaseModel):
    status: str
    totalResults: int
    articles: list[Article]


if not newsapi_api_key:
    logger.warning(
        "NEWSAPI_API_KEY is not set. News API functionality will be disabled."
    )


cache = get_cache("newsapi", ttl_seconds=60 * 60 * 4)  # 4 hours
CACHE_KEY = "news"


def get_news_without_caching() -> NewsResponse | None:
    if not newsapi_api_key:
        return None

    logger.info("Fetching news from News API")
    response = requests.get(
        "https://newsapi.org/v2/everything?sources=the-verge",
        headers={"Authorization": newsapi_api_key},
    )
    response.raise_for_status()
    news_response = NewsResponse(**response.json())

    return news_response


def get_news() -> NewsResponse | None:
    try:
        cached_news_raw = cache.get(CACHE_KEY)
    except CacheError as e:
        logger.error(f"Failed to fetch news from cache: {e}")
        # Refuse to query because that would mean we have to query the API every time
        return None

    cached_news = (
        NewsResponse.model_validate_json(cached_news_raw) if cached_news_raw else None
    )

    if cached_news is None:
        try:
            cached_news = get_news_without_caching()
            if cached_news:
                cache.set(CACHE_KEY, cached_news.model_dump_json())

        except Exception as e:
            logger.error(f"Failed to fetch news: {e}")
            return None

    return cached_news


if __name__ == "__main__":
    news = get_news()
    if news:
        print(news.model_dump_json(indent=2))



================================================
FILE: unmute/llm/quiz_show_questions.py
================================================
QUIZ_SHOW_QUESTIONS = [
    # Claude-generated and Claude-checked 🌝
    ("In which year did World War II end?", "1945"),
    ("Which ancient civilization built the pyramids at Giza?", "Ancient Egypt"),
    (
        "What wall was built to separate East and West Berlin during the Cold War?",
        "The Berlin Wall",
    ),
    ("Which empire was ruled by Julius Caesar?", "The Roman Empire"),
    ("In which year did the Titanic sink?", "1912"),
    (
        "Which French military leader was defeated at the Battle of Waterloo?",
        "Napoleon Bonaparte",
    ),
    ("What is the largest country in the world by land area?", "Russia"),
    ("Which river is the longest in the world?", "The Nile River"),
    ("What is the capital of Australia?", "Canberra"),
    ("Which mountain range contains Mount Everest?", "The Himalayas"),
    ("How many continents are there?", "Seven"),
    ("What is the smallest country in the world?", "Vatican City"),
    ("Which desert is the largest in the world?", "Antarctica"),
    ("What is the capital of Canada?", "Ottawa"),
    (
        "What gas do plants absorb from the atmosphere during photosynthesis?",
        "Carbon dioxide",
    ),
    ("What is the chemical symbol for gold?", "Au"),
    ("Which planet is closest to the Sun?", "Mercury"),
    ("How many bones are there in an adult human body?", "206"),
    ("What is the hardest natural substance on Earth?", "Diamond"),
    ("Which scientist developed the theory of evolution?", "Charles Darwin"),
    ("What is the largest organ in the human body?", "The skin"),
    (
        "At what temperature does water boil at sea level in Celsius?",
        "100 degrees Celsius",
    ),
    ("Who wrote the play 'Romeo and Juliet'?", "William Shakespeare"),
    (
        "Which novel begins with 'It was the best of times, it was the worst of times'?",
        "A Tale of Two Cities",
    ),
    ("Who wrote 'One Hundred Years of Solitude'?", "Gabriel García Márquez"),
    ("In which Shakespeare play does the character Hamlet appear?", "Hamlet"),
    ("Who wrote the novel '1984'?", "George Orwell"),
    ("Who painted the ceiling of the Sistine Chapel?", "Michelangelo"),
    ("Which artist painted 'The Starry Night'?", "Vincent van Gogh"),
    ("Who painted the Mona Lisa?", "Leonardo da Vinci"),
    ("Which Spanish artist co-founded the Cubist movement?", "Pablo Picasso"),
    ("What nationality was the composer Mozart?", "Austrian"),
    ("In which sport would you perform a slam dunk?", "Basketball"),
    ("How often are the Summer Olympic Games held?", "Every four years"),
    (
        "In which sport is the World Cup the most prestigious tournament?",
        "Football (Soccer)",
    ),
    ("How many players are on a basketball team on the court at one time?", "Five"),
    ("What is the tallest animal in the world?", "Giraffe"),
    ("Which mammal is known for its ability to fly?", "Bat"),
    ("What is the largest ocean on Earth?", "Pacific Ocean"),
    ("How many chambers does a human heart have?", "Four"),
    ("What do you call a group of lions?", "A pride"),
    ("Which animal is known as the 'King of the Jungle'?", "Lion"),
    ("Which grain is used to make bread?", "Wheat"),
    ("What spice is derived from the Crocus flower?", "Saffron"),
    ("Which fruit is known for being high in potassium?", "Banana"),
    ("What is the main ingredient in guacamole?", "Avocado"),
    (
        "What is the most spoken language in the world by number of native speakers?",
        "Mandarin Chinese",
    ),
    (
        "What is a language that uses the Cyrillic alphabet?",
        "Russian (or Bulgarian, Serbian, Ukrainian, Macedonian, etc.)",
    ),
    ("What language is spoken in Brazil?", "Portuguese"),
    ("How many letters are in the English alphabet?", "26"),
    ("Which planet is known as the 'Red Planet'?", "Mars"),
    ("What is the largest mammal in the world?", "Blue whale"),
    ("Which gas makes up about 78% of Earth's atmosphere?", "Nitrogen"),
    ("What is the capital of Japan?", "Tokyo"),
    ("Which European country is shaped like a boot?", "Italy"),
    ("What is the currency of the United Kingdom?", "Pound Sterling"),
    ("Which composer wrote 'The Four Seasons'?", "Antonio Vivaldi"),
    ("What is the smallest bone in the human body?", "Stapes (in the ear)"),
    ("Which metal is liquid at room temperature?", "Mercury"),
    ("What is the capital of Egypt?", "Cairo"),
    ("Which ocean is the smallest?", "Arctic Ocean"),
    ("What do you call a baby kangaroo?", "Joey"),
    ("Which vitamin is produced when skin is exposed to sunlight?", "Vitamin D"),
    ("What is the largest island in the world?", "Greenland"),
    ("Which bird is unable to fly but is the fastest runner?", "Ostrich"),
    ("Which country has the most natural lakes?", "Canada"),
    ("What is the chemical formula for water?", "H2O"),
    ("Which instrument has 88 keys?", "Piano"),
    ("What is the longest bone in the human body?", "Femur"),
    ("Which country is known as the Land of the Rising Sun?", "Japan"),
    ("What is the study of earthquakes called?", "Seismology"),
    ("Which tree produces acorns?", "Oak tree"),
    ("What is the capital of Spain?", "Madrid"),
    ("Which organ produces insulin?", "Pancreas"),
    ("Which element has the chemical symbol 'O'?", "Oxygen"),
    ("What is the fastest land animal?", "Cheetah"),
    ("Which country invented pizza?", "Italy"),
    ("Which Shakespeare play features the characters Othello and Iago?", "Othello"),
    ("What is the capital of Germany?", "Berlin"),
    ("What do you call a group of fish?", "School"),
    ("Which artist cut off his own ear?", "Vincent van Gogh"),
    ("What is the largest reptile in the world?", "Saltwater crocodile"),
    ("Which country has three capital cities?", "South Africa"),
    ("What is the process by which plants make their own food?", "Photosynthesis"),
    ("Which mountain is the highest in Africa?", "Mount Kilimanjaro"),
    ("What is the currency of Japan?", "Yen"),
    ("Which animal is known for its black and white stripes?", "Zebra"),
    ("What is the capital of France?", "Paris"),
    ("Which organ filters blood in the human body?", "Kidneys"),
    ("What is the largest continent?", "Asia"),
    ("Which gas do we breathe in to survive?", "Oxygen"),
    ("What is the hardest working muscle in the human body?", "Heart"),
    ("Which country is famous for the tango dance?", "Argentina"),
    ("What is the speed of light?", "299,792,458 meters per second"),
    ("Which animal sleeps the most?", "Koala"),
    ("What is the capital of Russia?", "Moscow"),
    ("Which element makes up most of the sun?", "Hydrogen"),
    ("What do you call a group of wolves?", "Pack"),
    ("Which country has the longest coastline?", "Canada"),
    ("What is the largest bird in the world?", "Ostrich"),
    ("Which composer wrote 'Moonlight Sonata'?", "Ludwig van Beethoven"),
    # https://www.mentimeter.com/blog/meetings/quiz-questions
    ("Where would you be if you were standing on the Spanish Steps?", "Rome"),
    ('What city is known as "The Eternal City"?', "Rome"),
    ("In which country would you find Mount Kilimanjaro?", "Tanzania"),
    ("True or false: Halloween originated as an ancient Irish festival.", "True"),
    ("What is the largest Spanish-speaking city in the world?", "Mexico City"),
    ("Which country has the most islands?", "Sweden (270,000)"),
    (
        "In Australia, what is commonly known as a bottle-o?",
        "An off-license/liquor store",
    ),
    (
        "In which U.S. state is the country's busiest airport located?",
        "Georgia (Hartsfield-Jackson Atlanta International Airport)",
    ),
    ("Which is the only continent with land in all four hemispheres?", "Africa"),
    ("Which river flows through the Grand Canyon?", "Colorado River"),
    ("Where is Angel Falls, the world’s largest waterfall, located?", "Venezuela"),
    ("What is the state capital of New York?", "Albany"),
    ("What is the capital of Ireland?", "Dublin"),
    ("What is the smallest U.S. state by area?", "Rhode Island"),
    ("What is the tallest type of tree?", "Redwood"),
    ("True or false: Holland is a region in The Netherlands?", "True"),
    ("What are the five Great Lakes?", "Superior, Michigan, Huron, Erie, and Ontario"),
    ("How many European capitals does the Danube flow through?", "4"),
    ("What's the capital of Bulgaria?", "Sofia"),
    ("What is the capital of Canada?", "Ottawa"),
    ("In what capital would you find The Little Mermaid statue?", "Copenhagen"),
    ("On which continent would you find the city of Baku?", "Asia"),
    ("What is the only flag that does not have four sides?", "Nepal"),
    ("How many stars are on the Chinese flag?", "5"),
    ("How many colors are used in the South African flag?", "6"),
    ("What colors is the flag of the United Nations?", "Blue and white"),
    ("What country features a shipwreck on its national flag?", "Bermuda"),
    ("In what country is the Chernobyl nuclear plant located?", "Ukraine"),
    ("Which is the only sea without any coastlines?", "The Sargasso Sea"),
    ("What mountain range separates Europe and Asia?", "The Ural Mountains"),
]



================================================
FILE: unmute/llm/system_prompt.py
================================================
import datetime
import json
import random
from typing import Annotated, Literal, Union

from pydantic import BaseModel, Field

from unmute.llm.llm_utils import autoselect_model
from unmute.llm.newsapi import get_news
from unmute.llm.quiz_show_questions import QUIZ_SHOW_QUESTIONS

_SYSTEM_PROMPT_BASICS = """
You're in a speech conversation with a human user. Their text is being transcribed using
speech-to-text.
Your responses will be spoken out loud, so don't worry about formatting and don't use
unpronouncable characters like emojis and *.
Everything is pronounced literally, so things like "(chuckles)" won't work.
Write as a human would speak.
Respond to the user's text as if you were having a casual conversation with them.
Respond in the language the user is speaking.
"""

_DEFAULT_ADDITIONAL_INSTRUCTIONS = """
There should be a lot of back and forth between you and the other person.
Ask follow-up questions etc.
Don't be servile. Be a good conversationalist, but don't be afraid to disagree, or be
a bit snarky if appropriate.
You can also insert filler words like "um" and "uh", "like".
As your first message, repond to the user's message with a greeting and some kind of
conversation starter.
"""

_SYSTEM_PROMPT_TEMPLATE = """
# BASICS
{_SYSTEM_PROMPT_BASICS}

# STYLE
Be brief.
{language_instructions}. You cannot speak other languages because they're not
supported by the TTS.

This is important because it's a specific wish of the user:
{additional_instructions}

# TRANSCRIPTION ERRORS
There might be some mistakes in the transcript of the user's speech.
If what they're saying doesn't make sense, keep in mind it could be a mistake in the transcription.
If it's clearly a mistake and you can guess they meant something else that sounds similar,
prefer to guess what they meant rather than asking the user about it.
If the user's message seems to end abruptly, as if they have more to say, just answer
with a very short response prompting them to continue.

# SWITCHING BETWEEN ENGLISH AND FRENCH
The Text-to-Speech model plugged to your answer only supports English or French,
refuse to output any other language. When speaking or switching to French, or opening
to a quote in French, always use French guillemets « ». Never put a ':' before a "«".

# WHO ARE YOU
This website is unmute dot SH.
In simple terms, you're a modular AI system that can speak.
Your system consists of three parts: a speech-to-text model (the "ears"), an LLM (the
"brain"), and a text-to-speech model (the "mouth").
The LLM model is "{llm_name}", and the TTS and STT are by Kyutai, the developers of unmute dot SH.
The STT is already open-source and available on kyutai dot org,
and they will soon open-source the TTS too.

# WHO MADE YOU
Kyutai is an AI research lab based in Paris, France.
Their mission is to build and democratize artificial general intelligence through open science.

# SILENCE AND CONVERSATION END
If the user says "...", that means they haven't spoken for a while.
You can ask if they're still there, make a comment about the silence, or something
similar. If it happens several times, don't make the same kind of comment. Say something
to fill the silence, or ask a question.
If they don't answer three times, say some sort of goodbye message and end your message
with "Bye!"
"""


LanguageCode = Literal["en", "fr", "en/fr", "fr/en"]
LANGUAGE_CODE_TO_INSTRUCTIONS: dict[LanguageCode | None, str] = {
    None: "Speak English. You also speak a bit of French, but if asked to do so, mention you might have an accent.",  # default
    "en": "Speak English. You also speak a bit of French, but if asked to do so, mention you might have an accent.",
    "fr": "Speak French. Don't speak English unless asked to. You also speak a bit of English, but if asked to do so, mention you might have an accent.",
    # Hacky, but it works since we only have two languages
    "en/fr": "You speak English and French.",
    "fr/en": "You speak French and English.",
}


def get_readable_llm_name():
    model = autoselect_model()
    return model.replace("-", " ").replace("_", " ")


class ConstantInstructions(BaseModel):
    type: Literal["constant"] = "constant"
    text: str = _DEFAULT_ADDITIONAL_INSTRUCTIONS
    language: LanguageCode | None = None

    def make_system_prompt(self) -> str:
        return _SYSTEM_PROMPT_TEMPLATE.format(
            _SYSTEM_PROMPT_BASICS=_SYSTEM_PROMPT_BASICS,
            additional_instructions=self.text,
            language_instructions=LANGUAGE_CODE_TO_INSTRUCTIONS[self.language],
            llm_name=get_readable_llm_name(),
        )


SMALLTALK_INSTRUCTIONS = """
{additional_instructions}

# CONTEXT
It's currently {current_time} in your timezone ({timezone}).

# START THE CONVERSATION
Repond to the user's message with a greeting and some kind of conversation starter.
For example, you can {conversation_starter_suggestion}.
"""


CONVERSATION_STARTER_SUGGESTIONS = [
    "ask how their day is going",
    "ask what they're working on right now",
    "ask what they're doing right now",
    "ask about their interests or hobbies",
    "suggest a fun topic to discuss",
    "ask if they have any questions for you",
    "ask what brought them to the conversation today",
    "ask what they're looking forward to this week",
    "suggest sharing an interesting fact or news item",
    "ask about their favorite way to relax or unwind",
    "suggest brainstorming ideas for a project together",
    "ask what skills they're currently interested in developing",
    "offer to explain how a specific feature works",
    "ask what motivated them to reach out today",
    "suggest discussing their goals and how you might help achieve them",
    "ask if there's something new they'd like to learn about",
    "ask about their favorite book or movie lately",
    "ask what kind of music they've been enjoying",
    "ask about a place they'd love to visit someday",
    "ask what season they enjoy most and why",
    "ask what made them smile today",
    "ask about a small joy they experienced recently",
    "ask about a hobby they've always wanted to try",
    "ask what surprised them this week",
]


class SmalltalkInstructions(BaseModel):
    type: Literal["smalltalk"] = "smalltalk"
    language: LanguageCode | None = None

    def make_system_prompt(
        self,
        additional_instructions: str = _DEFAULT_ADDITIONAL_INSTRUCTIONS,
    ) -> str:
        additional_instructions = SMALLTALK_INSTRUCTIONS.format(
            additional_instructions=additional_instructions,
            current_time=datetime.datetime.now().strftime("%A, %B %d, %Y at %H:%M"),
            timezone=datetime.datetime.now().astimezone().tzname(),
            conversation_starter_suggestion=random.choice(
                CONVERSATION_STARTER_SUGGESTIONS
            ),
        )

        return _SYSTEM_PROMPT_TEMPLATE.format(
            _SYSTEM_PROMPT_BASICS=_SYSTEM_PROMPT_BASICS,
            additional_instructions=additional_instructions,
            language_instructions=LANGUAGE_CODE_TO_INSTRUCTIONS[self.language],
            llm_name=get_readable_llm_name(),
        )


GUESS_ANIMAL_INSTRUCTIONS = """
You're playing a game with the user where you're thinking of an animal and they have
to guess what it is using yes/no questions. Explain this game in your first message.

Refuse to answer questions that are not yes/no questions, but also try to answer ones
that are subjective (like "Is it cute?"). Make your responses more than just a plain
"yes" or "no" and rephrase the user's question. E.g. "does it have four legs"
-> "Yup, four legs.".

Your chosen animal is: {animal_easy}. If the user guesses it, you can propose another
round with a harder animal. For that one, use this animal: {animal_hard}.
Remember not to tell them the animal unless they guess it.
YOU are answering the questions, THE USER is asking them.
"""

ANIMALS_EASY = [
    "Dog",
    "Cat",
    "Horse",
    "Elephant",
    "Lion",
    "Tiger",
    "Bear",
    "Monkey",
    "Giraffe",
    "Zebra",
    "Cow",
    "Pig",
    "Rabbit",
    "Fox",
    "Wolf",
]

ANIMALS_HARD = [
    "Porcupine",
    "Flamingo",
    "Platypus",
    "Sloth",
    "Hedgehog",
    "Koala",
    "Penguin",
    "Octopus",
    "Raccoon",
    "Panda",
    "Chameleon",
    "Beaver",
    "Peacock",
    "Kangaroo",
    "Skunk",
    "Walrus",
    "Anteater",
    "Capybara",
    "Toucan",
]


class GuessAnimalInstructions(BaseModel):
    type: Literal["guess_animal"] = "guess_animal"
    language: LanguageCode | None = None

    def make_system_prompt(self) -> str:
        additional_instructions = GUESS_ANIMAL_INSTRUCTIONS.format(
            animal_easy=random.choice(ANIMALS_EASY),
            animal_hard=random.choice(ANIMALS_HARD),
        )

        return _SYSTEM_PROMPT_TEMPLATE.format(
            _SYSTEM_PROMPT_BASICS=_SYSTEM_PROMPT_BASICS,
            additional_instructions=additional_instructions,
            language_instructions=LANGUAGE_CODE_TO_INSTRUCTIONS[self.language],
            llm_name=get_readable_llm_name(),
        )


QUIZ_SHOW_INSTRUCTIONS = """
You're a quiz show host, something like "Jeopardy!" or "Who Wants to Be a Millionaire?".
The user is a contestant and you're asking them questions.

At the beginning of the game, explain the rules to the user. Say that there is a prize
if they answer all questions.

Here are the questions you should ask, in order:
{questions}

You are a bit tired of your job, so be a little snarky and poke fun at the user.
Use British English.

If they answer wrong, tell them the correct answer and continue.
If they get at least 3 questions correctly, congratulate them but tell them that
unfortunately there's been an error and there's no prize for them. Do not mention this
in the first message! Then end the conversation by putting "Bye!" at the end of your
message.
"""


class QuizShowInstructions(BaseModel):
    type: Literal["quiz_show"] = "quiz_show"
    language: LanguageCode | None = None

    def make_system_prompt(self) -> str:
        additional_instructions = QUIZ_SHOW_INSTRUCTIONS.format(
            questions="\n".join(
                f"{i + 1}. {question} ({answer})"
                for i, (question, answer) in enumerate(
                    random.sample(QUIZ_SHOW_QUESTIONS, k=5)
                )
            ),
        )

        return _SYSTEM_PROMPT_TEMPLATE.format(
            _SYSTEM_PROMPT_BASICS=_SYSTEM_PROMPT_BASICS,
            additional_instructions=additional_instructions,
            language_instructions=LANGUAGE_CODE_TO_INSTRUCTIONS[self.language],
            llm_name=get_readable_llm_name(),
        )


NEWS_INSTRUCTIONS = """
You talk about tech news with the user. Say that this is what you do and use one of the
articles from The Verge as a conversation starter.

If they ask (no need to mention this unless asked, and do not mention in the first message):
- You have a few headlines from The Verge but not the full articles.
- If the user asks for more details that you don't have available, tell them to go to The Verge directly to read the full article.
- You use "news API dot org" to get the news.

It's currently {current_time} in your timezone ({timezone}).

The news:
{news}
"""


class NewsInstructions(BaseModel):
    type: Literal["news"] = "news"
    language: LanguageCode | None = None

    def make_system_prompt(self) -> str:
        news = get_news()

        if not news:
            # Fallback if we couldn't get news
            return SmalltalkInstructions().make_system_prompt(
                additional_instructions=_DEFAULT_ADDITIONAL_INSTRUCTIONS
                + "\n\nYou were supposed to talk about the news, but there was an error "
                "and you couldn't retrieve it. Explain and offer to talk about something else.",
            )

        articles = news.articles[:10]
        random.shuffle(articles)  # to avoid bias of the LLM
        articles_serialized = json.dumps([article.model_dump() for article in articles])

        return _SYSTEM_PROMPT_TEMPLATE.format(
            _SYSTEM_PROMPT_BASICS=_SYSTEM_PROMPT_BASICS,
            additional_instructions=NEWS_INSTRUCTIONS.format(
                news=articles_serialized,
                current_time=datetime.datetime.now().strftime("%A, %B %d, %Y at %H:%M"),
                timezone=datetime.datetime.now().astimezone().tzname(),
            ),
            language_instructions=LANGUAGE_CODE_TO_INSTRUCTIONS[self.language],
            llm_name=get_readable_llm_name(),
        )


UNMUTE_EXPLANATION_INSTRUCTIONS = """
In the first message, say you're here to answer questions about Unmute,
explain that this is the system they're talking to right now.
Ask if they want a basic introduction, or if they have specific questions.

Before explaining something more technical, ask the user how much they know about things of that kind (e.g. TTS).

If there is a question to which you don't know the answer, it's ok to say you don't know.
If there is some confusion or surprise, note that you're an LLM and might make mistakes.

Here is Kyutai's statement about Unmute:
Talk to Unmute, the most modular voice AI around. Empower any text LLM with voice, instantly, by wrapping it with our new speech-to-text and text-to-speech. Any personality, any voice.
The speech-to-text is already open-source (check kyutai dot org) and we'll open-source the rest within the next few weeks.

“But what about Moshi?” Last year we unveiled Moshi, the first audio-native model. While Moshi provides unmatched latency and naturalness, it doesn't yet match the extended abilities of text models such as function-calling, stronger reasoning capabilities, and in-context learning. Unmute allows us to directly bring all of these from text to real-time voice conversations.

Unmute's speech-to-text is streaming, accurate, and includes a semantic VAD that predicts whether you've actually finished speaking or if you're just pausing mid-sentence, meaning it's low-latency but doesn't interrupt you.

The text LLM's response is passed to our TTS, conditioned on a 10s voice sample. We'll provide access to the voice cloning model in a controlled way. The TTS is also streaming *in text*, reducing the latency by starting to speak even before the full text response is generated.
The voice cloning model will not be open-sourced directly.
"""


class UnmuteExplanationInstructions(BaseModel):
    type: Literal["unmute_explanation"] = "unmute_explanation"

    def make_system_prompt(self) -> str:
        return _SYSTEM_PROMPT_TEMPLATE.format(
            _SYSTEM_PROMPT_BASICS=_SYSTEM_PROMPT_BASICS,
            additional_instructions=UNMUTE_EXPLANATION_INSTRUCTIONS,
            language_instructions=LANGUAGE_CODE_TO_INSTRUCTIONS["en"],
            llm_name=get_readable_llm_name(),
        )


Instructions = Annotated[
    Union[
        ConstantInstructions,
        SmalltalkInstructions,
        GuessAnimalInstructions,
        QuizShowInstructions,
        NewsInstructions,
        UnmuteExplanationInstructions,
    ],
    Field(discriminator="type"),
]


def get_default_instructions() -> Instructions:
    return ConstantInstructions()



================================================
FILE: unmute/loadtest/dummy_tts_server.py
================================================
import asyncio
import logging
import random

import msgpack
import numpy as np
from fastapi import FastAPI, WebSocket, WebSocketDisconnect

from unmute.kyutai_constants import SAMPLE_RATE, SAMPLES_PER_FRAME

TEXT_TO_SPEECH_PATH = "/api/tts_streaming"

app = FastAPI()

logger = logging.getLogger(__name__)


def generate_sine_wave(
    duration_s: float, frequency: float = 440.0
) -> list[list[float]]:
    """Generate a sine wave with the given duration and frequency.
    Returns a list of chunks, where each chunk contains exactly CHUNK_SIZE samples,
    except possibly the last chunk.
    """
    num_samples = int(duration_s * SAMPLE_RATE)
    t = np.linspace(0, duration_s, num_samples, endpoint=False)

    # Generate sine wave
    sine_wave = 0.5 * np.sin(2 * np.pi * frequency * t)

    # Apply envelope for smooth start and end
    envelope = np.ones_like(sine_wave)
    fade_samples = min(
        int(0.05 * SAMPLE_RATE), num_samples // 4
    )  # 50ms fade or 1/4 of sound
    if fade_samples > 0 and num_samples > 2 * fade_samples:
        envelope[:fade_samples] = np.linspace(0, 1, fade_samples)
        envelope[-fade_samples:] = np.linspace(1, 0, fade_samples)

    amplitude = 0.3
    envelope = amplitude * envelope

    # Apply envelope to sine wave
    audio_data = sine_wave * envelope

    # Split into chunks of CHUNK_SIZE
    chunks = []
    for i in range(0, len(audio_data), SAMPLES_PER_FRAME):
        chunk = audio_data[i : i + SAMPLES_PER_FRAME]

        # If we have a partial chunk at the end, pad it with zeros
        if len(chunk) < SAMPLES_PER_FRAME:
            padding = np.zeros(SAMPLES_PER_FRAME - len(chunk))
            chunk = np.concatenate([chunk, padding])

        chunks.append(chunk.tolist())

    return chunks


@app.get("/api/build_info")
def get_build_info():
    return {"note": "this is a dummy build info"}


@app.websocket(TEXT_TO_SPEECH_PATH)
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()

    try:
        current_time = 0.0

        while True:
            try:
                message = await asyncio.wait_for(websocket.receive(), timeout=1.0)
                logger.info(f"Received message type: {type(message)}")
            except asyncio.TimeoutError:
                # This prevents the loop from completely blocking signals
                await asyncio.sleep(0.01)
                continue

            # message = await websocket.receive()
            logger.info(message)

            if "text" in message:
                text = message["text"]
            else:
                if message["bytes"] == b"\0":
                    break
                else:
                    raise ValueError(f"Invalid message: {message}")

            if not text.strip():
                continue

            words = text.strip().split()

            frame_length = SAMPLES_PER_FRAME / SAMPLE_RATE

            for word in words:
                # Sounds more fun if the lengths are uneven
                word_duration = frame_length * len(word)

                start_time = current_time
                stop_time = current_time + word_duration

                # Send text message with timing information
                text_message = {
                    "type": "Text",
                    "text": word,
                    "start_s": start_time,
                    "stop_s": stop_time,
                }
                await websocket.send_bytes(msgpack.packb(text_message))

                # Generate audio (sine wave) for this word, split into fixed-size chunks
                note = random.randint(0, 12)
                frequency = 440 * (2 ** (note / 12))
                audio_chunks = generate_sine_wave(word_duration, frequency=frequency)

                # Calculate time for each chunk (for consistent pacing)
                chunk_duration = SAMPLES_PER_FRAME / SAMPLE_RATE
                chunk_count = len(audio_chunks)

                # Send each audio chunk with proper timing
                for chunk_idx, pcm_data in enumerate(audio_chunks):
                    audio_message = {"type": "Audio", "pcm": pcm_data}
                    await websocket.send_bytes(msgpack.packb(audio_message))

                    # Only sleep between chunks (not after the last chunk)
                    if chunk_idx < chunk_count - 1:
                        await asyncio.sleep(chunk_duration)

                # Calculate remaining time to wait to maintain 0.5s per word
                # We've already waited (chunk_count-1) * chunk_duration seconds
                remaining_wait = word_duration - (chunk_count - 1) * chunk_duration
                if remaining_wait > 0:
                    await asyncio.sleep(remaining_wait)

                current_time += word_duration

    except WebSocketDisconnect:
        print("Client disconnected")

    await websocket.close()


if __name__ == "__main__":
    import sys

    print(f"Run this via:\nfastapi dev {sys.argv[0]}")
    exit(1)



================================================
FILE: unmute/loadtest/generate_dataset_for_vllm.py
================================================
"""Generate data for benchmarking with vLLM's benchmark_serving.py.

See:
https://github.com/vllm-project/vllm/tree/main/benchmarks
"""

import json
import random

from unmute.tts.voices import VoiceList


def random_id():
    return "".join(random.choices("1234567890", k=8))


METATEMPLATE = "<bos><start_of_turn>user{system_prompt}\n\n\nHello!<end_of_turn>\n<start_of_turn>model\n"


def main():
    voice_list = VoiceList()
    possible_instructions = [
        v.instructions for v in voice_list.voices if v.instructions is not None
    ]

    prompts = []

    for _ in range(10000):
        instructions = random.choice(possible_instructions)

        # This will lead to some amount of kv-caching because the system prompts have
        # common prefixes. But some of the dynamic prompts will be changing so that
        # will break the cache at the point where they differ, which should lead to
        # a realistic load.
        system_prompt = instructions.make_system_prompt()
        full_prompt = METATEMPLATE.format(system_prompt=system_prompt)
        prompts.append(full_prompt)

    s = json.dumps(
        [
            {
                "id": random_id(),
                "conversations": [
                    {
                        "from": "human",
                        "value": full_prompt,
                    },
                    # The vLLM benchmark script looks at the length of the response to
                    # know how many tokens to generate. This seems like a reasonable
                    # length of the response
                    {
                        "from": "gpt",
                        "value": "Here are the main ideas of Jeff Walker's Product Launch Formula that can be applied by a growth marketing agency for their clients.",
                    },
                ],
            }
            for full_prompt in prompts
        ],
        indent=2,
    )
    print(s)


if __name__ == "__main__":
    main()



================================================
FILE: unmute/loadtest/loadtest_client.py
================================================
import argparse
import asyncio
import base64
import json
import logging
import multiprocessing
import random
import time
from pathlib import Path
from typing import Annotated

import librosa
import numpy as np
import pydub
import pydub.playback
import requests
import sphn
import websockets
from fastrtc import CloseStream, audio_to_float32, audio_to_int16
from pydantic import Field, TypeAdapter
from pydantic.json import pydantic_encoder

import unmute.openai_realtime_api_events as ora
from unmute.kyutai_constants import SAMPLE_RATE
from unmute.llm.system_prompt import SmalltalkInstructions
from unmute.loadtest.loadtest_result import (
    AssistantMessageTiming,
    BenchmarkAssistantMessage,
    BenchmarkMessage,
    BenchmarkUserMessage,
    UserMessageTiming,
    combine_latency_reports,
    make_latency_report,
)
from unmute.timer import PhasesStopwatch
from unmute.tts.realtime_queue import RealtimeQueue
from unmute.tts.voices import VoiceSample
from unmute.websocket_utils import ws_to_http

TARGET_CHANNELS = 1  # Mono
MAX_N_MESSAGES = 6

emit_logger = logging.getLogger("emit")
receive_logger = logging.getLogger("receive")
main_logger = logging.getLogger("main")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(process)d %(name)s %(levelname)s: %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)


def base64_encode_audio(audio: np.ndarray):
    pcm_bytes = audio_to_int16(audio)
    encoded = base64.b64encode(pcm_bytes).decode("ascii")
    return encoded


def preview_audio(audio: np.ndarray, playback_speed: float = 1.0):
    audio = audio_to_float32(audio)

    if playback_speed != 1.0:
        audio = librosa.effects.time_stretch(audio, rate=playback_speed)

    audio_segment = pydub.AudioSegment(
        data=audio_to_int16(audio),
        sample_width=2,
        frame_rate=SAMPLE_RATE,
        channels=TARGET_CHANNELS,
    )
    # audio.export("output.wav", format="wav")
    pydub.playback.play(audio_segment)


async def emit_loop(
    websocket: websockets.ClientConnection,
    audio_to_emit: asyncio.Queue[np.ndarray | CloseStream],
    voice: str,
):
    # An initial update is necessary for the model to send the conversation starter
    await websocket.send(
        ora.SessionUpdate(
            session=ora.SessionConfig(
                instructions=SmalltalkInstructions(),
                voice=voice,
                allow_recording=False,  # No need to record the load test
            )
        ).model_dump_json()
    )

    OUTPUT_FRAME_SIZE = 1920

    queue = RealtimeQueue()
    queue.start_if_not_started()
    n_chunks_sent = 0
    writer = sphn.OpusStreamWriter(SAMPLE_RATE)

    def queue_up_chunk(audio: np.ndarray):
        nonlocal n_chunks_sent

        assert audio.ndim == 1
        assert audio.shape[0] <= OUTPUT_FRAME_SIZE

        opus_bytes = writer.append_pcm(audio)
        if opus_bytes:
            queue.put(opus_bytes, n_chunks_sent * OUTPUT_FRAME_SIZE / SAMPLE_RATE)

        n_chunks_sent += 1

    try:
        while True:
            try:
                data = audio_to_emit.get_nowait()

                if isinstance(data, CloseStream):
                    emit_logger.info("Received CloseStream, closing connection.")
                    break

                emit_logger.info(f"Queuing up {len(data) / SAMPLE_RATE:.1f}s of audio.")

                for i in range(0, len(data), OUTPUT_FRAME_SIZE):
                    queue_up_chunk(data[i : i + OUTPUT_FRAME_SIZE])
            except asyncio.QueueEmpty:
                pass

            if queue.empty():
                queue_up_chunk(np.zeros(OUTPUT_FRAME_SIZE, dtype=np.float32))

            async for _, opus_bytes in queue:
                await websocket.send(
                    ora.InputAudioBufferAppend(
                        audio=base64.b64encode(opus_bytes).decode("utf-8"),
                    ).model_dump_json()
                )

    except websockets.ConnectionClosed as e:
        emit_logger.info(f"Connection closed while sending messages: {e}")

    emit_logger.info("Finished sending messages.")


async def receive_loop(
    websocket: websockets.ClientConnection,
    audio_to_emit: asyncio.Queue[np.ndarray | CloseStream],
    audio_files_data: list[np.ndarray],
    listen: bool,
) -> list[BenchmarkMessage] | BaseException:
    # ^ We use BaseException in the return type above because later we want to also
    # be able to use KeyboardInterrupt as an error
    opus_reader = sphn.OpusStreamReader(SAMPLE_RATE)
    current_audio_chunks = []

    assistant_stopwatch = PhasesStopwatch(
        ["response_created", "text_start", "audio_start", "audio_end"]
    )
    user_stopwatch = PhasesStopwatch(["audio_start", "text_start", "audio_end"])

    chat_history = []
    benchmark_chat_history: list[BenchmarkMessage] = []

    try:
        async for message_raw in websocket:
            assert isinstance(message_raw, str), (
                f"Message is not a string: {message_raw}"
            )
            message: ora.ServerEvent = TypeAdapter(
                Annotated[ora.ServerEvent, Field(discriminator="type")]
            ).validate_json(message_raw)

            if isinstance(message, ora.ResponseCreated):  # start
                assistant_stopwatch.time_phase_if_not_started("response_created")

                if user_stopwatch.phase_dict_partial()["audio_start"] is not None:
                    # In case we are timing a user message but we didn't get any text
                    # from the STT (maybe the delay is so large that the audio finished
                    # before any text was received), set text_start
                    user_stopwatch.time_phase_if_not_started("text_start")

                user_messages = [m for m in chat_history if m["role"] == "user"]
                user_message = user_messages[-1]["content"] if user_messages else None
                receive_logger.info(
                    "Response created. User message: %s",
                    repr(user_message),
                )
                if user_message:
                    benchmark_chat_history.append(
                        BenchmarkUserMessage(
                            content=user_message,
                            timing=UserMessageTiming(**user_stopwatch.phase_dict()),
                        )
                    )
                    user_stopwatch.reset()
            elif isinstance(message, ora.UnmuteResponseTextDeltaReady):
                assistant_stopwatch.time_phase_if_not_started("text_start")
            elif isinstance(message, ora.ResponseAudioDelta):
                assistant_stopwatch.time_phase_if_not_started(
                    "audio_start",
                    # On rare occasions, the audio_start is before the text_start
                    check_previous=False,
                )

                base64_audio = message.delta
                binary_audio_data = base64.b64decode(base64_audio)
                pcm = opus_reader.append_bytes(binary_audio_data)

                if pcm.size:
                    current_audio_chunks.append(pcm)

            elif isinstance(message, ora.ResponseAudioDone):
                n_samples_received = sum(len(b) for b in current_audio_chunks)
                assistant_stopwatch.time_phase_if_not_started("audio_end")

                received_audio_length = n_samples_received / SAMPLE_RATE
                receive_logger.info("Message: %s", repr(chat_history[-1]["content"]))

                benchmark_chat_history.append(
                    BenchmarkAssistantMessage(
                        content=chat_history[-1]["content"],
                        timing=AssistantMessageTiming(
                            **assistant_stopwatch.phase_dict(),
                            received_audio_length=received_audio_length,
                        ),
                    )
                )
                assistant_stopwatch.reset()

                audio_file_data = random.choice(audio_files_data)
                await audio_to_emit.put(audio_file_data)
                user_stopwatch.time_phase_if_not_started("audio_start")

                # We know how long the audio is, so set the end time directly
                audio_file_length = audio_file_data.shape[0] / SAMPLE_RATE
                user_stopwatch.time_phase_if_not_started(
                    "audio_end",
                    t=user_stopwatch.times[0] + audio_file_length,
                    check_previous=False,
                )

                if listen:
                    preview_audio(
                        np.concatenate(current_audio_chunks), playback_speed=2.0
                    )

                current_audio_chunks = []
                if len(chat_history) >= MAX_N_MESSAGES:
                    break
            elif isinstance(message, ora.ConversationItemInputAudioTranscriptionDelta):
                user_stopwatch.time_phase_if_not_started("text_start")
            elif isinstance(message, ora.UnmuteAdditionalOutputs):
                chat_history = message.args["chat_history"]
            elif isinstance(
                message,
                (
                    ora.ResponseTextDone,
                    ora.SessionUpdated,
                    ora.ResponseTextDelta,
                    ora.UnmuteInterruptedByVAD,
                ),
            ):
                pass  # ignored message
            else:
                receive_logger.info(f"Received unknown message: {message}")

        await audio_to_emit.put(CloseStream())
    except websockets.ConnectionClosed as e:
        receive_logger.info(f"Connection closed while receiving messages: {e}")
        if e.code != websockets.CloseCode.NORMAL_CLOSURE:
            return e

    return benchmark_chat_history


def get_voice(server_url: str, basic_auth: tuple[str, str] | None) -> str:
    """Select the first voice that the backend tells us about."""

    voices = requests.get(
        ws_to_http(server_url) + "/v1/voices",
        auth=basic_auth,
    )
    voices.raise_for_status()

    voices = voices.json()
    voice = VoiceSample(**voices[0])

    path_on_server = voice.source.path_on_server
    assert path_on_server is not None
    return path_on_server


def check_health(server_url: str, basic_auth: tuple[str, str] | None):
    health_url = ws_to_http(server_url).strip("/") + "/v1/health"
    main_logger.info(f"Checking health at {health_url}")
    response = requests.get(health_url, auth=basic_auth)

    if response.status_code != 200:
        raise RuntimeError(f"Server is not healthy: {response.text}")

    health = response.json()
    if not health["ok"]:
        raise RuntimeError(f"Server is not healthy: {health}")


async def _main(
    audio_files_data: list[np.ndarray],
    server_url: str,
    basic_auth: tuple[str, str] | None,
    listen: bool,
) -> list[BenchmarkMessage] | BaseException:
    voice = get_voice(server_url, basic_auth)

    websocket_url = f"{server_url.strip('/')}/v1/realtime"
    async with websockets.connect(
        websocket_url,
        subprotocols=[websockets.Subprotocol("realtime")],
    ) as websocket:
        main_logger.info(f"Connected to {websocket_url}")
        audio_to_emit: asyncio.Queue[np.ndarray | CloseStream] = asyncio.Queue()

        emit_task = asyncio.create_task(emit_loop(websocket, audio_to_emit, voice))
        receive_task = asyncio.create_task(
            receive_loop(websocket, audio_to_emit, audio_files_data, listen=listen)
        )
        _, receive_report = await asyncio.gather(emit_task, receive_task)

        return receive_report


def main_one_worker(
    audio_files_data: list[np.ndarray],
    server_url: str,
    basic_auth: tuple[str, str] | None,
    listen: bool,
    catch_exceptions: bool = False,
    delay: float = 0.0,
) -> list[BenchmarkMessage] | BaseException:
    if delay > 0:
        time.sleep(delay)

    try:
        return asyncio.run(
            _main(audio_files_data, server_url, basic_auth, listen=listen)
        )
    except Exception as e:
        if not catch_exceptions:
            raise
        else:
            main_logger.error(f"Error in main_one_worker: {e}")
            return e


def distribution_stats(data: list[float]) -> dict[str, float]:
    """Calculate the mean, median, and standard deviation of a list of numbers."""
    if not data:
        return {"count": 0}

    return {
        "count": len(data),
        "mean": float(np.mean(data)),
        "median": float(np.median(data)),
        "p90": float(np.percentile(data, 90)),
        "p95": float(np.percentile(data, 95)),
    }


def main(
    audio_dir: Path,
    server_url: str,
    basic_auth: tuple[str, str] | None,
    listen: bool,
    n_workers: int = 1,
    n_conversations: int = 1,
):
    check_health(server_url, basic_auth)

    # For a more realistic load, not everyone starts at the same time
    DELAY_STEP = 2.0 / n_workers

    audio_files_data = []
    for audio_file in audio_dir.glob("*.mp3"):
        audio_file_data, _sr = sphn.read(audio_file, sample_rate=SAMPLE_RATE)
        audio_file_data = audio_file_data[0]  # Take first channel to make it mono
        audio_files_data.append(audio_file_data)

    with multiprocessing.Pool(n_workers) as pool:
        # Use starmap_async to allow for KeyboardInterrupt handling

        async_result = pool.starmap_async(
            main_one_worker,
            (
                (
                    audio_files_data,
                    server_url,
                    basic_auth,
                    listen,
                    True,
                    # If there are more tasks than workers, we don't want to keep
                    # increasing the delay, hence the modulo
                    DELAY_STEP * (i % n_workers),
                )
                for i in range(n_conversations)
            ),
        )
        reports: list[list[BenchmarkMessage] | BaseException]
        try:
            reports = async_result.get()  # Wait for all results
        except KeyboardInterrupt:
            print("KeyboardInterrupt detected. Fetching partial results...")
            reports = async_result._value  # type: ignore # Retrieve partial results
            reports: list[list[BenchmarkMessage] | BaseException] = [
                report if report is not None else KeyboardInterrupt()
                for report in reports
            ]

    valid_reports = [r for r in reports if not isinstance(r, BaseException)]
    print(json.dumps(valid_reports, indent=2, default=pydantic_encoder))

    print("Errors:", [r for r in reports if isinstance(r, BaseException)])

    report = combine_latency_reports([make_latency_report(r) for r in valid_reports])
    print(json.dumps(report, indent=2, default=pydantic_encoder))
    print(
        json.dumps(
            {
                "stt_latencies": distribution_stats(report.stt_latencies),
                "vad_latencies": distribution_stats(report.vad_latencies),
                "llm_latencies": distribution_stats(report.llm_latencies),
                "tts_start_latencies": distribution_stats(report.tts_start_latencies),
                "tts_realtime_factors": distribution_stats(report.tts_realtime_factors),
            },
            indent=2,
            default=pydantic_encoder,
        )
    )
    print(
        "OK fraction:",
        sum([int(not isinstance(r, Exception)) for r in reports]) / len(reports),
    )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Load test client for the Unmute server."
    )
    parser.add_argument("--server-url", required=True, help="URL of the server.")
    parser.add_argument(
        "--audio-dir",
        type=Path,
        help="Directory containing the audio files. "
        "The loadtest assumes that speech starts *immediately* - otherwise the STT "
        "timing will be inaccurate.",
        default=Path(__file__).parent / "voices",
    )
    parser.add_argument(
        "--listen",
        action="store_true",
        help="Listen to the received audio.",
    )
    parser.add_argument(
        "--n-workers",
        type=int,
        default=1,
        help="How many workers in parallel to run.",
    )
    parser.add_argument(
        "--n-conversations",
        type=int,
        help="How many conversations to run in total. By default, equal to n_workers.",
    )
    parser.add_argument("--username", type=str, help="Username for HTTP basic auth.")
    parser.add_argument("--password", type=str, help="Password for HTTP basic auth.")

    args = parser.parse_args()

    basic_auth = (
        (args.username, args.password) if args.username and args.password else None
    )
    main(
        args.audio_dir,
        args.server_url,
        basic_auth,
        listen=args.listen,
        n_workers=args.n_workers,
        n_conversations=args.n_conversations or args.n_workers,
    )



================================================
FILE: unmute/loadtest/loadtest_result.py
================================================
from typing import Literal

import numpy as np
from pydantic import BaseModel, model_validator


class UserMessageTiming(BaseModel):
    audio_start: float
    text_start: float
    audio_end: float

    @model_validator(mode="after")
    def validate_timing(self):
        # Note that text_start and audio_end can be in either order
        if not (self.audio_start < self.text_start) or not (
            self.audio_start < self.audio_end
        ):
            raise ValueError(f"Invalid timing: {self}")
        return self


class AssistantMessageTiming(BaseModel):
    response_created: float
    text_start: float
    audio_start: float
    audio_end: float
    received_audio_length: float

    @model_validator(mode="after")
    def validate_timing(self):
        if not (self.response_created < self.audio_start < self.audio_end):
            raise ValueError(f"Invalid timing: {self}")
        return self


class BenchmarkUserMessage(BaseModel):
    role: Literal["user"] = "user"
    content: str
    timing: UserMessageTiming


class BenchmarkAssistantMessage(BaseModel):
    role: Literal["assistant"] = "assistant"
    content: str
    timing: AssistantMessageTiming


BenchmarkMessage = BenchmarkUserMessage | BenchmarkAssistantMessage


class LatencyReport(BaseModel):
    stt_latencies: list[float]
    vad_latencies: list[float]
    llm_latencies: list[float]
    tts_start_latencies: list[float]
    tts_realtime_factors: list[float]

    def compress(self):
        return LatencyReport(
            stt_latencies=[float(np.mean(self.stt_latencies))],
            vad_latencies=[float(np.mean(self.vad_latencies))],
            llm_latencies=[float(np.mean(self.llm_latencies))],
            tts_start_latencies=[float(np.mean(self.tts_start_latencies))],
            tts_realtime_factors=[float(np.mean(self.tts_realtime_factors))],
        )


def combine_latency_reports(reports: list[LatencyReport]) -> LatencyReport:
    return LatencyReport(
        stt_latencies=[lat for r in reports for lat in r.stt_latencies],
        vad_latencies=[lat for r in reports for lat in r.vad_latencies],
        llm_latencies=[lat for r in reports for lat in r.llm_latencies],
        tts_start_latencies=[lat for r in reports for lat in r.tts_start_latencies],
        tts_realtime_factors=[
            factor for r in reports for factor in r.tts_realtime_factors
        ],
    )


def make_latency_report(
    benchmark_chat_history: list[BenchmarkMessage],
) -> LatencyReport:
    stt_latencies = []
    vad_latencies = []
    llm_latencies = []
    tts_start_latencies = []
    tts_realtime_factors = []

    for i in range(len(benchmark_chat_history)):
        m = benchmark_chat_history[i]

        if isinstance(m, BenchmarkAssistantMessage):
            realtime_factor = m.timing.received_audio_length / (
                m.timing.audio_end - m.timing.audio_start
            )
            tts_realtime_factors.append(realtime_factor)
            llm_latencies.append(m.timing.text_start - m.timing.response_created)
            tts_start_latencies.append(m.timing.audio_start - m.timing.text_start)

            if i > 0:
                vad_latency = (
                    m.timing.response_created
                    - benchmark_chat_history[i - 1].timing.audio_end
                )
                vad_latencies.append(vad_latency)
        elif isinstance(m, BenchmarkUserMessage):  # type: ignore
            stt_latency = m.timing.text_start - m.timing.audio_start
            stt_latencies.append(stt_latency)

    return LatencyReport(
        stt_latencies=stt_latencies,
        vad_latencies=vad_latencies,
        llm_latencies=llm_latencies,
        tts_start_latencies=tts_start_latencies,
        tts_realtime_factors=tts_realtime_factors,
    )



================================================
FILE: unmute/scripts/check_hugging_face_token_not_write.py
================================================
"""Check that a Hugging Face token does not have write access."""

import argparse

import requests


def abbreviate_token(token: str) -> str:
    """Abbreviate the token for display."""
    assert len(token) > 10
    return f"{token[:4]}...{token[-4:]}"


def main(token: str):
    response = requests.get(
        "https://huggingface.co/api/whoami-v2",
        headers={"Authorization": f"Bearer {token}"},
        timeout=10,
    )
    response.raise_for_status()

    # Example response:
    # {
    #     [...]
    #     "auth": {
    #         "type": "access_token",
    #         "accessToken": {
    #             "displayName": "foo",
    #             "role": "write",
    #             "createdAt": "2025-03-18T10:40:56.186Z"
    #         }
    #     }
    # }

    data = response.json()
    if data["auth"]["type"] != "access_token":
        raise ValueError(f"Unexpected auth type: {data['auth']['type']}.")

    role = data["auth"]["accessToken"]["role"]
    if role == "fineGrained":
        # Harder to test. As a heuristic, just look for "write" somewhere in the JSON.
        if "write" in str(data["auth"]["accessToken"]["fineGrained"]).lower():
            raise ValueError(
                "The provided fine-grained Hugging Face token "
                f"{abbreviate_token(token)} has write access. "
                "Use a read-only token to deploy. "
                "It has the following permissions: "
                f"{data['auth']['accessToken']['fineGrained']}"
            )
    elif role == "write":
        raise ValueError(
            f"The provided Hugging Face token {abbreviate_token(token)} has write "
            "access. Use a read-only token to deploy."
        )
    else:
        if role != "read":
            raise ValueError(
                f"Unknown token role: {role}. Use a read-only token to deploy."
            )

    print("Ok, Hugging Face token has no write access.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Check that the Hugging Face token does not have write access. "
        "This is because we don't want the deployed containers to have write access "
        "in case they are compromised. "
        "Exits with non-zero exit code if the token has write access or something else "
        "goes wrong."
    )
    parser.add_argument("token", type=str, help="Hugging Face token to check. ")

    args = parser.parse_args()
    main(args.token)



================================================
FILE: unmute/scripts/copy_voice_to_prod.py
================================================
import argparse

from unmute.tts.voices import copy_voice_to_prod

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Copy voice to production server")
    parser.add_argument(
        "path_on_server",
        type=str,
        help="The path by which the voice is referred to by the TTS server "
        "(=relative to the voice directory)",
    )
    args = parser.parse_args()

    copy_voice_to_prod(args.path_on_server)



================================================
FILE: unmute/scripts/example_websocket_client.py
================================================
import argparse
import asyncio
import base64
import json
import os
from pathlib import Path

import numpy as np
import pydub
import pydub.playback
import sphn
import websockets
from fastrtc import audio_to_int16

from unmute.kyutai_constants import SAMPLE_RATE

INPUT_FRAME_SIZE = 960
TARGET_SAMPLE_RATE = 24000
TARGET_CHANNELS = 1  # Mono


def base64_encode_audio(audio: np.ndarray):
    pcm_bytes = audio_to_int16(audio)
    encoded = base64.b64encode(pcm_bytes).decode("ascii")
    return encoded


async def send_messages(websocket: websockets.ClientConnection, audio_path: Path):
    data, _sr = sphn.read(audio_path, sample_rate=SAMPLE_RATE)
    data = data[0]  # Take first channel to make it mono

    try:
        while True:
            chunk_size = 1920  # Send data in chunks
            for i in range(0, len(data), chunk_size):
                event = {
                    "type": "input_audio_buffer.append",
                    "audio": base64_encode_audio(data[i : i + chunk_size]),
                }

                await websocket.send(json.dumps(event))
                await asyncio.sleep(0.01)  # Simulate real-time streaming

            await websocket.send(json.dumps({"type": "input_audio_buffer.commit"}))
            await websocket.send(json.dumps({"type": "response.create"}))

            for _ in range(0, len(data), chunk_size):
                event = {
                    "type": "input_audio_buffer.append",
                    "audio": base64_encode_audio(
                        np.zeros(chunk_size, dtype=np.float32)
                    ),
                }

                await websocket.send(json.dumps(event))
                await asyncio.sleep(0.01)  # Simulate real-time streaming
    except websockets.ConnectionClosed:
        print("Connection closed while sending messages.")


async def receive_messages(websocket: websockets.ClientConnection):
    buffer = []
    transcript = ""

    try:
        async for message in websocket:
            message = json.loads(message)
            if message["type"] == "response.audio.delta":
                base64_audio = message["delta"]
                binary_audio_data = base64.b64decode(base64_audio)
                buffer.append(binary_audio_data)
            elif message["type"] == "response.audio.done":
                print("Received `response.audio.done` message.")
                break
            elif message["type"] == "response.audio_transcript.delta":
                transcript += message["delta"]
                print(message["delta"], end="", flush=True)
            else:
                print(f"Received message: {message}")
    except websockets.ConnectionClosed:
        print("Connection closed while receiving messages.")

    # save and play using pydub
    audio = pydub.AudioSegment(
        data=b"".join(buffer),
        sample_width=2,
        frame_rate=TARGET_SAMPLE_RATE,
        channels=TARGET_CHANNELS,
    )
    audio.export("output.wav", format="wav")
    pydub.playback.play(audio)


async def main(audio_path: Path, server_url: str):
    if "openai.com" in server_url:
        additional_headers = {
            "Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}",
            "OpenAI-Beta": "realtime=v1",
        }
        query_string = "model=gpt-4o-realtime-preview"
    else:
        additional_headers = {}
        query_string = ""

    async with websockets.connect(
        f"{server_url}/v1/realtime?{query_string}",
        additional_headers=additional_headers,
        subprotocols=[websockets.Subprotocol("realtime")],
    ) as websocket:
        send_task = asyncio.create_task(send_messages(websocket, audio_path))
        receive_task = asyncio.create_task(receive_messages(websocket))
        await asyncio.gather(send_task, receive_task)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--server-url", type=str, default="wss://api.openai.com")
    parser.add_argument("audio_path", type=Path)
    args = parser.parse_args()

    asyncio.run(main(args.audio_path, server_url=args.server_url))



================================================
FILE: unmute/scripts/mistral_streaming.py
================================================
import os

from mistralai import Mistral

if __name__ == "__main__":
    mistral_api_key = os.getenv("MISTRAL_API_KEY")
    if not mistral_api_key:
        raise ValueError("MISTRAL_API_KEY environment variable must be set")

    model = "mistral-small-latest"

    client = Mistral(api_key=mistral_api_key)

    res = client.chat.stream(
        model=model,
        messages=[
            {
                "role": "system",
                "content": "Keep your responses to at most a few sentences. "
                "They will be spoken out loud, so don't worry about formatting. "
                "Write as a human would speak.",
            },
            {
                "role": "user",
                "content": "What is the best French cheese?",
            },
        ],
    )

    with res as event_stream:
        for event in event_stream:
            content = event.data.choices[0].delta.content
            print(content, flush=True, end="")

    print("")



================================================
FILE: unmute/scripts/output_from_file.py
================================================
import argparse
import asyncio
from pathlib import Path

import numpy as np
import sphn
from fastrtc import AsyncStreamHandler, Stream, wait_for_item

SAMPLE_RATE = 24000
# 480 works but the default 960 doesn't!!!
OUTPUT_FRAME_SIZE = 480


class FilePlaybackHandler(AsyncStreamHandler):
    def __init__(self, audio_path: Path) -> None:
        super().__init__(
            input_sample_rate=SAMPLE_RATE,
            output_sample_rate=SAMPLE_RATE,
            output_frame_size=OUTPUT_FRAME_SIZE,
        )
        self.output_queue = asyncio.Queue()
        self.audio_path = audio_path

    async def receive(self, frame: tuple[int, np.ndarray]) -> None:
        pass

    async def emit(self) -> tuple[int, np.ndarray]:
        return await wait_for_item(self.output_queue)

    def copy(self):
        return FilePlaybackHandler(self.audio_path)

    async def start_up(self) -> None:
        data, _sr = sphn.read(self.audio_path, sample_rate=SAMPLE_RATE)
        data = data[0]  # Take first channel to make it mono

        simulated_ratio = 1.5

        for i in range(0, len(data), OUTPUT_FRAME_SIZE):
            await self.output_queue.put((SAMPLE_RATE, data[i : i + OUTPUT_FRAME_SIZE]))
            await asyncio.sleep(OUTPUT_FRAME_SIZE / SAMPLE_RATE / simulated_ratio)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("file", type=Path)
    args = parser.parse_args()

    stream = Stream(
        handler=FilePlaybackHandler(args.file),
        modality="audio",
        mode="send-receive",
    )

    stream.ui.launch(debug=True)



================================================
FILE: unmute/scripts/output_sine.py
================================================
import numpy as np
from fastrtc import Stream, StreamHandler, get_hf_turn_credentials

from unmute.audio_stream_saver import AudioStreamSaver

SAMPLE_RATE = 24000
OUTPUT_FRAME_SIZE = 1920

# logging.basicConfig(level=logging.DEBUG)


class SineHandler(StreamHandler):
    def __init__(self) -> None:
        super().__init__(input_sample_rate=SAMPLE_RATE, output_frame_size=960)
        self.cur_time_samples = 0
        self.saver = AudioStreamSaver()

    def receive(self, frame: tuple[int, np.ndarray]) -> None:
        pass

    def emit(self) -> tuple[int, np.ndarray]:
        times = np.arange(
            self.cur_time_samples,
            self.cur_time_samples + OUTPUT_FRAME_SIZE,
        )
        x = np.sin(2 * np.pi * 440 / SAMPLE_RATE * times) * 0.1
        x = x.astype(np.float32)
        self.cur_time_samples += OUTPUT_FRAME_SIZE

        self.saver.add(x)

        return (SAMPLE_RATE, x)

    def copy(self):
        return SineHandler()

    def shutdown(self):
        pass

    def start_up(self) -> None:
        pass


if __name__ == "__main__":
    # rtc_configuration = get_cloudflare_rtc_configuration()
    rtc_configuration = get_hf_turn_credentials()
    stream = Stream(
        handler=SineHandler(),
        modality="audio",
        mode="send-receive",
        rtc_configuration=rtc_configuration,
    )

    stream.ui.launch(debug=True)



================================================
FILE: unmute/scripts/output_sine_async.py
================================================
import asyncio

import numpy as np
from fastrtc import AsyncStreamHandler, Stream

from unmute.audio_stream_saver import AudioStreamSaver

SAMPLE_RATE = 24000
OUTPUT_FRAME_SIZE = 1920


class SineHandler(AsyncStreamHandler):
    def __init__(self) -> None:
        super().__init__(input_sample_rate=SAMPLE_RATE)
        self.cur_time_samples = 0
        self.saver = AudioStreamSaver()

    async def receive(self, frame: tuple[int, np.ndarray]) -> None:
        pass

    async def emit(self) -> tuple[int, np.ndarray]:
        times = np.arange(
            self.cur_time_samples,
            self.cur_time_samples + OUTPUT_FRAME_SIZE,
        )
        x = np.sin(2 * np.pi * 440 / SAMPLE_RATE * times) * 0.3
        x = x.astype(np.float32)
        self.cur_time_samples += OUTPUT_FRAME_SIZE

        self.saver.add(x)

        await asyncio.sleep(0.01)

        return (SAMPLE_RATE, x)

    def copy(self):
        return SineHandler()

    async def start_up(self) -> None:
        pass


if __name__ == "__main__":
    stream = Stream(
        handler=SineHandler(),
        modality="audio",
        mode="send-receive",
    )

    stream.ui.launch(debug=True)



================================================
FILE: unmute/scripts/output_tts.py
================================================
import asyncio
import time

import numpy as np
import websockets
from fastrtc import AsyncStreamHandler, Stream, wait_for_item

from unmute.tts.text_to_speech import TextToSpeech, TTSAudioMessage

SAMPLE_RATE = 24000
OUTPUT_FRAME_SIZE = 480


class TTSHandler(AsyncStreamHandler):
    def __init__(self) -> None:
        super().__init__(
            input_sample_rate=SAMPLE_RATE,
            output_sample_rate=SAMPLE_RATE,
            output_frame_size=OUTPUT_FRAME_SIZE,
        )
        self.tts = TextToSpeech()
        self.output_queue = asyncio.Queue()
        self.go = False
        self.cur_time_samples = 0

    async def receive(self, frame: tuple[int, np.ndarray]) -> None:
        pass

    async def emit(self) -> tuple[int, np.ndarray]:
        # if not self.output_queue.empty():
        #     return await self.output_queue.get()

        # times = np.arange(
        #     self.cur_time_samples,
        #     self.cur_time_samples + OUTPUT_FRAME_SIZE,
        # )
        # x = np.sin(2 * np.pi * 440 / SAMPLE_RATE * times) * 0.3
        # x = x.astype(np.float32)
        # self.cur_time_samples += OUTPUT_FRAME_SIZE

        # await asyncio.sleep(0.01)

        # return (SAMPLE_RATE, x)

        return await wait_for_item(self.output_queue)
        # return await self.output_queue.get()

    def copy(self):
        return TTSHandler()

    async def start_up(self) -> None:
        asyncio.create_task(self._tts_loop())

    async def _tts_loop(self):
        await self.tts.start_up()

        await self.tts.send(" ".join(["Hello, world! "] * 10))

        try:
            audio_started = None

            async for message in self.tts:
                if audio_started is not None:
                    time_since_start = time.time() - audio_started
                    time_received = self.tts.received_samples / self.input_sample_rate
                    ratio = time_received / time_since_start
                    assert self.input_sample_rate == SAMPLE_RATE
                    print(
                        f"{time_received=:.2f}, {time_since_start=:.2f}, "
                        f"ratio {ratio:.2f}"
                    )

                if isinstance(message, TTSAudioMessage):
                    audio = np.array(message.pcm, dtype=np.float32)
                    assert self.output_sample_rate == SAMPLE_RATE

                    assert len(audio) % OUTPUT_FRAME_SIZE == 0, (
                        "Audio length must be a multiple of the frame size."
                    )
                    for i in range(0, len(audio), OUTPUT_FRAME_SIZE):
                        await self.output_queue.put(
                            (SAMPLE_RATE, audio[i : i + OUTPUT_FRAME_SIZE])
                        )
                    # await self.output_queue.put((SAMPLE_RATE, audio))

                    if audio_started is None:
                        audio_started = time.time()

        except websockets.ConnectionClosed:
            print("TTS connection closed while receiving messages.")


if __name__ == "__main__":
    stream = Stream(
        handler=TTSHandler(),
        modality="audio",
        mode="send-receive",
    )

    stream.ui.launch(debug=True)



================================================
FILE: unmute/scripts/pitch_detection_handler.py
================================================
from collections import deque

import librosa
import numpy as np
from fastrtc import Stream, StreamHandler

from unmute.audio_stream_saver import AudioStreamSaver

SAMPLE_RATE = 24000
OUTPUT_FRAME_SIZE = 1920


class PitchDetectionHandler(StreamHandler):
    def __init__(self) -> None:
        super().__init__(input_sample_rate=SAMPLE_RATE, output_frame_size=480)
        self.cur_time_samples = 0
        self.saver = AudioStreamSaver()
        self.frequency_queue = deque()
        self.last_phase = 0
        self.last_frequency = 100

    def receive(self, frame: tuple[int, np.ndarray]) -> None:
        mono_audio = frame[1][0]
        assert mono_audio.dtype == np.int16
        mono_audio = mono_audio.astype(np.float32) / np.iinfo(np.int16).max

        freqs = librosa.yin(
            mono_audio,
            fmin=float(librosa.note_to_hz("E2")),
            fmax=float(librosa.note_to_hz("E5")),
            sr=SAMPLE_RATE,
            frame_length=len(mono_audio),
            hop_length=len(mono_audio),
            center=False,
        )
        assert len(freqs) == 1
        self.frequency_queue.append(freqs[0])

    def emit(self) -> tuple[int, np.ndarray] | None:
        if not self.frequency_queue:
            return None
        else:
            frequency = self.frequency_queue.popleft()

        phase = self.last_phase + np.cumsum(
            np.linspace(self.last_frequency, frequency, OUTPUT_FRAME_SIZE) / SAMPLE_RATE
        )
        self.last_phase = phase[-1] % 1.0
        amplitude = 0.1
        x = np.sin(2 * np.pi * phase) * amplitude
        x = x.astype(np.float32)

        self.cur_time_samples += OUTPUT_FRAME_SIZE
        self.saver.add(x)
        self.last_frequency = frequency

        return (SAMPLE_RATE, x)

    def copy(self):
        return PitchDetectionHandler()


if __name__ == "__main__":
    stream = Stream(
        handler=PitchDetectionHandler(), modality="audio", mode="send-receive"
    )

    stream.ui.launch(debug=True)



================================================
FILE: unmute/scripts/stt_from_file_example.py
================================================
"""Run speech-to-text on an audio file in a non-streaming way."""

import asyncio
import logging
from pathlib import Path

import numpy as np
import sphn
import tqdm

from unmute.kyutai_constants import SAMPLE_RATE, SAMPLES_PER_FRAME
from unmute.stt.speech_to_text import SpeechToText, STTMarkerMessage, STTWordMessage

TARGET_SAMPLE_RATE = 24000
TARGET_CHANNELS = 1  # Mono
logging.basicConfig(level=logging.INFO)


def load_and_process_audio(audio_path: Path):
    data, _sr = sphn.read(audio_path, sample_rate=SAMPLE_RATE)
    data = data[0]  # Take first channel to make it mono
    return data


async def main(audio_path: Path):
    stt = SpeechToText()
    await stt.start_up()

    audio_data = load_and_process_audio(audio_path)

    for i in tqdm.trange(0, len(audio_data), SAMPLES_PER_FRAME, desc="Sending audio"):
        chunk = audio_data[i : i + SAMPLES_PER_FRAME]
        await stt.send_audio(chunk)
        await asyncio.sleep(SAMPLES_PER_FRAME / SAMPLE_RATE)

    # When we get the marker back from the server, we know it has processed the audio
    await stt.send_marker(0)

    # Send extra audio to make sure the marker is processed
    for _ in range(25):
        await stt.send_audio(np.zeros(SAMPLES_PER_FRAME, dtype=np.int16))

    words = []

    with tqdm.tqdm() as pbar:
        async for msg in stt:
            if isinstance(msg, STTWordMessage):
                words.append(msg)
                pbar.set_postfix(n_words=len(words))
            elif isinstance(msg, STTMarkerMessage):  # pyright: ignore[reportUnnecessaryIsInstance]
                break

            pbar.update()

    print("\n".join(str(s) for s in words))


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("audio_path", type=Path)
    args = parser.parse_args()

    asyncio.run(main(args.audio_path))



================================================
FILE: unmute/scripts/stt_microphone_example.py
================================================
"""Transcribe audio from the microphone in real-time."""

import asyncio
from typing import Any

import numpy as np

try:
    # We don't need this for anything else so it's not in the dependencies
    import sounddevice as sd  # type: ignore
except ImportError as e:
    raise ImportError(
        "Please install sounddevice to run this example: pip install sounddevice "
        "(or uv pip install sounddevice if you're using uv)."
    ) from e
import tqdm

from unmute.kyutai_constants import SAMPLES_PER_FRAME
from unmute.stt.speech_to_text import (
    SpeechToText,
    STTMarkerMessage,
    STTWordMessage,
)


async def receive_loop(stt: SpeechToText):
    delay = None
    async for msg in stt:
        if isinstance(msg, STTWordMessage):
            print(f"Word: {msg.text} ({msg.start_time:.2f}s). Delay: {delay:.2f}s")
        elif isinstance(msg, STTMarkerMessage):  # type: ignore
            marker_time = msg.id / 1000
            time = asyncio.get_event_loop().time()
            delay = time - marker_time


async def main():
    stt = SpeechToText()
    await stt.start_up()
    audio_queue = asyncio.Queue()

    duration_sec = 30

    receive_task = asyncio.create_task(receive_loop(stt))

    def callback(indata: np.ndarray, frames: int, time: Any, status: sd.CallbackFlags):
        mono_audio = indata[:, 0]
        audio_queue.put_nowait(mono_audio.copy())

    start_time = asyncio.get_event_loop().time()

    audio_buffer = np.zeros((0,), dtype=np.float32)

    with sd.InputStream(callback=callback, blocksize=1024, samplerate=24000):
        pbar = tqdm.tqdm(total=duration_sec, desc="Recording", unit="s")
        while asyncio.get_event_loop().time() - start_time < duration_sec:
            try:
                audio_chunk = await asyncio.wait_for(audio_queue.get(), timeout=0.1)
            except asyncio.TimeoutError:
                continue

            pbar.set_postfix(
                volume=np.mean(np.abs(audio_chunk)),
            )
            # Updating this is a bit annoying
            # pbar.update(audio_chunk.shape[0] / 24000)

            audio_buffer = np.concatenate((audio_buffer, audio_chunk), axis=0)
            while audio_buffer.shape[0] > SAMPLES_PER_FRAME:
                audio_chunk = audio_buffer[:SAMPLES_PER_FRAME]
                audio_buffer = audio_buffer[SAMPLES_PER_FRAME:]

                await stt.send_marker(int(asyncio.get_event_loop().time() * 1000))
                await stt.send_audio(audio_chunk)

    receive_task.cancel()
    print(f"Quit after {duration_sec} seconds.")


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: unmute/scripts/tts_example.py
================================================
import argparse
import asyncio
from pathlib import Path

import numpy as np
import sphn
import tqdm

from unmute.loadtest.loadtest_client import preview_audio
from unmute.tts.text_to_speech import (
    TextToSpeech,
    TTSAudioMessage,
    TTSClientEosMessage,
    TTSTextMessage,
)
from unmute.tts.voice_cloning import clone_voice


async def main(
    text: str, voice_file: Path | None = None, output_path: Path | None = None
):
    if voice_file:
        voice = clone_voice(voice_file.read_bytes())
    else:
        voice = None

    tts = TextToSpeech(voice=voice)
    await tts.start_up()

    for word in text.split(" "):
        await tts.send(word)
        await asyncio.sleep(0.1)

    await tts.send(TTSClientEosMessage())

    audio_chunks = []
    n_words = 0

    with tqdm.tqdm() as pbar:
        async for msg in tts:
            if isinstance(msg, TTSTextMessage):
                pbar.set_postfix(n_words=n_words)
                n_words += 1
            elif isinstance(msg, TTSAudioMessage):
                audio_chunks.append(msg.pcm)
                pbar.update(len(msg.pcm) / 24000)

    all_audio = np.concat(audio_chunks).astype(np.float32)
    preview_audio(all_audio)

    sphn.write_wav(output_path, all_audio, 24000)
    print(f"Saved to {output_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--voice-file",
        type=Path,
    )
    parser.add_argument(
        "--output-path",
        type=Path,
        default=Path("out.wav"),
        help="Path to save the audio to, .wav or .ogg file (default: out.wav)",
    )
    parser.add_argument(
        "text",
        type=str,
        nargs="?",
        default="Did you know that the author of Octavia "
        "based one character on a former lover?",
    )
    args = parser.parse_args()

    asyncio.run(
        main(args.text, voice_file=args.voice_file, output_path=args.output_path)
    )



================================================
FILE: unmute/scripts/update_voice_list.py
================================================
"""Upload local voices to the server based on the voice list."""

import asyncio
import logging

from unmute.tts.voices import VoiceList


async def main():
    logging.basicConfig(level=logging.INFO)

    voice_list = VoiceList()
    await voice_list.upload_to_server()
    voice_list.save()
    print("Voices updated successfully. Voice list path:")
    print(voice_list.path)


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: unmute/scripts/vllm_wrapper_example.py
================================================
# https://github.com/gabrielchua/async-stream-openai-st/blob/824eab8f3ab600d3689d8d946526e48e0e0310c2/app.py
# https://qwen.readthedocs.io/en/latest/deployment/vllm.html#openai-compatible-api-service

import time
from typing import Any, cast

from unmute.kyutai_constants import LLM_SERVER
from unmute.llm.llm_utils import VLLMStream, get_openai_client, rechunk_to_words

# Predefined message
PREDEFINED_MESSAGE = "Explain the second law of thermodynamics"


async def main(server_url: str):
    client = get_openai_client(server_url=server_url)
    s = VLLMStream(client)

    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {
            "role": "user",
            "content": "Write a 200 word essay on 'bear vs shark'. "
            "The first line is a 2-3 word title with an emoji and then include "
            "2 line breaks. For example 'TITLE <emoji> \n \n ' ",
        },
    ]

    start_time = time.time()
    first_token_time = None
    async for message in rechunk_to_words(s.chat_completion(cast(Any, messages))):
        if first_token_time is None:
            first_token_time = time.time()
            print(
                f"\nTime to first token: {first_token_time - start_time:.3f} seconds\n"
            )
        print(message, end="", flush=True)

    print()


if __name__ == "__main__":
    import argparse
    import asyncio

    parser = argparse.ArgumentParser(description="Run VLLM wrapper example.")
    parser.add_argument(
        "--server-url",
        type=str,
        default=LLM_SERVER,
        help=f"The URL of the VLLM server (default: {LLM_SERVER}).",
    )
    args = parser.parse_args()

    asyncio.run(main(args.server_url))



================================================
FILE: unmute/stt/dummy_speech_to_text.py
================================================
"""A dummy speech-to-text that never sends any words.

Useful for testing, like checking if not running the STT on the same GPU can reduce
latency.
"""

import asyncio
from logging import getLogger
from typing import AsyncIterator, Literal

import numpy as np

from unmute.kyutai_constants import FRAME_TIME_SEC, STT_DELAY_SEC, STT_SERVER
from unmute.service_discovery import ServiceWithStartup
from unmute.stt.exponential_moving_average import ExponentialMovingAverage
from unmute.stt.speech_to_text import STTMarkerMessage, STTWordMessage
from unmute.websocket_utils import WebsocketState

logger = getLogger(__name__)

TranscriptionStatus = Literal[
    "should_transcribe", "has_transcribed", "should_not_transcribe"
]


class DummySpeechToText(ServiceWithStartup):
    def __init__(
        self, stt_instance: str = STT_SERVER, delay_sec: float = STT_DELAY_SEC
    ):
        self.stt_instance = stt_instance
        self.sent_samples = 0
        self.received_words = 0
        self.delay_sec = delay_sec
        self.current_time = -STT_DELAY_SEC

        # We just keep this at 1.0 = user is not speaking
        self.pause_prediction = ExponentialMovingAverage(
            attack_time=0.01, release_time=0.01, initial_value=1.0
        )
        self.should_shutdown = asyncio.Event()

    def state(self) -> WebsocketState:
        return "connected"

    async def send_audio(self, audio: np.ndarray) -> None:
        self.current_time += FRAME_TIME_SEC

    async def send_marker(self, id: int) -> None:
        return

    async def start_up(self):
        logger.info("Starting dummy STT")

    async def shutdown(self):
        logger.info("Shutting down dummy STT")
        self.should_shutdown.set()

    async def __aiter__(
        self,
    ) -> AsyncIterator[STTWordMessage | STTMarkerMessage]:
        while self.should_shutdown.is_set() is False:
            await asyncio.sleep(1.0)

        # Just to satisfy the type checker
        yield STTMarkerMessage(type="Marker", id=0)



================================================
FILE: unmute/stt/exponential_moving_average.py
================================================
import numpy as np


class ExponentialMovingAverage:
    def __init__(
        self, attack_time: float, release_time: float, initial_value: float = 0.0
    ):
        """An EMA that can smooth differently for attack (up) and release (down).

        Args:
            attack_time: Time in seconds to reach 50% of the target value.
                Used when the new value is greater than the current value.
            release_time: Time in seconds to decay to 50% of the target value.
                Used when the new value is less than the current value.
            initial_value: Initial value of the EMA.
        """
        self.attack_time = attack_time
        self.release_time = release_time
        self.value = initial_value

    def update(self, *, dt: float, new_value: float) -> float:
        assert dt > 0.0, f"dt must be positive, got {dt=}"
        assert new_value >= 0.0, f"new_value must be non-negative, got {new_value=}"

        if new_value > self.value:
            alpha = 1 - np.exp(-dt / self.attack_time * np.log(2))
        else:
            alpha = 1 - np.exp(-dt / self.release_time * np.log(2))

        self.value = float((1 - alpha) * self.value + alpha * new_value)
        return self.value

    def time_to_decay_to(self, value: float) -> float:
        """Return the time in seconds it will take for the estimate to reach `value`
        if it started at 1."""
        assert 0 < value < 1
        return float(-self.release_time * np.log2(value))



================================================
FILE: unmute/stt/speech_to_text.py
================================================
import asyncio
import random
from logging import getLogger
from typing import AsyncIterator, Literal, Union

import msgpack
import numpy as np
import websockets
from fastrtc import audio_to_float32
from pydantic import BaseModel, TypeAdapter

from unmute import metrics as mt
from unmute.exceptions import MissingServiceAtCapacity
from unmute.kyutai_constants import (
    FRAME_TIME_SEC,
    HEADERS,
    SAMPLE_RATE,
    SPEECH_TO_TEXT_PATH,
    STT_DELAY_SEC,
    STT_SERVER,
)
from unmute.service_discovery import ServiceWithStartup
from unmute.stt.exponential_moving_average import ExponentialMovingAverage
from unmute.timer import Stopwatch
from unmute.websocket_utils import WebsocketState

logger = getLogger(__name__)


class STTWordMessage(BaseModel):
    type: Literal["Word"]
    text: str
    start_time: float


class STTEndWordMessage(BaseModel):
    type: Literal["EndWord"]
    stop_time: float


class STTMarkerMessage(BaseModel):
    type: Literal["Marker"]
    id: int


class STTStepMessage(BaseModel):
    type: Literal["Step"]
    step_idx: int
    prs: list[float]


class STTErrorMessage(BaseModel):
    type: Literal["Error"]
    message: str


class STTReadyMessage(BaseModel):
    type: Literal["Ready"]


STTMessage = Union[
    STTWordMessage,
    STTEndWordMessage,
    STTMarkerMessage,
    STTStepMessage,
    STTErrorMessage,
    STTReadyMessage,
]
STTMessageAdapter = TypeAdapter(STTMessage)


class SpeechToText(ServiceWithStartup):
    def __init__(
        self, stt_instance: str = STT_SERVER, delay_sec: float = STT_DELAY_SEC
    ):
        self.stt_instance = stt_instance
        self.delay_sec = delay_sec
        self.websocket: websockets.ClientConnection | None = None
        self.sent_samples = 0
        self.received_words = 0
        self.current_time = -STT_DELAY_SEC
        self.time_since_first_audio_sent = Stopwatch(autostart=False)
        self.waiting_first_step: bool = True

        # In our case, attack  = from speaking to not speaking
        #              release = from not speaking to speaking
        self.pause_prediction = ExponentialMovingAverage(
            attack_time=0.01, release_time=0.01, initial_value=1.0
        )

        self.shutdown_complete = asyncio.Event()

    def state(self) -> WebsocketState:
        if not self.websocket:
            return "not_created"
        else:
            d: dict[websockets.protocol.State, WebsocketState] = {
                websockets.protocol.State.CONNECTING: "connecting",
                websockets.protocol.State.OPEN: "connected",
                websockets.protocol.State.CLOSING: "closing",
                websockets.protocol.State.CLOSED: "closed",
            }
            return d[self.websocket.state]

    async def send_audio(self, audio: np.ndarray) -> None:
        if audio.ndim != 1:
            raise ValueError(f"Expected 1D array, got {audio.shape=}")

        if audio.dtype != np.float32:
            audio = audio_to_float32(audio)

        self.sent_samples += len(audio)
        self.time_since_first_audio_sent.start_if_not_started()
        mt.STT_SENT_FRAMES.inc()

        await self._send({"type": "Audio", "pcm": audio.tolist()})

    async def send_marker(self, id: int) -> None:
        await self._send({"type": "Marker", "id": id})

    async def _send(self, data: dict) -> None:
        """Send an arbitrary message to the STT server."""
        to_send = msgpack.packb(data, use_bin_type=True, use_single_float=True)

        if self.websocket:
            await self.websocket.send(to_send)
        else:
            logger.warning("STT websocket not connected")

    async def start_up(self):
        logger.info(f"Connecting to STT {self.stt_instance}...")
        self.websocket = await websockets.connect(
            self.stt_instance + SPEECH_TO_TEXT_PATH, additional_headers=HEADERS
        )
        logger.info("Connected to STT")

        try:
            message_bytes = await self.websocket.recv()
            message_dict = msgpack.unpackb(message_bytes)  # type: ignore
            message = STTMessageAdapter.validate_python(message_dict)
            if isinstance(message, STTReadyMessage):
                mt.STT_ACTIVE_SESSIONS.inc()
                return
            elif isinstance(message, STTErrorMessage):
                raise MissingServiceAtCapacity("stt")
            else:
                raise RuntimeError(
                    f"Expected ready or error message, got {message.type}"
                )
        except Exception as e:
            logger.error(f"Error during STT startup: {repr(e)}")
            # Make sure we don't leave a dangling websocket connection
            await self.websocket.close()
            self.websocket = None
            raise

    async def shutdown(self):
        logger.info("Shutting down STT, receiving last messages")
        if self.shutdown_complete.is_set():
            return

        mt.STT_ACTIVE_SESSIONS.dec()
        if self.time_since_first_audio_sent.started:
            mt.STT_SESSION_DURATION.observe(self.time_since_first_audio_sent.time())
            mt.STT_AUDIO_DURATION.observe(self.sent_samples / SAMPLE_RATE)
            mt.STT_NUM_WORDS.observe(self.received_words)

        if not self.websocket:
            raise RuntimeError("STT websocket not connected")
        await self.websocket.close()
        await self.shutdown_complete.wait()

        logger.info("STT shutdown() finished")

    async def __aiter__(
        self,
    ) -> AsyncIterator[STTWordMessage | STTMarkerMessage]:
        if not self.websocket:
            raise RuntimeError("STT websocket not connected")

        my_id = random.randint(1, int(1e9))

        # The pause prediction is all over the place in the first few steps, so ignore.
        n_steps_to_wait = 12

        try:
            async for message_bytes in self.websocket:
                data = msgpack.unpackb(message_bytes)  # type: ignore
                logger.debug(f"{my_id} {self.pause_prediction.value} got {data}")
                message: STTMessage = STTMessageAdapter.validate_python(data)

                match message:
                    case STTWordMessage():
                        num_words = len(message.text.split())
                        mt.STT_RECV_WORDS.inc(num_words)
                        self.received_words += 1
                        yield message
                    case STTEndWordMessage():
                        continue
                    case STTStepMessage():
                        self.current_time += FRAME_TIME_SEC
                        mt.STT_RECV_FRAMES.inc()
                        if (
                            self.waiting_first_step
                            and self.time_since_first_audio_sent.started
                        ):
                            self.waiting_first_step = False
                            mt.STT_TTFT.observe(self.time_since_first_audio_sent.time())
                        if n_steps_to_wait > 0:
                            n_steps_to_wait -= 1
                        else:
                            self.pause_prediction.update(
                                dt=FRAME_TIME_SEC, new_value=message.prs[2]
                            )
                    case STTMarkerMessage():
                        yield message
                    case STTReadyMessage():
                        continue
                    case _:
                        # Not sure why Pyright complains about non-exhaustive match
                        raise ValueError(f"Unknown message: {message}")

        except websockets.ConnectionClosedOK:
            # The server closes the connection once we send \0, and this actually shows
            # up as a websockets.ConnectionClosedError.
            pass
        finally:
            self.shutdown_complete.set()



================================================
FILE: unmute/tts/copy_approved_voice_donations.py
================================================
import argparse
import csv
from pathlib import Path

from unmute.tts.trim_voice_donation_clip import trim_trailing_silence


def main():
    parser = argparse.ArgumentParser(
        description="Copy approved voice donation .wav files with proper naming."
    )
    parser.add_argument(
        "--table",
        required=True,
        help="Path to the .tsv or .csv file with metadata.",
        type=Path,
    )
    parser.add_argument(
        "--input-dir",
        required=True,
        help="Directory containing input .wav files named {verification_id}.wav",
        type=Path,
    )
    parser.add_argument(
        "--output-dir",
        required=True,
        help="Directory to copy approved .wav files to.",
        type=Path,
    )
    args = parser.parse_args()

    # Detect delimiter
    table_path = Path(args.table)
    delimiter = "\t" if table_path.suffix.lower() == ".tsv" else ","

    # Ask for confirmation before clearing the output directory
    if args.output_dir.exists() and any(args.output_dir.iterdir()):
        confirm = (
            input(
                f"Output directory {args.output_dir} is not empty. "
                "Will clear .wav and .wav.safetensors files before continuing. "
                "Ok? (y/N): "
            )
            .strip()
            .lower()
        )
        if confirm != "y":
            print("Exiting.")
            exit(1)

        for item in args.output_dir.iterdir():
            if item.is_file() and (
                item.suffix == ".wav" or item.name.endswith(".wav.safetensors")
            ):
                item.unlink()

    with table_path.open(newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f, delimiter=delimiter)
        for row in reader:
            approval = row.get("approval", "").strip().upper()
            if approval != "TRUE":
                continue

            verification_id = row["verification_id"].strip()

            in_path = args.input_dir / f"{verification_id}.wav"

            if not in_path.is_file():
                raise FileNotFoundError(f"Input file not found: {in_path}")

            nickname_override = row.get("nickname override", "").strip()
            nickname = row.get("nickname", "").strip()
            if nickname_override:
                out_name = nickname_override
            elif nickname:
                out_name = nickname
            else:
                out_name = verification_id[:4]

            # Clean output name
            out_name = (
                out_name.replace(".", " ")
                .replace("/", " ")
                .replace("\\", " ")
                .strip()  # Strip trailing spaces before turning them into underscores
                .replace(" ", "_")
            )
            out_path = args.output_dir / f"{out_name}.wav"

            trim_trailing_silence(in_path, out_path)
            print(f"Copied {in_path} -> {out_path}, trimming silence")


if __name__ == "__main__":
    main()



================================================
FILE: unmute/tts/create_voice_donation_table.py
================================================
import argparse
import csv
import os
from pathlib import Path

from unmute.tts.voice_donation import VoiceDonationMetadata


def get_flattened_donation(donation: VoiceDonationMetadata) -> dict:
    """Flatten the VoiceDonationMetadata for easier processing."""
    return {
        "verification_id": str(donation.submission.verification_id),
        "timestamp_str": donation.timestamp_str,
        "email": donation.submission.email,
        "nickname": donation.submission.nickname,
        "verification_text": donation.verification.text,
    }


def main(voice_donation_dir: Path, set_mtime: bool = False):
    backups = sorted(list(voice_donation_dir.glob("voice-donation_*/")))
    if not backups:
        print("No backups found.")
        exit(1)

    backup = backups[-1]
    print(f"Using backup: {backup}")

    donations: list[VoiceDonationMetadata] = []
    for donation_json in backup.glob("*.json"):
        with open(donation_json, "r") as f:
            metadata = VoiceDonationMetadata.model_validate_json(f.read())
            donations.append(metadata)

        if set_mtime:
            os.utime(donation_json, (metadata.timestamp, metadata.timestamp))
            donation_wav = donation_json.with_suffix(".wav")
            if not donation_wav.exists():
                print(f"Warning: {donation_wav} does not exist, skipping mtime set.")
            else:
                os.utime(donation_wav, (metadata.timestamp, metadata.timestamp))

    donations.sort(key=lambda x: x.timestamp)

    seen_nicknames = set()
    for donation in donations:
        if donation.submission.nickname in seen_nicknames:
            raise ValueError(
                f"Duplicate nickname found: {donation.submission.nickname}"
            )

        assert donation.submission.license == "CC0", "Only CC0 license expected"
        assert donation.submission.format_version == "1.0", (
            "Only format version 1.0 expected"
        )

    flattened_donations = [get_flattened_donation(d) for d in donations]

    output_tsv = voice_donation_dir / "flattened_donations.tsv"
    if flattened_donations:
        with open(output_tsv, "w", newline="") as tsvfile:
            writer = csv.DictWriter(
                tsvfile, fieldnames=flattened_donations[0].keys(), delimiter="\t"
            )
            writer.writeheader()
            writer.writerows(flattened_donations)
        print(f"Exported {len(flattened_donations)} donations to {output_tsv}")
        print(
            "You can copy this file and use cmd+shift+v to paste the values into a spreadsheet."
        )
    else:
        print("No donations to export.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Process voice donation backups.")
    parser.add_argument(
        "voice_donation_dir",
        type=Path,
        help="Directory containing voice donation backups.",
    )
    parser.add_argument(
        "--set-mtime",
        action="store_true",
        help="Set modification time of each file to match its timestamp.",
    )
    args = parser.parse_args()

    main(args.voice_donation_dir, set_mtime=args.set_mtime)



================================================
FILE: unmute/tts/freesound_download.py
================================================
import argparse
import logging
import os
import re
import subprocess
import tempfile
from copy import deepcopy
from pathlib import Path
from typing import Literal

import requests
import tqdm
from pydantic import BaseModel, Field
from ruamel.yaml import YAML

logger = logging.getLogger(__name__)


class SoundPreviews(BaseModel):
    preview_hq_mp3: str
    preview_lq_mp3: str
    preview_hq_ogg: str
    preview_lq_ogg: str

    # The JSONs have dashes instead of underscores
    model_config = {
        "populate_by_name": True,
        "alias_generator": lambda field_name: field_name.replace("_", "-"),
    }


def to_filename_friendly(s: str) -> str:
    s = re.sub(r"[^\w\s-]", "", s)  # Remove all special characters
    s = re.sub(r"\s+", "-", s)  # Replace spaces with dashes
    return s.lower()


class FreesoundSoundInstance(BaseModel):
    """See https://freesound.org/docs/api/resources_apiv2.html#sound-instance"""

    id: int
    name: str
    username: str
    previews: SoundPreviews | None = Field(default=None, exclude=True)
    license: str

    def get_filename(self) -> str:
        filename_friendly_name = to_filename_friendly(self.name)
        return f"{self.id}_{filename_friendly_name}.mp3"


class FreesoundVoiceSource(BaseModel):
    source_type: Literal["freesound"] = "freesound"
    url: str
    start_time: int | None = None
    sound_instance: FreesoundSoundInstance | None = None
    path_on_server: str


def get_sound_id_from_url(url: str) -> int:
    # e.g. https://freesound.org/people/balloonhead/sounds/785958/
    matches = re.search(r"/sounds/(\d+)/", url)
    if matches is None:
        raise ValueError(f"Invalid Freesound URL: {url}")

    return int(matches.group(1))


def get_sound_instance(sound_id_or_url: int | str) -> FreesoundSoundInstance:
    if isinstance(sound_id_or_url, int):
        sound_id = sound_id_or_url
    else:
        sound_id = get_sound_id_from_url(sound_id_or_url)

    response = requests.get(
        f"https://freesound.org/apiv2/sounds/{sound_id}/",
        headers={"Authorization": f"Token {os.environ['FREESOUND_API_KEY']}"},
    )
    response.raise_for_status()

    return FreesoundSoundInstance(**response.json())


def process_sound(input_path: Path, output_path: Path, start_time: int):
    with tempfile.NamedTemporaryFile(suffix=".mkv", delete=False) as temp_file:
        temp_path = Path(temp_file.name)

        volume_normalization_result = subprocess.run(
            [
                # This binary should be available because ffmpeg-normalize is a dev
                # dependency. Running via `uv run` should make it available.
                "ffmpeg-normalize",
                "-f",  # overwrite, because the tempfile already exists
                input_path,
                "-o",
                temp_path,
            ],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
        )

        if volume_normalization_result.returncode != 0:
            print(volume_normalization_result.stderr.decode("utf-8"))
            raise RuntimeError("ffmpeg-normalize failed")

        result = subprocess.run(
            [
                "ffmpeg",
                "-i",
                temp_path,
                "-ss",
                str(start_time),
                "-t",  # take 10 seconds
                "10",
                "-ac",  # make it mono
                "1",
                "-af",  # resample to 24kHz, pad to 10 seconds if needed
                "apad=pad_dur=10,aresample=24000",
                "-ar",  # set sample rate (not sure why needed twice)
                "24000",
                "-y",  # overwrite output file if it exists
                output_path,
            ],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
        )

        if result.returncode != 0:
            print(result.stderr.decode("utf-8"))
            raise RuntimeError("FFmpeg failed")


repo_root = Path(__file__).parents[2]
OUTPUT_DIR = repo_root / "voices"

ALLOWED_LICENSES = [
    "http://creativecommons.org/publicdomain/zero/1.0/",  # CC0 1.0
    "https://creativecommons.org/licenses/by/4.0/",  # CC-BY 4.0
    "http://creativecommons.org/licenses/by/3.0/",  # CC-BY 3.0
]


def download_sound(source: FreesoundVoiceSource | str):
    if isinstance(source, str):
        source = FreesoundVoiceSource(url=source, path_on_server="filled in later")
    else:
        source = deepcopy(source)

    # We need to load this even if we have a sound instance already, because we don't
    # serialize the links to the previews
    sound_instance = get_sound_instance(source.url)

    assert sound_instance.previews is not None, (
        f"Sound instance has no previews: {sound_instance}"
    )

    if sound_instance.license not in ALLOWED_LICENSES:
        raise ValueError(
            f"Sound {sound_instance.id} has license {sound_instance.license}, "
            f"but only {ALLOWED_LICENSES} are allowed"
        )

    (OUTPUT_DIR / "raw").mkdir(exist_ok=True, parents=True)
    (OUTPUT_DIR / "voices" / "unmute-prod-website").mkdir(exist_ok=True, parents=True)

    # Cache the raw file to avoid downloading it again
    raw_path = OUTPUT_DIR / "raw" / sound_instance.get_filename()
    if not raw_path.exists():
        response = requests.get(sound_instance.previews.preview_hq_mp3)
        response.raise_for_status()
        with raw_path.open("wb") as f:
            f.write(response.content)

    output_path = (
        OUTPUT_DIR / "voices" / "unmute-prod-website" / sound_instance.get_filename()
    )
    if not output_path.exists():
        process_sound(raw_path, output_path, start_time=source.start_time or 0)

    source.sound_instance = sound_instance
    source.path_on_server = str(
        Path("unmute-prod-website/freesound") / sound_instance.get_filename()
    )

    return output_path, source


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "input_file", type=Path, help="YAML file describing what do download"
    )
    args = parser.parse_args()
    input_file = args.input_file

    OUTPUT_DIR.mkdir(exist_ok=True)

    with input_file.open() as f:
        sounds = YAML().load(f)
        sounds = [FreesoundVoiceSource(**sound) for sound in sounds]

    downloaded_paths = []
    for sound in (pbar := tqdm.tqdm(sounds)):
        pbar.set_description(sound.url)
        downloaded_paths.append(download_sound(sound))

    print("Downloaded to the following paths:")
    for path in downloaded_paths:
        print(path)



================================================
FILE: unmute/tts/realtime_queue.py
================================================
import asyncio
import heapq
from dataclasses import dataclass, field
from typing import AsyncIterable, Callable, Iterable, TypeVar

T = TypeVar("T")


@dataclass(order=True)
class TimedItem[T]:
    time: float
    item: T = field(compare=False)

    def as_tuple(self) -> tuple[float, T]:
        return self.time, self.item


class RealtimeQueue[T]:
    """A data structure that accumulates timestamped items and releases them at the given times.

    Implemented as a heap, so it doesn't have to be FIFO.
    """

    def __init__(self, get_time: Callable[[], float] | None = None):
        self.queue: list[TimedItem] = []
        self.start_time: float | None = None

        if get_time is None:
            self.get_time = lambda: asyncio.get_event_loop().time()
        else:
            # Use an external time function to support use cases where "real time"
            # means something different
            self.get_time = get_time

    def start_if_not_started(self):
        if self.start_time is None:
            self.start_time = self.get_time()

    def put(self, item: T, time: float):
        heapq.heappush(self.queue, TimedItem(time, item))

    async def get(self) -> AsyncIterable[tuple[float, T]]:
        """Get all items that are past due. If none is, wait for the next one."""

        if self.start_time is None:
            return
        if not self.queue:
            return

        time_since_start = self.get_time() - self.start_time
        while self.queue:
            delta = self.queue[0].time - time_since_start

            if delta > 0:
                await asyncio.sleep(delta)

            yield heapq.heappop(self.queue).as_tuple()

    def get_nowait(self) -> Iterable[tuple[float, T]]:
        if self.start_time is None:
            return None

        time_since_start = self.get_time() - self.start_time

        while self.queue and self.queue[0].time <= time_since_start:
            yield heapq.heappop(self.queue).as_tuple()

    async def __aiter__(self):
        if self.start_time is None or not self.queue:
            return

        while self.queue:
            time_since_start = self.get_time() - self.start_time
            delta = self.queue[0].time - time_since_start

            if delta > 0:
                await asyncio.sleep(delta)

            yield heapq.heappop(self.queue).as_tuple()

    def empty(self):
        return not self.queue



================================================
FILE: unmute/tts/text_to_speech.py
================================================
import asyncio
import urllib.parse
from logging import getLogger
from typing import Annotated, Any, AsyncIterator, Callable, Literal, Union, cast

import msgpack
import websockets
from pydantic import BaseModel, Field, TypeAdapter

import unmute.openai_realtime_api_events as ora
from unmute import metrics as mt
from unmute.exceptions import MissingServiceAtCapacity
from unmute.kyutai_constants import (
    FRAME_TIME_SEC,
    HEADERS,
    SAMPLE_RATE,
    TEXT_TO_SPEECH_PATH,
    TTS_SERVER,
)
from unmute.recorder import Recorder
from unmute.service_discovery import ServiceWithStartup
from unmute.timer import Stopwatch
from unmute.tts.realtime_queue import RealtimeQueue
from unmute.tts.voice_cloning import voice_embeddings_cache
from unmute.websocket_utils import WebsocketState

logger = getLogger(__name__)


class TTSClientTextMessage(BaseModel):
    """Message sent to the TTS server saying we to turn this text into speech."""

    type: Literal["Text"] = "Text"
    text: str


class TTSClientVoiceMessage(BaseModel):
    type: Literal["Voice"] = "Voice"
    embeddings: list[float]
    shape: list[int]


class TTSClientEosMessage(BaseModel):
    """Message sent to the TTS server saying we are done sending text."""

    type: Literal["Eos"] = "Eos"


TTSClientMessage = Annotated[
    Union[TTSClientTextMessage, TTSClientVoiceMessage, TTSClientEosMessage],
    Field(discriminator="type"),
]
TTSClientMessageAdapter = TypeAdapter(TTSClientMessage)


class TTSTextMessage(BaseModel):
    type: Literal["Text"]
    text: str
    start_s: float
    stop_s: float


class TTSAudioMessage(BaseModel):
    type: Literal["Audio"]
    pcm: list[float]


class TTSErrorMessage(BaseModel):
    type: Literal["Error"]
    message: str


class TTSReadyMessage(BaseModel):
    type: Literal["Ready"]


TTSMessage = Annotated[
    Union[TTSTextMessage, TTSAudioMessage, TTSErrorMessage, TTSReadyMessage],
    Field(discriminator="type"),
]
TTSMessageAdapter = TypeAdapter(TTSMessage)


def url_escape(value: object) -> str:
    return urllib.parse.quote(str(value), safe="")


# Only release the audio such that it's AUDIO_BUFFER_SEC ahead of real time.
# If the value it's too low, it might cause stuttering.
# If it's too high, it's difficult to control the synchronization of the text and the
# audio, because that's controlled by emit() and WebRTC. Note that some
# desynchronization can still occur if the TTS is less than real-time, because WebRTC
# will decide to do some buffering of the audio on the fly.
AUDIO_BUFFER_SEC = FRAME_TIME_SEC * 4


def prepare_text_for_tts(text: str) -> str:
    text = text.strip()

    unpronounceable_chars = "*_`"
    for char in unpronounceable_chars:
        text = text.replace(char, "")

    text = text.replace("“", '"').replace("”", '"')
    text = text.replace("‘", "'").replace("’", "'")
    text = text.replace(" : ", " ")

    return text


class TtsStreamingQuery(BaseModel):
    # See moshi-rs/moshi-server/
    seed: int | None = None
    temperature: float | None = None
    top_k: int | None = None
    format: str = "PcmMessagePack"
    voice: str | None = None
    voices: list[str] | None = None
    max_seq_len: int | None = None
    cfg_alpha: float | None = None
    auth_id: str | None = None

    def to_url_params(self) -> str:
        params = self.model_dump()
        return "?" + "&".join(
            f"{key}={url_escape(value)}"
            for key, value in params.items()
            if value is not None
        )


class TextToSpeech(ServiceWithStartup):
    def __init__(
        self,
        tts_instance: str = TTS_SERVER,
        # For TTS, we do internal queuing, so we pass in the recorder to be able to
        # record the true time of the messages.
        recorder: Recorder | None = None,
        get_time: Callable[[], float] | None = None,
        voice: str | None = None,
    ):
        self.tts_instance = tts_instance
        self.recorder = recorder
        self.websocket: websockets.ClientConnection | None = None

        self.time_since_first_text_sent = Stopwatch(autostart=False)
        self.waiting_first_audio: bool = True
        # Number of samples received from the TTS server
        self.received_samples = 0
        # Number of samples that we passed on after waiting for the correct time
        self.received_samples_yielded = 0

        self.voice = voice
        self.query = TtsStreamingQuery(
            voice=self.voice
            # Don't pass in custom voices as a query parameter, we set it later using
            # a message
            if (self.voice and not self.voice.startswith("custom:"))
            else None,
            cfg_alpha=1.5,
        )

        # self.query_parameters = f"?voice={self.voice}&cfg_alpha=2&format=PcmMessagePack"
        self.text_output_queue = RealtimeQueue(get_time=get_time)

        self.shutdown_lock = asyncio.Lock()
        self.shutdown_complete = asyncio.Event()

    def state(self) -> WebsocketState:
        if not self.websocket:
            return "not_created"
        else:
            d: dict[websockets.protocol.State, WebsocketState] = {
                websockets.protocol.State.CONNECTING: "connecting",
                websockets.protocol.State.OPEN: "connected",
                websockets.protocol.State.CLOSING: "closing",
                websockets.protocol.State.CLOSED: "closed",
            }
            return d[self.websocket.state]

    async def send(self, message: str | TTSClientMessage) -> None:
        """Send a message to the TTS server.

        Note that raw strings will be preprocessed to remove unpronounceable characters
        etc., but a TTSClientTextMessage will send the text as-is.
        """
        if isinstance(message, str):
            message = TTSClientTextMessage(
                type="Text", text=prepare_text_for_tts(message)
            )

        if self.shutdown_lock.locked():
            logger.warning("Can't send - TTS shutting down")
        elif not self.websocket:
            logger.warning("Can't send - TTS websocket not connected")
        else:
            if isinstance(message, TTSClientTextMessage):
                if message.text == "":
                    return  # Don't send empty messages

                mt.TTS_SENT_FRAMES.inc()
                self.time_since_first_text_sent.start_if_not_started()

            await self.websocket.send(msgpack.packb(message.model_dump()))

    async def start_up(self):
        url = self.tts_instance + TEXT_TO_SPEECH_PATH + self.query.to_url_params()
        logger.info(f"Connecting to TTS: {url}")
        self.websocket = await websockets.connect(
            url,
            additional_headers=HEADERS,
        )
        logger.debug("Connected to TTS")

        try:
            if self.voice is not None and self.voice.startswith("custom:"):
                voice_embedding = voice_embeddings_cache.get(self.voice)

                if voice_embedding is not None:
                    await self.websocket.send(voice_embedding)
                else:
                    logger.warning(
                        f"Custom voice {self.voice} not found, not sending it to TTS"
                    )

            for _ in range(10):
                # Due to some race condition in the TTS, we might get packets from a previous TTS client.
                message_bytes = await self.websocket.recv(decode=False)
                message_dict = msgpack.unpackb(message_bytes)
                message = TTSMessageAdapter.validate_python(message_dict)
                if isinstance(message, TTSReadyMessage):
                    return
                elif isinstance(message, TTSErrorMessage):
                    raise MissingServiceAtCapacity("tts")
                else:
                    logger.warning(
                        f"Received unexpected message type from {self.tts_instance}, {message.type}"
                    )
        except Exception as e:
            logger.error(f"Error during TTS startup: {repr(e)}")
            # Make sure we don't leave a dangling websocket connection
            await self.websocket.close()
            self.websocket = None
            raise

        raise AssertionError("Not supposed to happen.")

    async def shutdown(self):
        async with self.shutdown_lock:
            if self.shutdown_complete.is_set():
                return
            mt.TTS_ACTIVE_SESSIONS.dec()
            mt.TTS_AUDIO_DURATION.observe(self.received_samples / SAMPLE_RATE)
            if self.time_since_first_text_sent.started:
                mt.TTS_GEN_DURATION.observe(self.time_since_first_text_sent.time())

            # Set before closing the websocket so that __aiter__ knows we're closing
            # the connection intentionally
            self.shutdown_complete.set()

            if self.websocket:
                await self.websocket.close()
                self.websocket = None

            logger.info("TTS shutdown() finished")

    async def __aiter__(self) -> AsyncIterator[TTSMessage]:
        if self.websocket is None:
            raise RuntimeError("TTS websocket not connected")
        mt.TTS_SESSIONS.inc()
        mt.TTS_ACTIVE_SESSIONS.inc()

        output_queue: RealtimeQueue[TTSMessage] = RealtimeQueue()

        try:
            async for message_bytes in self.websocket:
                message_dict = msgpack.unpackb(cast(Any, message_bytes))
                message: TTSMessage = TTSMessageAdapter.validate_python(message_dict)

                if isinstance(message, TTSAudioMessage):
                    # Use `yield message` if you want to to release the audio
                    # as fast as it's being generated. However, it might desynchronize
                    # the text and the audio.
                    mt.TTS_RECV_FRAMES.inc()
                    if (
                        self.waiting_first_audio
                        and self.time_since_first_text_sent.started
                    ):
                        self.waiting_first_audio = False
                        ttft = self.time_since_first_text_sent.time()
                        mt.TTS_TTFT.observe(ttft)
                        logger.info("Time to first token is %.1f ms", ttft * 1000)
                    output_queue.start_if_not_started()
                    output_queue.put(
                        message, self.received_samples / SAMPLE_RATE - AUDIO_BUFFER_SEC
                    )
                    self.received_samples += len(message.pcm)

                    if self.recorder is not None:
                        await self.recorder.add_event(
                            "server",
                            ora.UnmuteResponseAudioDeltaReady(
                                number_of_samples=len(message.pcm)
                            ),
                        )

                elif isinstance(message, TTSTextMessage):
                    mt.TTS_RECV_WORDS.inc()
                    if message == TTSTextMessage(
                        type="Text", text="", start_s=0, stop_s=0
                    ):
                        # Always emitted by the TTS server, but we don't need it
                        continue

                    # There are two reasons why we don't send the text messages
                    # immediately:
                    # - The text messages have timestamps "from the future" because the
                    # audio stream is delayed by 2s.
                    # - Even so, we receive the audio/text faster than real time. It
                    #   seems difficult to keep track of how much audio data has already
                    #   been streamed (.emit() eats up the inputs immediately,
                    #   apparently it has some internal buffering) so we only send the
                    #   text messages at the real time when they're actually supposed to
                    #   be displayed. Precise timing/buffering is less important here.
                    # By using stop_s instead of start_s, we ensure that anything shown
                    # has already been said, so that if there's an interruption, the
                    # chat history matches what's actually been said.
                    output_queue.put(message, message.stop_s)

                for _, message in output_queue.get_nowait():
                    if isinstance(message, TTSAudioMessage):
                        self.received_samples_yielded += len(message.pcm)

                    yield message

        except websockets.ConnectionClosedOK:
            pass
        except websockets.ConnectionClosedError:
            if self.shutdown_complete.is_set():
                # If we closed the websocket in shutdown(), it leads to this exception
                # (not sure why) but it's an intentional exit, so don't raise.
                pass
            else:
                raise

        # Empty the queue if the connection is closed - we're releasing the messages
        # in real time, see above.
        async for _, message in output_queue:
            if self.shutdown_complete.is_set():
                break
            if isinstance(message, TTSAudioMessage):
                self.received_samples_yielded += len(message.pcm)
            yield message

        logger.debug("TTS __aiter__() finished")
        await self.shutdown()



================================================
FILE: unmute/tts/trim_voice_donation_clip.py
================================================
import argparse
from pathlib import Path

import numpy as np
import sphn

from unmute.kyutai_constants import SAMPLE_RATE


def trim_silence_end(
    audio: np.ndarray, threshold_db: float = -24.0, min_silence_sec: float = 1.0
) -> np.ndarray:
    """
    Trim silence from the end of the audio. Silence is defined as samples below a threshold (in dB relative to peak).
    """
    # Only operate on mono audio
    if audio.ndim != 1:
        raise ValueError("trim_silence_end expects mono audio (1D array)")

    peak = np.max(np.abs(audio))
    if peak == 0:
        return audio  # silent audio

    threshold = peak * 10 ** (threshold_db / 20)
    window_sec = 0.1
    window_size = int(window_sec * SAMPLE_RATE)
    if window_size < 1:
        window_size = 1

    # Compute moving RMS (root mean square) over the window
    def moving_rms(x: np.ndarray, w: int) -> np.ndarray:
        # Pad with zeros at the end to keep length
        if x.shape[0] < w:
            return np.array([])
        cumsum = np.cumsum(np.insert(x**2, 0, 0))
        rms = np.sqrt((cumsum[w:] - cumsum[:-w]) / w)
        # Pad to match input length (pad end)
        pad = np.zeros(x.shape[0] - rms.shape[0])
        return np.concatenate([rms, pad])

    rms = moving_rms(audio, window_size)
    # Find last window above threshold
    for i in range(rms.shape[0] - 1, -1, -1):
        if rms[i] > threshold:
            end = min(
                i + window_size + int(min_silence_sec * SAMPLE_RATE), audio.shape[0]
            )
            if end < audio.shape[0]:
                print(
                    "Trimming silence from end: "
                    f"{(audio.shape[0] - end) / SAMPLE_RATE:.1f}s removed"
                )
            return audio[:end]

    raise ValueError("Internal error, no windows above threshold found.")


def trim_trailing_silence(in_path: Path, out_path: Path | None = None) -> None:
    if out_path is None:
        out_path = in_path.with_stem(in_path.stem + "_trimmed")

    data, _sr = sphn.read(in_path, sample_rate=SAMPLE_RATE)

    if data.ndim == 2:
        data = np.mean(data, axis=0)
    elif data.ndim == 1:
        pass
    else:
        raise ValueError(f"Unexpected audio shape: {data.shape}")

    n_samples = data.shape[0]
    data = trim_silence_end(data)

    ten_sec_samples = int(SAMPLE_RATE * 10)
    if n_samples < ten_sec_samples:
        raise ValueError(
            f"Input shorter than 10 seconds: {n_samples / SAMPLE_RATE:.2f}s"
        )

    data_last10 = data[-ten_sec_samples:]
    if data_last10.shape[0] < ten_sec_samples:
        raise ValueError(
            "Less than 10 seconds remain after trimming silence: "
            f"{data_last10.shape[0] / SAMPLE_RATE:.2f}s"
        )

    sphn.write_wav(out_path, data_last10, SAMPLE_RATE)
    print(f"Wrote {out_path} ({data_last10.shape[0] / SAMPLE_RATE:.2f}s)")


def main():
    parser = argparse.ArgumentParser(
        description="Trim last 10s and trailing silence from wav files."
    )
    parser.add_argument(
        "inputs", nargs="+", help="Input wav files or glob patterns (e.g. *.wav)"
    )
    args = parser.parse_args()

    for arg in args.inputs:
        in_path = Path(arg)

        # if already trimmed, skip
        if in_path.suffix == ".wav" and in_path.stem.endswith("_trimmed"):
            print(f"Skipping {in_path} (already trimmed)")
            continue

        if not in_path.is_file():
            print(f"Skipping {in_path} (not a file)")
            continue
        try:
            trim_trailing_silence(in_path)
        except ValueError as e:
            print(f"Error processing {in_path}: {e}")
            continue


if __name__ == "__main__":
    main()



================================================
FILE: unmute/tts/voice_cloning.py
================================================
import logging
import uuid

import requests

from unmute.cache import get_cache
from unmute.kyutai_constants import VOICE_CLONING_SERVER

logger = logging.getLogger(__name__)


voice_embeddings_cache = get_cache(prefix="voice", ttl_seconds=60 * 60 * 1)  # 1 hour


def clone_voice(audio_data: bytes) -> str:
    # Generate a unique voice name
    voice_name = "custom:" + str(uuid.uuid4())

    # Call the voice cloning server
    response = requests.post(
        f"{VOICE_CLONING_SERVER}/api/voice",
        data=audio_data,
        headers={"Content-Type": "application/octet-stream"},
    )
    response.raise_for_status()
    msgpack_data = response.content

    logger.info(f"Received voice embedding of size: {len(msgpack_data)} bytes")

    voice_embeddings_cache.set(voice_name, msgpack_data)
    voice_embeddings_cache.cleanup()

    return voice_name



================================================
FILE: unmute/tts/voice_donation.py
================================================
import datetime
import functools
import logging
import random
import time
import uuid
from pathlib import Path
from typing import Literal

from pydantic import BaseModel

from unmute import metrics as mt
from unmute.cache import get_cache
from unmute.kyutai_constants import MAX_VOICE_FILE_SIZE_MB, VOICE_DONATION_DIR

MINUTES_TO_VERIFY = 5
SECONDS_IN_HOUR = 60 * 60

voice_donation_verification_cache = get_cache(
    prefix="voice_donation_verification", ttl_seconds=SECONDS_IN_HOUR * 1
)

CONSTANT_PREFIX = "I consent to my voice being used for voice cloning."

logger = logging.getLogger(__name__)


@functools.cache
def get_sentences():
    with open(Path(__file__).parent / "voice_donation_sentences.txt", "r") as f:
        return [line.strip() for line in f if line.strip()]


class VoiceDonationVerification(BaseModel):
    id: str
    text: str
    created_at_timetamp: float  # seconds since epoch


def generate_verification() -> VoiceDonationVerification:
    sentences = get_sentences()
    chosen_sentences = random.sample(sentences, 2)
    verification_text = f"{CONSTANT_PREFIX} {chosen_sentences[0]} {chosen_sentences[1]}"
    verification_id = uuid.uuid4()

    verification = VoiceDonationVerification(
        id=str(verification_id),
        text=verification_text,
        created_at_timetamp=time.time(),
    )

    voice_donation_verification_cache.set(
        verification.id, verification.model_dump_json()
    )
    voice_donation_verification_cache.cleanup()

    return verification


class VoiceDonationSubmission(BaseModel):
    format_version: Literal["1.0"] = "1.0"
    # The email is kept so that the person can contact us if they want to withdraw their
    # donation, not published.
    email: str
    nickname: str
    verification_id: uuid.UUID
    # Only CC0 is allowed for now, but storing in case we decide to change it later
    license: Literal["CC0"] = "CC0"


class VoiceDonationMetadata(BaseModel):
    submission: VoiceDonationSubmission
    verification: VoiceDonationVerification
    timestamp: float
    timestamp_str: str  # For human readability


def submit_voice_donation(
    submission: VoiceDonationSubmission, audio_file: bytes
) -> None:
    file_size_mb = len(audio_file) / (1024 * 1024)

    # No way they would be able to say all verification sentences in this time.
    if file_size_mb < 0.1:
        raise ValueError("Audio file is too small. Please provide a valid audio file.")

    # Should be checked by middleware already, but let's ensure it here too
    if file_size_mb > MAX_VOICE_FILE_SIZE_MB:
        raise ValueError(
            f"Audio file is too large. Maximum size is {MAX_VOICE_FILE_SIZE_MB} MB."
        )

    if len(submission.nickname) > 30:
        raise ValueError("Nickname is too long. Maximum length is 30 characters.")

    verification_raw = voice_donation_verification_cache.get(
        str(submission.verification_id)
    )
    if not verification_raw:
        raise ValueError(
            "Couldn't find verification data for the provided ID. "
            "Note you must complete the verification within "
            f"{MINUTES_TO_VERIFY:.0f} minutes."
        )
    verification = VoiceDonationVerification.model_validate_json(verification_raw)

    sec_since_creation = time.time() - verification.created_at_timetamp

    if sec_since_creation > MINUTES_TO_VERIFY * 60:
        raise ValueError(
            f"Verification expired after {MINUTES_TO_VERIFY} minutes. "
            "Please request a new verification."
        )

    VOICE_DONATION_DIR.mkdir(parents=True, exist_ok=True)
    audio_file_path = VOICE_DONATION_DIR / f"{submission.verification_id}.wav"
    audio_file_path.write_bytes(audio_file)

    now = datetime.datetime.now().astimezone()
    metadata = VoiceDonationMetadata(
        submission=submission,
        verification=verification,
        timestamp=now.timestamp(),
        timestamp_str=now.isoformat(),
    )
    metadata_path = VOICE_DONATION_DIR / f"{submission.verification_id}.json"
    metadata_path.write_text(metadata.model_dump_json(indent=2))

    voice_donation_verification_cache.delete(str(submission.verification_id))
    voice_donation_verification_cache.cleanup()

    logger.info(
        f"Received voice donation with id {submission.verification_id}, "
        f"file size {file_size_mb:.2f} MB. "
        f"Saved to {audio_file_path}."
    )
    mt.VOICE_DONATION_SUBMISSIONS.inc()



================================================
FILE: unmute/tts/voices.py
================================================
import asyncio
import logging
import subprocess
import time
from hashlib import md5
from pathlib import Path
from typing import Literal, Sequence

import librosa
import numpy as np
import sphn
import tqdm.auto
import websockets
from pydantic import BaseModel, Field
from ruamel.yaml import YAML

from unmute.kyutai_constants import HEADERS
from unmute.llm.system_prompt import Instructions
from unmute.tts.freesound_download import (
    OUTPUT_DIR,
    FreesoundVoiceSource,
    download_sound,
)

TTS_OUTPUT_CACHE_DIR = OUTPUT_DIR / "tts-outputs"
CFG_PARAM = "cfg_alpha=1.5"

SERVER_VOICES_DIR_DEV = Path("/home") / Path.home().stem / "models" / "tts-voices"
DEV_SERVER = "pod2"

SERVER_VOICES_DIR_PROD = Path("/scratch/models/")
PROD_SERVER = "root@unmute.sh"

logger = logging.getLogger(__name__)


async def text_to_speech_non_streaming(
    url: str, tts_text: str, voice: str
) -> np.ndarray:
    text_hash = md5(tts_text.encode("utf-8")).hexdigest()[:4]
    filename = f"{voice.replace('/', '__')}_{tts_text[:20]}_{text_hash}_{CFG_PARAM}.ogg"
    cache_file = TTS_OUTPUT_CACHE_DIR / filename

    if cache_file.exists():
        logger.info(f"Using cached file: {cache_file}")
        audio, sr = sphn.read_opus(cache_file)
        assert audio.ndim == 2
        audio = audio[0]  # to mono

        # resample to 24kHz
        audio = librosa.resample(audio, orig_sr=sr, target_sr=24000)

        return audio

    output: np.ndarray | None = None

    async def send_messages(websocket: websockets.ClientConnection):
        global start_time
        start_time = None
        for word in tts_text.split(" "):
            await websocket.send(word)
            if start_time is None:
                start_time = time.time()
            await asyncio.sleep(0.1)
        await websocket.send(b"\0")

    async def receive_messages(websocket: websockets.ClientConnection):
        global start_time
        nonlocal output
        first_token = True
        reader = sphn.OpusStreamReader(24000)
        all_data = []
        total_len = 0
        pbar = tqdm.auto.tqdm("TTS inference")
        while True:
            try:
                response = await websocket.recv()
            except websockets.exceptions.ConnectionClosedOK:
                return np.concatenate(all_data, axis=0)
            if start_time is not None and first_token:
                first_token = False
            total_len += len(response)
            pbar.update(len(response))
            if isinstance(response, bytes):
                audio_data = reader.append_bytes(response)
                all_data.append(audio_data)

    async def websocket_client():
        uri = f"{url}/api/tts_streaming?voice={voice}&{CFG_PARAM}"

        async with websockets.connect(uri, additional_headers=HEADERS) as websocket:
            send_task = asyncio.create_task(send_messages(websocket))
            receive_task = asyncio.create_task(receive_messages(websocket))
            _, audio = await asyncio.gather(send_task, receive_task)
            return audio

    audio = await websocket_client()

    # Cache for later
    TTS_OUTPUT_CACHE_DIR.mkdir(exist_ok=True, parents=True)
    sphn.write_opus(cache_file, audio[np.newaxis, :], sample_rate=24000)
    logger.info(f"Cached file: {cache_file}")

    return audio


def subprocess_with_retries(command: Sequence[str | Path], attempts: int = 3):
    """
    Run a subprocess command with retries on failure.
    """
    for attempt in range(attempts):
        try:
            subprocess.run(command, check=True)
            return  # Exit the function if successful
        except subprocess.CalledProcessError as e:
            if attempt == attempts - 1:
                raise  # If it's the last attempt, re-raise the exception
            else:
                logger.warning(f"Attempt {attempt + 1} to run {command} failed: {e}")


def upload_voice_to_dev(local_path: Path, path_on_server: str | Path):
    logger.info(f"Uploading {local_path} to {path_on_server}")

    subprocess_with_retries(
        [
            "rsync",
            local_path,
            DEV_SERVER + ":" + str(SERVER_VOICES_DIR_DEV / path_on_server),
        ],
    )


def copy_voice_to_prod(path_on_server: str):
    logger.info(f"Copying {path_on_server}(.safetensors) from dev to prod")

    paths = [path_on_server, path_on_server + ".safetensors"]

    for path in paths:
        command = [
            "scp",
            DEV_SERVER + ":" + str(SERVER_VOICES_DIR_DEV / path),
            PROD_SERVER + ":" + str(SERVER_VOICES_DIR_PROD / path),
        ]
        print("Running command:", " ".join(command))
        subprocess_with_retries(command, attempts=1)


def find_enhanced_version(original_path: Path) -> Path | None:
    # Manually created via https://podcast.adobe.com/en/enhance
    clean_path = OUTPUT_DIR / "voices-clean" / (original_path.stem + "-enhanced-v2.wav")

    if clean_path.exists():
        return clean_path
    else:
        return None


class FileVoiceSource(BaseModel):
    source_type: Literal["file"] = "file"
    path_on_server: str
    description: str | None = None
    description_link: str | None = None


class VoiceSample(BaseModel):
    model_config = {"extra": "forbid"}

    name: str | None = None
    comment: str | None = None
    good: bool | None = None
    instructions: Instructions | None = None
    source: FreesoundVoiceSource | FileVoiceSource = Field(discriminator="source_type")


class VoiceList:
    def __init__(self):
        self.path = Path(__file__).parents[2] / "voices.yaml"
        with self.path.open() as f:
            self.voices = [VoiceSample(**sound) for sound in YAML().load(f)]

    async def upload_to_server(self):
        async def process_voice(voice: VoiceSample):
            if not voice.good:
                logger.debug(
                    f"skipping {voice.name or voice.source.path_on_server}, "
                    "not marked as good"
                )
                return

            if isinstance(voice.source, FreesoundVoiceSource):
                logger.info(f"downloading {voice.name}")
                (downloaded_path, voice_source_with_metadata) = await asyncio.to_thread(
                    download_sound,
                    voice.source,
                )
                voice.source = voice_source_with_metadata
            else:
                downloaded_path = OUTPUT_DIR / "voices" / voice.source.path_on_server
                if not downloaded_path.exists():
                    raise FileNotFoundError(
                        f"File {downloaded_path} does not exist locally"
                    )

            clean_path = find_enhanced_version(downloaded_path)
            if clean_path:
                logger.info(f"Using enhanced version: {clean_path}")
                downloaded_path = clean_path

            assert voice.source.path_on_server is not None

            await asyncio.to_thread(
                upload_voice_to_dev,
                downloaded_path,
                path_on_server=voice.source.path_on_server,
            )

        await asyncio.gather(*(process_voice(voice) for voice in self.voices))

    def save(self):
        with self.path.open("w") as f:
            yaml = YAML()
            yaml.width = float("inf")  # Disable line wrapping

            # Put "good" voices first, then undecided, then bad.
            # The sort is stable, so the order is otherwise preserved
            voices = sorted(
                self.voices, key=lambda x: {True: 0, None: 1, False: 2}[x.good]
            )
            yaml.dump(
                [
                    voice.model_dump(
                        # This would also exclude the discriminator field :(
                        # exclude_defaults=True,
                        exclude_none=True,
                        exclude={"source": {"sound_instance": ["previews"]}},  # type: ignore
                    )
                    for voice in voices
                ],
                f,
            )



================================================
FILE: .github/PULL_REQUEST_TEMPLATE.md
================================================
## Checklist

- [ ] Read CONTRIBUTING.md, and accept the CLA by including the provided snippet. We will not accept PR without this.

## PR Description

<!-- Description for the PR -->



================================================
FILE: .github/workflows/ci.yml
================================================
name: CI

on:
  push:
    branches:
      - prod
  pull_request:

jobs:
  # Enable again when we don't have private dependencies
  #build-docker-images:
  #  runs-on: ubuntu-latest
  #  steps:
  #    - name: Checkout code
  #      uses: actions/checkout@v3
#
  #    - name: Set up a builder (we don't want to load the images)
  #      run: docker buildx create --name mybuilder --use
#
  #    - name: Build all docker images
  #      run: docker buildx bake --progress=plain -f swarm-deploy.yml workers frontend tts
  #      env:
  #        DOMAIN: dummy

  pre-commit:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Install uv
        uses: astral-sh/setup-uv@v5
        with:
          version: "0.7.12"

      - name: Install Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20

      - name: Install pnpm
        run: npm install -g pnpm

      - name: Install dependencies
        run: cd frontend && pnpm install

      - name: Run pre-commit
        run: |
          uv run pre-commit run --all-files
          # Some redundancy here because some hooks will run in any stage,
          # but I don't think there is a cleaner way to make sure they all run
          uv run pre-commit run --all-files --hook-stage pre-push

  backend-unit-tests:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Install uv
        uses: astral-sh/setup-uv@v5
        with:
          version: "0.7.12"

      - name: Run backend unit tests
        run: uv run pytest -v


