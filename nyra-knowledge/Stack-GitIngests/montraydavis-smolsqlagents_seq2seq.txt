Directory structure:
‚îî‚îÄ‚îÄ montraydavis-smolsqlagents/
    ‚îú‚îÄ‚îÄ Demos.ipynb
    ‚îú‚îÄ‚îÄ LICENSE
    ‚îú‚îÄ‚îÄ seed.sql
    ‚îú‚îÄ‚îÄ docs/
    ‚îÇ   ‚îú‚îÄ‚îÄ README.md
    ‚îÇ   ‚îú‚îÄ‚îÄ source_code_structure.md
    ‚îÇ   ‚îú‚îÄ‚îÄ agents/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BaseAgent.md
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BatchManager.md
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BusinessAgent.md
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CoreAgent.md
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ EntityRecognitionAgent.md
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FactoryAgent.md
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ IndexerAgent.md
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ IntegrationAgent.md
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ NL2SQLAgent.md
    ‚îÇ   ‚îú‚îÄ‚îÄ concepts/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agents.md
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ architecture.md
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompt_engineering.md
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vector_database.md
    ‚îÇ   ‚îú‚îÄ‚îÄ contributing/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ development.md
    ‚îÇ   ‚îú‚îÄ‚îÄ database/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DatabaseInspector.md
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DocumentationStore.md
    ‚îÇ   ‚îú‚îÄ‚îÄ getting_started/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ configuration.md
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ installation.md
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ quick_start.md
    ‚îÇ   ‚îú‚îÄ‚îÄ output/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DocumentationFormatter.md
    ‚îÇ   ‚îî‚îÄ‚îÄ vector/
    ‚îÇ       ‚îú‚îÄ‚îÄ EmbeddingsClient.md
    ‚îÇ       ‚îú‚îÄ‚îÄ SearchTools.md
    ‚îÇ       ‚îî‚îÄ‚îÄ VectorStore.md
    ‚îú‚îÄ‚îÄ smol-sql-agents/
    ‚îÇ   ‚îú‚îÄ‚îÄ API_DOCUMENTATION.md
    ‚îÇ   ‚îú‚îÄ‚îÄ di.py
    ‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt
    ‚îÇ   ‚îú‚îÄ‚îÄ run_tests.py
    ‚îÇ   ‚îú‚îÄ‚îÄ setup.py
    ‚îÇ   ‚îú‚îÄ‚îÄ simple_test.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_db_performance.py
    ‚îÇ   ‚îú‚îÄ‚îÄ test_structure.py
    ‚îÇ   ‚îú‚îÄ‚îÄ backend/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generate.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
    ‚îÇ   ‚îú‚îÄ‚îÄ src/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agents/
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ batch_manager.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ business.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ entity_recognition.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ factory.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ indexer.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ integration.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nl2sql.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ concepts/
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ loader.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ matcher.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ examples/
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ test_concepts.yaml
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tools/
    ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ factory.py
    ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ shared.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database/
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ inspector.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ persistence.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ output/
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ formatters.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ utils/
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validation/
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ business_validator.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ query_optimizer.py
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tsql_validator.py
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vector/
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ embeddings.py
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ search.py
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ store.py
    ‚îÇ   ‚îî‚îÄ‚îÄ tests/
    ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ       ‚îú‚îÄ‚îÄ test_agent_core.py
    ‚îÇ       ‚îú‚îÄ‚îÄ test_batch_manager.py
    ‚îÇ       ‚îú‚îÄ‚îÄ test_index_processed_documents.py
    ‚îÇ       ‚îú‚îÄ‚îÄ test_indexer_agent.py
    ‚îÇ       ‚îú‚îÄ‚îÄ test_recognize_entities.py
    ‚îÇ       ‚îú‚îÄ‚îÄ test_search_tools.py
    ‚îÇ       ‚îú‚îÄ‚îÄ test_vector_indexing_fallback.py
    ‚îÇ       ‚îî‚îÄ‚îÄ test_vector_indexing_retry.py
    ‚îú‚îÄ‚îÄ tutorial/
    ‚îÇ   ‚îú‚îÄ‚îÄ 00_TOC.md
    ‚îÇ   ‚îú‚îÄ‚îÄ 01_introduction.md
    ‚îÇ   ‚îú‚îÄ‚îÄ 02_agent_framework.md
    ‚îÇ   ‚îú‚îÄ‚îÄ 03_agent_implementation.md
    ‚îÇ   ‚îú‚îÄ‚îÄ 04_vector_database.md
    ‚îÇ   ‚îú‚îÄ‚îÄ 05_ai_agents.md
    ‚îÇ   ‚îú‚îÄ‚îÄ 06_prompt_engineering.md
    ‚îÇ   ‚îî‚îÄ‚îÄ 06_sql_generation.md
    ‚îî‚îÄ‚îÄ web-ui/
        ‚îú‚îÄ‚îÄ README.md
        ‚îî‚îÄ‚îÄ frontend/
            ‚îú‚îÄ‚îÄ README.md
            ‚îú‚îÄ‚îÄ package.json
            ‚îú‚îÄ‚îÄ public/
            ‚îÇ   ‚îú‚îÄ‚îÄ index.html
            ‚îÇ   ‚îú‚îÄ‚îÄ manifest.json
            ‚îÇ   ‚îî‚îÄ‚îÄ robots.txt
            ‚îî‚îÄ‚îÄ src/
                ‚îú‚îÄ‚îÄ App.css
                ‚îú‚îÄ‚îÄ App.js
                ‚îú‚îÄ‚îÄ App.test.js
                ‚îú‚îÄ‚îÄ index.css
                ‚îú‚îÄ‚îÄ index.js
                ‚îú‚îÄ‚îÄ reportWebVitals.js
                ‚îú‚îÄ‚îÄ setupTests.js
                ‚îî‚îÄ‚îÄ components/
                    ‚îú‚îÄ‚îÄ index.js
                    ‚îú‚îÄ‚îÄ documentation/
                    ‚îÇ   ‚îú‚îÄ‚îÄ DocumentationSidebar.js
                    ‚îÇ   ‚îú‚îÄ‚îÄ index.js
                    ‚îÇ   ‚îú‚îÄ‚îÄ ItemDetailsModal.css
                    ‚îÇ   ‚îî‚îÄ‚îÄ ItemDetailsModal.js
                    ‚îú‚îÄ‚îÄ documentation-explorer/
                    ‚îÇ   ‚îú‚îÄ‚îÄ DocumentationExplorer.css
                    ‚îÇ   ‚îú‚îÄ‚îÄ DocumentationExplorer.js
                    ‚îÇ   ‚îú‚îÄ‚îÄ index.js
                    ‚îÇ   ‚îú‚îÄ‚îÄ ItemDetailsModal.css
                    ‚îÇ   ‚îî‚îÄ‚îÄ ItemDetailsModal.js
                    ‚îú‚îÄ‚îÄ pages/
                    ‚îÇ   ‚îú‚îÄ‚îÄ documentation/
                    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md
                    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DocumentationPage.css
                    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.js
                    ‚îÇ   ‚îú‚îÄ‚îÄ relationships/
                    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md
                    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.js
                    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ RelationshipsPage.css
                    ‚îÇ   ‚îî‚îÄ‚îÄ schema/
                    ‚îÇ       ‚îú‚îÄ‚îÄ README.md
                    ‚îÇ       ‚îú‚îÄ‚îÄ index.js
                    ‚îÇ       ‚îî‚îÄ‚îÄ SchemaPage.css
                    ‚îú‚îÄ‚îÄ query/
                    ‚îÇ   ‚îú‚îÄ‚îÄ BusinessContext.js
                    ‚îÇ   ‚îú‚îÄ‚îÄ EntityRecognitionResults.js
                    ‚îÇ   ‚îú‚îÄ‚îÄ index.js
                    ‚îÇ   ‚îú‚îÄ‚îÄ OptimizationSuggestions.js
                    ‚îÇ   ‚îú‚îÄ‚îÄ QueryInput.js
                    ‚îÇ   ‚îú‚îÄ‚îÄ QueryPage.js
                    ‚îÇ   ‚îú‚îÄ‚îÄ QueryResults.js
                    ‚îÇ   ‚îú‚îÄ‚îÄ SQLAgentStatus.js
                    ‚îÇ   ‚îî‚îÄ‚îÄ SQLGeneration.js
                    ‚îî‚îÄ‚îÄ ui/
                        ‚îú‚îÄ‚îÄ index.js
                        ‚îú‚îÄ‚îÄ SplashScreen.js
                        ‚îî‚îÄ‚îÄ TopNavigation.js

================================================
FILE: Demos.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# SQL Documentation Agent - Interactive Demo

This notebook demonstrates the capabilities of the Smol-SQL Agents Suite for autonomous database documentation generation with semantic search capabilities.

## Features Demonstrated:
- üîç Database schema discovery
- üìù AI-powered documentation generation
- üîç Vector indexing with ChromaDB
- üîç Semantic search capabilities
- ‚ö° Batch processing for efficiency
- üíæ State persistence and resume capability
- üìä Cost estimation
- üîÑ Index management
"""

"""
## 1. Setup and Environment Configuration
"""

%pip install -r smol-sql-agents/requirements.txt
# Output:
#   Requirement already satisfied: smolagents in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from -r smol-sql-agents/requirements.txt (line 4)) (1.20.0)

#   Requirement already satisfied: litellm in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from -r smol-sql-agents/requirements.txt (line 7)) (1.74.9)

#   Requirement already satisfied: SQLAlchemy in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from -r smol-sql-agents/requirements.txt (line 10)) (2.0.41)

#   Requirement already satisfied: psycopg2-binary in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from -r smol-sql-agents/requirements.txt (line 13)) (2.9.10)

#   Requirement already satisfied: python-dotenv in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from -r smol-sql-agents/requirements.txt (line 16)) (1.1.1)

#   Requirement already satisfied: jinja2 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from -r smol-sql-agents/requirements.txt (line 19)) (3.1.6)

#   Requirement already satisfied: chromadb in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from -r smol-sql-agents/requirements.txt (line 22)) (1.0.15)

#   Requirement already satisfied: openai in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from -r smol-sql-agents/requirements.txt (line 23)) (1.97.1)

#   Requirement already satisfied: numpy in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from -r smol-sql-agents/requirements.txt (line 24)) (2.2.6)

#   Requirement already satisfied: tiktoken in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from -r smol-sql-agents/requirements.txt (line 25)) (0.9.0)

#   Requirement already satisfied: tenacity in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from -r smol-sql-agents/requirements.txt (line 26)) (9.1.2)

#   Requirement already satisfied: pyodbc in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from -r smol-sql-agents/requirements.txt (line 27)) (5.2.0)

#   Collecting sqlparse (from -r smol-sql-agents/requirements.txt (line 30))

#     Downloading sqlparse-0.5.3-py3-none-any.whl.metadata (3.9 kB)

#   Requirement already satisfied: dotenv in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from -r smol-sql-agents/requirements.txt (line 33)) (0.9.9)

#   Requirement already satisfied: pytest in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from -r smol-sql-agents/requirements.txt (line 36)) (8.4.1)

#   Requirement already satisfied: pytest-cov in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from -r smol-sql-agents/requirements.txt (line 37)) (6.2.1)

#   Requirement already satisfied: huggingface-hub>=0.31.2 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from smolagents->-r smol-sql-agents/requirements.txt (line 4)) (0.34.2)

#   Requirement already satisfied: requests>=2.32.3 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from smolagents->-r smol-sql-agents/requirements.txt (line 4)) (2.32.4)

#   Requirement already satisfied: rich>=13.9.4 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from smolagents->-r smol-sql-agents/requirements.txt (line 4)) (14.0.0)

#   Requirement already satisfied: pillow>=10.0.1 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from smolagents->-r smol-sql-agents/requirements.txt (line 4)) (11.3.0)

#   Requirement already satisfied: aiohttp>=3.10 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from litellm->-r smol-sql-agents/requirements.txt (line 7)) (3.12.14)

#   Requirement already satisfied: click in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from litellm->-r smol-sql-agents/requirements.txt (line 7)) (8.2.1)

#   Requirement already satisfied: httpx>=0.23.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from litellm->-r smol-sql-agents/requirements.txt (line 7)) (0.28.1)

#   Requirement already satisfied: importlib-metadata>=6.8.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from litellm->-r smol-sql-agents/requirements.txt (line 7)) (8.7.0)

#   Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from litellm->-r smol-sql-agents/requirements.txt (line 7)) (4.25.0)

#   Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from litellm->-r smol-sql-agents/requirements.txt (line 7)) (2.11.7)

#   Requirement already satisfied: tokenizers in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from litellm->-r smol-sql-agents/requirements.txt (line 7)) (0.21.4)

#   Requirement already satisfied: MarkupSafe>=2.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from jinja2->-r smol-sql-agents/requirements.txt (line 19)) (3.0.2)

#   Requirement already satisfied: attrs>=22.2.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->-r smol-sql-agents/requirements.txt (line 7)) (25.3.0)

#   Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->-r smol-sql-agents/requirements.txt (line 7)) (2025.4.1)

#   Requirement already satisfied: referencing>=0.28.4 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->-r smol-sql-agents/requirements.txt (line 7)) (0.36.2)

#   Requirement already satisfied: rpds-py>=0.7.1 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->-r smol-sql-agents/requirements.txt (line 7)) (0.26.0)

#   Requirement already satisfied: annotated-types>=0.6.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from pydantic<3.0.0,>=2.5.0->litellm->-r smol-sql-agents/requirements.txt (line 7)) (0.7.0)

#   Requirement already satisfied: pydantic-core==2.33.2 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from pydantic<3.0.0,>=2.5.0->litellm->-r smol-sql-agents/requirements.txt (line 7)) (2.33.2)

#   Requirement already satisfied: typing-extensions>=4.12.2 in c:\users\montr\appdata\roaming\python\python310\site-packages (from pydantic<3.0.0,>=2.5.0->litellm->-r smol-sql-agents/requirements.txt (line 7)) (4.14.1)

#   Requirement already satisfied: typing-inspection>=0.4.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from pydantic<3.0.0,>=2.5.0->litellm->-r smol-sql-agents/requirements.txt (line 7)) (0.4.1)

#   Requirement already satisfied: greenlet>=1 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from SQLAlchemy->-r smol-sql-agents/requirements.txt (line 10)) (3.2.3)

#   Requirement already satisfied: build>=1.0.3 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from chromadb->-r smol-sql-agents/requirements.txt (line 22)) (1.2.2.post1)

#   Requirement already satisfied: pybase64>=1.4.1 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from chromadb->-r smol-sql-agents/requirements.txt (line 22)) (1.4.2)

#   Requirement already satisfied: uvicorn>=0.18.3 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (0.23.1)

#   Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from chromadb->-r smol-sql-agents/requirements.txt (line 22)) (5.4.0)

#   Requirement already satisfied: onnxruntime>=1.14.1 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from chromadb->-r smol-sql-agents/requirements.txt (line 22)) (1.22.1)

#   Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from chromadb->-r smol-sql-agents/requirements.txt (line 22)) (1.35.0)

#   Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from chromadb->-r smol-sql-agents/requirements.txt (line 22)) (1.35.0)

#   Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from chromadb->-r smol-sql-agents/requirements.txt (line 22)) (1.35.0)

#   Requirement already satisfied: pypika>=0.48.9 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from chromadb->-r smol-sql-agents/requirements.txt (line 22)) (0.48.9)

#   Requirement already satisfied: tqdm>=4.65.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from chromadb->-r smol-sql-agents/requirements.txt (line 22)) (4.67.1)

#   Requirement already satisfied: overrides>=7.3.1 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from chromadb->-r smol-sql-agents/requirements.txt (line 22)) (7.7.0)

#   Requirement already satisfied: importlib-resources in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from chromadb->-r smol-sql-agents/requirements.txt (line 22)) (6.5.2)

#   Requirement already satisfied: grpcio>=1.58.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from chromadb->-r smol-sql-agents/requirements.txt (line 22)) (1.68.0)

#   Requirement already satisfied: bcrypt>=4.0.1 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from chromadb->-r smol-sql-agents/requirements.txt (line 22)) (4.3.0)

#   Requirement already satisfied: typer>=0.9.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from chromadb->-r smol-sql-agents/requirements.txt (line 22)) (0.16.0)

#   Requirement already satisfied: kubernetes>=28.1.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from chromadb->-r smol-sql-agents/requirements.txt (line 22)) (33.1.0)

#   Requirement already satisfied: pyyaml>=6.0.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from chromadb->-r smol-sql-agents/requirements.txt (line 22)) (6.0.2)

#   Requirement already satisfied: mmh3>=4.0.1 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from chromadb->-r smol-sql-agents/requirements.txt (line 22)) (5.1.0)

#   Requirement already satisfied: orjson>=3.9.12 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from chromadb->-r smol-sql-agents/requirements.txt (line 22)) (3.11.0)

#   Requirement already satisfied: six>=1.5 in c:\users\montr\appdata\roaming\python\python310\site-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (1.17.0)

#   Requirement already satisfied: python-dateutil>=2.2 in c:\users\montr\appdata\roaming\python\python310\site-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (2.9.0.post0)

#   Requirement already satisfied: backoff>=1.10.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (2.2.1)

#   Requirement already satisfied: distro>=1.5.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (1.9.0)

#   Requirement already satisfied: charset_normalizer<4,>=2 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from requests>=2.32.3->smolagents->-r smol-sql-agents/requirements.txt (line 4)) (3.4.2)

#   Requirement already satisfied: idna<4,>=2.5 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from requests>=2.32.3->smolagents->-r smol-sql-agents/requirements.txt (line 4)) (3.10)

#   Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from requests>=2.32.3->smolagents->-r smol-sql-agents/requirements.txt (line 4)) (1.26.20)

#   Requirement already satisfied: certifi>=2017.4.17 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from requests>=2.32.3->smolagents->-r smol-sql-agents/requirements.txt (line 4)) (2025.7.14)

#   Requirement already satisfied: anyio<5,>=3.5.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from openai->-r smol-sql-agents/requirements.txt (line 23)) (4.9.0)

#   Requirement already satisfied: jiter<1,>=0.4.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from openai->-r smol-sql-agents/requirements.txt (line 23)) (0.10.0)

#   Requirement already satisfied: sniffio in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from openai->-r smol-sql-agents/requirements.txt (line 23)) (1.3.1)

#   Requirement already satisfied: exceptiongroup>=1.0.2 in c:\users\montr\appdata\roaming\python\python310\site-packages (from anyio<5,>=3.5.0->openai->-r smol-sql-agents/requirements.txt (line 23)) (1.3.0)

#   Requirement already satisfied: httpcore==1.* in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from httpx>=0.23.0->litellm->-r smol-sql-agents/requirements.txt (line 7)) (1.0.9)

#   Requirement already satisfied: h11>=0.16 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from httpcore==1.*->httpx>=0.23.0->litellm->-r smol-sql-agents/requirements.txt (line 7)) (0.16.0)

#   Requirement already satisfied: regex>=2022.1.18 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from tiktoken->-r smol-sql-agents/requirements.txt (line 25)) (2024.11.6)

#   Requirement already satisfied: colorama>=0.4 in c:\users\montr\appdata\roaming\python\python310\site-packages (from pytest->-r smol-sql-agents/requirements.txt (line 36)) (0.4.6)

#   Requirement already satisfied: iniconfig>=1 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from pytest->-r smol-sql-agents/requirements.txt (line 36)) (2.1.0)

#   Requirement already satisfied: packaging>=20 in c:\users\montr\appdata\roaming\python\python310\site-packages (from pytest->-r smol-sql-agents/requirements.txt (line 36)) (25.0)

#   Requirement already satisfied: pluggy<2,>=1.5 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from pytest->-r smol-sql-agents/requirements.txt (line 36)) (1.6.0)

#   Requirement already satisfied: pygments>=2.7.2 in c:\users\montr\appdata\roaming\python\python310\site-packages (from pytest->-r smol-sql-agents/requirements.txt (line 36)) (2.19.2)

#   Requirement already satisfied: tomli>=1 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from pytest->-r smol-sql-agents/requirements.txt (line 36)) (2.2.1)

#   Requirement already satisfied: coverage>=7.5 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from coverage[toml]>=7.5->pytest-cov->-r smol-sql-agents/requirements.txt (line 37)) (7.10.1)

#   Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from aiohttp>=3.10->litellm->-r smol-sql-agents/requirements.txt (line 7)) (2.6.1)

#   Requirement already satisfied: aiosignal>=1.4.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from aiohttp>=3.10->litellm->-r smol-sql-agents/requirements.txt (line 7)) (1.4.0)

#   Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from aiohttp>=3.10->litellm->-r smol-sql-agents/requirements.txt (line 7)) (5.0.1)

#   Requirement already satisfied: frozenlist>=1.1.1 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from aiohttp>=3.10->litellm->-r smol-sql-agents/requirements.txt (line 7)) (1.7.0)

#   Requirement already satisfied: multidict<7.0,>=4.5 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from aiohttp>=3.10->litellm->-r smol-sql-agents/requirements.txt (line 7)) (6.6.3)

#   Requirement already satisfied: propcache>=0.2.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from aiohttp>=3.10->litellm->-r smol-sql-agents/requirements.txt (line 7)) (0.3.2)

#   Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from aiohttp>=3.10->litellm->-r smol-sql-agents/requirements.txt (line 7)) (1.20.1)

#   Requirement already satisfied: pyproject_hooks in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from build>=1.0.3->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (1.2.0)

#   Requirement already satisfied: filelock in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from huggingface-hub>=0.31.2->smolagents->-r smol-sql-agents/requirements.txt (line 4)) (3.18.0)

#   Requirement already satisfied: fsspec>=2023.5.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from huggingface-hub>=0.31.2->smolagents->-r smol-sql-agents/requirements.txt (line 4)) (2025.7.0)

#   Requirement already satisfied: zipp>=3.20 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from importlib-metadata>=6.8.0->litellm->-r smol-sql-agents/requirements.txt (line 7)) (3.23.0)

#   Requirement already satisfied: google-auth>=1.0.1 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from kubernetes>=28.1.0->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (2.40.3)

#   Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from kubernetes>=28.1.0->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (1.8.0)

#   Requirement already satisfied: requests-oauthlib in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from kubernetes>=28.1.0->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (2.0.0)

#   Requirement already satisfied: oauthlib>=3.2.2 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from kubernetes>=28.1.0->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (3.3.1)

#   Requirement already satisfied: durationpy>=0.7 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from kubernetes>=28.1.0->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (0.10)

#   Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (5.5.2)

#   Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (0.4.2)

#   Requirement already satisfied: rsa<5,>=3.1.4 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (4.9.1)

#   Requirement already satisfied: pyasn1>=0.1.3 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (0.6.1)

#   Requirement already satisfied: coloredlogs in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from onnxruntime>=1.14.1->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (15.0.1)

#   Requirement already satisfied: flatbuffers in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from onnxruntime>=1.14.1->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (25.2.10)

#   Requirement already satisfied: protobuf in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from onnxruntime>=1.14.1->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (5.29.5)

#   Requirement already satisfied: sympy in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from onnxruntime>=1.14.1->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (1.14.0)

#   Requirement already satisfied: googleapis-common-protos~=1.57 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (1.70.0)

#   Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.35.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (1.35.0)

#   Requirement already satisfied: opentelemetry-proto==1.35.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (1.35.0)

#   Requirement already satisfied: opentelemetry-semantic-conventions==0.56b0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from opentelemetry-sdk>=1.2.0->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (0.56b0)

#   Requirement already satisfied: markdown-it-py>=2.2.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from rich>=13.9.4->smolagents->-r smol-sql-agents/requirements.txt (line 4)) (3.0.0)

#   Requirement already satisfied: mdurl~=0.1 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->smolagents->-r smol-sql-agents/requirements.txt (line 4)) (0.1.2)

#   Requirement already satisfied: shellingham>=1.3.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from typer>=0.9.0->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (1.5.4)

#   Requirement already satisfied: httptools>=0.5.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (0.6.4)

#   Requirement already satisfied: watchfiles>=0.13 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (1.1.0)

#   Requirement already satisfied: websockets>=10.4 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (15.0.1)

#   Requirement already satisfied: humanfriendly>=9.1 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (10.0)

#   Requirement already satisfied: pyreadline3 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (3.5.4)

#   Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\users\montr\appdata\local\programs\python\python310\lib\site-packages (from sympy->onnxruntime>=1.14.1->chromadb->-r smol-sql-agents/requirements.txt (line 22)) (1.3.0)

#   Downloading sqlparse-0.5.3-py3-none-any.whl (44 kB)

#   Installing collected packages: sqlparse

#   Successfully installed sqlparse-0.5.3

#   Note: you may need to restart the kernel to use updated packages.

#     WARNING: The script sqlformat.exe is installed in 'c:\Users\montr\AppData\Local\Programs\Python\Python310\Scripts' which is not on PATH.

#     Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.


import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# Add the project root to Python path
project_root = Path.cwd()
sys.path.append(str(project_root) + "/smol-sql-agents")

# Load environment variables
load_dotenv()

# Import the framework components
from src.database.inspector import DatabaseInspector
from src.agents.core import PersistentDocumentationAgent
from src.output.formatters import DocumentationFormatter
from src.agents.batch_manager import BatchIndexingManager
from src.vector.search import search_table_documentation, search_relationship_documentation

print("‚úÖ Framework imports successful")
print(f"üìÅ Project root: {project_root}")

"""
## 2. Environment Configuration Check
"""

# Check environment configuration
print("üîß Environment Configuration Check")
print("=" * 50)

# Check required environment variables
required_vars = [
    'OPENAI_API_KEY',
    'DATABASE_URL'
]

for var in required_vars:
    value = os.getenv(var)
    if value:
        # Mask sensitive values
        if 'API_KEY' in var:
            display_value = value[:8] + "..." + value[-4:] if len(value) > 12 else "***"
        else:
            display_value = value
        print(f"‚úÖ {var}: {display_value}")
    else:
        print(f"‚ùå {var}: Not set")

# Check optional environment variables
optional_vars = [
    'LOG_LEVEL',
    'EMBEDDING_BATCH_SIZE',
    'EMBEDDING_MAX_RETRIES'
]

print("\nüìã Optional Configuration:")
for var in optional_vars:
    value = os.getenv(var, "Not set (using default)")
    print(f"  {var}: {value}")

"""
## 3. Database Connection and Schema Discovery
"""

# Initialize database inspector
print("üîç Database Schema Discovery")
print("=" * 50)

try:
    inspector = DatabaseInspector()
    
    # Discover all tables
    tables = inspector.get_all_table_names()
    print(f"üìä Found {len(tables)} tables:")
    for i, table in enumerate(tables[:10], 1):  # Show first 10
        print(f"  {i}. {table}")
    if len(tables) > 10:
        print(f"  ... and {len(tables) - 10} more tables")
    
    # Discover relationships
    relationships = inspector.get_all_foreign_key_relationships()
    print(f"\nüîó Found {len(relationships)} relationships:")
    for i, rel in enumerate(relationships[:5], 1):  # Show first 5
        print(f"  {i}. {rel['constrained_table']} -> {rel['referred_table']}")
    if len(relationships) > 5:
        print(f"  ... and {len(relationships) - 5} more relationships")
    
    # Show sample table schema
    if tables:
        sample_table = tables[0]
        print(f"\nüìã Sample schema for '{sample_table}':")
        schema = inspector.get_table_schema(sample_table)
        print(f"  Columns: {len(schema.get('columns', []))}")
        for col in schema.get('columns', [])[:3]:  # Show first 3 columns
            print(f"    - {col.get('name')}: {col.get('type')}")
        if len(schema.get('columns', [])) > 3:
            print(f"    ... and {len(schema.get('columns', [])) - 3} more columns")
    
    print("\n‚úÖ Database connection and schema discovery successful!")
    
except Exception as e:
    print(f"‚ùå Database connection failed: {e}")
    print("\nüí° Make sure your DATABASE_URL is correctly configured.")

"""
## 4. Agent Initialization and Vector Indexing Status
"""

# Initialize the main agent
print("ü§ñ Agent Initialization")
print("=" * 50)

try:
    agent = PersistentDocumentationAgent()
    
    print(f"‚úÖ Agent initialized successfully")
    print(f"üìä Vector indexing available: {agent.vector_indexing_available}")
    print(f"üîç Indexer agent available: {'Yes' if agent.indexer_agent else 'No'}")
    print(f"üíæ Database store available: {'Yes' if agent.store else 'No'}")
    print(f"üß† LLM model available: {'Yes' if agent.llm_model else 'No'}")
    
    if agent.vector_indexing_available:
        print("\nüéâ Vector indexing is available!")
        print("You can use advanced features like:")
        print("  - Semantic search")
        print("  - Batch processing")
        print("  - Cost estimation")
    else:
        print("\n‚ö†Ô∏è  Vector indexing is not available.")
        print("Basic documentation generation will still work.")
    
except Exception as e:
    print(f"‚ùå Agent initialization failed: {e}")
    print("\nüí° Check your OpenAI API key and database connection.")

"""
## 5. Documentation Generation Demo
"""

# Demo: Process a single table
print("üìù Documentation Generation Demo")
print("=" * 50)

if 'inspector' in locals() and 'agent' in locals():
    try:
        # Get a sample table
        tables = inspector.get_all_table_names()
        if tables:
            sample_table = tables[0]
            print(f"üéØ Processing table: {sample_table}")
            
            # Process the table documentation
            agent.process_table_documentation(sample_table)
            
            print(f"‚úÖ Successfully processed table: {sample_table}")
            print("\nüìã Generated documentation includes:")
            print("  - Business purpose inference")
            print("  - Schema analysis")
            print("  - Column descriptions")
            
            # Check if it was indexed (if vector indexing is available)
            if agent.vector_indexing_available:
                print("  - Vector indexing (for semantic search)")
            
        else:
            print("‚ùå No tables found in database")
            
    except Exception as e:
        print(f"‚ùå Table processing failed: {e}")
else:
    print("‚ùå Agent or inspector not initialized")

"""
## 6. Vector Indexing and Search Demo
"""

# Demo: Vector indexing and semantic search
print("üîç Vector Indexing and Search Demo")
print("=" * 50)

if 'agent' in locals() and agent.vector_indexing_available:
    try:
        # Check indexing status
        print("üìä Checking vector indexing status...")
        
        # Get indexing statistics
        batch_manager = BatchIndexingManager(agent.indexer_agent)
        stats = batch_manager.get_processing_stats(agent.store)
        
        print(f"\nüìà Indexing Statistics:")
        print(f"  Pending tables: {stats['pending_tables']}")
        print(f"  Pending relationships: {stats['pending_relationships']}")
        print(f"  Total pending: {stats['total_pending']}")
        print(f"  Batch size: {stats['batch_size']}")
        
        # Perform a sample search
        print("\nüîç Performing sample search...")
        search_query = "user"  # Simple search term
        
        results = agent.indexer_agent.search_documentation(search_query, "all")
        
        if results:
            total_results = results.get("total_results", 0)
            print(f"‚úÖ Search completed: {total_results} results found")
            
            # Display results
            if results.get("tables"):
                print(f"\nüìä Tables ({len(results['tables'])} results):")
                for i, table in enumerate(results["tables"][:3], 1):  # Show first 3
                    table_name = table.get('content', {}).get('name', 'Unknown')
                    similarity = table.get('score', 0)
                    print(f"  {i}. {table_name} (similarity: {similarity:.3f})")
            
            if results.get("relationships"):
                print(f"\nüîó Relationships ({len(results['relationships'])} results):")
                for i, rel in enumerate(results["relationships"][:3], 1):  # Show first 3
                    rel_name = rel.get('content', {}).get('name', 'Unknown')
                    similarity = rel.get('score', 0)
                    print(f"  {i}. {rel_name} (similarity: {similarity:.3f})")
        else:
            print("‚ùå Search returned no results")
            
    except Exception as e:
        print(f"‚ùå Vector indexing demo failed: {e}")
else:
    print("‚ö†Ô∏è  Vector indexing is not available")
    print("This demo requires vector indexing to be enabled.")

"""
## 7. Entity Recognition Demo
"""

# Entity recognition demo
print("üîç Entity Recognition Demo")
print("=" * 50)

if 'agent' in locals() and agent.vector_indexing_available:
    try:
        # Import entity recognition agent
        from src.agents.entity_recognition import EntityRecognitionAgent
        
        # Initialize entity recognition agent
        entity_agent = EntityRecognitionAgent(agent.indexer_agent)
        print("‚úÖ Entity recognition agent initialized")
        
        # Sample queries for entity recognition
        sample_queries = [
            "customers information"
        ]
        
        print("\nüîç Available sample queries:")
        for i, query in enumerate(sample_queries, 1):
            print(f"  {i}. '{query}'")
        
        print("\nüí° Running entity recognition for all sample queries...\n")
        
        for idx, demo_query in enumerate(sample_queries, 1):
            print("=" * 50)
            print(f"üéØ Entity Recognition {idx}: '{demo_query}'")
            try:
                results = entity_agent.recognize_entities(demo_query)
                
                if results and results.get("success"):
                    confidence = results.get("confidence", 0.0)
                    analysis = results.get("analysis", "No analysis available")
                    applicable_entities = results.get("applicable_entities", [])
                    
                    print(f"‚úÖ Success! Confidence Score: {confidence:.2f}")
                    print(f"Analysis: {analysis}")
                    
                    if applicable_entities:
                        print(f"\nüìä Applicable Entities ({len(applicable_entities)} found):")
                        for i, entity in enumerate(applicable_entities, 1):
                            table_name = entity.get("table_name", "Unknown")
                            relevance_score = entity.get("relevance_score", 0.0)
                            business_purpose = entity.get("business_purpose", "")
                            recommendation = entity.get("recommendation", "")
                            
                            print(f"\n  {i}. {table_name}")
                            print(f"     Relevance: {relevance_score:.3f}")
                            print(f"     Purpose: {business_purpose}")
                            if recommendation:
                                print(f"     Recommendation: {recommendation}")
                    else:
                        print("\nüìä No applicable entities found.")
                        
                    # Show recommendations if available
                    recommendations = results.get("recommendations", [])
                    if recommendations:
                        print(f"\nüí° Entity Recommendations ({len(recommendations)} items):")
                        for i, rec in enumerate(recommendations, 1):
                            table_name = rec.get("table_name", "Unknown")
                            relevance_score = rec.get("relevance_score", 0.0)
                            business_purpose = rec.get("business_purpose", "")
                            print(f"  {i}. {table_name} (relevance: {relevance_score:.3f})")
                            if business_purpose:
                                print(f"     Purpose: {business_purpose}")
                else:
                    error = results.get("error", "Unknown error") if results else "No results returned"
                    print(f"‚ùå Entity recognition failed: {error}")
                    if results and results.get("details"):
                        print(f"Details: {results['details']}")
                        
            except Exception as e:
                print(f"‚ùå Entity recognition failed: {e}")
            print("=" * 50 + "\n")
            
        # Quick entity lookup demo
        print("\n" + "=" * 60)
        print("üöÄ Quick Entity Lookup Demo")
        print("=" * 60)
        
        quick_query = "customer information"
        print(f"Query: '{quick_query}'")
        
        try:
            table_names = entity_agent.quick_entity_lookup(quick_query, threshold=0.3)
            
            if table_names:
                print(f"‚úÖ Found {len(table_names)} relevant tables:")
                for i, table_name in enumerate(table_names, 1):
                    print(f"  {i}. {table_name}")
            else:
                print("‚ùå No relevant tables found above threshold.")
                
        except Exception as e:
            print(f"‚ùå Quick lookup failed: {e}")
            
    except Exception as e:
        print(f"‚ùå Entity recognition demo failed: {e}")
else:
    print("‚ö†Ô∏è  Entity recognition requires vector indexing")
    print("Enable vector indexing to use entity recognition features.")



================================================
FILE: LICENSE
================================================
MIT License

Copyright (c) 2025 Montray Davis

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.



================================================
FILE: seed.sql
================================================
CREATE TABLE branches (
    branch_id INT PRIMARY KEY AUTO_INCREMENT,
    branch_name VARCHAR(255) NOT NULL,
    address VARCHAR(255) NOT NULL,
    city VARCHAR(100) NOT NULL,
    state VARCHAR(50) NOT NULL,
    zip_code VARCHAR(20) NOT NULL,
    phone_number VARCHAR(20)
);

INSERT INTO branches (branch_name, address, city, state, zip_code, phone_number) VALUES
('Downtown Central', '123 Main St', 'Metropolis', 'TX', '78701', '512-555-0101'),
('Northside Financial', '456 Oak Ave', 'Metropolis', 'TX', '78758', '512-555-0102'),
('West Lake Hills', '789 Pine Ln', 'West Lake', 'TX', '78746', '512-555-0103');

CREATE TABLE employees (
    employee_id INT PRIMARY KEY AUTO_INCREMENT,
    branch_id INT,
    first_name VARCHAR(100) NOT NULL,
    last_name VARCHAR(100) NOT NULL,
    position VARCHAR(100) NOT NULL,
    hire_date DATE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    FOREIGN KEY (branch_id) REFERENCES branches(branch_id)
);

INSERT INTO employees (branch_id, first_name, last_name, position, hire_date, email) VALUES
(1, 'Alice', 'Johnson', 'Branch Manager', '2018-05-20', 'alice.j@fakebank.com'),
(1, 'Bob', 'Williams', 'Teller', '2022-08-15', 'bob.w@fakebank.com'),
(2, 'Charlie', 'Brown', 'Loan Officer', '2020-02-10', 'charlie.b@fakebank.com'),
(3, 'Diana', 'Miller', 'Teller', '2023-01-30', 'diana.m@fakebank.com');

CREATE TABLE customers (
    customer_id INT PRIMARY KEY AUTO_INCREMENT,
    first_name VARCHAR(100) NOT NULL,
    last_name VARCHAR(100) NOT NULL,
    date_of_birth DATE NOT NULL,
    address VARCHAR(255) NOT NULL,
    city VARCHAR(100) NOT NULL,
    state VARCHAR(50) NOT NULL,
    zip_code VARCHAR(20) NOT NULL,
    phone_number VARCHAR(20),
    email VARCHAR(255) UNIQUE NOT NULL,
    join_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

INSERT INTO customers (first_name, last_name, date_of_birth, address, city, state, zip_code, phone_number, email) VALUES
('John', 'Smith', '1985-04-12', '101 Maple Dr', 'Metropolis', 'TX', '78704', '512-555-1122', 'john.smith@email.com'),
('Jane', 'Doe', '1992-08-22', '202 Birch Rd', 'Metropolis', 'TX', '78759', '512-555-3344', 'jane.doe@email.com'),
('Peter', 'Jones', '1978-11-30', '303 Cedar Blvd', 'West Lake', 'TX', '78746', '512-555-5566', 'peter.jones@email.com');

CREATE TABLE accounts (
    account_id INT PRIMARY KEY AUTO_INCREMENT,
    customer_id INT,
    branch_id INT,
    account_type ENUM('Checking', 'Savings', 'Money Market', 'Certificate of Deposit') NOT NULL,
    account_number VARCHAR(20) UNIQUE NOT NULL,
    balance DECIMAL(15, 2) NOT NULL DEFAULT 0.00,
    open_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    status ENUM('Active', 'Closed', 'Frozen') NOT NULL DEFAULT 'Active',
    FOREIGN KEY (customer_id) REFERENCES customers(customer_id),
    FOREIGN KEY (branch_id) REFERENCES branches(branch_id)
);

INSERT INTO accounts (customer_id, branch_id, account_type, account_number, balance, status) VALUES
(1, 1, 'Checking', 'CHK000123456', 5450.75, 'Active'),
(1, 1, 'Savings', 'SAV000123457', 25300.00, 'Active'),
(2, 2, 'Checking', 'CHK000789012', 8210.50, 'Active'),
(3, 3, 'Money Market', 'MM000345678', 150250.20, 'Active');

CREATE TABLE transactions (
    transaction_id INT PRIMARY KEY AUTO_INCREMENT,
    account_id INT,
    transaction_type ENUM('Deposit', 'Withdrawal', 'Transfer', 'Fee', 'Interest') NOT NULL,
    amount DECIMAL(15, 2) NOT NULL,
    transaction_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    description VARCHAR(255),
    FOREIGN KEY (account_id) REFERENCES accounts(account_id)
);

INSERT INTO transactions (account_id, transaction_type, amount, transaction_date, description) VALUES
(1, 'Deposit', 1200.00, '2025-07-20 10:00:00', 'Paycheck'),
(2, 'Deposit', 5000.00, '2025-07-21 11:30:00', 'Initial Deposit'),
(1, 'Withdrawal', 50.00, '2025-07-22 15:45:00', 'ATM Withdrawal'),
(3, 'Withdrawal', 2500.00, '2025-07-23 09:15:00', 'Online Shopping'),
(1, 'Transfer', -200.00, '2025-07-25 14:00:00', 'Transfer to account SAV000123457'),
(2, 'Transfer', 200.00, '2025-07-25 14:00:00', 'Transfer from account CHK000123456'),
(4, 'Interest', 150.25, '2025-07-28 01:00:00', 'Monthly Interest Earned');

CREATE TABLE loans (
    loan_id INT PRIMARY KEY AUTO_INCREMENT,
    customer_id INT,
    loan_type ENUM('Mortgage', 'Auto', 'Personal') NOT NULL,
    principal_amount DECIMAL(15, 2) NOT NULL,
    interest_rate DECIMAL(5, 4) NOT NULL,
    term_months INT NOT NULL,
    start_date DATE NOT NULL,
    status ENUM('Active', 'Paid Off', 'Default') NOT NULL DEFAULT 'Active',
    FOREIGN KEY (customer_id) REFERENCES customers(customer_id)
);

INSERT INTO loans (customer_id, loan_type, principal_amount, interest_rate, term_months, start_date, status) VALUES
(1, 'Auto', 25000.00, 0.0450, 60, '2023-06-15', 'Active'),
(3, 'Mortgage', 450000.00, 0.0625, 360, '2024-02-01', 'Active');

CREATE TABLE cards (
    card_id INT PRIMARY KEY AUTO_INCREMENT,
    account_id INT,
    card_number VARCHAR(20) UNIQUE NOT NULL,
    card_type ENUM('Debit', 'Credit') NOT NULL,
    expiration_date VARCHAR(5) NOT NULL,
    cvv VARCHAR(4) NOT NULL,
    status ENUM('Active', 'Lost', 'Stolen', 'Expired') NOT NULL DEFAULT 'Active',
    FOREIGN KEY (account_id) REFERENCES accounts(account_id)
);

INSERT INTO cards (account_id, card_number, card_type, expiration_date, cvv) VALUES
(1, '4111222233334444', 'Debit', '08/28', '123'),
(3, '377788889999000', 'Credit', '11/27', '4567');


================================================
FILE: docs/README.md
================================================
# smol-sql-agents Documentation

Welcome to the official documentation for smol-sql-agents! This documentation provides comprehensive guides and references for using and developing with the smol-sql-agents framework.

## üìö Table of Contents

1. [Getting Started](./getting_started/README.md)
   - [Installation](./getting_started/installation.md)
   - [Quick Start](./getting_started/quick_start.md)
   - [Configuration](./getting_started/configuration.md)

2. [Core Concepts](./concepts/README.md)
   - [Architecture Overview](./concepts/architecture.md)
   - [Agents](./concepts/agents.md)
   - [Vector Database](./concepts/vector_database.md)
   - [Prompt Engineering](./concepts/prompt_engineering.md)

3. [Guides](./guides/README.md)
   - [Creating Custom Agents](./guides/custom_agents.md)
   - [Working with Databases](./guides/database_operations.md)
   - [Performance Optimization](./guides/performance.md)
   - [Testing](./guides/testing.md)

4. [API Reference](./api/README.md)
   - [Agents](./api/agents/README.md)
   - [Database](./api/database/README.md)
   - [Vector](./api/vector/README.md)
   - [Utils](./api/utils/README.md)

5. [Contributing](./contributing/README.md)
   - [Development Setup](./contributing/development.md)
   - [Code Style](./contributing/code_style.md)
   - [Pull Request Process](./contributing/pull_requests.md)

6. [Troubleshooting](./troubleshooting/README.md)
   - [Common Issues](./troubleshooting/common_issues.md)
   - [Debugging](./troubleshooting/debugging.md)
   - [Performance Tuning](./troubleshooting/performance_tuning.md)

## üîç Quick Navigation

- [Source Code Structure](./source_code_structure.md)
- [Changelog](../CHANGELOG.md)
- [License](../LICENSE)

## üí° Need Help?

If you need assistance or have questions, please [open an issue](https://github.com/yourusername/smol-sql-agents/issues) on our GitHub repository.



================================================
FILE: docs/source_code_structure.md
================================================
# Source Code Structure

This document provides an overview of the source code organization and key components of the smol-sql-agents project.

## üìÅ Project Structure

```
src/
‚îú‚îÄ‚îÄ __init__.py         # Package initialization
‚îú‚îÄ‚îÄ agents/            # Core agent implementations
‚îú‚îÄ‚îÄ database/          # Database interaction layer
‚îú‚îÄ‚îÄ output/            # Output formatters and handlers
‚îú‚îÄ‚îÄ prompts/           # Prompt templates and management
‚îú‚îÄ‚îÄ utils/             # Utility functions and helpers
‚îú‚îÄ‚îÄ validation/        # Data validation logic
‚îî‚îÄ‚îÄ vector/            # Vector operations and storage
```

## üìÇ utils/ Directory

The `utils` directory contains reusable utility functions and helper classes used throughout the application.

### Key Components

- **config_loader.py**: Loads and validates configuration from environment variables
- **logger.py**: Centralized logging configuration and utilities
- **file_helpers.py**: File I/O operations and path utilities
- **async_helpers.py**: Asynchronous operation utilities and decorators
- **string_utils.py**: String manipulation and formatting helpers

### Example Usage

```python
from src.utils.logger import get_logger
from src.utils.config_loader import get_config

logger = get_logger(__name__)
config = get_config()
```

## üìÇ validation/ Directory

The `validation` directory contains data validation schemas and validation logic.

### Key Components

- **schemas/**: Pydantic models for request/response validation
  - `database.py`: Database connection and query validation
  - `agent.py`: Agent configuration and execution validation
  - `vector.py`: Vector operation validation
- **validators/**: Custom validation functions
  - `sql_validator.py`: SQL syntax and safety validation
  - `data_validator.py`: Data type and format validation

### Example Usage

```python
from src.validation.schemas.database import DatabaseConnection
from src.validation.validators.sql_validator import validate_sql_query

# Validate database connection
connection = DatabaseConnection(
    host="localhost",
    port=5432,
    database="mydb",
    username="user"
)

# Validate SQL query
safe_query = validate_sql_query("SELECT * FROM users WHERE id = %s")
```

## üìÇ prompts/ Directory

The `prompts` directory contains all prompt templates and prompt management logic.

### Key Components

- **templates/**: Prompt templates for different agent types
  - `sql_generation.txt`: Templates for SQL generation tasks
  - `query_understanding.txt`: Templates for natural language understanding
  - `result_explanation.txt`: Templates for explaining query results
- **prompt_manager.py**: Manages loading and rendering of prompt templates
- **prompt_utils.py**: Helper functions for prompt construction

### Example Prompt Template (`prompts/templates/sql_generation.txt`)

```markdown$s$
Given the following database schema:

{schema}

Generate a SQL query to: {user_query}

Return only the SQL query without any additional text.
```

### Example Usage

```python
from src.prompts.prompt_manager import PromptManager

pm = PromptManager()
prompt = pm.get_prompt(
    "sql_generation",
    schema=schema_info,
    user_query="Find all active users"
)
```

## üèóÔ∏è Package Initialization

The `__init__.py` files are used to define Python packages and their public API.

### Root `__init__.py`

```python
# Core exports
from .agents import (
    BaseAgent,
    NL2SQLAgent,
    BusinessAgent,
    CoreAgent,
    EntityRecognitionAgent
)

# Database components
from .database import DatabaseInspector, DocumentationStore

# Vector operations
from .vector import VectorStore, EmbeddingsClient

__version__ = "0.1.0"
__all__ = [
    # Agents
    'BaseAgent',
    'NL2SQLAgent',
    'BusinessAgent',
    'CoreAgent',
    'EntityRecognitionAgent',
    # Database
    'DatabaseInspector',
    'DocumentationStore',
    # Vector
    'VectorStore',
    'EmbeddingsClient'
]
```

## üîÑ Module Initialization

Each subpackage (agents, database, etc.) contains its own `__init__.py` that follows a similar pattern:

```python
# agents/__init__.py
from .base_agent import BaseAgent
from .nl2sql_agent import NL2SQLAgent
from .business_agent import BusinessAgent

__all__ = [
    'BaseAgent',
    'NL2SQLAgent',
    'BusinessAgent'
]
```

## üîç Code Organization Principles

1. **Single Responsibility**: Each module and class has a single responsibility
2. **Separation of Concerns**: Clear boundaries between different components
3. **Dependency Injection**: Dependencies are injected where possible for testability
4. **Type Hints**: Extensive use of Python type hints for better IDE support
5. **Documentation**: All public APIs include docstrings following Google style

## üõ†Ô∏è Development Guidelines

- Add new utility functions to the appropriate module in `utils/`
- Keep validation logic in the `validation/` directory
- Store all prompt templates in the `prompts/` directory
- Update `__init__.py` files to expose new public APIs
- Maintain backward compatibility when modifying existing code

## üìö Related Documentation

- [Database Components](./database/DatabaseInspector.md)
- [Agent Architecture](./agents/BaseAgent.md)
- [Vector Operations](./vector/VectorStore.md)



================================================
FILE: docs/agents/BaseAgent.md
================================================
# Base Agent

The Base Agent provides the foundational class and mixins that all other agents inherit from in the SQL Documentation suite. It implements common functionality, dependency injection, tool management, and shared component integration to eliminate code duplication and ensure consistent agent behavior.

## üéØ What It Does

The Base Agent provides the foundation for all agent implementations:

- **Common Initialization**: Standardized agent setup with LLM models and tools
- **Dependency Injection**: Unified database tools and shared component integration
- **Tool Management**: Automatic tool validation and integration
- **Mixins Support**: Caching and validation mixins for enhanced functionality
- **Error Handling**: Comprehensive error handling and validation
- **Resource Management**: Efficient resource allocation and cleanup

## üîÑ Agent Architecture

```markdown
BaseAgent (ABC) ‚Üí Specific Agent Classes
‚îú‚îÄ‚îÄ CachingMixin ‚Üí Performance optimization
‚îú‚îÄ‚îÄ ValidationMixin ‚Üí Data validation
‚îî‚îÄ‚îÄ Shared Components ‚Üí LLM, Database Tools
```

1. **Abstract Base Class**: Defines interface and common functionality
2. **Mixin Classes**: Provide optional functionality (caching, validation)
3. **Shared Components**: LLM models, database tools, and other resources
4. **Tool Integration**: Automatic tool validation and setup
5. **Error Handling**: Comprehensive error management and recovery

## üöÄ Usage Examples

### Command Line Interface

```bash
# Base agent is not used directly from command line
# It provides foundation for all other agents

# All agents inherit from BaseAgent
python main.py --core-agent          # Uses BaseAgent foundation
python main.py --indexer-agent       # Uses BaseAgent foundation
python main.py --entity-agent        # Uses BaseAgent foundation
```

### Programmatic Usage

```python
from src.agents.base import BaseAgent, CachingMixin, ValidationMixin

# Create custom agent inheriting from BaseAgent
class CustomAgent(BaseAgent):
    def _setup_agent_components(self):
        """Setup agent-specific components."""
        self.custom_component = CustomComponent()
    
    def _setup_tools(self):
        """Setup agent tools."""
        @tool
        def custom_tool(param: str) -> Dict:
            return {"result": f"Processed: {param}"}
        
        self.tools = [custom_tool]

# Create agent with mixins
class AdvancedAgent(BaseAgent, CachingMixin, ValidationMixin):
    def __init__(self):
        CachingMixin.__init__(self, cache_size=100)
        ValidationMixin.__init__(self)
        super().__init__(
            shared_llm_model=None,
            additional_imports=['json', 'yaml'],
            agent_name="Advanced Agent"
        )
```

## üìä Class Structure

### BaseAgent Class

```python
class BaseAgent(ABC):
    def __init__(self, shared_llm_model=None, additional_imports=None, 
                 agent_name="Base Agent", database_tools=None):
        # Common initialization
        self.agent_name = agent_name
        self.database_tools = database_tools
        
        # LLM model setup
        if shared_llm_model:
            self.llm_model = shared_llm_model
        else:
            self._initialize_llm_model()
        
        # Agent-specific setup
        self._setup_agent_components()
        self._setup_tools()
        
        # Tool integration and validation
        self._integrate_database_tools()
        self._validate_tools()
        
        # CodeAgent initialization
        self.agent = CodeAgent(
            model=self.llm_model,
            tools=self.tools,
            additional_authorized_imports=additional_imports or []
        )
```

### CachingMixin

```python
class CachingMixin:
    def __init__(self, cache_size: int = 50):
        self._cache = {}
        self._cache_size = cache_size
    
    def _get_cache_key(self, key_string: str) -> str:
        """Generate MD5 cache key."""
        import hashlib
        return hashlib.md5(key_string.lower().strip().encode()).hexdigest()
    
    def _get_cached_result(self, cache_key: str) -> Optional[Dict]:
        """Get cached result if available."""
        return self._cache.get(cache_key)
    
    def _cache_result(self, cache_key: str, result: Dict):
        """Cache result with size management."""
        self._cache[cache_key] = result
        if len(self._cache) > self._cache_size:
            # Remove oldest entries
            oldest_keys = list(self._cache.keys())[:10]
            for key in oldest_keys:
                del self._cache[key]
```

### ValidationMixin

```python
class ValidationMixin:
    def __init__(self):
        self.validators = {}
    
    def add_validator(self, name: str, validator):
        """Add a validator function."""
        self.validators[name] = validator
    
    def validate(self, data: Any, validator_name: str) -> bool:
        """Validate data using specified validator."""
        if validator_name not in self.validators:
            return True  # Default to valid if no validator
        
        try:
            return self.validators[validator_name](data)
        except Exception as e:
            logger.error(f"Validation error for '{validator_name}': {e}")
            return False
```

## üß† Core Algorithm

The Base Agent implements sophisticated initialization and management:

### Initialization Flow
```python
def __init__(self, shared_llm_model=None, additional_imports=None, 
             agent_name="Base Agent", database_tools=None):
    # 1. Store basic configuration
    self.agent_name = agent_name
    self.database_tools = database_tools
    
    # 2. Initialize LLM model (shared or new)
    if shared_llm_model:
        self.llm_model = shared_llm_model
    else:
        self._initialize_llm_model()
    
    # 3. Setup agent-specific components
    self._setup_agent_components()
    
    # 4. Setup tools
    self.tools = []
    self._setup_tools()
    
    # 5. Integrate database tools
    if self.database_tools:
        self._integrate_database_tools()
    
    # 6. Validate tools
    self._validate_tools()
    
    # 7. Initialize CodeAgent
    self.agent = CodeAgent(
        model=self.llm_model,
        tools=self.tools,
        additional_authorized_imports=additional_imports or []
    )
```

### Tool Integration
```python
def _integrate_database_tools(self):
    """Integrate unified database tools into agent tools list."""
    if hasattr(self.database_tools, 'create_tools'):
        try:
            database_tools_list = self.database_tools.create_tools()
            self.tools.extend(database_tools_list)
            logger.info(f"Integrated {len(database_tools_list)} database tools")
        except Exception as e:
            logger.error(f"Failed to integrate database tools: {e}")
```

### Tool Validation
```python
def _validate_tools(self):
    """Validate that all tools are properly decorated."""
    from smolagents.tools import Tool
    
    for i, tool_func in enumerate(self.tools):
        if not isinstance(tool_func, Tool):
            logger.error(f"Tool at index {i} is not properly decorated")
            raise ValueError(f"All tools must be instances of Tool")
```

## ‚öôÔ∏è Configuration

### Environment Variables

```env
# Required for base agent
OPENAI_API_KEY="your-api-key-here"

# Optional: Agent settings
AGENT_CACHE_SIZE="50"
AGENT_VALIDATION_TIMEOUT="30"

# Optional: Logging configuration
LOG_LEVEL="INFO"
LOG_FILE="base_agent.log"
```

### Initialization Parameters

```python
BaseAgent(
    shared_llm_model=None,          # Optional: Shared LLM model
    additional_imports=None,         # Optional: Additional imports for CodeAgent
    agent_name="Base Agent",        # Optional: Agent name for logging
    database_tools=None              # Optional: Database tools instance
)

# Mixin initialization
CachingMixin(cache_size=50)         # Optional: Cache size
ValidationMixin()                    # No parameters required
```

## üéØ Use Cases

### 1. Custom Agent Creation

Create custom agents inheriting from BaseAgent:

```python
class CustomDocumentationAgent(BaseAgent):
    def _setup_agent_components(self):
        """Setup custom components."""
        self.documentation_store = DocumentationStore()
        self.vector_store = SQLVectorStore()
    
    def _setup_tools(self):
        """Setup custom tools."""
        @tool
        def process_documentation(table_name: str) -> Dict:
            return {"success": True, "table": table_name}
        
        self.tools = [process_documentation]

# Use custom agent
custom_agent = CustomDocumentationAgent(
    shared_llm_model=shared_model,
    agent_name="Custom Documentation Agent"
)
```

### 2. Agent with Mixins

Create agents with caching and validation:

```python
class AdvancedAnalysisAgent(BaseAgent, CachingMixin, ValidationMixin):
    def __init__(self):
        # Initialize mixins
        CachingMixin.__init__(self, cache_size=100)
        ValidationMixin.__init__(self)
        
        # Initialize base agent
        super().__init__(
            shared_llm_model=None,
            additional_imports=['pandas', 'numpy'],
            agent_name="Advanced Analysis Agent"
        )
        
        # Add validators
        self.add_validator("syntax", self._validate_syntax)
        self.add_validator("security", self._validate_security)
    
    def _setup_agent_components(self):
        """Setup analysis components."""
        self.analyzer = DataAnalyzer()
    
    def _setup_tools(self):
        """Setup analysis tools."""
        @tool
        def analyze_data(query: str) -> Dict:
            # Use caching
            cache_key = self._get_cache_key(query)
            cached_result = self._get_cached_result(cache_key)
            if cached_result:
                return cached_result
            
            # Perform analysis
            result = self.analyzer.analyze(query)
            
            # Cache result
            self._cache_result(cache_key, result)
            return result
```

### 3. Shared Component Usage

Use shared components across agents:

```python
# Create shared components
shared_llm = OpenAIModel(model_id="gpt-4o-mini", api_key=api_key)
shared_database_tools = DatabaseToolsFactory.create_database_tools(inspector)

# Create agents with shared components
agent1 = CustomAgent(
    shared_llm_model=shared_llm,
    database_tools=shared_database_tools
)

agent2 = AnotherAgent(
    shared_llm_model=shared_llm,
    database_tools=shared_database_tools
)

# Verify shared components
assert agent1.llm_model is agent2.llm_model
assert agent1.database_tools is agent2.database_tools
```

### 4. Tool Integration

Integrate database tools automatically:

```python
class DatabaseAgent(BaseAgent):
    def _setup_agent_components(self):
        """Setup database components."""
        self.query_optimizer = QueryOptimizer()
    
    def _setup_tools(self):
        """Setup database tools."""
        @tool
        def optimize_query(sql: str) -> Dict:
            return {"optimized_sql": self.query_optimizer.optimize(sql)}
        
        self.tools = [optimize_query]

# Database tools are automatically integrated
db_agent = DatabaseAgent(
    database_tools=unified_database_tools
)

# Check integrated tools
print(f"Total tools: {len(db_agent.tools)}")
print(f"Database tools: {[t for t in db_agent.tools if 'database' in str(t)]}")
```

## üîç Integration with Other Agents

The Base Agent provides foundation for all other agents:

- **All Agents**: Inherit from BaseAgent for common functionality
- **LLM Models**: Shared OpenAI model instances
- **Database Tools**: Unified database tool integration
- **Tool Management**: Automatic tool validation and setup
- **Error Handling**: Comprehensive error management

## üéñÔ∏è Advanced Features

### Intelligent Resource Management

- Shared LLM model instances across agents
- Automatic database tool integration
- Memory-efficient component reuse
- Automatic resource cleanup

### Tool Validation and Integration

- Automatic tool decoration validation
- Database tool integration
- Tool list management
- Import authorization

### Mixin Support

- Caching functionality for performance
- Validation framework for data integrity
- Extensible mixin architecture
- Configurable mixin behavior

### Error Handling

- Comprehensive error logging
- Tool validation error handling
- Resource initialization error recovery
- Graceful degradation mechanisms

## üìà Performance Characteristics

- **Agent Initialization**: 0.1-0.3 seconds for new agent instances
- **Tool Integration**: Sub-second for database tool integration
- **Memory Usage**: Efficient shared component management
- **Tool Validation**: Fast tool decoration validation
- **Scalability**: Supports hundreds of agent instances
- **Resource Efficiency**: 60-80% reduction through component sharing

## üö¶ Prerequisites

1. **Environment Configuration**: Valid OpenAI API key
2. **Agent Dependencies**: All agent classes must be importable
3. **Tool Dependencies**: Properly decorated tool functions
4. **Database Tools**: Functional database tool implementations
5. **Dependencies**: All required packages from requirements.txt

## üîß Error Handling

### Common Error Scenarios

1. **Initialization Errors**: Missing API keys, invalid configurations
2. **Tool Validation Errors**: Improperly decorated tools
3. **Database Integration Errors**: Missing database tool methods
4. **Resource Errors**: Memory or file system issues

### Error Response Handling

```python
# Handle base agent errors
try:
    agent = CustomAgent(
        shared_llm_model=shared_model,
        database_tools=database_tools
    )
except ValueError as e:
    print(f"Configuration error: {e}")
    # Check environment variables and configuration
except ImportError as e:
    print(f"Import error: {e}")
    # Check dependencies
except Exception as e:
    print(f"Base agent error: {e}")
    # Implement fallback mechanisms
```

### Recovery Mechanisms

1. **Configuration Fallback**: Use default configurations when custom configs fail
2. **Tool Fallback**: Skip tool integration when database tools fail
3. **Resource Fallback**: Use minimal resources when shared components fail
4. **Validation Bypass**: Continue with warnings when validation fails

## üîÑ Agent Lifecycle

### Initialization Phase
```python
# Agent is created with shared components
agent = CustomAgent(
    shared_llm_model=shared_model,
    database_tools=database_tools
)

# Components are validated and integrated
assert agent.llm_model is shared_model
assert agent.database_tools is database_tools
assert len(agent.tools) > 0
```

### Usage Phase
```python
# Agent is used for processing
result = agent.agent.run("Process this query")

# Tools are automatically available
# Database tools are integrated
# Caching and validation work automatically
```

### Cleanup Phase
```python
# Agent cleanup (automatic in most cases)
# Shared components are managed by factory
# Memory is automatically cleaned up
```

---

The Base Agent provides the essential foundation for all agent implementations in the SQL Documentation suite, offering standardized initialization, tool management, shared component integration, and comprehensive error handling to ensure consistent, reliable, and efficient agent behavior across the entire system. 


================================================
FILE: docs/agents/BatchManager.md
================================================
# Batch Manager

The Batch Manager is an optimization component that provides efficient batch processing for OpenAI embeddings generation. It works with the Indexer Agent to process multiple documents simultaneously, reducing API calls and costs while maintaining high throughput for large-scale database documentation.

## üéØ What It Does

The Batch Manager optimizes vector indexing operations for SQL documentation:

- **Efficient Batch Processing**: Groups multiple documents into optimal batch sizes for OpenAI API calls
- **Cost Estimation**: Provides accurate cost estimates before processing large datasets
- **Progress Tracking**: Monitors processing progress with detailed statistics and logging
- **Error Recovery**: Handles failures gracefully with comprehensive error reporting
- **Resource Optimization**: Minimizes OpenAI API usage through intelligent batching strategies
- **Scalable Processing**: Handles databases with thousands of tables and relationships efficiently

## üîÑ Processing Flow

```markdown
Documentation Store ‚Üí Batch Manager ‚Üí Grouped Batches ‚Üí Indexer Agent ‚Üí OpenAI API ‚Üí Vector Storage
```

1. **Data Collection**: Retrieves pending tables and relationships from documentation store
2. **Batch Grouping**: Organizes items into optimal batch sizes for API efficiency
3. **Cost Estimation**: Calculates expected OpenAI API costs before processing
4. **Batch Processing**: Processes multiple documents per API call using the Indexer Agent
5. **Progress Monitoring**: Tracks completion status and success rates across batches
6. **Result Aggregation**: Combines individual processing results into comprehensive reports

## üöÄ Usage Examples

### Command Line Interface

```bash
# Use batch processing for documentation generation (default)
python main.py --batch-index

# Estimate costs before processing
python main.py --estimate-costs

# Individual processing (non-batched)
python main.py --individual-index

# Rebuild indexes using batch processing
python main.py --rebuild-indexes
```

### Programmatic Usage

```python
from src.agents.batch_manager import BatchIndexingManager
from src.agents.indexer import SQLIndexerAgent
from src.vector.store import SQLVectorStore
from src.database.persistence import DocumentationStore

# Initialize components
vector_store = SQLVectorStore()
indexer_agent = SQLIndexerAgent(vector_store)
batch_manager = BatchIndexingManager(indexer_agent)
doc_store = DocumentationStore()

# Get processing statistics
stats = batch_manager.get_processing_stats(doc_store)
print(f"Pending items: {stats['total_pending']}")
print(f"Estimated cost: ${stats['total_estimated_cost']:.6f}")

# Process tables in batches
table_results = batch_manager.batch_process_pending_tables(doc_store)
successful_tables = sum(1 for success in table_results.values() if success)
print(f"Processed {successful_tables}/{len(table_results)} tables successfully")

# Process relationships in batches
rel_results = batch_manager.batch_process_pending_relationships(doc_store)
successful_rels = sum(1 for success in rel_results.values() if success)
print(f"Processed {successful_rels}/{len(rel_results)} relationships successfully")

# Estimate costs for custom text list
texts = ["Table documentation text", "Relationship documentation text"]
cost_estimate = batch_manager.estimate_embedding_costs(texts)
print(f"Estimated cost for {len(texts)} texts: ${cost_estimate['estimated_cost_usd']:.6f}")
```

## üìä Response Structure

### Processing Statistics Response

```json
{
  "pending_tables": 150,
  "pending_relationships": 75,
  "total_pending": 225,
  "estimated_table_cost": 0.000180,
  "estimated_relationship_cost": 0.000090,
  "total_estimated_cost": 0.000270,
  "batch_size": 100,
  "estimated_batches": 3
}
```

### Batch Processing Results

```json
{
  "table_processing_results": {
    "users": true,
    "orders": true,
    "products": false,
    "categories": true
  },
  "relationship_processing_results": {
    "users_orders_fk": true,
    "orders_products_fk": true,
    "products_categories_fk": false
  },
  "summary": {
    "total_tables_processed": 4,
    "successful_tables": 3,
    "total_relationships_processed": 3,
    "successful_relationships": 2,
    "overall_success_rate": 0.714
  }
}
```

### Cost Estimation Response

```json
{
  "total_texts": 50,
  "estimated_tokens": 12500,
  "estimated_cost_usd": 0.000250,
  "cost_per_text": 0.000005
}
```

## ‚öôÔ∏è Configuration

### Environment Variables

```env
# Batch processing settings
EMBEDDING_BATCH_SIZE="100"          # Documents per batch (default: 100)
EMBEDDING_MAX_RETRIES="3"           # Maximum retry attempts (default: 3)

# Required for underlying components
OPENAI_API_KEY="your-api-key-here"
DATABASE_URL="your-database-connection-string"

# Optional: Logging configuration
LOG_LEVEL="INFO"
LOG_FILE="batch_processing.log"
```

### Initialization Parameters

```python
BatchIndexingManager(
    indexer_agent                    # Required: SQLIndexerAgent instance
)

# Method parameters
batch_process_pending_tables(
    doc_store                       # Required: DocumentationStore instance
)

batch_process_pending_relationships(
    doc_store                       # Required: DocumentationStore instance  
)

get_processing_stats(
    doc_store                       # Required: DocumentationStore instance
)

estimate_embedding_costs(
    texts                          # Required: List of text strings
)
```

## üéØ Use Cases

### 1. Large Database Processing

Efficiently process databases with hundreds or thousands of tables:

```python
# Get statistics before processing
stats = batch_manager.get_processing_stats(doc_store)
print(f"Will process {stats['total_pending']} items in {stats['estimated_batches']} batches")
print(f"Estimated cost: ${stats['total_estimated_cost']:.6f}")

# Process in optimized batches
if stats['total_estimated_cost'] < 0.10:  # Cost threshold check
    table_results = batch_manager.batch_process_pending_tables(doc_store)
    rel_results = batch_manager.batch_process_pending_relationships(doc_store)
```

### 2. Cost-Conscious Processing

Estimate and control OpenAI API costs before processing:

```python
# Check costs before processing
stats = batch_manager.get_processing_stats(doc_store)
if stats['total_estimated_cost'] > 1.00:  # $1.00 threshold
    print(f"Warning: Estimated cost ${stats['total_estimated_cost']:.6f} exceeds threshold")
    user_confirm = input("Continue? (y/n): ")
    if user_confirm.lower() != 'y':
        return

# Proceed with batch processing
results = batch_manager.batch_process_pending_tables(doc_store)
```

### 3. Progress Monitoring

Monitor large-scale processing operations:

```python
import time

# Start processing
print("Starting batch processing...")
start_time = time.time()

table_results = batch_manager.batch_process_pending_tables(doc_store)
rel_results = batch_manager.batch_process_pending_relationships(doc_store)

# Calculate metrics
end_time = time.time()
processing_time = end_time - start_time
total_items = len(table_results) + len(rel_results)
successful_items = sum(table_results.values()) + sum(rel_results.values())

print(f"Processing completed in {processing_time:.2f} seconds")
print(f"Success rate: {successful_items}/{total_items} ({successful_items/total_items*100:.1f}%)")
```

### 4. Custom Batch Sizing

Optimize batch sizes for different scenarios:

```python
# Small batches for rate-limited scenarios
small_batch_manager = BatchIndexingManager(indexer_agent)
small_batch_manager.batch_size = 25

# Large batches for high-throughput scenarios  
large_batch_manager = BatchIndexingManager(indexer_agent)
large_batch_manager.batch_size = 200

# Process with appropriate batch size
results = small_batch_manager.batch_process_pending_tables(doc_store)
```

## üîç Integration with Other Components

The Batch Manager orchestrates several components for optimal processing:

- **Indexer Agent**: Provides the core indexing functionality for individual documents
- **Documentation Store**: Supplies pending items and stores processing metadata
- **Vector Store**: Receives the processed embeddings and document metadata
- **OpenAI API**: Handles the actual embedding generation with optimized batch calls
- **Logger**: Provides detailed progress tracking and error reporting

## üéñÔ∏è Advanced Features

### Intelligent Cost Estimation

- Token-based cost calculation using OpenAI pricing models
- Accurate estimation before processing large datasets
- Cost-per-item breakdown for detailed analysis
- Support for different embedding models and pricing tiers

### Adaptive Batch Processing

- Dynamic batch sizing based on content length and complexity
- Automatic retry logic with exponential backoff for transient failures
- Graceful error handling with partial batch recovery
- Progress tracking with detailed success/failure reporting

### Resource Optimization

- Memory-efficient processing of large document sets
- Optimized API usage to minimize costs and latency
- Configurable batch sizes for different scenarios
- Built-in rate limiting and throttling support

### Error Recovery and Reporting

- Comprehensive error logging with actionable details
- Graceful degradation when individual items fail
- Detailed success/failure statistics for monitoring
- Retry mechanisms for transient API failures

## üìà Performance Characteristics

- **Throughput**: 50-200 documents per minute depending on content size and batch configuration
- **Cost Efficiency**: 40-60% reduction in API calls compared to individual processing
- **Memory Usage**: Minimal memory footprint with streaming batch processing
- **Scalability**: Handles databases with 10,000+ tables efficiently
- **Error Recovery**: Continues processing after individual failures without stopping entire batches
- **API Optimization**: Intelligent batching reduces rate limiting issues

## üö¶ Prerequisites

1. **Indexer Agent**: Functional SQLIndexerAgent instance for document processing
2. **Documentation Store**: DocumentationStore with pending items to process
3. **OpenAI API Access**: Valid API key with sufficient credits for embedding generation
4. **Vector Storage**: Configured vector store (ChromaDB) for embedding persistence
5. **Dependencies**: All required packages from requirements.txt

## üîß Error Handling

### Common Error Scenarios

1. **API Rate Limiting**: Automatic retry with exponential backoff for rate limit errors
2. **Network Issues**: Connection timeout handling with retry mechanisms
3. **Invalid Documents**: Graceful handling of malformed document data
4. **Memory Constraints**: Efficient memory usage for large batch processing
5. **Partial Failures**: Continue processing remaining items when individual documents fail

### Error Response Handling

```python
# Handle batch processing errors
try:
    results = batch_manager.batch_process_pending_tables(doc_store)
    
    # Check for failures
    failed_tables = [table for table, success in results.items() if not success]
    if failed_tables:
        print(f"Failed to process tables: {failed_tables}")
        
        # Retry failed items individually
        for table in failed_tables:
            try:
                # Process individually for detailed error info
                success = indexer_agent.index_table_documentation(table_data)
                if success:
                    print(f"Retry successful for table: {table}")
            except Exception as e:
                print(f"Retry failed for table {table}: {e}")
                
except Exception as e:
    print(f"Batch processing failed: {e}")
    # Fall back to individual processing
    pending_tables = doc_store.get_pending_tables()
    for table in pending_tables:
        try:
            agent.process_table_documentation(table)
        except Exception as table_error:
            print(f"Individual processing failed for {table}: {table_error}")
```

### Retry Mechanisms

The Batch Manager includes sophisticated retry logic:

1. **API Rate Limits**: Exponential backoff with jitter for rate limit responses
2. **Network Failures**: Connection retry with timeout escalation
3. **Transient Errors**: Automatic retry for temporary API issues
4. **Batch Failures**: Automatic fallback to individual item processing
5. **Resource Constraints**: Memory and resource management for large datasets

---

The Batch Manager is essential for efficient large-scale processing in the SQL Documentation suite, providing cost-effective and scalable vector indexing capabilities while maintaining high reliability and comprehensive error handling.



================================================
FILE: docs/agents/BusinessAgent.md
================================================
# Business Agent

The Business Agent (BusinessContextAgent) is an intelligent component that gathers business context and domain knowledge for SQL generation. It uses concept matching, business rules, and domain expertise to provide rich business context that enhances the accuracy and relevance of generated SQL queries.

## üéØ What It Does

The Business Agent provides sophisticated business context gathering:

- **Concept Matching**: Matches user queries to business concepts and domain knowledge
- **Business Rules**: Applies business rules and constraints to SQL generation
- **Domain Expertise**: Leverages domain-specific knowledge for better query understanding
- **Join Validation**: Validates that required table joins can be satisfied
- **Example Retrieval**: Finds relevant examples and patterns for similar queries
- **Entity Coverage**: Analyzes how well business concepts cover identified entities

## üîÑ Processing Flow

```markdown
User Query ‚Üí Entity Analysis ‚Üí Concept Loading ‚Üí Concept Matching ‚Üí Business Rules ‚Üí Join Validation ‚Üí Context Assembly
```

1. **Entity Analysis**: Analyzes identified database entities for business relevance
2. **Concept Loading**: Loads business concepts and domain knowledge for entities
3. **Concept Matching**: Matches user query to relevant business concepts
4. **Business Rules**: Applies business rules and constraints
5. **Join Validation**: Validates that required table relationships can be satisfied
6. **Context Assembly**: Combines all business context into comprehensive response

## üöÄ Usage Examples

### Command Line Interface

```bash
# Gather business context for customer analytics
python main.py --business-context "customer analytics" --entities "customers,orders"

# Business context with custom concepts directory
python main.py --business-context "user behavior" --concepts-dir "custom/concepts"

# Business context for specific business domain
python main.py --business-context "sales performance" --domain "retail"
```

### Programmatic Usage

```python
from src.agents.business import BusinessContextAgent
from src.agents.factory import agent_factory

# Get business agent from factory
business_agent = agent_factory.get_business_agent()

# Gather business context
applicable_entities = ["customers", "orders", "products"]
business_context = business_agent.gather_business_context(
    user_query="customer retention analysis",
    applicable_entities=applicable_entities
)

# Check business context results
if business_context["success"]:
    print(f"Matched concepts: {business_context['matched_concepts']}")
    print(f"Business instructions: {business_context['business_instructions']}")
    print(f"Entity coverage: {business_context['entity_coverage']}")
else:
    print(f"Business context failed: {business_context['error']}")
```

## üìä Response Structure

### Successful Business Context Response

```json
{
  "success": true,
  "matched_concepts": [
    {
      "name": "customer_retention",
      "description": "Customer retention and loyalty analysis",
      "target_entities": ["customers", "orders"],
      "required_joins": ["customers.customer_id = orders.customer_id"],
      "similarity": 0.85
    },
    {
      "name": "customer_analytics",
      "description": "Customer behavior and analytics",
      "target_entities": ["customers", "orders", "products"],
      "required_joins": ["customers.customer_id = orders.customer_id", "orders.product_id = products.product_id"],
      "similarity": 0.72
    }
  ],
  "business_instructions": [
    {
      "concept": "customer_retention",
      "instructions": "Use customer_id for joins, filter by date ranges, calculate retention rates",
      "similarity": 0.85
    },
    {
      "concept": "customer_analytics",
      "instructions": "Include customer demographics, order patterns, and product preferences",
      "similarity": 0.72
    }
  ],
  "relevant_examples": [
    {
      "example": "SELECT customer_id, COUNT(DISTINCT order_date) as order_frequency FROM orders GROUP BY customer_id",
      "similarity": 0.78,
      "concept_name": "customer_retention"
    }
  ],
  "join_validation": {
    "customer_retention": {
      "valid": true,
      "satisfied_joins": ["customers.customer_id = orders.customer_id"],
      "unsatisfied_joins": []
    },
    "customer_analytics": {
      "valid": true,
      "satisfied_joins": ["customers.customer_id = orders.customer_id", "orders.product_id = products.product_id"],
      "unsatisfied_joins": []
    }
  },
  "entity_coverage": {
    "total_entities": 3,
    "entities_with_concepts": 3
  }
}
```

### Empty Business Context Response

```json
{
  "success": true,
  "matched_concepts": [],
  "business_instructions": [],
  "relevant_examples": [],
  "join_validation": {},
  "entity_coverage": {
    "total_entities": 0,
    "entities_with_concepts": 0
  }
}
```

### Error Response

```json
{
  "success": false,
  "error": "Failed to load business concepts",
  "matched_concepts": [],
  "business_instructions": [],
  "relevant_examples": [],
  "join_validation": {},
  "entity_coverage": {
    "total_entities": 0,
    "entities_with_concepts": 0
  }
}
```

## üß† Processing Algorithm

The Business Agent implements sophisticated concept matching and business rule application:

### Stage 1: Concept Loading
```python
def gather_business_context(self, user_query: str, applicable_entities: List[str]):
    # Load concepts for identified entities
    concepts = self.concept_loader.get_concepts_for_entities(applicable_entities)
    
    # Match concepts to user query
    matched_concepts = self.concept_matcher.match_concepts_to_query(user_query, concepts)
    
    # Extract business instructions and examples
    business_instructions = self._extract_business_instructions(matched_concepts)
    relevant_examples = self._get_relevant_examples(matched_concepts, user_query)
    
    # Validate joins
    join_validation = self._validate_required_joins(applicable_entities, matched_concepts)
    
    return self._format_business_context(matched_concepts, business_instructions, 
                                       relevant_examples, join_validation)
```

### Stage 2: Concept Matching
```python
def match_concepts_to_query(self, user_query: str, concepts: List[BusinessConcept]):
    # Use semantic similarity to match concepts
    matches = []
    for concept in concepts:
        similarity = self._calculate_concept_similarity(user_query, concept)
        if similarity > 0.5:  # Threshold for relevance
            matches.append((concept, similarity))
    
    # Sort by similarity score
    return sorted(matches, key=lambda x: x[1], reverse=True)
```

### Stage 3: Join Validation
```python
def _validate_required_joins(self, entities: List[str], matched_concepts: List[Tuple]):
    validation_results = {}
    
    for concept, similarity in matched_concepts:
        required_entities = self._extract_entities_from_joins(concept.required_joins)
        available_entities = set(entities)
        missing_entities = required_entities - available_entities
        
        validation_results[concept.name] = {
            "valid": len(missing_entities) == 0,
            "missing_entities": list(missing_entities),
            "satisfied_joins": [join for join in concept.required_joins 
                              if self._can_satisfy_join(join, available_entities)],
            "unsatisfied_joins": [join for join in concept.required_joins 
                                 if not self._can_satisfy_join(join, available_entities)]
        }
    
    return validation_results
```

## ‚öôÔ∏è Configuration

### Environment Variables

```env
# Required for business context
OPENAI_API_KEY="your-api-key-here"

# Optional: Business concepts directory
CONCEPTS_DIRECTORY="src/agents/concepts"

# Optional: Concept matching settings
CONCEPT_SIMILARITY_THRESHOLD="0.5"
MAX_CONCEPTS_PER_QUERY="5"

# Optional: Logging configuration
LOG_LEVEL="INFO"
LOG_FILE="business_agent.log"
```

### Initialization Parameters

```python
BusinessContextAgent(
    indexer_agent=None,              # Optional: SQLIndexerAgent for semantic search
    concepts_dir="src/agents/concepts",  # Optional: Concepts directory path
    shared_llm_model=None,          # Optional: Shared LLM model
    shared_concept_loader=None,      # Optional: Shared concept loader
    shared_concept_matcher=None,     # Optional: Shared concept matcher
    database_tools=None              # Optional: Database tools
)

# Method parameters
gather_business_context(
    user_query,                      # Required: Natural language query
    applicable_entities              # Required: List of entity names
)
```

## üéØ Use Cases

### 1. Customer Analytics Context

Gather business context for customer analysis:

```python
# Customer analytics query
business_context = business_agent.gather_business_context(
    user_query="customer behavior analysis",
    applicable_entities=["customers", "orders", "products"]
)

# Check matched concepts
for concept in business_context["matched_concepts"]:
    print(f"Concept: {concept['name']}")
    print(f"Description: {concept['description']}")
    print(f"Similarity: {concept['similarity']}")
    print(f"Required joins: {concept['required_joins']}")
```

### 2. Sales Performance Analysis

Business context for sales analysis:

```python
# Sales performance query
business_context = business_agent.gather_business_context(
    user_query="sales performance by region",
    applicable_entities=["sales", "regions", "products", "customers"]
)

# Apply business instructions
for instruction in business_context["business_instructions"]:
    print(f"Business rule: {instruction['instructions']}")
    print(f"Concept: {instruction['concept']}")
    print(f"Confidence: {instruction['similarity']}")
```

### 3. Join Validation

Validate that required joins can be satisfied:

```python
# Check join validation
join_validation = business_context["join_validation"]

for concept_name, validation in join_validation.items():
    if validation["valid"]:
        print(f"‚úÖ {concept_name}: All joins satisfied")
    else:
        print(f"‚ùå {concept_name}: Missing entities {validation['missing_entities']}")
        print(f"   Unsatisfied joins: {validation['unsatisfied_joins']}")
```

### 4. Entity Coverage Analysis

Analyze how well business concepts cover entities:

```python
# Check entity coverage
coverage = business_context["entity_coverage"]
print(f"Total entities: {coverage['total_entities']}")
print(f"Entities with concepts: {coverage['entities_with_concepts']}")
print(f"Coverage percentage: {coverage['entities_with_concepts']/coverage['total_entities']*100:.1f}%")
```

## üîç Integration with Other Agents

The Business Agent works seamlessly with other components:

- **Entity Recognition Agent**: Receives identified entities for context analysis
- **Concept Loader**: Loads business concepts and domain knowledge
- **Concept Matcher**: Matches queries to relevant business concepts
- **NL2SQL Agent**: Provides business context for SQL generation
- **Integration Agent**: Contributes business context to complete pipeline

## üéñÔ∏è Advanced Features

### Intelligent Concept Matching

- Semantic similarity using OpenAI embeddings
- Multi-factor concept relevance scoring
- Configurable similarity thresholds
- Support for complex business domains

### Business Rule Application

- Domain-specific business rules and constraints
- Join requirement validation
- Entity relationship analysis
- Business logic enforcement

### Example Retrieval

- Similar example identification
- Pattern matching for query optimization
- Historical query analysis
- Best practice recommendations

### Join Validation

- Automatic join requirement analysis
- Entity availability checking
- Join satisfaction validation
- Missing entity identification

## üìà Performance Characteristics

- **Concept Loading**: 0.1-0.5 seconds for concept retrieval
- **Concept Matching**: 1-3 seconds for semantic matching
- **Join Validation**: Sub-second for join analysis
- **Memory Usage**: Efficient concept caching and reuse
- **Scalability**: Handles complex business domains with hundreds of concepts
- **Accuracy**: High precision concept matching with configurable thresholds

## üö¶ Prerequisites

1. **Business Concepts**: YAML-based business concept definitions
2. **OpenAI API Access**: Valid API key for semantic matching
3. **Entity Recognition**: Functional entity recognition for input
4. **Concept Components**: Concept loader and matcher components
5. **Dependencies**: All required packages from requirements.txt

## üîß Error Handling

### Common Error Scenarios

1. **Concept Loading Failures**: Missing concept files, invalid YAML
2. **Matching Errors**: API failures, invalid similarity calculations
3. **Join Validation Errors**: Invalid join syntax, missing entities
4. **Entity Coverage Issues**: No concepts found for entities

### Error Response Handling

```python
# Handle business context errors
try:
    business_context = business_agent.gather_business_context(
        user_query="customer analysis",
        applicable_entities=["customers", "orders"]
    )
    
    if not business_context["success"]:
        error = business_context.get("error", "Unknown error")
        
        if "Failed to load concepts" in error:
            print("Business concepts not found")
            # Use default business context
        elif "Concept matching failed" in error:
            print("Unable to match business concepts")
            # Continue with minimal context
        elif "Join validation failed" in error:
            print("Join requirements cannot be satisfied")
            # Provide alternative suggestions
            
except Exception as e:
    print(f"Business context gathering failed: {e}")
    # Implement fallback mechanisms
```

### Recovery Mechanisms

1. **Concept Fallback**: Use default concepts when loading fails
2. **Matching Fallback**: Use keyword matching when semantic matching fails
3. **Join Fallback**: Provide simplified joins when complex joins fail
4. **Entity Fallback**: Use entity name matching when concept matching fails

---

The Business Agent provides sophisticated business context gathering with intelligent concept matching, business rule application, and comprehensive join validation, ensuring that generated SQL queries incorporate relevant domain knowledge and business constraints for accurate and meaningful results. 


================================================
FILE: docs/agents/CoreAgent.md
================================================
# Core Agent

The Core Agent is the central orchestrator of the SQL Documentation suite that manages autonomous documentation generation, persistence, and vector indexing. It combines LLM-powered analysis with database schema discovery to create comprehensive documentation for tables and relationships.

## üéØ What It Does

The Core Agent serves as the primary interface for SQL documentation operations:

- **Autonomous Documentation**: Generates business purpose and schema documentation using GPT-4
- **Database Discovery**: Inspects database schemas to identify tables and relationships
- **Persistence Management**: Maintains documentation state across sessions with resume capability
- **Vector Indexing**: Integrates with Indexer Agent for semantic search capabilities
- **Error Handling**: Provides robust error handling and retry mechanisms
- **State Management**: Tracks processing progress and enables session resumption

## üîÑ Documentation Flow

```markdown
Database Schema ‚Üí Core Agent ‚Üí LLM Analysis ‚Üí Documentation Store ‚Üí Vector Indexing ‚Üí Searchable Knowledge Base
```

1. **Schema Discovery**: Inspects database to identify tables and relationships
2. **LLM Processing**: Uses GPT-4 to analyze schema and infer business purpose
3. **Documentation Generation**: Creates structured documentation with business context
4. **Persistence**: Saves documentation to SQLite database for state management
5. **Vector Indexing**: Indexes documentation for semantic search capabilities
6. **Resume Capability**: Tracks progress and enables interrupted session resumption

## üöÄ Usage Examples

### Command Line Interface

```bash
# Generate documentation for all tables and relationships
python main.py

# Resume interrupted documentation generation
python main.py --resume

# Process specific table
python main.py --table users

# Process specific relationship
python main.py --relationship "users_orders_fk"

# Generate documentation with batch indexing
python main.py --batch-index
```

### Programmatic Usage

```python
from src.agents.core import PersistentDocumentationAgent

# Initialize the core agent
agent = PersistentDocumentationAgent()

# Process a single table
agent.process_table_documentation("users")

# Process a relationship
relationship = {
    "id": 1,
    "constrained_table": "orders",
    "referred_table": "users",
    "constrained_columns": "user_id",
    "referred_columns": "id"
}
agent.process_relationship_documentation(relationship)

# Index all processed documents
agent.index_processed_documents()

# Check vector indexing availability
if agent.vector_indexing_available:
    print("Vector indexing is available for semantic search")
```

## üìä Response Structure

### Table Documentation Response

```json
{
  "business_purpose": "Stores user account information and authentication data",
  "schema_data": {
    "table_name": "users",
    "columns": [
      {
        "name": "id",
        "type": "INTEGER",
        "nullable": false,
        "primary_key": true,
        "default": null
      },
      {
        "name": "email",
        "type": "VARCHAR(255)",
        "nullable": false,
        "primary_key": false,
        "default": null
      }
    ]
  }
}
```

### Relationship Documentation Response

```json
{
  "relationship_type": "one-to-many",
  "documentation": "Each user can have multiple orders, establishing a customer-order relationship"
}
```

### Processing Status Response

```python
{
  "vector_indexing_available": True,
  "indexer_agent": "Available",
  "database_store": "Available",
  "llm_model": "Available",
  "processed_tables": 15,
  "processed_relationships": 8
}
```

## üß† LLM Processing Algorithm

The agent uses GPT-4 for intelligent documentation generation:

```markdown
Model: gpt-4o-mini
Context Window: 128K tokens
Processing: Schema analysis + Business purpose inference
Output: Structured JSON documentation
```

### Table Processing Steps

1. **Schema Retrieval**: Extract table schema using database inspector
2. **LLM Analysis**: Send schema to GPT-4 for business purpose inference
3. **JSON Generation**: Generate structured documentation in JSON format
4. **Validation**: Validate required fields and data structure
5. **Persistence**: Save to documentation store
6. **Vector Indexing**: Index for semantic search (if available)

### Relationship Processing Steps

1. **Relationship Data**: Extract foreign key relationship information
2. **LLM Analysis**: Analyze relationship type and business context
3. **Documentation Generation**: Create relationship documentation
4. **Validation**: Ensure proper relationship type classification
5. **Persistence**: Save relationship documentation
6. **Vector Indexing**: Index for semantic search (if available)

## ‚öôÔ∏è Configuration

### Environment Variables

```env
# Required for LLM processing
OPENAI_API_KEY="your-api-key-here"

# Required for database connection
DATABASE_URL="sqlite:///__bin__/data/documentation.db"

# Optional: Logging configuration
LOG_LEVEL="INFO"
LOG_FILE="agent.log"

# Optional: Vector indexing settings
EMBEDDING_BATCH_SIZE="100"
EMBEDDING_MAX_RETRIES="3"
```

### Initialization Parameters

```python
PersistentDocumentationAgent(
    # No parameters required - uses environment variables
)

# Method parameters
process_table_documentation(
    table_name                 # Required: Name of table to process
)

process_relationship_documentation(
    relationship               # Required: Dict with relationship data
)

index_processed_documents(
    # No parameters - indexes all processed documents
)
```

## üéØ Use Cases

### 1. Autonomous Documentation Generation

Generate comprehensive documentation for entire databases:

```python
# Process all tables in database
tables = agent.db_inspector.get_all_table_names()
for table in tables:
    agent.process_table_documentation(table)

# Process all relationships
relationships = agent.db_inspector.get_all_foreign_key_relationships()
for rel in relationships:
    agent.process_relationship_documentation(rel)
```

### 2. Incremental Processing

Process specific tables or relationships:

```python
# Process specific table
agent.process_table_documentation("customers")

# Process specific relationship
relationship = {
    "id": 1,
    "constrained_table": "orders",
    "referred_table": "customers",
    "constrained_columns": "customer_id",
    "referred_columns": "id"
}
agent.process_relationship_documentation(relationship)
```

### 3. Resume Capability

Continue interrupted processing sessions:

```python
# Check what's been processed
pending_tables = agent.store.get_pending_tables()
pending_relationships = agent.store.get_pending_relationships()

# Resume processing
for table in pending_tables:
    agent.process_table_documentation(table)
```

### 4. Vector Indexing Integration

Index processed documents for semantic search:

```python
# Index all processed documents
agent.index_processed_documents()

# Check indexing status
if agent.vector_indexing_available:
    print("Vector indexing is working")
    # Use indexer agent for search
    results = agent.indexer_agent.search_documentation("user data")
```

## üîç Integration with Other Agents

The Core Agent orchestrates all other components:

- **Database Inspector**: Discovers schema information
- **Documentation Store**: Manages persistence and state
- **Indexer Agent**: Provides vector indexing capabilities
- **LLM Model**: Powers intelligent documentation generation
- **Vector Store**: Enables semantic search functionality

## üéñÔ∏è Advanced Features

### Intelligent Schema Analysis

- Automatic business purpose inference from table names and columns
- Relationship type classification (one-to-one, one-to-many, many-to-many)
- Context-aware documentation generation
- Multi-language schema support

### State Management

- Persistent session tracking across restarts
- Progress monitoring and reporting
- Resume capability for large databases
- Error recovery and retry mechanisms

### Error Handling

- Graceful degradation when vector indexing unavailable
- Comprehensive error logging and reporting
- Retry logic for transient failures
- Fallback mechanisms for edge cases

### Performance Optimization

- Efficient batch processing capabilities
- Memory-conscious processing for large schemas
- Parallel processing where possible
- Caching of frequently accessed data

## üìà Performance Characteristics

- **Documentation Generation**: 2-5 seconds per table/relationship
- **Schema Discovery**: Sub-second for typical databases
- **Vector Indexing**: ~100ms per document when available
- **Memory Usage**: Minimal footprint with efficient data structures
- **Scalability**: Handles databases with thousands of tables
- **Resume Capability**: Instant session restoration

## üö¶ Prerequisites

1. **OpenAI API Access**: Valid API key for GPT-4 processing
2. **Database Connection**: Accessible database with schema information
3. **Dependencies**: All required packages from requirements.txt
4. **Storage**: SQLite database for persistence
5. **Vector Store**: ChromaDB for semantic search (optional)

## üîß Error Handling

### Common Error Scenarios

1. **LLM Processing Errors**: JSON parsing failures, invalid responses
2. **Database Connection Issues**: Connection timeouts, schema access problems
3. **Vector Indexing Failures**: API rate limits, ChromaDB issues
4. **Validation Errors**: Missing required fields, invalid data structures

### Error Response Format

```json
{
  "success": false,
  "error": "Invalid JSON response from LLM",
  "table_name": "users",
  "raw_response": "Generated response text",
  "suggestion": "Check LLM response format and retry"
}
```

### Retry Mechanisms

1. **LLM Processing**: Automatic retry for transient API errors
2. **Vector Indexing**: Exponential backoff for rate limits
3. **Database Operations**: Connection retry with timeout handling
4. **Validation Errors**: Detailed error messages with correction suggestions

## üîÑ Session Management

### Progress Tracking

```python
# Get processing progress
progress = agent.store.get_generation_progress()
print(f"Tables: {progress['tables']}")
print(f"Relationships: {progress['relationships']}")

# Check specific item status
is_processed = agent.store.is_table_processed("users")
is_processed = agent.store.is_relationship_processed(relationship)
```

### Resume Capability

```python
# Start new session
session_id = agent.store.start_generation_session(
    database_url, tables, relationships
)

# Resume existing session
agent = PersistentDocumentationAgent()  # Automatically resumes
```

---

The Core Agent is the heart of the SQL Documentation suite, providing intelligent, autonomous documentation generation with robust state management and seamless integration with vector indexing for semantic search capabilities.



================================================
FILE: docs/agents/EntityRecognitionAgent.md
================================================
# Entity Recognition Agent

The Entity Recognition Agent is an intelligent component that determines the most applicable database entities (tables) for a given user query. It leverages semantic search capabilities from the Indexer Agent to provide contextual entity recommendations.

## üéØ What It Does

The Entity Recognition Agent analyzes user queries and determines which database tables are most relevant based on:

- **Semantic Similarity**: Uses OpenAI embeddings to find tables with similar meaning to the user's query
- **Business Purpose Matching**: Analyzes how well table purposes align with user intent
- **Table Name Relevance**: Evaluates direct mentions or conceptual matches in table names
- **Relevance Scoring**: Provides weighted relevance scores combining multiple factors
- **Entity Recommendations**: Identifies the most applicable entities for the user's query

## üîÑ Query Flow

```markdown
User Query ‚Üí Entity Recognition Agent ‚Üí Indexer Agent ‚Üí Search Results ‚Üí Entity Analysis ‚Üí Entity Rankings
```

1. **User Input**: Natural language query (e.g., "customer information")
2. **Semantic Search**: Uses Indexer Agent to search table documentation
3. **Relevance Analysis**: Calculates multi-factor relevance scores
4. **Entity Filtering**: Identifies highly relevant entities (score > 0.5)
5. **Rankings**: Returns ranked list of applicable entities with relevance scores

## üöÄ Usage Examples

### Command Line Interface

```bash
# Comprehensive entity recognition with recommendations
python main.py --recognize-entities "customer data" "I want to analyze user behavior"

# Quick entity lookup with custom threshold
python main.py --quick-lookup "order information" 0.8

# Search for user-related entities
python main.py --recognize-entities "user authentication"
```

### Programmatic Usage

```python
from src.agents.entity_recognition import EntityRecognitionAgent
from src.agents.core import PersistentDocumentationAgent

# Initialize agents
doc_agent = PersistentDocumentationAgent()
entity_agent = EntityRecognitionAgent(doc_agent.indexer_agent)

# Comprehensive entity recognition
results = entity_agent.recognize_entities(
    user_query="customer information",
    user_intent="I want to analyze customer demographics",
    max_entities=5
)

# Quick table lookup
relevant_tables = entity_agent.quick_entity_lookup(
    "user data", 
    threshold=0.7
)

# Get detailed entity information
entity_details = entity_agent.get_entity_details(relevant_tables)
```

## üìä Response Structure

### Comprehensive Entity Recognition Response

```json
{
  "success": true,
  "applicable_entities": [
    {
      "table_name": "users",
      "business_purpose": "Stores user account information",
      "relevance_score": 0.92,
      "relevance_factors": {
        "semantic_similarity": 0.95,
        "business_purpose_match": 0.85,
        "table_name_relevance": 1.0
      },
      "recommendation": "Highly relevant - strongly recommended for your query"
    }
  ],
  "recommendations": [
    {
      "priority": 1,
      "table_name": "users",
      "relevance_score": 0.92,
      "business_purpose": "Stores user account information",
      "recommendation": "Highly relevant - strongly recommended for your query"
    }
  ],
  "analysis": "Found 1 applicable entities for intent: 'customer information'. Top match is 'users' with average relevance score of 0.92.",
  "confidence": 0.92,
  "total_entities_analyzed": 3
}
```

### Quick Lookup Response

```python
["users", "user_profiles", "customer_data"]  # List of relevant table names
```

## üß† Relevance Scoring Algorithm

The agent uses a weighted scoring system to determine entity relevance:

```markdown
Overall Relevance = (Semantic Similarity √ó 0.5) + 
                   (Business Purpose Match √ó 0.3) + 
                   (Table Name Relevance √ó 0.2)
```

### Scoring Components

1. **Semantic Similarity (50% weight)**: OpenAI embedding cosine similarity
2. **Business Purpose Match (30% weight)**: Keyword overlap analysis
3. **Table Name Relevance (20% weight)**: Direct and partial name matching

### Relevance Thresholds

- **‚â• 0.8**: Highly relevant - strongly recommended
- **‚â• 0.6**: Relevant - good match
- **‚â• 0.4**: Moderately relevant - may contain useful information
- **‚â• 0.2**: Low relevance - tangentially related
- **< 0.2**: Not relevant - unlikely to be useful

## ‚öôÔ∏è Configuration

### Environment Variables

```env
# Required for entity recognition
OPENAI_API_KEY="your-api-key-here"

# Optional: Customize embedding model
OPENAI_EMBEDDING_MODEL="text-embedding-3-small"

# Optional: Batch processing settings
EMBEDDING_BATCH_SIZE="100"
EMBEDDING_MAX_RETRIES="3"
```

### Initialization Parameters

```python
EntityRecognitionAgent(
    indexer_agent,              # Required: SQLIndexerAgent instance
)

# Method parameters
recognize_entities(
    user_query,                 # Required: Natural language query
    user_intent=None,           # Optional: Specific user intent
    max_entities=5              # Optional: Maximum entities to return
)

quick_entity_lookup(
    user_query,                 # Required: Natural language query
    threshold=0.7               # Optional: Minimum relevance threshold
)
```

## üéØ Use Cases

### 1. Entity Discovery

Identify which tables contain data relevant to your query:

```bash
python main.py --recognize-entities "sales performance" "I need quarterly revenue data"
```

### 2. Database Exploration

Discover relevant tables when exploring an unfamiliar database:

```bash
python main.py --quick-lookup "inventory management" 0.6
```

### 3. Schema Navigation

Find tables related to specific business domains:

```bash
python main.py --recognize-entities "customer support" "I want to analyze ticket resolution times"
```

### 4. Data Source Identification

Identify the most relevant data sources before further analysis:

```bash
python main.py --recognize-entities "user engagement" "I need to create a dashboard"
```

## üîç Integration with Other Agents

The Entity Recognition Agent works seamlessly with other components:

- **Indexer Agent**: Provides semantic search capabilities
- **Documentation Agent**: Uses documented table information
- **Batch Manager**: Supports efficient processing of large databases
- **Vector Store**: Leverages OpenAI embeddings for similarity search

## üéñÔ∏è Advanced Features

### Natural Language Processing

- Handles complex user intents and queries
- Understands synonyms and related concepts
- Supports multi-word entity matching

### Smart Entity Scoring

- Multi-factor relevance analysis
- Context-aware entity recommendations
- Business-purpose alignment assessment

### Confidence Assessment

- Overall analysis confidence scoring
- Individual entity confidence metrics
- Threshold-based filtering for high-quality results

### Error Handling

- Graceful degradation when vector indexing unavailable
- Comprehensive error reporting with actionable feedback
- Fallback mechanisms for edge cases

## üìà Performance Characteristics

- **Response Time**: Sub-second for quick lookups, 2-5 seconds for comprehensive analysis
- **Accuracy**: High relevance scoring accuracy with multi-factor analysis
- **Scalability**: Handles databases with hundreds of tables efficiently
- **Memory Usage**: Minimal memory footprint with efficient vector operations

## üö¶ Prerequisites

1. **Vector Indexing**: Requires functional vector indexing (ChromaDB + OpenAI embeddings)
2. **Documented Tables**: Tables must be processed by the Documentation Agent
3. **API Access**: Valid OpenAI API key for embeddings generation
4. **Dependencies**: All required packages from requirements.txt

---

The Entity Recognition Agent enhances the SQL Documentation suite by providing intelligent entity discovery and relevance scoring, making database exploration more intuitive and efficient for users who need to identify the most applicable tables for their data needs.



================================================
FILE: docs/agents/FactoryAgent.md
================================================
# Factory Agent

The Factory Agent (AgentFactory) is a centralized factory pattern implementation that manages the creation, lifecycle, and dependency injection of all agent instances in the SQL Documentation suite. It provides efficient resource management through shared components and ensures consistent agent initialization across the application.

## üéØ What It Does

The Factory Agent provides centralized agent management and dependency injection:

- **Agent Creation**: Creates and manages all agent instances with proper initialization
- **Shared Components**: Manages shared LLM models, database tools, and concept components
- **Dependency Injection**: Handles complex dependencies between agents and components
- **Resource Optimization**: Reuses shared components to minimize memory usage and API calls
- **Lifecycle Management**: Provides agent reset and cleanup capabilities for testing
- **Singleton Pattern**: Ensures single instances of expensive components like LLM models

## üîÑ Factory Flow

```markdown
Agent Request ‚Üí Factory Check ‚Üí Component Creation ‚Üí Agent Initialization ‚Üí Dependency Injection ‚Üí Agent Return
```

1. **Agent Request**: Client requests specific agent instance
2. **Factory Check**: Factory checks if agent already exists
3. **Component Creation**: Creates shared components if needed (LLM, database tools)
4. **Agent Initialization**: Initializes agent with proper dependencies
5. **Dependency Injection**: Injects shared components and dependencies
6. **Agent Return**: Returns configured agent instance

## üöÄ Usage Examples

### Command Line Interface

```bash
# Use factory to get main documentation agent
python main.py --factory-agent main

# Get SQL pipeline with all dependencies
python main.py --factory-pipeline

# Get specific agent with custom configuration
python main.py --factory-agent nl2sql --database-tools custom
```

### Programmatic Usage

```python
from src.agents.factory import agent_factory

# Get main documentation agent
main_agent = agent_factory.get_main_agent()

# Get indexer agent with shared LLM model
indexer_agent = agent_factory.get_indexer_agent()

# Get entity recognition agent
entity_agent = agent_factory.get_entity_agent()

# Get business context agent with custom concepts directory
business_agent = agent_factory.get_business_agent("custom/concepts")

# Get NL2SQL agent with custom database tools
nl2sql_agent = agent_factory.get_nl2sql_agent(custom_database_tools)

# Get batch manager
batch_manager = agent_factory.get_batch_manager()

# Get complete SQL pipeline
sql_pipeline = agent_factory.get_sql_pipeline()

# Get all agents at once
all_agents = agent_factory.get_all_agents()
```

## üìä Factory Structure

### Agent Registry

```python
{
    "main_agent": PersistentDocumentationAgent,
    "indexer_agent": SQLIndexerAgent,
    "entity_agent": EntityRecognitionAgent,
    "business_agent": BusinessContextAgent,
    "nl2sql_agent": NL2SQLAgent,
    "batch_manager": BatchIndexingManager,
    "sql_pipeline": SQLAgentPipeline
}
```

### Shared Components

```python
{
    "shared_llm_model": OpenAIModel,
    "unified_database_tools": DatabaseTools,
    "shared_concept_loader": ConceptLoader,
    "shared_concept_matcher": ConceptMatcher
}
```

### Factory Configuration

```python
{
    "_instances": {},              # Agent instances cache
    "_shared_components": {},      # Shared component cache
    "_shared_llm_model": None,    # Shared LLM model
    "_unified_database_tools": None  # Unified database tools
}
```

## üß† Factory Algorithm

The Factory Agent implements sophisticated dependency management:

### Component Creation Strategy
```python
def get_shared_llm_model(self):
    """Get or create shared LLM model."""
    if not self._shared_llm_model:
        # Create new LLM model only if not exists
        self._shared_llm_model = OpenAIModel(
            model_id="gpt-4o-mini", 
            api_key=os.getenv("OPENAI_API_KEY")
        )
    return self._shared_llm_model
```

### Agent Creation with Dependencies
```python
def get_entity_agent(self) -> EntityRecognitionAgent:
    """Get or create entity recognition agent."""
    if "entity_agent" not in self._instances:
        # Create with shared dependencies
        self._instances["entity_agent"] = EntityRecognitionAgent(
            self.get_indexer_agent(),  # Shared indexer
            shared_llm_model=self.get_shared_llm_model(),  # Shared LLM
            database_tools=self.get_unified_database_tools()  # Shared tools
        )
    return self._instances["entity_agent"]
```

### Complex Dependency Resolution
```python
def get_business_agent(self, concepts_dir: str = "src/agents/concepts"):
    """Get or create business context agent with complex dependencies."""
    if "business_agent" not in self._instances:
        # Get shared components
        shared_concept_loader = self._get_shared_component("concept_loader", concepts_dir)
        shared_concept_matcher = self._get_shared_component("concept_matcher", self.get_indexer_agent())
        
        # Create with all shared dependencies
        self._instances["business_agent"] = BusinessContextAgent(
            indexer_agent=self.get_indexer_agent(),
            concepts_dir=concepts_dir,
            shared_llm_model=self.get_shared_llm_model(),
            shared_concept_loader=shared_concept_loader,
            shared_concept_matcher=shared_concept_matcher,
            database_tools=self.get_unified_database_tools()
        )
    return self._instances["business_agent"]
```

## ‚öôÔ∏è Configuration

### Environment Variables

```env
# Required for factory components
OPENAI_API_KEY="your-api-key-here"
DATABASE_URL="your-database-connection-string"

# Optional: Factory settings
FACTORY_CACHE_SIZE="100"
FACTORY_RESET_ON_ERROR="true"

# Optional: Component paths
CONCEPTS_DIRECTORY="src/agents/concepts"
VECTOR_STORE_PATH="__bin__/data/vector_indexes"

# Optional: Logging configuration
LOG_LEVEL="INFO"
LOG_FILE="factory.log"
```

### Factory Initialization

```python
# Global factory instance (automatically created)
agent_factory = AgentFactory()

# Factory methods
get_main_agent() -> PersistentDocumentationAgent
get_indexer_agent() -> SQLIndexerAgent
get_entity_agent() -> EntityRecognitionAgent
get_business_agent(concepts_dir: str) -> BusinessContextAgent
get_nl2sql_agent(database_tools=None) -> NL2SQLAgent
get_batch_manager() -> BatchIndexingManager
get_sql_pipeline(database_tools=None) -> SQLAgentPipeline
get_all_agents() -> Dict[str, Any]
reset() -> None
```

## üéØ Use Cases

### 1. Standard Agent Creation

Create agents with automatic dependency management:

```python
# Get agents with shared components
main_agent = agent_factory.get_main_agent()
indexer_agent = agent_factory.get_indexer_agent()
entity_agent = agent_factory.get_entity_agent()

# All agents share the same LLM model and database tools
print(f"Main agent LLM: {main_agent.llm_model}")
print(f"Indexer agent LLM: {indexer_agent.llm_model}")
print(f"Same LLM instance: {main_agent.llm_model is indexer_agent.llm_model}")
```

### 2. Custom Configuration

Create agents with custom configurations:

```python
# Custom concepts directory
business_agent = agent_factory.get_business_agent("custom/business_concepts")

# Custom database tools
custom_tools = CustomDatabaseTools()
nl2sql_agent = agent_factory.get_nl2sql_agent(custom_tools)

# Custom SQL pipeline
custom_pipeline = agent_factory.get_sql_pipeline(custom_tools)
```

### 3. Complete Pipeline Creation

Create complete SQL generation pipeline:

```python
# Get complete pipeline with all dependencies
pipeline = agent_factory.get_sql_pipeline()

# Pipeline includes all required agents
print(f"Entity agent: {pipeline.entity_agent}")
print(f"Business agent: {pipeline.business_agent}")
print(f"NL2SQL agent: {pipeline.nl2sql_agent}")

# All agents share components
print(f"Shared LLM: {pipeline.entity_agent.llm_model}")
print(f"Shared database tools: {pipeline.database_tools}")
```

### 4. Testing and Reset

Reset factory for testing scenarios:

```python
# Get agents for testing
main_agent = agent_factory.get_main_agent()
entity_agent = agent_factory.get_entity_agent()

# Perform tests
test_results = run_tests(main_agent, entity_agent)

# Reset factory for clean state
agent_factory.reset()

# Verify reset
print(f"Main agent after reset: {agent_factory.get_main_agent()}")
print(f"New instance: {agent_factory.get_main_agent() is not main_agent}")
```

### 5. Resource Management

Monitor and manage shared resources:

```python
# Get all agents to see resource usage
all_agents = agent_factory.get_all_agents()

# Check shared components
shared_llm = agent_factory.get_shared_llm_model()
shared_tools = agent_factory.get_unified_database_tools()

print(f"Total agents: {len(all_agents)}")
print(f"Shared LLM model: {shared_llm}")
print(f"Shared database tools: {shared_tools}")

# All agents use the same shared components
for name, agent in all_agents.items():
    if hasattr(agent, 'llm_model'):
        assert agent.llm_model is shared_llm
    if hasattr(agent, 'database_tools'):
        assert agent.database_tools is shared_tools
```

## üîç Integration with Other Agents

The Factory Agent manages all agent dependencies:

- **All Agents**: Creates and manages all agent instances
- **Shared LLM Model**: Provides single OpenAI model instance
- **Database Tools**: Manages unified database tool instances
- **Concept Components**: Handles shared concept loaders and matchers
- **Vector Store**: Manages vector store instances for indexing

## üéñÔ∏è Advanced Features

### Intelligent Caching

- Singleton pattern for expensive components (LLM models)
- Instance caching for all agent types
- Automatic cleanup and resource management
- Configurable cache sizes and TTL

### Dependency Injection

- Automatic dependency resolution
- Shared component management
- Complex dependency chains
- Circular dependency prevention

### Resource Optimization

- Shared LLM models across all agents
- Unified database tools for consistency
- Memory-efficient component reuse
- Automatic resource cleanup

### Testing Support

- Complete factory reset capability
- Isolated agent creation for testing
- Dependency mocking support
- Clean state management

## üìà Performance Characteristics

- **Agent Creation**: 0.1-0.5 seconds for new agent instances
- **Shared Component Access**: Sub-second for cached components
- **Memory Usage**: 60-80% reduction through component sharing
- **API Efficiency**: Single LLM model instance across all agents
- **Scalability**: Handles hundreds of agent instances efficiently
- **Resource Management**: Automatic cleanup and memory optimization

## üö¶ Prerequisites

1. **Environment Configuration**: Valid OpenAI API key and database connection
2. **Agent Dependencies**: All agent classes must be importable
3. **Component Dependencies**: Database tools, vector store, and concept components
4. **Dependencies**: All required packages from requirements.txt
5. **File System**: Proper file paths for concepts and vector storage

## üîß Error Handling

### Common Error Scenarios

1. **Import Errors**: Missing agent or component dependencies
2. **Configuration Errors**: Invalid API keys or database connections
3. **Resource Errors**: Memory or file system issues
4. **Dependency Errors**: Circular dependencies or missing components

### Error Response Handling

```python
# Handle factory errors
try:
    agent = agent_factory.get_main_agent()
except ValueError as e:
    print(f"Configuration error: {e}")
    # Check environment variables
except ImportError as e:
    print(f"Import error: {e}")
    # Check dependencies
except Exception as e:
    print(f"Factory error: {e}")
    # Reset factory and retry
    agent_factory.reset()
    agent = agent_factory.get_main_agent()
```

### Recovery Mechanisms

1. **Factory Reset**: Complete reset for testing and error recovery
2. **Component Recreation**: Automatic recreation of failed components
3. **Dependency Retry**: Retry mechanism for complex dependency resolution
4. **Fallback Configuration**: Default configurations when custom configs fail

## üîÑ Factory Lifecycle

### Initialization Phase
```python
# Factory is automatically initialized
agent_factory = AgentFactory()

# Components are created on-demand
llm_model = agent_factory.get_shared_llm_model()
database_tools = agent_factory.get_unified_database_tools()
```

### Usage Phase
```python
# Agents are created and cached
main_agent = agent_factory.get_main_agent()
indexer_agent = agent_factory.get_indexer_agent()

# Shared components are reused
assert main_agent.llm_model is indexer_agent.llm_model
```

### Cleanup Phase
```python
# Reset factory for testing or cleanup
agent_factory.reset()

# All instances and shared components are cleared
assert len(agent_factory._instances) == 0
assert agent_factory._shared_llm_model is None
```

---

The Factory Agent provides efficient, centralized management of all agent instances with intelligent dependency injection, shared component optimization, and comprehensive lifecycle management, ensuring optimal resource usage and consistent agent behavior across the SQL Documentation suite. 


================================================
FILE: docs/agents/IndexerAgent.md
================================================
# Indexer Agent

The Indexer Agent is an intelligent vector indexing component that manages OpenAI embeddings for SQL documentation. It provides semantic search capabilities and natural language processing for database tables and relationships using ChromaDB as the vector store.

## üéØ What It Does

The Indexer Agent handles vector indexing and semantic search operations for SQL documentation:

- **Vector Indexing**: Creates and manages OpenAI embeddings for table and relationship documentation
- **Semantic Search**: Enables natural language queries to find relevant database entities
- **Batch Processing**: Efficiently indexes multiple documents using batch operations
- **Index Management**: Provides status monitoring and maintenance operations
- **Natural Language Interface**: Processes indexing instructions in plain English
- **ChromaDB Integration**: Uses ChromaDB for persistent vector storage with similarity search

## üîÑ Indexing Flow

```markdown
Documentation Data ‚Üí Indexer Agent ‚Üí OpenAI Embeddings ‚Üí ChromaDB Storage ‚Üí Semantic Search Results
```

1. **Input**: Table or relationship documentation with business purpose and schema
2. **Embedding Generation**: Uses OpenAI's text-embedding-3-small model (3072 dimensions)
3. **Vector Storage**: Stores embeddings in ChromaDB collections with metadata
4. **Similarity Search**: Enables cosine similarity search for natural language queries
5. **Results**: Returns ranked search results with relevance scores

## üöÄ Usage Examples

### Command Line Interface

```bash
# Search for tables related to user management
python main.py --search "user authentication" --type table

# Search for relationships involving customer data
python main.py --search "customer relationships" --type relationship

# Get indexing status and statistics
python main.py --index-status

# Batch index all processed documents
python main.py --batch-index
```

### Programmatic Usage

```python
from src.agents.indexer import SQLIndexerAgent
from src.vector.store import SQLVectorStore

# Initialize the indexer agent
vector_store = SQLVectorStore()
indexer_agent = SQLIndexerAgent(vector_store)

# Index table documentation
table_data = {
    "name": "users",
    "business_purpose": "Stores user account information and authentication data",
    "schema": {"columns": [...]},
    "type": "table"
}
success = indexer_agent.index_table_documentation(table_data)

# Search for relevant documentation
results = indexer_agent.search_documentation("user authentication", "table")

# Process natural language instructions
result = indexer_agent.process_indexing_instruction(
    "Index the customer table with business purpose for customer management"
)

# Batch index multiple tables
tables_data = [table1_data, table2_data, table3_data]
batch_results = indexer_agent.batch_index_tables(tables_data)
```

## üìä Response Structure

### Search Results Response

```json
{
  "success": true,
  "query": "user authentication",
  "doc_type": "table",
  "tables": [
    {
      "content": {
        "name": "users",
        "business_purpose": "Stores user account information and authentication data",
        "schema": {...}
      },
      "score": 0.92,
      "id": "users_table"
    }
  ],
  "relationships": [],
  "total_results": 1
}
```

### Indexing Status Response

```json
{
  "success": true,
  "table_index_count": 15,
  "relationship_index_count": 8,
  "total_indexed_documents": 23,
  "vector_dimensions": 3072
}
```

### Natural Language Instruction Response

```json
{
  "success": true,
  "operation": "index_table_documentation",
  "message": "Successfully indexed table: customers",
  "table_name": "customers",
  "vector_dimensions": 3072,
  "details": "Table documentation indexed with OpenAI embeddings"
}
```

## üß† Embedding and Search Algorithm

The agent uses OpenAI's text-embedding-3-small model with the following characteristics:

```markdown
Model: text-embedding-3-small
Dimensions: 3072
Distance Metric: Cosine Distance
Similarity Calculation: 1.0 - (distance / 2.0)
```

### Search Process

1. **Query Embedding**: Convert user query to 3072-dimensional vector
2. **Similarity Search**: Find most similar vectors in ChromaDB collections
3. **Score Conversion**: Convert cosine distance to similarity score (0-1)
4. **Ranking**: Sort results by similarity score (highest first)
5. **Filtering**: Apply relevance thresholds and result limits

### Indexing Process

1. **Document Preparation**: Validate and structure documentation data
2. **Text Embedding**: Generate embeddings for business purpose and schema
3. **Metadata Storage**: Store table/relationship metadata with embeddings
4. **Collection Management**: Organize embeddings in separate table/relationship collections
5. **Persistence**: Save to ChromaDB for persistent storage

## ‚öôÔ∏è Configuration

### Environment Variables

```env
# Required for OpenAI embeddings
OPENAI_API_KEY="your-api-key-here"

# Optional: Customize embedding model
OPENAI_EMBEDDING_MODEL="text-embedding-3-small"

# Optional: Batch processing settings
EMBEDDING_BATCH_SIZE="100"
EMBEDDING_MAX_RETRIES="3"

# Optional: ChromaDB settings
CHROMA_PERSIST_DIRECTORY="__bin__/data/vector_indexes"
```

### Initialization Parameters

```python
SQLIndexerAgent(
    vector_store,              # Required: SQLVectorStore instance
)

# Method parameters
index_table_documentation(
    table_data                 # Required: Dict with name, business_purpose, schema, type
)

search_documentation(
    query,                     # Required: Natural language search query
    doc_type="all"            # Optional: 'all', 'table', or 'relationship'
)

process_indexing_instruction(
    instruction                # Required: Natural language instruction
)
```

## üéØ Use Cases

### 1. Semantic Search

Find relevant tables using natural language:

```python
# Search for user-related tables
results = indexer_agent.search_documentation("user management", "table")

# Search for customer relationships
results = indexer_agent.search_documentation("customer data", "relationship")
```

### 2. Batch Indexing

Efficiently index multiple documents:

```python
# Index all processed tables
tables_data = [table1, table2, table3, ...]
batch_results = indexer_agent.batch_index_tables(tables_data)

# Index all relationships
relationships_data = [rel1, rel2, rel3, ...]
batch_results = indexer_agent.batch_index_relationships(relationships_data)
```

### 3. Natural Language Instructions

Process indexing operations using plain English:

```python
# Index a table with natural language
result = indexer_agent.process_indexing_instruction(
    "Index the orders table with business purpose for order management"
)

# Search using natural language
result = indexer_agent.process_indexing_instruction(
    "Search for tables related to customer information"
)
```

### 4. Index Management

Monitor and maintain vector indexes:

```python
# Get indexing status
status = indexer_agent.get_indexing_status()

# Update existing index
success = indexer_agent.update_table_index("users", updated_data)

# Remove from index
success = indexer_agent.remove_from_index("old_table", "table")
```

## üîç Integration with Other Agents

The Indexer Agent works seamlessly with other components:

- **Core Agent**: Receives documentation data for indexing
- **Entity Recognition Agent**: Provides search capabilities for entity discovery
- **Batch Manager**: Supports efficient batch processing operations
- **Vector Store**: Manages ChromaDB collections and embeddings
- **Embeddings Client**: Handles OpenAI API interactions

## üéñÔ∏è Advanced Features

### Intelligent Document Processing

- Validates documentation structure before indexing
- Handles both table and relationship documentation formats
- Supports metadata enrichment and organization

### Semantic Search Capabilities

- Natural language query understanding
- Multi-factor relevance scoring
- Configurable search result filtering
- Support for different document types

### Batch Processing Optimization

- Efficient bulk indexing operations
- Error handling and retry mechanisms
- Progress tracking and status reporting
- Memory-efficient processing for large datasets

### Index Management

- Real-time status monitoring
- Index health and performance metrics
- Update and removal operations
- Collection organization and maintenance

## üìà Performance Characteristics

- **Embedding Generation**: ~100ms per document with OpenAI API
- **Search Response Time**: Sub-second for typical queries
- **Index Storage**: Efficient ChromaDB persistence with metadata
- **Batch Processing**: 10-50 documents per second depending on API limits
- **Memory Usage**: Minimal memory footprint with streaming operations
- **Scalability**: Handles databases with thousands of tables efficiently

## üö¶ Prerequisites

1. **OpenAI API Access**: Valid API key for embeddings generation
2. **ChromaDB**: Local or remote ChromaDB instance for vector storage
3. **Documentation Data**: Tables and relationships must be documented by Core Agent
4. **Dependencies**: All required packages from requirements.txt
5. **Vector Store**: Properly configured SQLVectorStore with ChromaDB

## üîß Error Handling

### Common Error Scenarios

1. **API Rate Limits**: Automatic retry with exponential backoff
2. **Invalid Documentation**: Validation errors with detailed feedback
3. **ChromaDB Issues**: Graceful degradation with error reporting
4. **Network Problems**: Connection timeout handling and retry logic

### Error Response Format

```json
{
  "success": false,
  "error": "OpenAI API rate limit exceeded",
  "details": "Rate limit of 3000 requests per minute exceeded",
  "retry_after": 60,
  "suggestion": "Wait 60 seconds before retrying"
}
```

---

The Indexer Agent provides the foundation for semantic search capabilities in the SQL Documentation suite, enabling intelligent discovery and retrieval of database documentation through natural language queries and vector similarity search. 



================================================
FILE: docs/agents/IntegrationAgent.md
================================================
# Integration Agent

The Integration Agent (SQLAgentPipeline) is the central orchestrator that coordinates the complete SQL generation pipeline from natural language queries to validated SQL statements. It seamlessly integrates Entity Recognition, Business Context, and NL2SQL agents to provide end-to-end SQL generation capabilities.

## üéØ What It Does

The Integration Agent orchestrates a multi-step pipeline for intelligent SQL generation:

- **Pipeline Orchestration**: Coordinates Entity Recognition ‚Üí Business Context ‚Üí SQL Generation workflow
- **Agent Integration**: Manages shared instances of Entity Recognition, Business Context, and NL2SQL agents
- **Error Handling**: Provides comprehensive error handling and fallback mechanisms for each pipeline step
- **Response Formatting**: Delivers consistent, structured responses with detailed pipeline status
- **State Management**: Maintains pipeline state and enables step-by-step debugging
- **Performance Optimization**: Uses shared agent instances to minimize resource usage

## üîÑ Pipeline Flow

```markdown
User Query ‚Üí Entity Recognition ‚Üí Business Context ‚Üí SQL Generation ‚Üí Validation ‚Üí Final Response
```

1. **Entity Recognition**: Identifies relevant database tables and relationships
2. **Business Context**: Gathers business rules and domain knowledge
3. **SQL Generation**: Creates SQL queries using context and entity information
4. **Validation**: Validates SQL syntax, security, and business compliance
5. **Response Formatting**: Returns comprehensive results with pipeline status

## üöÄ Usage Examples

### Command Line Interface

```bash
# Complete pipeline processing
python main.py --pipeline "Show me customer orders from last month"

# Pipeline with custom intent
python main.py --pipeline "Get user analytics" --intent "I need to analyze user engagement metrics"

# Pipeline with specific entities
python main.py --pipeline "customer data analysis" --entities "users,orders,customers"
```

### Programmatic Usage

```python
from src.agents.integration import SQLAgentPipeline
from src.agents.factory import agent_factory

# Get pipeline from factory
pipeline = agent_factory.get_sql_pipeline()

# Process user query through complete pipeline
result = pipeline.process_user_query(
    user_query="Show me customer orders from last month",
    user_intent="I need to analyze recent customer purchasing patterns"
)

# Check pipeline results
if result["success"]:
    print(f"Generated SQL: {result['sql_generation']['generated_sql']}")
    print(f"Entities found: {result['entity_recognition']['entities']}")
    print(f"Business concepts: {result['business_context']['matched_concepts']}")
else:
    print(f"Pipeline failed: {result['error']}")
```

## üìä Response Structure

### Complete Pipeline Response

```json
{
  "success": true,
  "pipeline_summary": {
    "entity_recognition_success": true,
    "business_context_success": true,
    "sql_generation_success": true,
    "sql_validation_success": true
  },
  "entity_recognition": {
    "entities": ["users", "orders", "customers"],
    "confidence": 0.92
  },
  "business_context": {
    "matched_concepts": [
      {
        "name": "customer_analytics",
        "description": "Customer behavior analysis",
        "similarity": 0.85
      }
    ],
    "business_instructions": [
      {
        "concept": "customer_analytics",
        "instructions": "Use customer_id for joins, filter by date ranges",
        "similarity": 0.85
      }
    ]
  },
  "sql_generation": {
    "generated_sql": "SELECT c.customer_name, o.order_date, o.total_amount FROM customers c JOIN orders o ON c.customer_id = o.customer_id WHERE o.order_date >= DATEADD(month, -1, GETDATE())",
    "is_valid": true,
    "validation": {
      "syntax_valid": true,
      "business_compliant": true,
      "security_valid": true,
      "performance_issues": []
    },
    "query_execution": {
      "success": true,
      "total_rows": 150,
      "sample_data": {...}
    }
  }
}
```

### Pipeline Error Response

```json
{
  "success": false,
  "error": "Entity recognition failed",
  "step": "entity_recognition",
  "details": "No relevant entities found for the query"
}
```

## üß† Pipeline Algorithm

The Integration Agent implements a sophisticated multi-step pipeline:

### Step 1: Entity Recognition
```python
def _execute_entity_recognition(self, user_query: str, user_intent: str):
    # Use Entity Recognition Agent to identify relevant tables
    entity_results = self.entity_agent.recognize_entities_optimized(
        user_query, user_intent, max_entities=10
    )
    # Extract and validate entity information
    return self._format_entity_results(entity_results)
```

### Step 2: Business Context Gathering
```python
def _gather_business_context(self, user_query: str, entity_results: Dict):
    # Extract entities from recognition results
    entities = self._extract_entities(entity_results)
    # Use Business Context Agent to gather domain knowledge
    business_context = self.business_agent.gather_business_context(
        user_query, entities
    )
    return business_context
```

### Step 3: SQL Generation
```python
def _generate_sql(self, user_query: str, business_context: Dict, entity_context: Dict):
    # Build entity context for SQL generation
    entity_context_for_sql = self._build_entity_context(entity_results)
    # Use NL2SQL Agent to generate validated SQL
    sql_results = self.nl2sql_agent.generate_sql_optimized(
        user_query, business_context, entity_context_for_sql
    )
    return sql_results
```

### Step 4: Response Formatting
```python
def _format_final_response(self, entity_results: Dict, business_context: Dict, sql_results: Dict):
    # Combine all pipeline results into comprehensive response
    return {
        "success": True,
        "pipeline_summary": {...},
        "entity_recognition": {...},
        "business_context": {...},
        "sql_generation": {...}
    }
```

## ‚öôÔ∏è Configuration

### Environment Variables

```env
# Required for pipeline components
OPENAI_API_KEY="your-api-key-here"

# Required for database connection
DATABASE_URL="your-database-connection-string"

# Optional: Pipeline settings
EMBEDDING_BATCH_SIZE="100"
EMBEDDING_MAX_RETRIES="3"

# Optional: Logging configuration
LOG_LEVEL="INFO"
LOG_FILE="pipeline.log"
```

### Initialization Parameters

```python
SQLAgentPipeline(
    indexer_agent,                    # Required: SQLIndexerAgent instance
    database_tools,                   # Required: DatabaseTools instance
    shared_entity_agent=None,         # Optional: Shared EntityRecognitionAgent
    shared_business_agent=None,       # Optional: Shared BusinessContextAgent
    shared_nl2sql_agent=None         # Optional: Shared NL2SQLAgent
)

# Method parameters
process_user_query(
    user_query,                       # Required: Natural language query
    user_intent=None                  # Optional: Specific user intent
)
```

## üéØ Use Cases

### 1. Complete SQL Generation Pipeline

Generate SQL from natural language with full context:

```python
# Process complex business query
result = pipeline.process_user_query(
    user_query="Show me customer retention rates by month",
    user_intent="I need to analyze customer loyalty patterns for Q4 planning"
)

if result["success"]:
    sql = result["sql_generation"]["generated_sql"]
    print(f"Generated SQL: {sql}")
    
    # Check validation results
    validation = result["sql_generation"]["validation"]
    if validation["syntax_valid"] and validation["business_compliant"]:
        print("SQL is valid and business-compliant")
```

### 2. Step-by-Step Pipeline Debugging

Debug individual pipeline steps:

```python
# Step 1: Entity Recognition
entity_results = pipeline._execute_entity_recognition(
    "customer analytics", "I need user behavior data"
)
print(f"Found entities: {entity_results['entities']}")

# Step 2: Business Context
business_context = pipeline._gather_business_context(
    "customer analytics", entity_results
)
print(f"Business concepts: {business_context['matched_concepts']}")

# Step 3: SQL Generation
entity_context = pipeline._build_entity_context(entity_results)
sql_results = pipeline._generate_sql(
    "customer analytics", business_context, entity_context
)
print(f"Generated SQL: {sql_results['generated_sql']}")
```

### 3. Pipeline Performance Monitoring

Monitor pipeline performance and success rates:

```python
# Track pipeline performance
pipeline_results = []
queries = ["customer data", "order analysis", "user metrics"]

for query in queries:
    result = pipeline.process_user_query(query)
    pipeline_results.append({
        "query": query,
        "success": result["success"],
        "pipeline_summary": result.get("pipeline_summary", {}),
        "entities_found": len(result.get("entity_recognition", {}).get("entities", [])),
        "sql_generated": bool(result.get("sql_generation", {}).get("generated_sql"))
    })

# Analyze results
success_rate = sum(1 for r in pipeline_results if r["success"]) / len(pipeline_results)
print(f"Pipeline success rate: {success_rate:.2%}")
```

### 4. Custom Pipeline Configuration

Configure pipeline with specific agent instances:

```python
# Create custom agent instances
from src.agents.entity_recognition import EntityRecognitionAgent
from src.agents.business import BusinessContextAgent
from src.agents.nl2sql import NL2SQLAgent

custom_entity_agent = EntityRecognitionAgent(indexer_agent)
custom_business_agent = BusinessContextAgent(indexer_agent)
custom_nl2sql_agent = NL2SQLAgent(database_tools)

# Create pipeline with custom agents
custom_pipeline = SQLAgentPipeline(
    indexer_agent=indexer_agent,
    database_tools=database_tools,
    shared_entity_agent=custom_entity_agent,
    shared_business_agent=custom_business_agent,
    shared_nl2sql_agent=custom_nl2sql_agent
)
```

## üîç Integration with Other Agents

The Integration Agent orchestrates all other agents:

- **Entity Recognition Agent**: Identifies relevant database entities
- **Business Context Agent**: Gathers domain knowledge and business rules
- **NL2SQL Agent**: Generates and validates SQL queries
- **Indexer Agent**: Provides semantic search capabilities
- **Database Tools**: Enables schema inspection and query execution

## üéñÔ∏è Advanced Features

### Intelligent Error Handling

- Comprehensive error tracking for each pipeline step
- Graceful degradation when individual steps fail
- Detailed error reporting with actionable feedback
- Automatic retry mechanisms for transient failures

### Performance Optimization

- Shared agent instances to minimize resource usage
- Caching of intermediate results for repeated queries
- Parallel processing where possible
- Memory-efficient processing for large datasets

### Pipeline Monitoring

- Real-time pipeline status tracking
- Step-by-step success/failure reporting
- Performance metrics and timing information
- Detailed logging for debugging and optimization

### Response Consistency

- Standardized response format across all pipeline steps
- Comprehensive status reporting for each component
- Detailed validation results and error information
- Structured data for easy integration with other systems

## üìà Performance Characteristics

- **Pipeline Response Time**: 5-15 seconds for complete pipeline execution
- **Entity Recognition**: 1-3 seconds for entity identification
- **Business Context**: 2-5 seconds for context gathering
- **SQL Generation**: 3-8 seconds for query generation and validation
- **Memory Usage**: Efficient memory management with shared components
- **Scalability**: Handles complex queries with multiple entities and business rules

## üö¶ Prerequisites

1. **All Agent Components**: Requires functional Entity Recognition, Business Context, and NL2SQL agents
2. **Database Connection**: Accessible database with schema information
3. **OpenAI API Access**: Valid API key for LLM processing
4. **Vector Indexing**: Functional vector store for semantic search
5. **Dependencies**: All required packages from requirements.txt

## üîß Error Handling

### Common Error Scenarios

1. **Entity Recognition Failures**: No relevant entities found for query
2. **Business Context Errors**: Unable to match business concepts
3. **SQL Generation Issues**: Invalid SQL syntax or business rule violations
4. **Validation Failures**: Security, performance, or compliance issues

### Error Response Handling

```python
# Handle pipeline errors
try:
    result = pipeline.process_user_query("complex query")
    
    if not result["success"]:
        error_step = result.get("step", "unknown")
        error_message = result.get("error", "Unknown error")
        
        if error_step == "entity_recognition":
            print(f"Entity recognition failed: {error_message}")
            # Fallback to manual entity specification
        elif error_step == "business_context":
            print(f"Business context failed: {error_message}")
            # Continue with minimal business context
        elif error_step == "sql_generation":
            print(f"SQL generation failed: {error_message}")
            # Provide alternative query suggestions
            
except Exception as e:
    print(f"Pipeline execution failed: {e}")
    # Implement fallback mechanisms
```

### Recovery Mechanisms

1. **Entity Recognition Fallback**: Use table name matching when semantic search fails
2. **Business Context Fallback**: Continue with minimal context when concept matching fails
3. **SQL Generation Fallback**: Provide simplified queries when complex generation fails
4. **Validation Bypass**: Allow execution with warnings when validation fails

---

The Integration Agent provides the complete orchestration layer for intelligent SQL generation, seamlessly coordinating multiple specialized agents to deliver accurate, validated SQL queries from natural language input with comprehensive business context and domain knowledge. 


================================================
FILE: docs/agents/NL2SQLAgent.md
================================================
# NL2SQL Agent

The NL2SQL Agent is an intelligent component that converts natural language queries into validated SQL statements. It combines LLM-powered query generation with comprehensive validation including syntax checking, security analysis, performance optimization, and business compliance verification.

## üéØ What It Does

The NL2SQL Agent provides sophisticated natural language to SQL conversion:

- **Query Generation**: Converts natural language to T-SQL using GPT-4 with context awareness
- **Parallel Validation**: Simultaneously validates syntax, security, performance, and business compliance
- **Caching System**: Implements intelligent caching to avoid redundant validation for similar queries
- **Query Execution**: Tests generated SQL with sample data and provides execution results
- **Error Recovery**: Handles generation failures with detailed error reporting and suggestions
- **Business Integration**: Incorporates business context and domain knowledge into SQL generation

## üîÑ Processing Flow

```markdown
Natural Language Query ‚Üí Context Building ‚Üí LLM Generation ‚Üí SQL Extraction ‚Üí Parallel Validation ‚Üí Query Execution ‚Üí Final Response
```

1. **Context Building**: Combines user query with business context and entity information
2. **LLM Generation**: Uses GPT-4 to generate SQL with schema and business rules
3. **SQL Extraction**: Extracts clean SQL from LLM response using multiple patterns
4. **Parallel Validation**: Simultaneously validates syntax, security, performance, and business rules
5. **Query Execution**: Tests SQL with sample data and provides execution statistics
6. **Response Formatting**: Returns comprehensive results with validation details

## üöÄ Usage Examples

### Command Line Interface

```bash
# Generate SQL from natural language
python main.py --nl2sql "Show me customer orders from last month"

# Generate SQL with business context
python main.py --nl2sql "customer analytics" --context "business_rules.yaml"

# Generate SQL with specific entities
python main.py --nl2sql "user metrics" --entities "users,orders,profiles"
```

### Programmatic Usage

```python
from src.agents.nl2sql import NL2SQLAgent
from src.agents.factory import agent_factory

# Get NL2SQL agent from factory
nl2sql_agent = agent_factory.get_nl2sql_agent()

# Generate SQL with business context
business_context = {
    "matched_concepts": [
        {"name": "customer_analytics", "instructions": "Use customer_id for joins"}
    ],
    "business_instructions": [
        {"concept": "customer_analytics", "instructions": "Filter by date ranges"}
    ]
}

entity_context = {
    "entities": ["customers", "orders"],
    "entity_descriptions": {
        "customers": "Customer account information",
        "orders": "Customer order data"
    },
    "confidence_scores": {"customers": 0.9, "orders": 0.8}
}

result = nl2sql_agent.generate_sql_optimized(
    user_query="Show me customer orders from last month",
    business_context=business_context,
    entity_context=entity_context
)

if result["success"]:
    print(f"Generated SQL: {result['generated_sql']}")
    print(f"Validation: {result['validation']}")
    print(f"Execution: {result['query_execution']}")
else:
    print(f"Generation failed: {result['error']}")
```

## üìä Response Structure

### Successful SQL Generation Response

```json
{
  "success": true,
  "generated_sql": "SELECT c.customer_name, o.order_date, o.total_amount FROM customers c JOIN orders o ON c.customer_id = o.customer_id WHERE o.order_date >= DATEADD(month, -1, GETDATE()) ORDER BY o.order_date DESC",
  "validation": {
    "syntax_valid": true,
    "business_compliant": true,
    "security_valid": true,
    "performance_issues": []
  },
  "query_execution": {
    "success": true,
    "total_rows": 150,
    "returned_rows": 150,
    "truncated": false,
    "sample_data": {
      "sample_rows": [
        {"customer_name": "John Doe", "order_date": "2024-01-15", "total_amount": 299.99},
        {"customer_name": "Jane Smith", "order_date": "2024-01-14", "total_amount": 149.50}
      ],
      "columns": ["customer_name", "order_date", "total_amount"],
      "numeric_stats": {
        "total_amount": {
          "min": 25.00,
          "max": 599.99,
          "avg": 187.45
        }
      }
    }
  },
  "is_valid": true
}
```

### Validation Error Response

```json
{
  "success": true,
  "generated_sql": "SELECT * FROM customers WHERE customer_id = 123",
  "validation": {
    "syntax_valid": true,
    "business_compliant": false,
    "security_valid": false,
    "performance_issues": ["No WHERE clause optimization", "Missing indexes"]
  },
  "query_execution": {
    "success": false,
    "error": "Access denied: Insufficient permissions"
  },
  "is_valid": false
}
```

### Generation Error Response

```json
{
  "success": false,
  "error": "No valid SQL generated from LLM response",
  "generated_sql": "",
  "is_valid": false
}
```

## üß† Processing Algorithm

The NL2SQL Agent implements a sophisticated multi-stage processing pipeline:

### Stage 1: Context Building
```python
def _build_query_prompt(self, user_query: str, business_context: Dict, entity_context: Dict):
    # Combine user query with business rules and entity information
    schema_info = self._format_schema_info(entity_context.get("table_schemas", {}))
    business_instructions = business_context.get("business_instructions", [])
    
    return f"""
    Generate T-SQL for: {user_query}
    Schema: {schema_info}
    Business Rules: {business_instructions}
    Instructions: Use tools to verify schema and test query
    """
```

### Stage 2: LLM Generation
```python
def generate_sql_optimized(self, user_query: str, business_context: Dict, entity_context: Dict):
    # Build comprehensive prompt
    prompt = self._build_query_prompt(user_query, business_context, entity_context)
    
    # Generate SQL using LLM
    response = self.agent.run(prompt)
    generated_sql = self._extract_sql_from_response(response)
    
    # Check cache for validation results
    cache_key = self._get_cache_key(f"{generated_sql}:{hash(str(business_context))}")
    cached_validation = self._get_cached_result(cache_key)
    
    if cached_validation:
        return self._format_response_with_cache(generated_sql, cached_validation)
    
    # Perform parallel validation
    return self._execute_parallel_validation(generated_sql, business_context)
```

### Stage 3: Parallel Validation
```python
def _execute_parallel_validation(self, sql: str, business_context: Dict):
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        futures = {
            "syntax": executor.submit(self.validate, sql, "syntax"),
            "business": executor.submit(self._check_business_compliance, sql, business_context),
            "security": executor.submit(self.validate, sql, "security"),
            "performance": executor.submit(self.validate, sql, "performance"),
            "execution": executor.submit(self._execute_query_impl, sql, 100)
        }
        
        results = {name: future.result() for name, future in futures.items()}
        return self._format_validation_response(sql, results)
```

### Stage 4: SQL Extraction
```python
def _extract_sql_from_response(self, response) -> Optional[str]:
    # Multiple extraction patterns for robust SQL extraction
    patterns = [
        r'```sql\s*(.*?)\s*```',           # Code blocks
        r'final_answer\s*\(\s*["\']([^"\']*)["\']',  # Tool calls
        r'SELECT.*?;',                      # Direct SQL
        r'FROM.*?WHERE.*?',                # Partial SQL
    ]
    
    for pattern in patterns:
        match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)
        if match:
            return match.group(1).strip()
    
    return None
```

## ‚öôÔ∏è Configuration

### Environment Variables

```env
# Required for LLM processing
OPENAI_API_KEY="your-api-key-here"

# Required for database connection
DATABASE_URL="your-database-connection-string"

# Optional: Caching settings
CACHE_SIZE="50"
CACHE_TTL="3600"

# Optional: Validation settings
VALIDATION_TIMEOUT="30"
MAX_EXECUTION_ROWS="100"

# Optional: Logging configuration
LOG_LEVEL="INFO"
LOG_FILE="nl2sql.log"
```

### Initialization Parameters

```python
NL2SQLAgent(
    database_tools,              # Required: DatabaseTools instance
    shared_llm_model=None        # Optional: Shared LLM model instance
)

# Method parameters
generate_sql_optimized(
    user_query,                  # Required: Natural language query
    business_context,            # Required: Business context dictionary
    entity_context              # Required: Entity context dictionary
)
```

## üéØ Use Cases

### 1. Basic SQL Generation

Generate SQL from simple natural language queries:

```python
# Simple query generation
result = nl2sql_agent.generate_sql_optimized(
    user_query="Show me all customers",
    business_context={},
    entity_context={"entities": ["customers"]}
)

if result["success"]:
    print(f"SQL: {result['generated_sql']}")
    print(f"Valid: {result['is_valid']}")
```

### 2. Complex Business Queries

Generate SQL with business rules and domain knowledge:

```python
# Complex business query
business_context = {
    "matched_concepts": [
        {
            "name": "customer_retention",
            "instructions": "Calculate customer retention rates by month"
        }
    ],
    "business_instructions": [
        {
            "concept": "customer_retention",
            "instructions": "Use customer_id for joins, filter active customers only"
        }
    ]
}

result = nl2sql_agent.generate_sql_optimized(
    user_query="Calculate customer retention rates by month",
    business_context=business_context,
    entity_context={"entities": ["customers", "orders"]}
)
```

### 3. Query Validation and Testing

Validate and test generated SQL:

```python
# Generate and validate SQL
result = nl2sql_agent.generate_sql_optimized(
    user_query="Get top 10 customers by order value",
    business_context={},
    entity_context={"entities": ["customers", "orders"]}
)

# Check validation results
validation = result["validation"]
if validation["syntax_valid"]:
    print("SQL syntax is valid")
if validation["security_valid"]:
    print("SQL passes security checks")
if validation["business_compliant"]:
    print("SQL complies with business rules")

# Check execution results
execution = result["query_execution"]
if execution["success"]:
    print(f"Query returned {execution['total_rows']} rows")
    print(f"Sample data: {execution['sample_data']}")
```

### 4. Caching and Performance

Leverage caching for improved performance:

```python
# First query (will be cached)
result1 = nl2sql_agent.generate_sql_optimized(
    user_query="Show customer orders",
    business_context={},
    entity_context={"entities": ["customers", "orders"]}
)

# Similar query (will use cache)
result2 = nl2sql_agent.generate_sql_optimized(
    user_query="Get customer orders",
    business_context={},
    entity_context={"entities": ["customers", "orders"]}
)

# Clear cache if needed
nl2sql_agent.clear_cache()
```

## üîç Integration with Other Agents

The NL2SQL Agent works seamlessly with other components:

- **Business Context Agent**: Receives business rules and domain knowledge
- **Entity Recognition Agent**: Uses identified entities for context
- **Database Tools**: Executes queries and inspects schema
- **Validation Components**: Performs syntax, security, and performance checks
- **Caching System**: Optimizes performance for repeated queries

## üéñÔ∏è Advanced Features

### Intelligent Caching

- MD5-based cache keys for efficient storage
- Configurable cache size with automatic cleanup
- Cache invalidation based on business context changes
- Performance optimization for repeated queries

### Parallel Validation

- Concurrent validation of syntax, security, performance, and business rules
- ThreadPoolExecutor for efficient parallel processing
- Comprehensive validation reporting with detailed feedback
- Graceful handling of validation failures

### SQL Extraction Patterns

- Multiple regex patterns for robust SQL extraction
- Support for code blocks, tool calls, and direct SQL
- Fallback mechanisms for edge cases
- Detailed logging for extraction debugging

### Query Execution Testing

- Safe query execution with row limits
- Sample data generation with statistics
- Numeric analysis for quantitative results
- Error handling for execution failures

## üìà Performance Characteristics

- **SQL Generation**: 2-5 seconds for typical queries
- **Parallel Validation**: 1-3 seconds for comprehensive validation
- **Query Execution**: 0.5-2 seconds for sample data retrieval
- **Cache Hit Rate**: 60-80% for similar queries
- **Memory Usage**: Efficient caching with automatic cleanup
- **Scalability**: Handles complex queries with multiple joins and conditions

## üö¶ Prerequisites

1. **Database Connection**: Accessible database with schema information
2. **OpenAI API Access**: Valid API key for LLM processing
3. **Database Tools**: Functional database tools for query execution
4. **Validation Components**: Syntax, security, and business validators
5. **Dependencies**: All required packages from requirements.txt

## üîß Error Handling

### Common Error Scenarios

1. **LLM Generation Failures**: Invalid responses, API errors, timeout issues
2. **SQL Extraction Failures**: Unable to extract SQL from LLM response
3. **Validation Errors**: Syntax errors, security violations, business rule violations
4. **Execution Failures**: Database connection issues, permission problems

### Error Response Handling

```python
# Handle generation errors
try:
    result = nl2sql_agent.generate_sql_optimized(
        user_query="complex query",
        business_context={},
        entity_context={}
    )
    
    if not result["success"]:
        error = result.get("error", "Unknown error")
        
        if "No valid SQL generated" in error:
            print("LLM failed to generate valid SQL")
            # Try with simplified prompt
        elif "Validation failed" in error:
            print("Generated SQL failed validation")
            # Provide alternative query suggestions
        elif "Execution failed" in error:
            print("Query execution failed")
            # Check database connection and permissions
            
except Exception as e:
    print(f"NL2SQL processing failed: {e}")
    # Implement fallback mechanisms
```

### Recovery Mechanisms

1. **LLM Fallback**: Retry with simplified prompts when complex generation fails
2. **Extraction Fallback**: Use multiple patterns when primary extraction fails
3. **Validation Bypass**: Allow execution with warnings when validation fails
4. **Cache Invalidation**: Clear cache when validation results are inconsistent

---

The NL2SQL Agent provides sophisticated natural language to SQL conversion with comprehensive validation, intelligent caching, and robust error handling, making it the core component for intelligent query generation in the SQL Documentation suite. 


================================================
FILE: docs/concepts/agents.md
================================================
# Agents

Agents are the core components of smol-sql-agents that handle different types of natural language to SQL conversions and database interactions.

## Agent Types

### 1. Base Agent

The foundation for all agents, providing common functionality.

**Key Features:**

- Shared initialization
- Common error handling
- Logging and monitoring
- Base prompt templates

### 2. NL2SQL Agent

Converts natural language queries into SQL.

**Usage Example:**

```python
from smol_sql_agents import NL2SQLAgent

agent = NL2SQLAgent()
result = agent.execute(
    query="Show me all orders over $1000 from last month",
    schema=database_schema
)
```

**Features:**

- Natural language understanding
- SQL generation
- Query validation
- Schema-aware query building

### 3. Business Agent

Specialized in generating business insights from data.

**Usage Example:**

```python
from smol_sql_agents import BusinessAgent

agent = BusinessAgent()
insights = agent.analyze(
    question="What are the sales trends for this quarter?",
    schema=database_schema
)
```

**Features:**

- Business metric calculation
- Trend analysis
- KPI reporting
- Data visualization suggestions

### 4. Analytics Agent

Performs complex data analysis tasks.

**Usage Example:**

```python
from smol_sql_agents import AnalyticsAgent

agent = AnalyticsAgent()
analysis = agent.analyze(
    question="Perform cohort analysis on our user base",
    schema=database_schema
)
```

**Features:**

- Statistical analysis
- Pattern recognition
- Predictive modeling
- Data segmentation

## Agent Lifecycle

1. **Initialization**
   - Load configuration
   - Initialize LLM client
   - Set up database connections

2. **Execution**
   - Parse user query
   - Generate SQL
   - Execute query
   - Process results

3. **Response**
   - Format output
   - Add explanations
   - Include metadata

## Custom Agents

You can create custom agents by extending the `BaseAgent` class:

```python
from smol_sql_agents import BaseAgent

class CustomAgent(BaseAgent):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.agent_type = "custom"
    
    async def execute(self, query: str, **kwargs):
        # Custom implementation
        pass
```

## Agent Configuration

Agents can be configured using environment variables or programmatically:

```python
from smol_sql_agents import NL2SQLAgent

agent = NL2SQLAgent(
    model_name="gpt-4",
    temperature=0.1,
    max_retries=3,
    timeout=30
)
```

## Error Handling

Agents implement robust error handling:

- Invalid SQL generation
- Database connection issues
- Rate limiting
- Timeout handling

## Performance Considerations

- **Caching**: Query results and embeddings are cached
- **Batching**: Multiple operations in single transactions
- **Async**: Non-blocking operations for better throughput

## Best Practices

1. **Agent Selection**: Choose the most specific agent for your task
2. **Error Handling**: Implement appropriate error handling for agent operations
3. **Monitoring**: Track agent performance and usage
4. **Testing**: Test agents with various query types and edge cases

## Related Documentation

- [Architecture Overview](./architecture.md)
- [Prompt Engineering](./prompt_engineering.md)
- [API Reference](../api/agents/README.md)



================================================
FILE: docs/concepts/architecture.md
================================================
# Architecture Overview

This document provides a high-level overview of the smol-sql-agents architecture, its components, and how they interact.

## System Architecture

```mermaid
graph TD
    A[User Query] --> B[Agent Manager]
    B --> C[Agent Selection]
    C --> D[NL2SQL Agent]
    C --> E[Business Agent]
    C --> F[Analytics Agent]
    D --> G[Query Generator]
    E --> G
    F --> G
    G --> H[Database]
    H --> I[Result Processor]
    I --> J[Response Formatter]
    J --> K[User]
    
    M[Vector Store] <--> G
    N[Prompt Templates] <--> D & E & F
    O[Validation] <--> G
```

## Core Components

### 1. Agent Layer

- **Agent Manager**: Routes requests to appropriate agents
- **Specialized Agents**:
  - `NL2SQLAgent`: Converts natural language to SQL
  - `BusinessAgent`: Provides business insights
  - `AnalyticsAgent`: Handles data analysis tasks

### 2. Data Access Layer

- **Database Inspector**: Schema introspection and metadata
- **Query Executor**: Handles database operations
- **Connection Pool**: Manages database connections

### 3. LLM Integration

- **Prompt Management**: Template handling and rendering
- **Model Interface**: Abstraction over LLM providers
- **Response Processing**: Parsing and validating LLM outputs

### 4. Vector Operations

- **Embeddings**: Text to vector conversion
- **Similarity Search**: Finding similar content
- **Vector Store**: Storage and retrieval of embeddings

## Data Flow

1. **Request Handling**:
   - User submits a natural language query
   - Request is authenticated and validated
   - Appropriate agent is selected based on query type

2. **Query Processing**:
   - Agent generates SQL using LLM
   - Query is validated and optimized
   - Database connection is acquired from pool

3. **Execution**:
   - Query is executed against the database
   - Results are processed and formatted
   - Vector store is updated if needed

4. **Response Generation**:
   - Results are formatted for presentation
   - Additional context is added (explanations, suggestions)
   - Response is returned to the user

## Scalability Considerations

- **Horizontal Scaling**: Stateless design allows adding more instances
- **Caching**: Query results and embeddings are cached when possible
- **Async Operations**: Non-blocking I/O for database and LLM calls
- **Connection Pooling**: Efficient database connection management

## Security Model

- **Authentication**: API key or OAuth2
- **Authorization**: Role-based access control
- **Data Protection**: Encryption at rest and in transit
- **Audit Logging**: All operations are logged

## Error Handling

- **Validation Errors**: Invalid inputs are caught early
- **Retry Logic**: Transient failures are automatically retried
- **Fallback Mechanisms**: Alternative strategies when primary approach fails
- **Detailed Logging**: Comprehensive error context for debugging

## Performance Considerations

- **Query Optimization**: SQL optimization before execution
- **Batch Processing**: Handling multiple operations in single transactions
- **Resource Management**: Efficient memory and connection usage
- **Monitoring**: Performance metrics and health checks

## Integration Points

- **Databases**: PostgreSQL, MySQL, SQLite
- **Vector Stores**: Pinecone, Weaviate, FAISS
- **LLM Providers**: OpenAI, Anthropic, Local models
- **Monitoring**: Prometheus, Grafana, OpenTelemetry



================================================
FILE: docs/concepts/prompt_engineering.md
================================================

# Prompt Engineering

This guide covers prompt engineering best practices and customization options in smol-sql-agents.

## Overview

Prompts are templates that guide the LLM in generating SQL queries from natural language. The system includes a flexible prompt management system that allows for customization and versioning of prompts.

## Prompt Structure

### Core Components

1. **System Message**: Sets the behavior and role of the AI
2. **Schema Context**: Database schema information
3. **Examples**: Few-shot learning examples
4. **User Query**: The natural language input
5. **Constraints**: Rules and limitations for the output

## Default Prompts

The system includes several built-in prompt templates:

1. **sql_generation**: Converts natural language to SQL
2. **query_understanding**: Improves query interpretation
3. **result_explanation**: Explains query results
4. **error_recovery**: Handles and recovers from errors

## Customizing Prompts

### Method 1: Template Files

Create a new template file in the `prompts/templates` directory:

```jinja
# prompts/templates/custom_sql_generation.txt
You are a SQL expert. Given the following database schema:

{{ schema }}

Generate a SQL query to: {{ user_query }}

Rules:
- Only return the SQL query
- Use proper indentation
- Include comments for complex logic
```

### Method 2: Programmatic Override

```python
from smol_sql_agents.prompts import PromptManager

# Initialize with custom template directory
pm = PromptManager(template_dir="path/to/custom/templates")

# Or override specific templates
pm.register_template(
    name="custom_sql_generation",
    template_path="path/to/template.txt"
)
```

## Best Practices

### 1. Be Explicit

- Clearly specify the output format
- Include examples of desired behavior
- Define constraints and rules

### 2. Schema Context

- Include relevant table and column information
- Add descriptions for ambiguous column names
- Specify primary and foreign key relationships

### 3. Error Prevention

- Add validation rules in the prompt
- Include common error cases in examples
- Specify fallback behavior

## Advanced Techniques

### Dynamic Prompt Construction

```python
from string import Template

def build_dynamic_prompt(schema, query, examples=None):
    template = """
    Database Schema:
    $schema
    
    Examples:
    $examples
    
    Generate SQL for: $query
    """
    
    return Template(template).substitute(
        schema=schema,
        examples=examples or "No examples provided",
        query=query
    )
```

### Few-shot Learning

Include multiple examples in your prompt:

```markdown
Example 1:
Question: Find all active users
SQL: SELECT * FROM users WHERE status = 'active';

Example 2:
Question: Get total sales by product category
SQL: SELECT category, SUM(amount) FROM sales GROUP BY category;
```

## Testing Prompts

1. **Unit Testing**
   - Test with various query types
   - Verify SQL syntax correctness
   - Check handling of edge cases

2. **A/B Testing**
   - Compare performance of different prompts
   - Measure success rates
   - Collect user feedback

## Version Control

- Store prompt templates in version control
- Use semantic versioning for prompt changes
- Maintain a changelog of prompt modifications

## Prompt Architecture

### Input/Output Analysis

#### 1. SQL Generation Prompt

**Location**: `src/agents/nl2sql.py`  
**Purpose**: Converts natural language to SQL queries

**Inputs**:

- `user_query`: Natural language question/request
- `business_context`: Business rules and instructions
- `entity_context`: Database schema information

**Output Format**:

```typescript
{
  "sql": "SELECT ...",  // Generated SQL query
  "validation": {
    "is_valid": boolean,
    "errors": string[]
  }
}
```

**Key Features**:

- Schema-aware query generation
- Business rule integration
- Built-in validation
- Tool usage instructions

#### 2. Table Documentation Prompt

**Location**: `src/agents/core.py`  
**Purpose**: Generates documentation for database tables

**Inputs**:

- `table_name`: Name of the table to document
- `schema_data`: Table structure from database inspection

**Output Format**:

```typescript
{
  "business_purpose": string,
  "schema_data": {
    "table_name": string,
    "columns": string[]
  }
}
```

**Key Features**:

- Automated schema analysis
- Business context inference
- Structured JSON output
- Schema validation

#### 3. Indexing Instruction Prompt

**Location**: `src/agents/indexer.py`  
**Purpose**: Processes natural language indexing commands

**Inputs**:

- `instruction`: Natural language command
- `documentation_data`: Content to be indexed

**Output Format**:

```typescript
{
  "operation": "index" | "search" | "status",
  "target": "table" | "relationship" | "all",
  "data": any  // Operation-specific data
}
```

**Key Features**:

- Natural language command processing
- Multiple operation support
- Structured response format
- Error handling

### System Architecture with Prompt Flow

```mermaid
flowchart TD
    %% User Interaction
    UI[User Interface] -->|Natural Language Query| NL2SQL
    
    %% Main Prompt Processing
    subgraph "Prompt Processing"
        NL2SQL[NL2SQL Agent] -->|Generate SQL| SQL[SQL Generation]
        SQL -->|Validate| VALID[SQL Validation]
        VALID -->|Execute| DB[(Database)]
        
        %% Context Management
        CTX[Context Manager] <--> NL2SQL
        CTX <-->|Update| HIST[Conversation History]
        
        %% Template Application
        SQL -->|Apply Template| TMPL[Prompt Template]
        TMPL -->|Fill Variables| SQL
    end
    
    %% Documentation Flow
    DB -->|Schema Changes| DOC[Documentation Agent]
    DOC -->|Generate| DOCTMPL[Doc Template]
    DOCTMPL -->|Structured Data| VEC[Vector Store]
    
    %% Search and Retrieval
    UI -->|Search| SRCH[Search Agent]
    SRCH -->|Query| VEC
    VEC -->|Context| SRCH
    
    %% Validation Services
    VALID -->|Check Syntax| SYN[Syntax Validator]
    VALID -->|Check Business Rules| BIZ[Business Validator]
    
    %% Feedback Loop
    DB -->|Results| UI
    SRCH -->|Results| UI
    
    %% Error Handling
    VALID -->|Error| ERROR[Error Handler]
    ERROR -->|Retry| SQL
    
    %% Styling
    classDef agent fill:#e1f5fe,stroke:#01579b
    classDef storage fill:#e8f5e9,stroke:#2e7d32
    classDef service fill:#f3e5f5,stroke:#6a1b9a
    classDef process fill:#fff3e0,stroke:#e65100
    
    class NL2SQL,DOC,SRCH agent
    class DB,VEC storage
    class SYN,BIZ,ERROR service
    class TMPL,DOCTMPL,CTX,HIST process
```

### Component Interactions

1. **NL2SQL Agent**
   - Processes natural language queries
   - Coordinates SQL generation and validation
   - Handles query execution and result formatting

2. **Documentation Agent**
   - Manages database schema documentation
   - Generates and updates table documentation
   - Maintains schema versioning

3. **Indexing Agent**
   - Handles vector embeddings
   - Manages document search
   - Maintains search indexes

4. **Support Services**
   - Business validation rules
   - T-SQL syntax validation
   - Documentation storage and retrieval

## Implementation Details

### Prompt Templates

1. **SQL Generation Template**

```sql
-- Generate T-SQL for: {user_query}
-- Schema: {schema_info}
-- Business Rules: {business_rules}

-- Instructions:
-- 1. Verify schema with get_table_schema_unified_tool()
-- 2. Test with execute_query_and_return_results()
-- 3. Return final SQL using final_answer()
```

2. **Documentation Template**

```json
{
  "instruction": "Document table: {table_name}",
  "schema": "{schema_data}",
  "requirements": [
    "business_purpose: Clear table purpose",
    "schema_data: Table structure"
  ]
}
```

3. **Indexing Template**

```json
{
  "instruction": "Process: {instruction}",
  "operations": [
    "index: Index table/relationship",
    "search: Search documentation",
    "status: Get status"
  ],
  "response_format": "JSON"
}
```

### Prompt Examples

#### SQL Generation Example

**Input**:

```python
user_query = "Show me all active customers"
```

**Prompt**:

```sql
-- Generate T-SQL for: Show me all active customers
-- Schema: customers(id, name, email, status), orders(id, customer_id, amount, order_date)
-- Business Rules: Active customers have status = 'active'

-- Instructions:
-- 1. Verify schema with get_table_schema_unified_tool()
-- 2. Test with execute_query_and_return_results()
-- 3. Return final SQL using final_answer()
```

#### Table Documentation Example

**Input**:

```python
table_name = "customers"
```

**Prompt**:

```json
{
  "instruction": "Document table: customers",
  "schema": {
    "columns": ["id", "name", "email", "status"],
    "primary_key": ["id"],
    "foreign_keys": []
  },
  "requirements": [
    "business_purpose: Clear table purpose",
    "schema_data: Table structure"
  ]
}
```

#### Indexing Example

**Input**:

```python
instruction = "Index the customers table with their latest order information"
```

**Prompt**:

```json
{
  "instruction": "Index the customers table with their latest order information",
  "operations": [
    "index: Index table/relationship",
    "search: Search documentation",
    "status: Get status"
  ],
  "response_format": "JSON"
}
```

## Common Pitfalls

1. **Overly Complex Prompts**
   - Keep prompts focused and concise
   - Break down complex tasks into smaller prompts

2. **Insufficient Context**
   - Include all necessary schema information
   - Provide clear examples

3. **Vague Instructions**
   - Be specific about output formats
   - Define expected behavior for edge cases

## Related Documentation

- [Agents](./agents.md)
- [API Reference](../api/prompts/README.md)
- [Troubleshooting Guide](../troubleshooting/common_issues.md#prompt-related-issues)



================================================
FILE: docs/concepts/vector_database.md
================================================
# Vector Database

This document explains the vector database capabilities in smol-sql-agents, including embedding generation, storage, and similarity search.

## Overview

The vector database enables semantic search and similarity comparisons by storing and querying vector embeddings of text data.

## Key Components

### 1. Embeddings Client

Handles text-to-vector conversion using various embedding models.

**Features:**

- Multiple model support (OpenAI, Hugging Face, etc.)
- Batch processing
- Caching layer
- Normalization and preprocessing

### 2. Vector Store

Manages storage and retrieval of vector embeddings.

**Supported Backends:**

- In-memory (for development)
- Pinecone
- Weaviate
- FAISS
- Chroma

### 3. Search Tools

Provides similarity search and nearest neighbor lookups.

**Search Types:**

- Exact search
- Approximate Nearest Neighbor (ANN)
- Hybrid search (combining vector and metadata)

## Usage Examples

### Initialization

```python
from smol_sql_agents.vector import VectorStore, EmbeddingsClient

# Initialize embeddings client
embeddings = EmbeddingsClient(model="text-embedding-ada-002")

# Initialize vector store
vector_store = VectorStore(
    store_type="pinecone",
    index_name="my-index",
    dimension=1536  # Depends on the embedding model
)
```

### Generating and Storing Embeddings

```python
# Generate embeddings
texts = ["First document", "Second document"]
embeddings_list = await embeddings.embed_documents(texts)

# Store in vector store
ids = ["doc1", "doc2"]
metadata = [{"source": "example"}, {"source": "example"}]
await vector_store.add_vectors(ids, embeddings_list, metadata)
```

### Similarity Search

```python
# Find similar documents
query = "Find similar documents"
query_embedding = await embeddings.embed_query(query)
results = await vector_store.similarity_search(
    query_embedding=query_embedding,
    k=5  # Number of results
)
```

## Integration with SQL

### Vector-Enhanced Queries

```python
from smol_sql_agents import NL2SQLAgent

agent = NL2SQLAgent()
result = agent.execute(
    query="Find products similar to 'wireless headphones' in the electronics category",
    schema=database_schema,
    vector_search=True
)
```

## Performance Considerations

### Indexing

- **Batch Processing**: Process documents in batches for better performance
- **Parallel Processing**: Use async/await for concurrent operations
- **Incremental Updates**: Update indexes incrementally when possible

### Query Optimization

- **Filtering**: Apply metadata filters before vector search
- **Pagination**: Implement result pagination for large result sets
- **Caching**: Cache frequent queries and their results

## Best Practices

1. **Dimensionality**: Choose appropriate embedding dimensions for your use case
2. **Normalization**: Normalize vectors for consistent similarity scores
3. **Metadata**: Include rich metadata for better filtering
4. **Monitoring**: Track query performance and index size

## Troubleshooting

### Common Issues

1. **Dimension Mismatch**
   - Verify embedding model dimensions match vector store configuration
   - Check for model version changes

2. **Performance Problems**
   - Optimize batch sizes
   - Consider using approximate nearest neighbor search for large datasets

3. **Memory Usage**
   - Monitor vector store memory consumption
   - Use disk-based storage for large datasets

## Related Documentation

- [Architecture Overview](./architecture.md)
- [API Reference](../api/vector/README.md)
- [Performance Tuning Guide](../troubleshooting/performance_tuning.md)



================================================
FILE: docs/contributing/development.md
================================================
# Development Setup

This guide will help you set up a development environment for smol-sql-agents.

## Prerequisites

- Python 3.8+
- Git
- Poetry (for dependency management)
- Docker (for local database testing)

## Setup Instructions

### 1. Fork and Clone the Repository

```bash
git clone https://github.com/yourusername/smol-sql-agents.git
cd smol-sql-agents
```

### 2. Set Up Python Environment

```bash
# Install Poetry if you haven't already
pip install poetry

# Install dependencies
poetry install --with dev

# Activate the virtual environment
poetry shell
```

### 3. Set Up Pre-commit Hooks

```bash
pre-commit install
```

### 4. Configure Environment Variables

Create a `.env` file in the project root:

```ini
# Database
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/test_db

# OpenAI
OPENAI_API_KEY=your_api_key_here

# Logging
LOG_LEVEL=DEBUG
```

### 5. Start Local Services

```bash
# Start PostgreSQL in Docker
docker-compose up -d postgres

# Run database migrations
alembic upgrade head
```

## Running Tests

```bash
# Run all tests
pytest

# Run tests with coverage
pytest --cov=src --cov-report=term-missing

# Run a specific test file
pytest tests/test_agents.py -v
```

## Code Style

We use:
- Black for code formatting
- isort for import sorting
- flake8 for linting
- mypy for type checking

```bash
# Format code
black .

# Sort imports
isort .

# Run linter
flake8

# Type checking
mypy src
```

## Documentation

We use MkDocs for documentation:

```bash
# Install docs dependencies
pip install -r docs/requirements.txt

# Serve docs locally
mkdocs serve
```

## Making Changes

1. Create a new branch:
   ```bash
   git checkout -b feature/your-feature-name
   ```

2. Make your changes and commit them:
   ```bash
   git add .
   git commit -m "Add your commit message"
   ```

3. Push your changes:
   ```bash
   git push origin feature/your-feature-name
   ```

4. Open a pull request on GitHub

## Debugging

### VS Code Launch Configuration

Add this to your `.vscode/launch.json`:

```json
{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python: Current File",
            "type": "python",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal",
            "justMyCode": true,
            "envFile": "${workspaceFolder}/.env"
        }
    ]
}
```

### Common Issues

1. **Database Connection Issues**
   - Verify PostgreSQL is running
   - Check `.env` file for correct credentials
   - Run `alembic upgrade head` to apply migrations

2. **Dependency Issues**
   - Run `poetry install` to ensure all dependencies are installed
  - Delete `poetry.lock` and run `poetry install` if needed

## Performance Profiling

```python
import cProfile
import pstats

def profile_function():
    # Your code here
    pass

if __name__ == "__main__":
    with cProfile.Profile() as pr:
        profile_function()
    
    stats = pstats.Stats(pr)
    stats.sort_stats(pstats.SortKey.TIME)
    stats.print_stats(10)  # Show top 10 time-consuming functions
```

## Release Process

1. Update version in `pyproject.toml`
2. Update `CHANGELOG.md`
3. Create a release tag:
   ```bash
   git tag -a v1.0.0 -m "Version 1.0.0"
   git push origin v1.0.0
   ```
4. Create a GitHub release with release notes

## Getting Help

- Check the [GitHub Issues](https://github.com/yourusername/smol-sql-agents/issues)
- Join our [Discord/Slack] channel
- Contact the maintainers at [email/contact info]



================================================
FILE: docs/database/DatabaseInspector.md
================================================
# Database Inspector

The Database Inspector is a SQLAlchemy-based component that provides comprehensive database schema introspection capabilities. It automatically discovers and analyzes database structures, extracting table information, column details, and relationship mappings while filtering out system-specific tables.

## üéØ What It Does

The Database Inspector performs automated database schema discovery and analysis:

- **Schema Discovery**: Automatically identifies all user-defined tables in the database
- **Column Analysis**: Extracts detailed column information including types, constraints, and nullability
- **Primary Key Detection**: Identifies primary key columns and constraints for each table
- **Relationship Mapping**: Discovers all foreign key relationships across the database
- **System Table Filtering**: Excludes database system tables from analysis
- **Cross-Platform Support**: Works with multiple database systems through SQLAlchemy

## üîÑ Inspection Flow

```markdown
Database Connection ‚Üí SQLAlchemy Engine ‚Üí Schema Reflection ‚Üí Metadata Extraction ‚Üí Filtered Results
```

1. **Connection Establishment**: Creates SQLAlchemy engine from DATABASE_URL
2. **Schema Reflection**: Uses SQLAlchemy inspector to reflect database metadata
3. **Table Discovery**: Identifies all tables and filters out system tables
4. **Column Analysis**: Extracts detailed schema information for each table
5. **Relationship Discovery**: Maps foreign key constraints across all tables
6. **Result Formatting**: Structures data for consumption by documentation agents

## üöÄ Usage Examples

### Command Line Interface

```bash
# Database inspection is automatically performed during documentation generation
python main.py

# Resume functionality relies on previous inspection results
python main.py --resume
```

### Programmatic Usage

```python
from src.database.inspector import DatabaseInspector

# Initialize the inspector
inspector = DatabaseInspector()

# Get all user-defined table names
tables = inspector.get_all_table_names()
print(f"Found {len(tables)} user tables: {tables}")

# Get detailed schema for a specific table
user_schema = inspector.get_table_schema("users")
print(f"Users table has {len(user_schema['columns'])} columns")

# Discover all foreign key relationships
relationships = inspector.get_all_foreign_key_relationships()
print(f"Found {len(relationships)} foreign key relationships")

# Example: Process all tables
for table_name in tables:
    schema = inspector.get_table_schema(table_name)
    print(f"\nTable: {schema['table_name']}")
    
    # Show primary keys
    pk_columns = [col['name'] for col in schema['columns'] if col['primary_key']]
    print(f"Primary Keys: {pk_columns}")
    
    # Show column details
    for column in schema['columns']:
        nullable = "NULL" if column['nullable'] else "NOT NULL"
        print(f"  {column['name']}: {column['type']} {nullable}")
```

## üìä Response Structure

### Table Names Response

```python
[
    "users",
    "orders", 
    "products",
    "categories",
    "order_items"
]
```

### Table Schema Response

```json
{
  "table_name": "users",
  "columns": [
    {
      "name": "id",
      "type": "INTEGER",
      "nullable": false,
      "primary_key": true,
      "default": null
    },
    {
      "name": "username",
      "type": "VARCHAR(50)",
      "nullable": false,
      "primary_key": false,
      "default": null
    },
    {
      "name": "email",
      "type": "VARCHAR(255)",
      "nullable": false,
      "primary_key": false,
      "default": null
    },
    {
      "name": "created_at",
      "type": "TIMESTAMP",
      "nullable": true,
      "primary_key": false,
      "default": "CURRENT_TIMESTAMP"
    }
  ]
}
```

### Foreign Key Relationships Response

```json
[
  {
    "constrained_table": "orders",
    "constrained_columns": ["user_id"],
    "referred_table": "users",
    "referred_columns": ["id"]
  },
  {
    "constrained_table": "order_items", 
    "constrained_columns": ["order_id"],
    "referred_table": "orders",
    "referred_columns": ["id"]
  },
  {
    "constrained_table": "order_items",
    "constrained_columns": ["product_id"],
    "referred_table": "products", 
    "referred_columns": ["id"]
  }
]
```

## ‚öôÔ∏è Configuration

### Environment Variables

```env
# Required: Database connection string
DATABASE_URL="mssql+pyodbc:///?odbc_connect=DRIVER={ODBC Driver 17 for SQL Server};SERVER=localhost\SQLEXPRESS;DATABASE=master;Trusted_Connection=yes"

# Alternative examples for different databases:
# PostgreSQL
# DATABASE_URL="postgresql://user:password@localhost:5432/database"

# MySQL  
# DATABASE_URL="mysql+pymysql://user:password@localhost:3306/database"

# SQLite
# DATABASE_URL="sqlite:///path/to/database.db"

# Optional: Logging configuration
LOG_LEVEL="INFO"
LOG_FILE="inspector.log"
```

### Supported Database Systems

- **SQL Server**: Via `pyodbc` driver with ODBC connection strings
- **PostgreSQL**: Via `psycopg2-binary` driver
- **MySQL**: Via `pymysql` driver  
- **SQLite**: Built-in support
- **Oracle**: Via `cx_Oracle` driver (additional setup required)

## üéØ Use Cases

### 1. Database Discovery

Automatically discover and catalog database structures:

```python
inspector = DatabaseInspector()

# Get overview of database
tables = inspector.get_all_table_names()
relationships = inspector.get_all_foreign_key_relationships()

print(f"Database contains:")
print(f"  - {len(tables)} user tables")
print(f"  - {len(relationships)} foreign key relationships")

# Analyze table complexity
for table in tables:
    schema = inspector.get_table_schema(table)
    column_count = len(schema['columns'])
    pk_count = sum(1 for col in schema['columns'] if col['primary_key'])
    
    print(f"  {table}: {column_count} columns, {pk_count} primary keys")
```

### 2. Schema Analysis

Perform detailed analysis of table structures:

```python
# Analyze column types across database
type_distribution = {}
nullable_stats = {"nullable": 0, "not_nullable": 0}

for table_name in inspector.get_all_table_names():
    schema = inspector.get_table_schema(table_name)
    
    for column in schema['columns']:
        # Track column types
        col_type = str(column['type']).split('(')[0]  # Remove length specifications
        type_distribution[col_type] = type_distribution.get(col_type, 0) + 1
        
        # Track nullability
        if column['nullable']:
            nullable_stats["nullable"] += 1
        else:
            nullable_stats["not_nullable"] += 1

print("Column type distribution:", type_distribution)
print("Nullability statistics:", nullable_stats)
```

### 3. Relationship Analysis

Map and analyze database relationships:

```python
# Build relationship graph
relationships = inspector.get_all_foreign_key_relationships()

# Group by parent table
parent_tables = {}
child_tables = {}

for rel in relationships:
    parent = rel['referred_table']
    child = rel['constrained_table']
    
    if parent not in parent_tables:
        parent_tables[parent] = []
    parent_tables[parent].append(child)
    
    if child not in child_tables:
        child_tables[child] = []
    child_tables[child].append(parent)

# Find tables with most relationships
most_connected = max(parent_tables.items(), key=lambda x: len(x[1]))
print(f"Most connected table: {most_connected[0]} (parent to {len(most_connected[1])} tables)")

# Find orphan tables (no relationships)
all_tables = set(inspector.get_all_table_names())
connected_tables = set(parent_tables.keys()) | set(child_tables.keys())
orphan_tables = all_tables - connected_tables
print(f"Orphan tables (no relationships): {orphan_tables}")
```

### 4. Data Quality Assessment

Assess schema quality and design patterns:

```python
# Check for common design patterns and potential issues
issues = []

for table_name in inspector.get_all_table_names():
    schema = inspector.get_table_schema(table_name)
    
    # Check for primary key
    has_pk = any(col['primary_key'] for col in schema['columns'])
    if not has_pk:
        issues.append(f"Table '{table_name}' has no primary key")
    
    # Check for common audit columns
    column_names = [col['name'].lower() for col in schema['columns']]
    if 'created_at' not in column_names and 'created_date' not in column_names:
        issues.append(f"Table '{table_name}' missing created timestamp")
    
    # Check for ID column naming
    id_columns = [col for col in schema['columns'] if col['name'].lower() == 'id']
    if not id_columns and has_pk:
        pk_columns = [col['name'] for col in schema['columns'] if col['primary_key']]
        if len(pk_columns) == 1 and not pk_columns[0].endswith('_id'):
            issues.append(f"Table '{table_name}' has unusual primary key name: {pk_columns[0]}")

print("Schema quality issues found:")
for issue in issues:
    print(f"  - {issue}")
```

## üîç Integration with Other Components

The Database Inspector provides foundational data for the entire system:

- **Core Agent**: Uses inspection results to identify tables and relationships for documentation
- **Documentation Store**: Receives inspection data for session initialization
- **Batch Manager**: Processes tables and relationships discovered by the inspector
- **Entity Recognition**: Searches among tables identified by the inspector

## üéñÔ∏è Advanced Features

### System Table Filtering

Automatically excludes database-specific system tables:

- **SQL Server**: Tables starting with `spt_`, `MSreplication_`
- **PostgreSQL**: Tables starting with `pg_`
- **General**: Any table starting with system prefixes

### Cross-Platform Compatibility

- **Driver Abstraction**: Uses SQLAlchemy for database-agnostic operations
- **Connection Pooling**: Efficient connection management
- **Error Handling**: Database-specific error handling and recovery
- **Metadata Caching**: Optimized metadata reflection and caching

### Schema Reflection Optimization

- **Lazy Loading**: Metadata loaded only when needed
- **Selective Reflection**: Can reflect specific tables or schemas
- **Connection Reuse**: Efficient connection pooling for multiple operations
- **Memory Management**: Optimized memory usage for large schemas

## üìà Performance Characteristics

- **Connection Time**: Sub-second connection establishment for local databases
- **Schema Reflection**: 1-5 seconds for typical databases (100-500 tables)
- **Large Databases**: Handles databases with 1000+ tables efficiently
- **Memory Usage**: Minimal memory footprint with lazy loading
- **Network Efficiency**: Optimized queries minimize network round trips

## üö¶ Prerequisites

1. **Database Access**: Valid database connection with read permissions
2. **SQLAlchemy Driver**: Appropriate database driver installed (pyodbc, psycopg2, etc.)
3. **Network Connectivity**: Access to database server if remote
4. **Schema Permissions**: READ access to information_schema or equivalent
5. **Dependencies**: SQLAlchemy and database-specific drivers from requirements.txt

## üîß Error Handling

### Common Error Scenarios

1. **Connection Failures**: Database server unavailable or credentials invalid
2. **Permission Issues**: Insufficient privileges to read schema information
3. **Driver Problems**: Missing or incompatible database drivers
4. **Network Issues**: Timeout or connectivity problems with remote databases
5. **Schema Changes**: Database schema modifications during inspection

### Error Recovery

```python
try:
    inspector = DatabaseInspector()
    tables = inspector.get_all_table_names()
    print(f"Successfully connected to database with {len(tables)} tables")
    
except ValueError as e:
    if "DATABASE_URL" in str(e):
        print("Error: DATABASE_URL environment variable not set")
        print("Please set DATABASE_URL with your database connection string")
    else:
        print(f"Configuration error: {e}")
        
except Exception as e:
    print(f"Database connection failed: {e}")
    print("Please check:")
    print("  - Database server is running")
    print("  - Connection string is correct")
    print("  - Network connectivity")
    print("  - Database permissions")
```

### Connection String Examples

```python
# SQL Server with Windows Authentication
DATABASE_URL = "mssql+pyodbc:///?odbc_connect=DRIVER={ODBC Driver 17 for SQL Server};SERVER=localhost;DATABASE=mydb;Trusted_Connection=yes"

# SQL Server with SQL Authentication  
DATABASE_URL = "mssql+pyodbc://user:password@localhost/mydb?driver=ODBC+Driver+17+for+SQL+Server"

# PostgreSQL
DATABASE_URL = "postgresql://user:password@localhost:5432/mydb"

# MySQL
DATABASE_URL = "mysql+pymysql://user:password@localhost:3306/mydb"

# SQLite
DATABASE_URL = "sqlite:///path/to/database.db"
```



================================================
FILE: docs/database/DocumentationStore.md
================================================
# Documentation Store

The Documentation Store is a SQLite-based persistence layer that manages the state and progress of documentation generation. It provides comprehensive session management, progress tracking, and resume capabilities for large-scale database documentation projects.

## üéØ What It Does

The Documentation Store handles persistent state management for documentation generation:

- **Session Management**: Tracks documentation generation sessions with resume capability
- **Progress Monitoring**: Maintains detailed progress information for tables and relationships
- **State Persistence**: Stores documentation content, metadata, and processing status
- **Resume Functionality**: Enables interrupted documentation generation to continue seamlessly
- **Duplicate Prevention**: Prevents reprocessing of already completed items
- **Metadata Storage**: Preserves generation timestamps, source information, and completion status

## üîÑ Storage Flow

```markdown
Generation Session ‚Üí Table/Relationship Processing ‚Üí SQLite Storage ‚Üí Progress Tracking ‚Üí Resume Capability
```

1. **Session Initialization**: Creates new generation session with source database metadata
2. **Item Registration**: Registers all pending tables and relationships for processing
3. **Progress Tracking**: Updates processing status as items are completed
4. **Content Storage**: Saves generated documentation content and metadata
5. **State Queries**: Provides current progress and pending items for resume functionality
6. **Completion Tracking**: Marks items as completed to prevent reprocessing

## üöÄ Usage Examples

### Programmatic Usage

```python
from src.database.persistence import DocumentationStore

# Initialize the documentation store
store = DocumentationStore()

# Start a new documentation generation session
db_url = "postgresql://user:pass@localhost/mydb" 
tables = ["users", "orders", "products"]
relationships = [
    {"constrained_table": "orders", "constrained_columns": ["user_id"], 
     "referred_table": "users", "referred_columns": ["id"]}
]

session_id = store.start_generation_session(db_url, tables, relationships)
print(f"Started session {session_id}")

# Save table documentation
table_name = "users"
schema_data = {
    "table_name": "users",
    "columns": [
        {"name": "id", "type": "INTEGER", "primary_key": True},
        {"name": "username", "type": "VARCHAR(50)", "nullable": False}
    ]
}
business_purpose = "Stores user account information and authentication data"
documentation = "## Users\n\nThis table manages user accounts..."

store.save_table_documentation(table_name, schema_data, business_purpose, documentation)

# Save relationship documentation  
relationship_id = "users_orders_fk"
relationship_type = "one-to-many"
rel_documentation = "Each user can have multiple orders"

store.save_relationship_documentation(relationship_id, relationship_type, rel_documentation)

# Check processing status
pending_tables = store.get_pending_tables()
pending_relationships = store.get_pending_relationships()
print(f"Pending: {len(pending_tables)} tables, {len(pending_relationships)} relationships")

# Get progress statistics
progress = store.get_generation_progress()
print("Progress:", progress)

# Check if specific items are processed
is_processed = store.is_table_processed("users")
print(f"Users table processed: {is_processed}")
```

### Resume Functionality

```python
# Initialize store - automatically detects existing session
store = DocumentationStore()

# Get items that still need processing
pending_tables = store.get_pending_tables()
pending_relationships = store.get_pending_relationships()

print(f"Resuming with {len(pending_tables)} pending tables")

# Continue processing only unfinished items
for table in pending_tables:
    if not store.is_table_processed(table):
        # Process table documentation
        # ... processing logic ...
        store.save_table_documentation(table, schema_data, purpose, docs)

# Check overall progress
progress = store.get_generation_progress()
print("Current progress:", progress)
```

### Data Retrieval

```python
# Get all processed tables
all_tables = store.get_all_tables()
print(f"Total processed tables: {len(all_tables)}")

# Get detailed information for specific table
table_info = store.get_table_info("users")
if table_info:
    print(f"Table: {table_info['table_name']}")
    print(f"Purpose: {table_info['business_purpose']}")
    print(f"Status: {table_info['status']}")

# Get relationship information
relationship_info = store.get_relationship_info("users_orders_fk")
if relationship_info:
    print(f"Relationship: {relationship_info['id']}")
    print(f"Type: {relationship_info['relationship_type']}")
    print(f"Documentation: {relationship_info['documentation']}")

# Get all processed relationships
all_relationships = store.get_all_relationships()
print(f"Total processed relationships: {len(all_relationships)}")
```

## üìä Database Schema

### Table Structure

```sql
-- Session and progress tracking
CREATE TABLE processing_state (
    id INTEGER PRIMARY KEY,
    phase TEXT NOT NULL,
    status TEXT NOT NULL,  -- 'pending', 'completed', 'failed'
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    error_message TEXT
);

-- Table documentation storage
CREATE TABLE table_metadata (
    table_name TEXT PRIMARY KEY,
    schema_data TEXT NOT NULL,  -- JSON
    business_purpose TEXT,
    documentation TEXT,         -- Generated markdown section
    processed_at TIMESTAMP,
    status TEXT DEFAULT 'pending'
);

-- Relationship documentation storage  
CREATE TABLE relationship_metadata (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    constrained_table TEXT NOT NULL,
    constrained_columns TEXT NOT NULL,  -- JSON array
    referred_table TEXT NOT NULL,
    referred_columns TEXT NOT NULL,     -- JSON array
    relationship_type TEXT,             -- inferred type
    documentation TEXT,                 -- Generated markdown section
    processed_at TIMESTAMP,
    status TEXT DEFAULT 'pending'
);

-- Generation session metadata
CREATE TABLE generation_metadata (
    id INTEGER PRIMARY KEY,
    source_database_url TEXT NOT NULL,
    started_at TIMESTAMP NOT NULL,
    completed_at TIMESTAMP,
    total_tables INTEGER,
    total_relationships INTEGER,
    status TEXT DEFAULT 'in_progress'
);
```

### Status Values

- **pending**: Item registered but not yet processed
- **completed**: Item successfully processed and documented
- **failed**: Item processing failed with error
- **in_progress**: Generation session currently active

## ‚öôÔ∏è Configuration

### Initialization Parameters

```python
DocumentationStore(
    db_path="__bin__/data/documentation.db"  # Optional: Custom database path
)

# Method parameters
start_generation_session(
    db_url,                    # Required: Source database URL
    tables,                    # Required: List of table names
    relationships              # Required: List of relationship dictionaries
)

save_table_documentation(
    table_name,               # Required: Name of the table
    schema_data,              # Required: Dictionary with schema information
    business_purpose,         # Required: Inferred business purpose
    documentation             # Required: Generated markdown documentation
)

save_relationship_documentation(
    relationship_id,          # Required: Unique relationship identifier
    relationship_type,        # Required: Type of relationship
    documentation            # Required: Generated relationship documentation
)
```

### File Storage

```markdown
__bin__/
‚îî‚îÄ‚îÄ data/
    ‚îî‚îÄ‚îÄ documentation.db     # SQLite database file
```

## üéØ Use Cases

### 1. Session Management

Start and manage documentation generation sessions:

```python
store = DocumentationStore()

# Start new session
tables = ["users", "orders", "products", "categories"]
relationships = [
    {"constrained_table": "orders", "constrained_columns": ["user_id"], 
     "referred_table": "users", "referred_columns": ["id"]},
    {"constrained_table": "orders", "constrained_columns": ["product_id"],
     "referred_table": "products", "referred_columns": ["id"]}
]

session_id = store.start_generation_session(
    "postgresql://localhost/ecommerce",
    tables,
    relationships
)

print(f"Session {session_id} started with {len(tables)} tables and {len(relationships)} relationships")
```

### 2. Progress Monitoring

Track documentation generation progress:

```python
# Monitor progress throughout generation
progress = store.get_generation_progress()

table_stats = progress['tables']
rel_stats = progress['relationships']

print("Table Progress:")
for status, count in table_stats.items():
    print(f"  {status}: {count}")

print("Relationship Progress:")  
for status, count in rel_stats.items():
    print(f"  {status}: {count}")

# Calculate completion percentage
total_tables = sum(table_stats.values())
completed_tables = table_stats.get('completed', 0)
table_completion = (completed_tables / total_tables * 100) if total_tables > 0 else 0

print(f"Tables: {table_completion:.1f}% complete")
```

### 3. Resume Functionality

Resume interrupted documentation generation:

```python
store = DocumentationStore()

# Check if there are pending items (indicating previous session)
pending_tables = store.get_pending_tables()
pending_relationships = store.get_pending_relationships()

if pending_tables or pending_relationships:
    print(f"Resuming previous session...")
    print(f"  Pending tables: {len(pending_tables)}")
    print(f"  Pending relationships: {len(pending_relationships)}")
    
    # Process only pending items
    for table in pending_tables:
        if not store.is_table_processed(table):
            print(f"Processing table: {table}")
            # ... process table ...
            store.save_table_documentation(table, schema, purpose, docs)
            
    for relationship in pending_relationships:
        rel_id = relationship['id']
        if not store.is_relationship_processed(relationship):
            print(f"Processing relationship: {rel_id}")
            # ... process relationship ...
            store.save_relationship_documentation(rel_id, rel_type, docs)
else:
    print("No pending items found - starting fresh generation")
```

### 4. Data Export and Analysis

Export and analyze stored documentation:

```python
# Export all table documentation
all_tables = store.get_all_tables()
table_docs = {}

for table_name in all_tables:
    table_info = store.get_table_info(table_name)
    if table_info:
        table_docs[table_name] = {
            'business_purpose': table_info['business_purpose'],
            'documentation': table_info['documentation'],
            'schema': table_info['schema_data']
        }

print(f"Exported documentation for {len(table_docs)} tables")

# Analyze documentation quality
total_chars = sum(len(info['documentation']) for info in table_docs.values())
avg_chars = total_chars / len(table_docs) if table_docs else 0

print(f"Average documentation length: {avg_chars:.0f} characters")

# Find tables with minimal documentation
short_docs = [name for name, info in table_docs.items() 
              if len(info['documentation']) < 100]
print(f"Tables with short documentation: {short_docs}")
```

## üîç Integration with Other Components

The Documentation Store serves as the central persistence layer:

- **Core Agent**: Saves generated documentation and checks processing status
- **Batch Manager**: Retrieves pending items for efficient batch processing  
- **Documentation Formatter**: Reads stored documentation for output generation
- **Main Application**: Manages sessions and provides resume functionality

## üéñÔ∏è Advanced Features

### Intelligent Resume Logic

- **Duplicate Detection**: Prevents reprocessing of completed items
- **Session Continuity**: Maintains session state across application restarts
- **Progress Preservation**: Tracks partial completion for large databases
- **Error Recovery**: Handles corrupted or incomplete processing gracefully

### Data Integrity

- **ACID Compliance**: SQLite transactions ensure data consistency
- **Foreign Key Constraints**: Maintains referential integrity between tables
- **Unique Constraints**: Prevents duplicate entries and processing conflicts
- **Timestamp Tracking**: Comprehensive audit trail of processing activities

### Performance Optimization

- **Indexed Queries**: Optimized database indexes for fast lookups
- **Batch Operations**: Efficient bulk insert and update operations
- **Connection Pooling**: Reused database connections for better performance
- **Memory Management**: Minimal memory footprint with efficient queries

## üìà Performance Characteristics

- **Database Size**: Handles documentation for 1000+ tables efficiently
- **Query Performance**: Sub-millisecond queries for status checks
- **Storage Efficiency**: Compact SQLite storage with minimal overhead
- **Concurrent Access**: Thread-safe operations for concurrent processing
- **Scalability**: Linear scaling with database size

## üö¶ Prerequisites

1. **SQLite Support**: Python sqlite3 module (included in standard library)
2. **File System Access**: Write permissions to data directory
3. **Disk Space**: Minimal space requirements (typically <100MB for large databases)
4. **Path Creation**: Ability to create directories for database storage

## üîß Error Handling

### Common Error Scenarios

1. **Database Corruption**: SQLite database file corruption or locking issues
2. **Disk Space**: Insufficient disk space for database operations
3. **Permission Issues**: File system permission problems
4. **Concurrent Access**: Multiple processes accessing same database file
5. **Schema Migration**: Database schema changes between versions

### Error Recovery

```python
try:
    store = DocumentationStore()
    session_id = store.start_generation_session(db_url, tables, relationships)
    
except sqlite3.OperationalError as e:
    if "database is locked" in str(e):
        print("Database is locked by another process")
        print("Please ensure no other documentation processes are running")
    elif "disk" in str(e).lower():
        print("Disk space or permission issue")
        print("Please check available disk space and write permissions")
    else:
        print(f"Database error: {e}")
        
except Exception as e:
    print(f"Documentation store initialization failed: {e}")
    
    # Attempt to recover with backup or fresh database
    backup_path = "__bin__/data/documentation_backup.db"
    if os.path.exists(backup_path):
        print("Attempting recovery from backup...")
        # Copy backup to main location
    else:
        print("Starting with fresh documentation database...")
        # Initialize new database
```

### Data Recovery

```python
# Check database integrity
def check_database_integrity(store):
    try:
        # Test basic operations
        progress = store.get_generation_progress()
        pending_tables = store.get_pending_tables()
        
        print("Database integrity check passed")
        return True
        
    except Exception as e:
        print(f"Database integrity check failed: {e}")
        return False

# Repair corrupted data
def repair_database(store):
    """Attempt to repair common database issues."""
    try:
        # Reset failed processing states
        with sqlite3.connect(store.db_path) as conn:
            conn.execute("""
                UPDATE table_metadata 
                SET status = 'pending' 
                WHERE status = 'failed'
            """)
            conn.execute("""
                UPDATE relationship_metadata 
                SET status = 'pending' 
                WHERE status = 'failed'
            """)
            conn.commit()
            
        print("Database repair completed")
        return True
        
    except Exception as e:
        print(f"Database repair failed: {e}")
        return False
```



================================================
FILE: docs/getting_started/configuration.md
================================================
# Configuration Reference

This document provides a comprehensive reference for all configuration options available in smol-sql-agents.

## Environment Variables

### Database Configuration

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `DATABASE_URL` | Yes | - | Database connection string (e.g., `postgresql://user:pass@host:port/dbname`) |
| `DB_POOL_SIZE` | No | 5 | Connection pool size |
| `DB_MAX_OVERFLOW` | No | 10 | Maximum overflow size for connection pool |

### LLM Configuration

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `OPENAI_API_KEY` | Yes | - | Your OpenAI API key |
| `OPENAI_MODEL` | No | `gpt-4` | OpenAI model to use |
| `OPENAI_TEMPERATURE` | No | `0.1` | Sampling temperature (0-2) |
| `OPENAI_MAX_TOKENS` | No | `2048` | Maximum number of tokens to generate |

### Logging Configuration

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `LOG_LEVEL` | No | `INFO` | Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL) |
| `LOG_FORMAT` | No | `%(asctime)s - %(name)s - %(levelname)s - %(message)s` | Log message format |

### Agent Configuration

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `AGENT_TIMEOUT` | No | `30` | Agent execution timeout in seconds |
| `MAX_RETRIES` | No | `3` | Maximum number of retries for agent operations |

## Configuration File

You can also use a `config.yaml` file in your project root:

```yaml
database:
  url: ${DATABASE_URL}
  pool_size: 5
  max_overflow: 10

openai:
  api_key: ${OPENAI_API_KEY}
  model: gpt-4
  temperature: 0.1
  max_tokens: 2048

logging:
  level: INFO
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

agent:
  timeout: 30
  max_retries: 3
```

## Programmatic Configuration

You can also configure the application programmatically:

```python
from smol_sql_agents import configure

configure(
    database_url="postgresql://user:pass@localhost:5432/dbname",
    openai_api_key="your-api-key",
    log_level="INFO",
    agent_timeout=30
)
```

## Best Practices

1. **Sensitive Information**: Never commit API keys or database credentials to version control. Use environment variables for sensitive data.
2. **Environment-Specific Configs**: Maintain separate configurations for development, testing, and production environments.
3. **Validation**: The application validates all configuration values on startup and will raise descriptive errors for invalid values.
4. **Performance Tuning**: Adjust pool sizes and timeouts based on your application's requirements and database capabilities.

## Troubleshooting

- **Connection Issues**: Verify your database URL and network connectivity
- **Authentication Failures**: Double-check API keys and database credentials
- **Performance Problems**: Adjust pool sizes and timeouts as needed

For more information, see the [Troubleshooting Guide](../troubleshooting/README.md).



================================================
FILE: docs/getting_started/installation.md
================================================
# Installation Guide

This guide will help you install and set up smol-sql-agents in your environment.

## Prerequisites

- Python 3.8 or higher
- pip (Python package manager)
- Git (for development)

## Installation Options

### Using pip (Recommended)

```bash
pip install smol-sql-agents
```

### From Source

1. Clone the repository:

   ```bash
   git clone https://github.com/yourusername/smol-sql-agents.git
   cd smol-sql-agents
   ```

2. Create and activate a virtual environment:

   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. Install with development dependencies:

   ```bash
   pip install -e ".[dev]"
   ```

## Verify Installation

```python
import smol_sql_agents
print(smol_sql_agents.__version__)
```

## Configuration

Create a `.env` file in your project root:

```ini
# Database Configuration
DATABASE_URL=postgresql://user:password@localhost:5432/your_database

# LLM Configuration
OPENAI_API_KEY=your_openai_api_key

# Logging
LOG_LEVEL=INFO
```

## Next Steps

- [Quick Start Guide](./quick_start.md)
- [Configuration Reference](./configuration.md)



================================================
FILE: docs/getting_started/quick_start.md
================================================
# Quick Start Guide

This guide will help you get started with smol-sql-agents by walking you through a simple example.

## Basic Usage

### 1. Import Required Modules

```python
from smol_sql_agents import NL2SQLAgent, DatabaseInspector
from dotenv import load_dotenv
import os

# Load environment variables
load_dotenv()
```

### 2. Initialize the Database Inspector

```python
# Initialize database inspector
db_url = os.getenv("DATABASE_URL")
inspector = DatabaseInspector(db_url)

# Get database schema
schema = inspector.get_schema()
```

### 3. Create and Run an Agent

```python
# Initialize the NL2SQL agent
agent = NL2SQLAgent()

# Execute a natural language query
result = agent.execute(
    query="Show me all active users who made a purchase in the last 30 days",
    schema=schema
)

print("Generated SQL:", result["sql"])
print("Query Results:", result["results"])
```

## Example: End-to-End Workflow

```python
from smol_sql_agents import BusinessAgent, DatabaseInspector

# Initialize components
db_inspector = DatabaseInspector(os.getenv("DATABASE_URL"))
agent = BusinessAgent()

# Analyze business data
business_insights = agent.analyze(
    question="What are our top-selling products by region?",
    schema=db_inspector.get_schema()
)

print("Analysis Results:", business_insights)
```

## Next Steps

- Learn more about [Core Concepts](../concepts/README.md)
- Explore the [API Reference](../api/README.md)
- Check out [Advanced Usage](../guides/README.md)



================================================
FILE: docs/output/DocumentationFormatter.md
================================================
# Documentation Formatter

The Documentation Formatter generates professional, structured documentation from processed SQL database information. It transforms raw documentation data into multiple output formats including Markdown and HTML with proper styling, organization, and comprehensive metadata.

## üéØ What It Does

The Documentation Formatter provides comprehensive document generation capabilities:

- **Multi-Format Output**: Generates documentation in Markdown and HTML formats
- **Professional Styling**: Creates visually appealing documentation with proper formatting
- **Structured Organization**: Organizes content into logical sections with hierarchical structure
- **Metadata Integration**: Includes generation timestamps, statistics, and progress information
- **Template-Based Rendering**: Uses Jinja2 templates for flexible and customizable output
- **Data Aggregation**: Combines table and relationship documentation into cohesive documents
- **Responsive Design**: HTML output includes mobile-friendly styling
- **Export Ready**: Generates documentation suitable for sharing and archival

## üîÑ Generation Flow

```markdown
SQLite Documentation Store ‚Üí Data Loading ‚Üí Template Rendering ‚Üí Formatted Output ‚Üí File Generation
```

1. **Data Collection**: Retrieves all processed documentation from SQLite storage
2. **Content Organization**: Structures tables and relationships into logical sections
3. **Metadata Assembly**: Compiles generation statistics and session information
4. **Template Processing**: Applies Jinja2 templates for consistent formatting
5. **Output Generation**: Creates formatted files in specified output directory
6. **Quality Assurance**: Validates output format and completeness

## üöÄ Usage Examples

### Basic Documentation Generation

```python
from src.output.formatters import DocumentationFormatter

# Initialize formatter
formatter = DocumentationFormatter()

# Generate Markdown documentation
markdown_content = formatter.generate_documentation('markdown')
print(f"Generated {len(markdown_content)} characters of Markdown")

# Generate HTML documentation
html_content = formatter.generate_documentation('html')
print(f"Generated {len(html_content)} characters of HTML")

# Save to files
with open('database_docs.md', 'w', encoding='utf-8') as f:
    f.write(markdown_content)

with open('database_docs.html', 'w', encoding='utf-8') as f:
    f.write(html_content)
```

### Custom Database Path

```python
# Use custom database location
formatter = DocumentationFormatter(db_path="custom/path/documentation.db")

# Generate documentation from custom location
documentation = formatter.generate_documentation('markdown')
print("Generated documentation from custom database")
```

### Automated Documentation Pipeline

```python
from pathlib import Path
from src.agents.core import PersistentDocumentationAgent
from src.output.formatters import DocumentationFormatter

def generate_complete_documentation():
    """Complete documentation generation pipeline."""
    
    # Step 1: Generate documentation data
    print("Processing database schema...")
    agent = PersistentDocumentationAgent()
    
    # Process all tables and relationships
    # (This would typically be done by the main process)
    
    # Step 2: Generate formatted output
    print("Generating formatted documentation...")
    formatter = DocumentationFormatter()
    
    # Create output directory
    output_dir = Path("__bin__/output")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Generate both formats
    formats = ['markdown', 'html']
    generated_files = []
    
    for fmt in formats:
        print(f"Generating {fmt.upper()} format...")
        content = formatter.generate_documentation(fmt)
        
        # Write to file
        file_path = output_dir / f"database_docs.{fmt}"
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        generated_files.append(str(file_path))
        print(f"Saved: {file_path}")
    
    return generated_files

# Run complete pipeline
files = generate_complete_documentation()
print(f"Generated documentation files: {files}")
```

## üìä Output Structure

### Markdown Documentation Format

```markdown
# Database Knowledge Base

Generated on: 2024-01-15 14:30:22
Total Tables: 15
Total Relationships: 8

# Tables

## users

Stores user account information and authentication data for the application

| Column | Type | Primary Key | Nullable |
|--------|------|-------------|----------|
| id | INTEGER | Yes | No |
| username | VARCHAR(255) | No | No |
| email | VARCHAR(255) | No | No |
| created_at | TIMESTAMP | No | Yes |

## orders

Customer order information including transaction details and status tracking

| Column | Type | Primary Key | Nullable |
|--------|------|-------------|----------|
| id | INTEGER | Yes | No |
| user_id | INTEGER | No | No |
| total_amount | DECIMAL(10,2) | No | No |
| order_date | TIMESTAMP | No | No |

# Relationships

### users.id ‚Üí orders.user_id

Each user can have multiple orders, establishing a customer-order relationship for transaction tracking

### orders.id ‚Üí order_items.order_id

Each order contains multiple line items with product details and quantities
```

### HTML Documentation Format

```html
<!DOCTYPE html>
<html>
<head>
    <title>Database Knowledge Base</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }
        table { border-collapse: collapse; width: 100%; margin: 20px 0; }
        th, td { border: 1px solid #ddd; padding: 12px; text-align: left; }
        th { background-color: #f2f2f2; font-weight: bold; }
        .metadata { background-color: #f9f9f9; padding: 15px; margin-bottom: 30px; border-radius: 5px; }
        h1 { color: #333; border-bottom: 2px solid #333; padding-bottom: 10px; }
        h2 { color: #555; margin-top: 30px; }
        h3 { color: #777; }
        .table-description { margin: 10px 0; font-style: italic; color: #666; }
    </style>
</head>
<body>
    <h1>Database Knowledge Base</h1>
    
    <div class="metadata">
        <strong>Generated:</strong> 2024-01-15 14:30:22<br>
        <strong>Tables:</strong> 15<br>
        <strong>Relationships:</strong> 8
    </div>
    
    <h1>Tables</h1>
    
    <h2>users</h2>
    <div class="table-description">
        Stores user account information and authentication data for the application
    </div>
    
    <table>
        <tr><th>Column</th><th>Type</th><th>Primary Key</th><th>Nullable</th></tr>
        <tr><td>id</td><td>INTEGER</td><td>Yes</td><td>No</td></tr>
        <tr><td>username</td><td>VARCHAR(255)</td><td>No</td><td>No</td></tr>
        <tr><td>email</td><td>VARCHAR(255)</td><td>No</td><td>No</td></tr>
        <tr><td>created_at</td><td>TIMESTAMP</td><td>No</td><td>Yes</td></tr>
    </table>
</body>
</html>
```

## ‚öôÔ∏è Configuration

### Initialization Parameters

```python
DocumentationFormatter(
    db_path="__bin__/data/documentation.db"    # Optional: Custom database path
)

# Method parameters
generate_documentation(
    format_type='markdown'                     # Required: 'markdown' or 'html'
)
```

### Template Customization

```python
# Access and modify templates
formatter = DocumentationFormatter()

# View current templates
print("Available templates:")
for format_type in formatter.templates.keys():
    print(f"  - {format_type}")

# Customize template (example: add custom CSS)
custom_html_template = formatter.templates['html'].replace(
    "body { font-family: Arial, sans-serif;",
    "body { font-family: 'Helvetica Neue', Arial, sans-serif;"
)

# Update template
formatter.templates['html'] = custom_html_template

# Generate with custom template
custom_html = formatter.generate_documentation('html')
```

### Output Directory Management

```python
import os
from pathlib import Path

def setup_documentation_output():
    """Set up organized output directory structure."""
    
    base_dir = Path("__bin__/output")
    
    # Create directory structure
    directories = [
        base_dir,
        base_dir / "markdown",
        base_dir / "html", 
        base_dir / "archive",
        base_dir / "assets"
    ]
    
    for directory in directories:
        directory.mkdir(parents=True, exist_ok=True)
    
    return base_dir

def generate_timestamped_docs():
    """Generate documentation with timestamp."""
    from datetime import datetime
    
    base_dir = setup_documentation_output()
    formatter = DocumentationFormatter()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Generate both formats with timestamps
    for fmt in ['markdown', 'html']:
        content = formatter.generate_documentation(fmt)
        
        # Save with timestamp
        filename = f"database_docs_{timestamp}.{fmt}"
        file_path = base_dir / fmt / filename
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        print(f"Generated: {file_path}")
        
        # Also save as latest
        latest_path = base_dir / f"database_docs.{fmt}"
        with open(latest_path, 'w', encoding='utf-8') as f:
            f.write(content)

generate_timestamped_docs()
```

## üéØ Use Cases

### 1. Automated Documentation Publishing

```python
def publish_documentation():
    """Generate and publish documentation to multiple destinations."""
    
    formatter = DocumentationFormatter()
    
    # Generate content
    markdown_content = formatter.generate_documentation('markdown')
    html_content = formatter.generate_documentation('html')
    
    # Save to local files
    Path("docs").mkdir(exist_ok=True)
    
    with open("docs/README.md", 'w', encoding='utf-8') as f:
        f.write(markdown_content)
    
    with open("docs/index.html", 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    # Additional publishing destinations
    destinations = {
        "wiki": markdown_content,
        "confluence": html_content,
        "sharepoint": html_content
    }
    
    for dest, content in destinations.items():
        print(f"Ready for publishing to {dest}: {len(content)} characters")
    
    return destinations

# Publish to multiple platforms
publish_destinations = publish_documentation()
```

### 2. Documentation Quality Validation

```python
def validate_documentation_quality():
    """Validate generated documentation for completeness and quality."""
    
    formatter = DocumentationFormatter()
    
    # Load raw data for comparison
    data = formatter._load_documentation_data()
    
    # Generate formatted output
    markdown_content = formatter.generate_documentation('markdown')
    html_content = formatter.generate_documentation('html')
    
    # Validation checks
    validation_results = {
        "table_count_match": True,
        "relationship_count_match": True,
        "content_completeness": True,
        "format_validity": True,
        "issues": []
    }
    
    # Check table count consistency
    expected_tables = len(data['tables'])
    table_sections = markdown_content.count('## ')
    if table_sections != expected_tables:
        validation_results["table_count_match"] = False
        validation_results["issues"].append(f"Table count mismatch: expected {expected_tables}, found {table_sections}")
    
    # Check relationship sections
    expected_relationships = len(data['relationships'])
    relationship_sections = markdown_content.count('###')
    if relationship_sections != expected_relationships:
        validation_results["relationship_count_match"] = False
        validation_results["issues"].append(f"Relationship count mismatch: expected {expected_relationships}, found {relationship_sections}")
    
    # Check for missing content
    if not markdown_content.strip():
        validation_results["content_completeness"] = False
        validation_results["issues"].append("Generated Markdown content is empty")
    
    if not html_content.strip():
        validation_results["content_completeness"] = False
        validation_results["issues"].append("Generated HTML content is empty")
    
    # Validate HTML structure
    if "<!DOCTYPE html>" not in html_content:
        validation_results["format_validity"] = False
        validation_results["issues"].append("HTML output missing DOCTYPE declaration")
    
    # Report results
    if all([validation_results["table_count_match"], 
            validation_results["relationship_count_match"],
            validation_results["content_completeness"],
            validation_results["format_validity"]]):
        print("‚úÖ Documentation validation passed")
    else:
        print("‚ùå Documentation validation failed:")
        for issue in validation_results["issues"]:
            print(f"   - {issue}")
    
    return validation_results

# Validate documentation quality
validation = validate_documentation_quality()
```

### 3. Custom Template Development

```python
def create_custom_template():
    """Create custom documentation template."""
    
    # Custom Markdown template with enhanced formatting
    custom_markdown_template = """# üìä {{ metadata.database_name or 'Database' }} Documentation

> **Generated:** {{ metadata.completed_at or 'In Progress' }}  
> **Tables:** {{ metadata.total_tables }} | **Relationships:** {{ metadata.total_relationships }}

---

## üìã Table of Contents
{% for table in tables %}
- [{{ table.name }}](#{{ table.name|lower|replace('_', '-') }})
{% endfor %}

---

## üóÇÔ∏è Database Tables

{% for table in tables %}
### {{ table.name }}

> {{ table.purpose }}

**Schema Information:**

| Column | Data Type | Constraints | Description |
|--------|-----------|-------------|-------------|
{% for column in table.schema.columns -%}
| `{{ column.name }}` | {{ column.type }} | {{ 'PK' if column.primary_key else '' }}{{ ', NOT NULL' if not column.nullable else '' }} | {{ column.description or 'N/A' }} |
{% endfor %}

---
{% endfor %}

## üîó Relationships

{% for rel in relationships %}
### {{ rel.constrained_table }} ‚Üí {{ rel.referred_table }}

**Type:** {{ rel.type|title }}  
**Foreign Key:** `{{ rel.constrained_table }}.{{ rel.constrained_columns|join(',') }}` ‚Üí `{{ rel.referred_table }}.{{ rel.referred_columns|join(',') }}`

{{ rel.documentation }}

---
{% endfor %}

---
**Documentation generated by SQL Documentation Agent**
"""

    # Apply custom template
    formatter = DocumentationFormatter()
    formatter.templates['markdown'] = custom_markdown_template
    
    # Generate with custom formatting
    enhanced_docs = formatter.generate_documentation('markdown')
    
    with open('enhanced_docs.md', 'w', encoding='utf-8') as f:
        f.write(enhanced_docs)
    
    print("Generated enhanced documentation with custom template")

create_custom_template()
```

### 4. Multi-Format Export Pipeline

```python
def comprehensive_export():
    """Generate documentation in multiple formats for different use cases."""
    
    formatter = DocumentationFormatter()
    
    # Standard formats
    formats = {
        'markdown': {
            'extension': 'md',
            'content': formatter.generate_documentation('markdown'),
            'use_case': 'Git repositories, wikis, documentation sites'
        },
        'html': {
            'extension': 'html', 
            'content': formatter.generate_documentation('html'),
            'use_case': 'Web publishing, internal portals, presentations'
        }
    }
    
    # Generate summary report
    summary = {
        'generation_timestamp': datetime.now().isoformat(),
        'formats_generated': list(formats.keys()),
        'file_sizes': {},
        'content_stats': {}
    }
    
    # Process each format
    output_dir = Path("__bin__/export")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    for format_name, format_info in formats.items():
        content = format_info['content']
        file_path = output_dir / f"database_docs.{format_info['extension']}"
        
        # Write file
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        # Collect statistics
        summary['file_sizes'][format_name] = len(content)
        summary['content_stats'][format_name] = {
            'characters': len(content),
            'lines': content.count('\n'),
            'use_case': format_info['use_case']
        }
        
        print(f"Exported {format_name}: {file_path} ({len(content):,} characters)")
    
    # Save summary
    import json
    with open(output_dir / "export_summary.json", 'w') as f:
        json.dump(summary, f, indent=2)
    
    return summary

# Run comprehensive export
export_summary = comprehensive_export()
print(f"Export completed: {export_summary['formats_generated']}")
```

## üîç Data Loading and Processing

### SQLite Data Retrieval

```python
# The formatter loads data using SQL queries to ensure data integrity
def examine_data_loading():
    """Examine how the formatter loads data from SQLite."""
    
    formatter = DocumentationFormatter()
    data = formatter._load_documentation_data()
    
    print("Loaded documentation data structure:")
    print(f"  Tables: {len(data['tables'])}")
    print(f"  Relationships: {len(data['relationships'])}")
    print(f"  Metadata: {data['metadata']}")
    
    # Examine table data structure
    if data['tables']:
        sample_table = data['tables'][0]
        print(f"\nSample table structure:")
        print(f"  Name: {sample_table['name']}")
        print(f"  Purpose: {sample_table['purpose'][:100]}...")
        print(f"  Columns: {len(sample_table['schema']['columns'])}")
    
    # Examine relationship data structure
    if data['relationships']:
        sample_rel = data['relationships'][0]
        print(f"\nSample relationship structure:")
        print(f"  Tables: {sample_rel['constrained_table']} ‚Üí {sample_rel['referred_table']}")
        print(f"  Type: {sample_rel['type']}")
        print(f"  Documentation: {sample_rel['documentation'][:100]}...")

examine_data_loading()
```

## üéñÔ∏è Advanced Features

### Template Engine Integration

- **Jinja2 Powered**: Full Jinja2 template engine support with filters and expressions
- **Custom Filters**: Built-in filters for formatting table names, data types, and constraints
- **Conditional Logic**: Template logic for handling optional content and variations
- **Loop Optimization**: Efficient rendering of large table and relationship sets

### Content Organization

- **Hierarchical Structure**: Logical organization with proper heading levels
- **Cross-References**: Internal links and references between sections
- **Metadata Integration**: Comprehensive statistics and generation information
- **Content Validation**: Ensures all processed data is included in output

### Responsive Design

- **Mobile-Friendly HTML**: CSS that adapts to different screen sizes
- **Print Optimization**: Print-friendly styling for physical documentation
- **Accessibility**: Proper semantic HTML and ARIA attributes
- **Modern Styling**: Professional appearance with clean, readable design

## üìà Performance Characteristics

- **Generation Speed**: Processes 100+ tables in under 5 seconds
- **Memory Efficiency**: Minimal memory usage with streaming template rendering
- **Output Size**: Typical documentation ranges from 50KB to 5MB depending on database size
- **Template Performance**: Sub-second rendering for complex templates
- **File I/O**: Efficient file writing with proper encoding handling
- **Concurrent Safe**: Thread-safe operations for parallel documentation generation

## üö¶ Prerequisites

1. **Documentation Store**: SQLite database with processed table and relationship data
2. **Template Engine**: Jinja2 package for template processing
3. **File System Access**: Write permissions for output directory creation
4. **Python Environment**: Python 3.8+ for optimal compatibility
5. **Dependencies**: All required packages from requirements.txt

## üîß Error Handling

### Common Error Scenarios

```python
def robust_documentation_generation():
    """Generate documentation with comprehensive error handling."""
    
    try:
        formatter = DocumentationFormatter()
        
        # Validate database exists and is accessible
        data = formatter._load_documentation_data()
        
        if not data['tables'] and not data['relationships']:
            print("‚ö†Ô∏è  Warning: No documentation data found")
            print("   Make sure tables and relationships have been processed")
            return None
        
        # Generate documentation
        for format_type in ['markdown', 'html']:
            try:
                content = formatter.generate_documentation(format_type)
                
                if not content.strip():
                    print(f"‚ùå Error: Empty {format_type} content generated")
                    continue
                
                # Save with error handling
                output_path = f"database_docs.{format_type}"
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                print(f"‚úÖ Generated {format_type}: {output_path}")
                
            except Exception as e:
                print(f"‚ùå Failed to generate {format_type}: {e}")
                continue
        
    except FileNotFoundError:
        print("‚ùå Documentation database not found")
        print("   Run documentation generation first: python main.py")
    except PermissionError:
        print("‚ùå Permission denied writing output files")
        print("   Check file permissions and directory access")
    except Exception as e:
        print(f"‚ùå Unexpected error: {e}")
        print("   Check logs for detailed error information")

# Use robust generation
robust_documentation_generation()
```

### Template Error Handling

```python
def handle_template_errors():
    """Handle template rendering errors gracefully."""
    
    from jinja2 import Environment, BaseLoader, TemplateError
    
    formatter = DocumentationFormatter()
    
    try:
        # Test template rendering
        data = formatter._load_documentation_data()
        template = Environment(loader=BaseLoader()).from_string(
            formatter.templates['markdown']
        )
        
        result = template.render(**data)
        print("‚úÖ Template rendering successful")
        return result
        
    except TemplateError as e:
        print(f"‚ùå Template error: {e}")
        print("   Check template syntax and variable names")
    except KeyError as e:
        print(f"‚ùå Missing template variable: {e}")
        print("   Ensure all required data is available")
    except Exception as e:
        print(f"‚ùå Rendering error: {e}")
        print("   Check template logic and data structure")
    
    return None

# Test template handling
handle_template_errors()
```

---

The Documentation Formatter provides the final step in the SQL Documentation suite, transforming processed database information into professional, shareable documentation that can be easily consumed by developers, architects, and stakeholders across different platforms and use cases.



================================================
FILE: docs/vector/EmbeddingsClient.md
================================================
# OpenAI Embeddings Client

The OpenAI Embeddings Client is a robust wrapper for OpenAI's embeddings API that provides efficient embedding generation with automatic error handling, retry mechanisms, and batch processing capabilities for SQL documentation.

## üéØ What It Does

The OpenAI Embeddings Client handles all interactions with OpenAI's embeddings API:

- **Single Embedding Generation**: Creates embeddings for individual text documents
- **Batch Processing**: Efficiently processes multiple texts in optimized batches
- **Error Handling**: Automatic retry with exponential backoff for transient failures
- **Token Management**: Intelligent token counting and text truncation for API limits
- **Text Preparation**: Cleans and prepares text for optimal embedding generation
- **Rate Limiting**: Built-in handling of API rate limits with retry strategies
- **Cost Optimization**: Minimizes API calls through efficient batching

## üîÑ Processing Flow

```
Text Input ‚Üí Text Preparation ‚Üí Token Validation ‚Üí OpenAI API ‚Üí Embedding Vector ‚Üí Response
```

1. **Text Preparation**: Cleans whitespace and prepares text for embedding generation
2. **Token Counting**: Validates text length against OpenAI's token limits (8,000 tokens)
3. **API Request**: Sends request to OpenAI embeddings endpoint with retry logic
4. **Vector Extraction**: Extracts 3072-dimensional embedding vector from response
5. **Error Handling**: Automatic retry for rate limits and transient failures
6. **Batch Coordination**: Groups multiple requests for efficient API usage

## üöÄ Usage Examples

### Single Embedding Generation

```python
from src.vector.embeddings import OpenAIEmbeddingsClient

# Initialize client
client = OpenAIEmbeddingsClient()

# Generate embedding for single text
text = "Stores user account information including username and email"
embedding = client.generate_embedding(text)

print(f"Generated embedding with {len(embedding)} dimensions")
# Output: Generated embedding with 3072 dimensions
```

### Batch Embedding Generation

```python
# Generate embeddings for multiple texts efficiently
texts = [
    "User account table with authentication data",
    "Order processing table with transaction details", 
    "Product catalog with pricing information",
    "Customer relationship management data"
]

embeddings = client.generate_embeddings_batch(texts)
print(f"Generated {len(embeddings)} embeddings")

# Process results
for i, (text, embedding) in enumerate(zip(texts, embeddings)):
    print(f"Text {i+1}: {len(embedding)} dimensions")
```

### Advanced Configuration

```python
import os

# Configure client with custom settings
os.environ["OPENAI_EMBEDDING_MODEL"] = "text-embedding-3-small"
os.environ["EMBEDDING_BATCH_SIZE"] = "50"
os.environ["EMBEDDING_MAX_RETRIES"] = "5"

client = OpenAIEmbeddingsClient()

# Generate embedding with automatic text preparation
long_text = """
    This is a very long text that might exceed token limits and contains
    extra    whitespace    that needs    to be    cleaned up before
    sending to the OpenAI API for embedding generation.
""" * 100  # Simulate very long text

embedding = client.generate_embedding(long_text)
print("Successfully handled long text with automatic truncation")
```

## üìä Response Structure

### Single Embedding Response

```python
# Returns List[float] with 3072 dimensions
embedding = client.generate_embedding("Sample text")
print(type(embedding))          # <class 'list'>
print(len(embedding))           # 3072
print(embedding[:5])            # [0.12345, -0.67890, 0.11111, ...]
```

### Batch Embeddings Response

```python
# Returns List[List[float]]
embeddings = client.generate_embeddings_batch(["text1", "text2", "text3"])
print(type(embeddings))         # <class 'list'>
print(len(embeddings))          # 3
print(len(embeddings[0]))       # 3072
```

## ‚öôÔ∏è Configuration

### Environment Variables

```env
# Required
OPENAI_API_KEY="your-api-key-here"

# Optional - Model Configuration
OPENAI_EMBEDDING_MODEL="text-embedding-3-small"    # Default model

# Optional - Batch Processing
EMBEDDING_BATCH_SIZE="100"                         # Documents per batch
EMBEDDING_MAX_RETRIES="3"                          # Maximum retry attempts

# Optional - Performance Tuning
OPENAI_REQUEST_TIMEOUT="30"                        # Request timeout in seconds
OPENAI_MAX_WORKERS="5"                             # Concurrent workers for batch processing
```

### Model Options

```python
# Available OpenAI embedding models
models = {
    "text-embedding-3-small": {
        "dimensions": 3072,
        "cost_per_1k_tokens": 0.00002,
        "max_tokens": 8191
    },
    "text-embedding-3-large": {
        "dimensions": 3072, 
        "cost_per_1k_tokens": 0.00013,
        "max_tokens": 8191
    },
    "text-embedding-ada-002": {
        "dimensions": 3072,
        "cost_per_1k_tokens": 0.0001,
        "max_tokens": 8191
    }
}
```

## üéØ Use Cases

### 1. Document Indexing

```python
# Index database table documentation
client = OpenAIEmbeddingsClient()

table_documentation = """
Table: users
Purpose: Stores user account information and authentication data
Columns: id (integer, primary key), username (varchar), email (varchar), created_at (timestamp)
"""

embedding = client.generate_embedding(table_documentation)
# Store embedding in vector database for semantic search
```

### 2. Bulk Processing

```python
# Process large numbers of documents efficiently
def process_documentation_batch(documents):
    client = OpenAIEmbeddingsClient()
    
    # Prepare texts for embedding
    texts = [f"{doc['type']}: {doc['name']} - {doc['description']}" 
             for doc in documents]
    
    # Generate embeddings in batches
    embeddings = client.generate_embeddings_batch(texts)
    
    # Return paired results
    return list(zip(documents, embeddings))

# Process 500 documents efficiently
documents = get_pending_documents()  # Returns list of document dicts
results = process_documentation_batch(documents)
```

### 3. Text Preparation and Validation

```python
# Handle various text formats and edge cases
client = OpenAIEmbeddingsClient()

# Test token counting
text = "Sample documentation text for token counting"
token_count = client._count_tokens(text)
print(f"Text has {token_count} tokens")

# Test text preparation
messy_text = """
    This    has    extra    whitespace
    
    
    And multiple line breaks
    
"""
cleaned_text = client._prepare_text_for_embedding(messy_text)
print(f"Cleaned: '{cleaned_text}'")

# Test text truncation
very_long_text = "word " * 10000  # Simulate very long text
truncated = client._truncate_text(very_long_text, max_tokens=1000)
print(f"Truncated from {len(very_long_text)} to {len(truncated)} characters")
```

### 4. Error Handling and Retries

```python
# Robust error handling for production use
client = OpenAIEmbeddingsClient()

def safe_generate_embedding(text, max_attempts=3):
    for attempt in range(max_attempts):
        try:
            embedding = client.generate_embedding(text)
            return {"success": True, "embedding": embedding}
        except Exception as e:
            print(f"Attempt {attempt + 1} failed: {e}")
            if attempt == max_attempts - 1:
                return {"success": False, "error": str(e)}
            time.sleep(2 ** attempt)  # Exponential backoff

# Use safe embedding generation
result = safe_generate_embedding("Important document text")
if result["success"]:
    print(f"Generated embedding: {len(result['embedding'])} dimensions")
else:
    print(f"Failed to generate embedding: {result['error']}")
```

## üîç Integration with Vector Store

### Seamless Integration

```python
from src.vector.store import SQLVectorStore
from src.vector.embeddings import OpenAIEmbeddingsClient

# The embeddings client is automatically integrated
vector_store = SQLVectorStore()
print(f"Using embeddings client: {type(vector_store.embeddings_client)}")

# Client is used automatically during document addition
vector_store.add_table_document("users", {
    "name": "users",
    "business_purpose": "User account management",
    "schema": {"columns": ["id", "username", "email"]}
})
```

## üéñÔ∏è Advanced Features

### Intelligent Text Preparation

- **Whitespace Normalization**: Removes extra spaces and line breaks
- **Token-Aware Truncation**: Preserves meaning while staying within API limits
- **Encoding Optimization**: Uses tiktoken for accurate token counting
- **Content Validation**: Ensures text is suitable for embedding generation

### Retry and Error Recovery

- **Exponential Backoff**: Intelligent retry timing for rate limits
- **Transient Error Handling**: Automatic retry for temporary API issues
- **Network Resilience**: Handles connection timeouts and network errors
- **Graceful Degradation**: Meaningful error messages for permanent failures

### Batch Optimization

- **Dynamic Batch Sizing**: Configurable batch sizes for different scenarios
- **Memory Management**: Efficient processing of large document sets
- **Progress Tracking**: Optional progress reporting for long-running operations
- **Cost Optimization**: Minimizes API calls while maximizing throughput

### Performance Monitoring

```python
import time

# Monitor embedding generation performance
client = OpenAIEmbeddingsClient()

start_time = time.time()
texts = ["Sample text"] * 100

# Time batch processing
embeddings = client.generate_embeddings_batch(texts)
batch_time = time.time() - start_time

print(f"Processed {len(texts)} texts in {batch_time:.2f} seconds")
print(f"Rate: {len(texts)/batch_time:.1f} embeddings/second")

# Compare with individual processing
start_time = time.time()
individual_embeddings = [client.generate_embedding(text) for text in texts[:10]]
individual_time = time.time() - start_time

print(f"Individual processing: {10/individual_time:.1f} embeddings/second")
print(f"Batch speedup: {(10/individual_time)/(len(texts)/batch_time):.1f}x")
```

## üìà Performance Characteristics

- **Throughput**: 100-500 embeddings per minute depending on text length and batch size
- **Latency**: 100-500ms per embedding for individual requests
- **Batch Efficiency**: 5-10x speedup compared to individual requests
- **Memory Usage**: Minimal memory footprint with streaming processing
- **Token Accuracy**: 99.9% accurate token counting using tiktoken
- **Error Recovery**: 95% success rate after retries for transient failures

## üö¶ Prerequisites

1. **OpenAI API Access**: Valid API key with embeddings access
2. **Network Connectivity**: Stable internet connection for API requests
3. **Dependencies**: openai, tiktoken, tenacity packages installed
4. **Python Version**: Python 3.8+ for optimal compatibility
5. **API Credits**: Sufficient OpenAI credits for embedding generation

## üîß Error Handling

### Common Error Scenarios

```python
from tenacity import RetryError
import openai

try:
    embedding = client.generate_embedding("Sample text")
except openai.RateLimitError as e:
    print(f"Rate limit exceeded: {e}")
    print("Consider reducing batch size or adding delays")
except openai.AuthenticationError as e:
    print(f"Invalid API key: {e}")
    print("Check OPENAI_API_KEY environment variable")
except openai.InvalidRequestError as e:
    print(f"Invalid request: {e}")
    print("Check text content and length")
except RetryError as e:
    print(f"Max retries exceeded: {e}")
    print("Check network connection and API status")
except Exception as e:
    print(f"Unexpected error: {e}")
    print("Check logs for detailed error information")
```

### Debugging and Monitoring

```python
import logging

# Enable debug logging
logging.getLogger("src.vector.embeddings").setLevel(logging.DEBUG)

# Monitor token usage
client = OpenAIEmbeddingsClient()
text = "Sample documentation for monitoring"

print(f"Original text length: {len(text)} characters")
print(f"Token count: {client._count_tokens(text)} tokens")

prepared_text = client._prepare_text_for_embedding(text)
print(f"Prepared text: '{prepared_text}'")

# Generate embedding with monitoring
embedding = client.generate_embedding(text)
print(f"Generated embedding: {len(embedding)} dimensions")
```

---

The OpenAI Embeddings Client provides the foundation for all semantic search capabilities in the SQL Documentation suite, offering reliable, efficient, and cost-effective embedding generation with comprehensive error handling and optimization features.



================================================
FILE: docs/vector/SearchTools.md
================================================
# Semantic Search Tools

The Semantic Search Tools provide high-level search functionality using OpenAI embeddings for intelligent discovery of database documentation. These tools enable natural language queries to find relevant tables, relationships, and documentation content with similarity scoring.

## üéØ What It Does

The Semantic Search Tools offer advanced search capabilities for SQL documentation:

- **Natural Language Queries**: Search using plain English rather than exact keyword matching
- **Semantic Similarity**: Find conceptually related content even without exact word matches
- **Multi-Type Search**: Search across tables, relationships, or all documentation types
- **Relevance Scoring**: Provides similarity scores to rank search results by relevance
- **Combined Results**: Intelligently merges and ranks results from different documentation types
- **Flexible Filtering**: Supports different search scopes and result limits
- **Performance Optimization**: Efficient search algorithms for large documentation sets

## üîÑ Search Flow

```markdown
Natural Language Query ‚Üí Embedding Generation ‚Üí Vector Similarity Search ‚Üí Ranked Results ‚Üí Formatted Output
```

1. **Query Processing**: Converts natural language query to embedding vector
2. **Similarity Search**: Compares query embedding against indexed documentation
3. **Relevance Scoring**: Calculates cosine similarity scores for ranking
4. **Result Filtering**: Applies type filters and relevance thresholds
5. **Result Ranking**: Sorts combined results by similarity score
6. **Response Formatting**: Structures results for consistent consumption

## üöÄ Usage Examples

### Table Documentation Search

```python
from src.vector.search import search_table_documentation

# Search for user-related tables
results = search_table_documentation("user account information", limit=5)

for result in results:
    print(f"Table: {result['table_name']}")
    print(f"Purpose: {result['business_purpose']}")
    print(f"Similarity: {result['similarity_score']:.3f}")
    print(f"Columns: {[col['name'] for col in result['schema']['columns']]}")
    print("---")
```

### Relationship Documentation Search

```python
from src.vector.search import search_relationship_documentation

# Search for foreign key relationships
results = search_relationship_documentation("customer order relationships", limit=3)

for result in results:
    print(f"Relationship: {result['relationship_id']}")
    print(f"Type: {result['relationship_type']}")
    print(f"Tables: {' ‚Üí '.join(result['tables_involved'])}")
    print(f"Description: {result['documentation']}")
    print(f"Similarity: {result['similarity_score']:.3f}")
    print("---")
```

### Comprehensive Search Across All Types

```python
from src.vector.search import semantic_search_all_documentation

# Search across all documentation types
results = semantic_search_all_documentation("inventory management", limit=10)

print(f"Found {results['total_results']} total results")

# Display table results
if results['tables']:
    print(f"\nTables ({len(results['tables'])} results):")
    for table in results['tables']:
        print(f"  ‚Ä¢ {table['table_name']}: {table['business_purpose']}")
        print(f"    Similarity: {table['similarity_score']:.3f}")

# Display relationship results  
if results['relationships']:
    print(f"\nRelationships ({len(results['relationships'])} results):")
    for rel in results['relationships']:
        print(f"  ‚Ä¢ {rel['relationship_id']}: {rel['documentation']}")
        print(f"    Similarity: {rel['similarity_score']:.3f}")
```

## üìä Response Structure

### Table Search Results

```json
[
  {
    "table_name": "users",
    "business_purpose": "Stores user account information and authentication data",
    "similarity_score": 0.924,
    "schema": {
      "columns": [
        {"name": "id", "type": "integer", "primary_key": true},
        {"name": "username", "type": "varchar", "nullable": false},
        {"name": "email", "type": "varchar", "nullable": false}
      ]
    }
  },
  {
    "table_name": "user_profiles",
    "business_purpose": "Extended user profile information and preferences",
    "similarity_score": 0.856,
    "schema": {
      "columns": [
        {"name": "user_id", "type": "integer", "primary_key": false},
        {"name": "first_name", "type": "varchar", "nullable": true},
        {"name": "last_name", "type": "varchar", "nullable": true}
      ]
    }
  }
]
```

### Relationship Search Results

```json
[
  {
    "relationship_id": "users_orders_fk",
    "relationship_type": "one-to-many",
    "documentation": "Each user can have multiple orders, establishing customer-order relationship",
    "tables_involved": ["users", "orders"],
    "similarity_score": 0.889
  },
  {
    "relationship_id": "orders_items_fk", 
    "relationship_type": "one-to-many",
    "documentation": "Each order contains multiple line items with product details",
    "tables_involved": ["orders", "order_items"],
    "similarity_score": 0.743
  }
]
```

### Combined Search Results

```json
{
  "tables": [
    {
      "table_name": "inventory",
      "business_purpose": "Product inventory levels and stock management",
      "similarity_score": 0.912,
      "schema": {"columns": [...]}
    }
  ],
  "relationships": [
    {
      "relationship_id": "products_inventory_fk",
      "relationship_type": "one-to-one", 
      "documentation": "Links products to their current inventory levels",
      "tables_involved": ["products", "inventory"],
      "similarity_score": 0.867
    }
  ],
  "total_results": 2
}
```

## ‚öôÔ∏è Configuration

### Search Parameters

```python
# Function parameters for search customization
search_table_documentation(
    query,                    # Required: Natural language search query
    limit=5                   # Optional: Maximum results to return (default: 5)
)

search_relationship_documentation(
    query,                    # Required: Natural language search query  
    limit=5                   # Optional: Maximum results to return (default: 5)
)

semantic_search_all_documentation(
    query,                    # Required: Natural language search query
    limit=10                  # Optional: Total results across all types (default: 10)
)
```

### Environment Dependencies

```env
# Required for underlying components
OPENAI_API_KEY="your-api-key-here"
DATABASE_URL="your-database-connection-string"

# Optional: Search optimization
SEARCH_SIMILARITY_THRESHOLD="0.5"        # Minimum similarity for results
SEARCH_MAX_RESULTS="50"                   # Global maximum results
SEARCH_CACHE_TTL="300"                    # Cache results for 5 minutes
```

## üéØ Use Cases

### 1. Interactive Documentation Exploration

```python
def interactive_search():
    while True:
        query = input("Search documentation (or 'quit' to exit): ")
        if query.lower() == 'quit':
            break
            
        results = semantic_search_all_documentation(query, limit=8)
        
        if results['total_results'] == 0:
            print("No results found. Try different search terms.")
            continue
            
        print(f"\nFound {results['total_results']} results for '{query}':")
        
        # Show top tables
        for i, table in enumerate(results['tables'][:3], 1):
            print(f"{i}. Table: {table['table_name']}")
            print(f"   Purpose: {table['business_purpose']}")
            print(f"   Relevance: {table['similarity_score']:.1%}")
        
        # Show top relationships
        for i, rel in enumerate(results['relationships'][:2], len(results['tables'][:3]) + 1):
            print(f"{i}. Relationship: {rel['relationship_id']}")
            print(f"   Description: {rel['documentation']}")
            print(f"   Relevance: {rel['similarity_score']:.1%}")

# Run interactive search
interactive_search()
```

### 2. Domain-Specific Search

```python
# Search for specific business domains
domains = {
    "user_management": ["user authentication", "account management", "user profiles"],
    "order_processing": ["order fulfillment", "payment processing", "transaction data"],
    "inventory": ["product catalog", "stock levels", "warehouse management"],
    "analytics": ["reporting data", "metrics tracking", "business intelligence"]
}

def search_by_domain(domain_name):
    if domain_name not in domains:
        print(f"Unknown domain: {domain_name}")
        return
        
    domain_queries = domains[domain_name]
    all_results = []
    
    for query in domain_queries:
        results = semantic_search_all_documentation(query, limit=5)
        all_results.extend(results['tables'])
        all_results.extend(results['relationships'])
    
    # Remove duplicates and sort by similarity
    unique_results = {r.get('table_name') or r.get('relationship_id'): r for r in all_results}
    sorted_results = sorted(unique_results.values(), 
                          key=lambda x: x['similarity_score'], reverse=True)
    
    print(f"Domain '{domain_name}' results:")
    for result in sorted_results[:10]:
        name = result.get('table_name') or result.get('relationship_id')
        purpose = result.get('business_purpose') or result.get('documentation')
        print(f"  ‚Ä¢ {name}: {purpose[:100]}...")

# Search specific domains
search_by_domain("user_management")
search_by_domain("order_processing")
```

### 3. Relevance Threshold Filtering

```python
def filtered_search(query, min_similarity=0.7):
    """Search with relevance filtering."""
    results = semantic_search_all_documentation(query, limit=20)
    
    # Filter results by similarity threshold
    filtered_tables = [
        table for table in results['tables'] 
        if table['similarity_score'] >= min_similarity
    ]
    
    filtered_relationships = [
        rel for rel in results['relationships']
        if rel['similarity_score'] >= min_similarity  
    ]
    
    if not filtered_tables and not filtered_relationships:
        print(f"No results above {min_similarity:.1%} similarity threshold")
        print("Consider lowering the threshold or refining your search query")
        return
    
    print(f"High-relevance results (>{min_similarity:.1%} similarity):")
    
    for table in filtered_tables:
        print(f"Table: {table['table_name']} ({table['similarity_score']:.1%})")
        print(f"  {table['business_purpose']}")
    
    for rel in filtered_relationships:
        print(f"Relationship: {rel['relationship_id']} ({rel['similarity_score']:.1%})")
        print(f"  {rel['documentation']}")

# Search with high relevance threshold
filtered_search("customer data processing", min_similarity=0.8)
```

### 4. Search Result Analysis

```python
def analyze_search_patterns(queries):
    """Analyze search patterns and result quality."""
    analysis = {
        "total_queries": len(queries),
        "avg_results_per_query": 0,
        "high_relevance_count": 0,
        "no_results_count": 0,
        "top_scoring_results": []
    }
    
    total_results = 0
    all_scores = []
    
    for query in queries:
        results = semantic_search_all_documentation(query, limit=10)
        result_count = results['total_results']
        total_results += result_count
        
        if result_count == 0:
            analysis["no_results_count"] += 1
            continue
            
        # Collect similarity scores
        for table in results['tables']:
            score = table['similarity_score']
            all_scores.append(score)
            if score > 0.8:
                analysis["high_relevance_count"] += 1
                analysis["top_scoring_results"].append({
                    "query": query,
                    "result": table['table_name'],
                    "score": score
                })
        
        for rel in results['relationships']:
            score = rel['similarity_score']
            all_scores.append(score)
            if score > 0.8:
                analysis["high_relevance_count"] += 1
                analysis["top_scoring_results"].append({
                    "query": query,
                    "result": rel['relationship_id'],
                    "score": score
                })
    
    analysis["avg_results_per_query"] = total_results / len(queries)
    analysis["avg_similarity_score"] = sum(all_scores) / len(all_scores) if all_scores else 0
    analysis["top_scoring_results"].sort(key=lambda x: x['score'], reverse=True)
    
    return analysis

# Analyze search effectiveness
test_queries = [
    "user authentication", "order processing", "product catalog",
    "customer relationships", "inventory management", "payment data"
]

analysis = analyze_search_patterns(test_queries)
print(f"Search Analysis:")
print(f"  Average results per query: {analysis['avg_results_per_query']:.1f}")
print(f"  Average similarity score: {analysis['avg_similarity_score']:.3f}")
print(f"  High relevance results: {analysis['high_relevance_count']}")
print(f"  Queries with no results: {analysis['no_results_count']}")
```

## üîç Integration with Core Components

### With Documentation Agent

```python
from src.agents.core import PersistentDocumentationAgent
from src.vector.search import semantic_search_all_documentation

# Search after documentation generation
agent = PersistentDocumentationAgent()

# Generate documentation first
agent.process_table_documentation("users")
agent.process_table_documentation("orders")

# Then search the generated documentation
results = semantic_search_all_documentation("user order data")
print(f"Found {results['total_results']} results in generated documentation")
```

### With Entity Recognition

```python
from src.agents.entity_recognition import EntityRecognitionAgent
from src.vector.search import search_table_documentation

# Use search tools for entity discovery
indexer_agent = SQLIndexerAgent(SQLVectorStore())
entity_agent = EntityRecognitionAgent(indexer_agent)

# Compare search approaches
query = "customer information"

# Direct search
direct_results = search_table_documentation(query, limit=5)

# Entity recognition search
entity_results = entity_agent.quick_entity_lookup(query, threshold=0.7)

print("Direct search results:")
for result in direct_results:
    print(f"  {result['table_name']}: {result['similarity_score']:.3f}")

print("\nEntity recognition results:")
for table_name in entity_results:
    print(f"  {table_name}")
```

## üéñÔ∏è Advanced Features

### Intelligent Result Ranking

- **Cross-Type Scoring**: Compares similarity scores across tables and relationships
- **Relevance Thresholds**: Filters low-quality results automatically
- **Result Deduplication**: Removes duplicate results in combined searches
- **Score Normalization**: Ensures consistent scoring across different content types

### Search Optimization

- **Query Enhancement**: Automatic query expansion for better results
- **Result Caching**: Caches frequent searches for improved performance
- **Batch Processing**: Efficient handling of multiple simultaneous searches
- **Memory Management**: Optimized for large documentation sets

### Flexible Search Scopes

```python
# Search specific content types
table_results = search_table_documentation("user data")
relationship_results = search_relationship_documentation("foreign keys")

# Combined search with custom limits
all_results = semantic_search_all_documentation("database schema", limit=15)

# Custom result filtering
def search_with_custom_filter(query, content_filter=None):
    results = semantic_search_all_documentation(query, limit=20)
    
    if content_filter:
        # Apply custom filtering logic
        if content_filter == "high_relevance":
            results['tables'] = [t for t in results['tables'] if t['similarity_score'] > 0.8]
            results['relationships'] = [r for r in results['relationships'] if r['similarity_score'] > 0.8]
        elif content_filter == "primary_keys":
            results['tables'] = [t for t in results['tables'] 
                               if any(col.get('primary_key') for col in t['schema']['columns'])]
    
    results['total_results'] = len(results['tables']) + len(results['relationships'])
    return results
```

## üìà Performance Characteristics

- **Search Speed**: Sub-second response times for typical queries
- **Accuracy**: 85-95% relevance for domain-specific queries
- **Scalability**: Handles databases with 1000+ documented entities efficiently
- **Memory Usage**: Minimal memory footprint with efficient vector operations
- **Concurrent Searches**: Supports multiple simultaneous search operations
- **Result Quality**: Consistently high-quality results with similarity-based ranking

## üö¶ Prerequisites

1. **Indexed Documentation**: Tables and relationships must be processed and indexed
2. **Vector Store**: Functional ChromaDB with embeddings data
3. **OpenAI Access**: Valid API key for query embedding generation
4. **Agent Infrastructure**: PersistentDocumentationAgent with vector indexing enabled
5. **Dependencies**: All required packages from requirements.txt

## üîß Error Handling

### Search Error Scenarios

```python
def robust_search(query, fallback_to_keyword=True):
    """Search with comprehensive error handling."""
    try:
        results = semantic_search_all_documentation(query)
        return {"success": True, "results": results}
        
    except Exception as e:
        print(f"Semantic search failed: {e}")
        
        if fallback_to_keyword and "embedding" in str(e).lower():
            # Fall back to keyword-based search
            print("Falling back to keyword search...")
            return keyword_fallback_search(query)
        
        return {"success": False, "error": str(e), "results": {"tables": [], "relationships": [], "total_results": 0}}

def keyword_fallback_search(query):
    """Simple keyword-based fallback search."""
    # Implement basic keyword matching as fallback
    # This would search table names and descriptions for exact matches
    return {"success": True, "results": {"tables": [], "relationships": [], "total_results": 0}, "fallback": True}

# Use robust search with fallback
result = robust_search("user authentication data")
if result["success"]:
    if result.get("fallback"):
        print("Used keyword fallback search")
    print(f"Found {result['results']['total_results']} results")
else:
    print(f"Search failed: {result['error']}")
```

---

The Semantic Search Tools provide the intelligent search capabilities that make the SQL Documentation suite truly powerful, enabling users to discover relevant database entities and relationships using natural language queries with high accuracy and performance.



================================================
FILE: docs/vector/VectorStore.md
================================================
# Vector Store

The Vector Store is a ChromaDB-based component that manages OpenAI embeddings for SQL documentation. It provides persistent vector storage, semantic search capabilities, and efficient similarity matching for both table and relationship documentation.

## üéØ What It Does

The Vector Store handles vector embeddings and semantic search for SQL documentation:

- **Vector Storage**: Persistent storage of OpenAI embeddings using ChromaDB
- **Semantic Search**: Fast similarity search using cosine distance metrics
- **Document Management**: Add, update, and remove documentation with metadata
- **Dual Indexes**: Separate indexes for table and relationship documentation
- **Local Persistence**: ChromaDB-based local storage without external dependencies
- **Metadata Enrichment**: Rich metadata storage alongside vector embeddings

## üîÑ Vector Flow

```markdown
Documentation Content ‚Üí Text Preparation ‚Üí OpenAI Embeddings ‚Üí ChromaDB Storage ‚Üí Similarity Search
```

1. **Document Preparation**: Converts documentation to searchable text format
2. **Embedding Generation**: Creates 3072-dimensional OpenAI embeddings
3. **Metadata Preparation**: Structures metadata for ChromaDB compatibility  
4. **Vector Storage**: Persists embeddings and metadata in ChromaDB collections
5. **Search Processing**: Converts queries to embeddings for similarity search
6. **Result Ranking**: Returns ranked results with similarity scores

## üöÄ Usage Examples

### Programmatic Usage

```python
from src.vector.store import SQLVectorStore

# Initialize the vector store
vector_store = SQLVectorStore()

# Create indexes for tables and relationships
vector_store.create_table_index()
vector_store.create_relationship_index()

# Add table documentation
table_data = {
    "name": "users",
    "business_purpose": "Stores user account information and authentication data",
    "schema": {
        "columns": ["id", "username", "email", "created_at"]
    },
    "type": "table"
}

vector_store.add_table_document("users", table_data)

# Add relationship documentation
relationship_data = {
    "name": "users_orders_rel",
    "type": "one-to-many", 
    "documentation": "Each user can have multiple orders",
    "tables": ["users", "orders"]
}

vector_store.add_relationship_document("users_orders_fk", relationship_data)

# Search for relevant tables
table_results = vector_store.search_tables("user authentication", limit=5)
for result in table_results:
    print(f"Table: {result['content']['name']} (score: {result['score']:.3f})")
    print(f"Purpose: {result['content']['business_purpose']}")

# Search for relevant relationships  
rel_results = vector_store.search_relationships("user orders", limit=3)
for result in rel_results:
    print(f"Relationship: {result['content']['name']} (score: {result['score']:.3f})")
    print(f"Documentation: {result['content']['documentation']}")
```

### Advanced Search Operations

```python
# Combined search across both indexes
def comprehensive_search(vector_store, query, limit=10):
    """Search across both tables and relationships."""
    
    # Search tables and relationships separately
    table_results = vector_store.search_tables(query, limit=limit//2)
    rel_results = vector_store.search_relationships(query, limit=limit//2)
    
    # Combine and sort by score
    all_results = []
    
    for result in table_results:
        all_results.append({
            "type": "table",
            "name": result["content"]["name"],
            "score": result["score"],
            "content": result["content"]
        })
    
    for result in rel_results:
        all_results.append({
            "type": "relationship", 
            "name": result["content"]["name"],
            "score": result["score"],
            "content": result["content"]
        })
    
    # Sort by relevance score
    all_results.sort(key=lambda x: x["score"], reverse=True)
    
    return all_results[:limit]

# Use comprehensive search
results = comprehensive_search(vector_store, "customer data management")
for result in results:
    print(f"{result['type']}: {result['name']} (score: {result['score']:.3f})")
```

### Batch Operations

```python
# Add multiple documents efficiently
tables_to_add = [
    {
        "name": "customers",
        "business_purpose": "Customer information and contact details",
        "schema": {"columns": ["id", "name", "email", "phone"]},
        "type": "table"
    },
    {
        "name": "orders", 
        "business_purpose": "Customer order tracking and history",
        "schema": {"columns": ["id", "customer_id", "total", "order_date"]},
        "type": "table"
    }
]

# Add each table to the index
for table_data in tables_to_add:
    vector_store.add_table_document(table_data["name"], table_data)
    print(f"Added table: {table_data['name']}")

# Verify storage
for table_data in tables_to_add:
    results = vector_store.search_tables(table_data["name"], limit=1)
    if results and results[0]["score"] > 0.9:
        print(f"‚úì {table_data['name']} successfully stored and searchable")
```

## üìä Response Structure

### Search Results Response

```json
[
  {
    "id": "users",
    "content": {
      "name": "users",
      "business_purpose": "Stores user account information and authentication data",
      "schema_data": {
        "columns": ["id", "username", "email", "created_at"]
      },
      "type": "table",
      "columns": ["id", "username", "email", "created_at"],
      "column_count": 4
    },
    "score": 0.923
  },
  {
    "id": "user_profiles", 
    "content": {
      "name": "user_profiles",
      "business_purpose": "Extended user profile information and preferences",
      "schema_data": {
        "columns": ["user_id", "first_name", "last_name", "bio"]
      },
      "type": "table",
      "columns": ["user_id", "first_name", "last_name", "bio"],
      "column_count": 4
    },
    "score": 0.856
  }
]
```

### Document Metadata Structure

```json
{
  "type": "table",
  "name": "users",
  "description": "Stores user account information",
  "columns": ["id", "username", "email", "created_at"],
  "column_count": 4,
  "business_purpose": "User authentication and profile management",
  "schema_data": {
    "table_name": "users",
    "columns": [
      {"name": "id", "type": "INTEGER", "primary_key": true},
      {"name": "username", "type": "VARCHAR(50)", "nullable": false}
    ]
  }
}
```

## ‚öôÔ∏è Configuration

### Environment Variables

```env
# Required for OpenAI embeddings
OPENAI_API_KEY="your-api-key-here"

# Optional: Customize embedding model  
OPENAI_EMBEDDING_MODEL="text-embedding-3-small"

# Optional: ChromaDB storage location
CHROMA_PERSIST_DIRECTORY="__bin__/data/vector_indexes"

# Optional: Batch processing settings
EMBEDDING_BATCH_SIZE="100"
EMBEDDING_MAX_RETRIES="3"
```

### Initialization Parameters

```python
SQLVectorStore(
    base_path="__bin__/data/vector_indexes",  # Optional: Custom storage path
    vector_index_factory=None                # Optional: Custom index factory
)

# Method parameters
add_table_document(
    table_name,                # Required: Unique table identifier
    content                    # Required: Dictionary with table documentation
)

add_relationship_document(
    relationship_id,           # Required: Unique relationship identifier  
    content                   # Required: Dictionary with relationship documentation
)

search_tables(
    query,                    # Required: Natural language search query
    limit=5                   # Optional: Maximum results to return
)

search_relationships(
    query,                    # Required: Natural language search query
    limit=5                   # Optional: Maximum results to return
)
```

### Storage Structure

```markdown
__bin__/data/vector_indexes/
‚îú‚îÄ‚îÄ chroma.sqlite3           # ChromaDB database file
‚îú‚îÄ‚îÄ tables/                  # Table embeddings collection
‚îî‚îÄ‚îÄ relationships/           # Relationship embeddings collection
```

## üéØ Use Cases

### 1. Semantic Documentation Search

Find relevant documentation using natural language queries:

```python
# Search for authentication-related tables
auth_results = vector_store.search_tables("user authentication login", limit=5)

print("Authentication-related tables:")
for result in auth_results:
    table_name = result["content"]["name"]
    purpose = result["content"]["business_purpose"]
    score = result["score"]
    
    print(f"  {table_name} (relevance: {score:.3f})")
    print(f"    Purpose: {purpose}")

# Search for specific relationship types
rel_results = vector_store.search_relationships("foreign key constraints", limit=5)

print("\nRelationship information:")
for result in rel_results:
    rel_name = result["content"]["name"]
    documentation = result["content"]["documentation"]
    score = result["score"]
    
    print(f"  {rel_name} (relevance: {score:.3f})")
    print(f"    Description: {documentation}")
```

### 2. Content Discovery

Discover related documentation based on context:

```python
def find_related_content(vector_store, table_name, limit=5):
    """Find content related to a specific table."""
    
    # Get the table's documentation first
    table_results = vector_store.search_tables(table_name, limit=1)
    if not table_results:
        return {"tables": [], "relationships": []}
    
    table_info = table_results[0]["content"]
    business_purpose = table_info.get("business_purpose", "")
    
    # Search for related tables using the business purpose
    related_tables = vector_store.search_tables(business_purpose, limit=limit)
    # Exclude the original table
    related_tables = [t for t in related_tables if t["content"]["name"] != table_name]
    
    # Search for related relationships
    search_terms = f"{table_name} {business_purpose}"
    related_relationships = vector_store.search_relationships(search_terms, limit=limit)
    
    return {
        "tables": related_tables[:limit],
        "relationships": related_relationships[:limit]
    }

# Find content related to users table
related = find_related_content(vector_store, "users")
print(f"Found {len(related['tables'])} related tables")
print(f"Found {len(related['relationships'])} related relationships")
```

### 3. Documentation Quality Assessment

Analyze documentation coverage and quality:

```python
def analyze_documentation_coverage(vector_store):
    """Analyze the quality and coverage of stored documentation."""
    
    # Test search quality with common terms
    test_queries = [
        "user management",
        "order processing", 
        "product catalog",
        "authentication",
        "foreign key relationships"
    ]
    
    coverage_report = {
        "total_queries": len(test_queries),
        "queries_with_results": 0,
        "average_top_score": 0.0,
        "query_results": {}
    }
    
    total_top_scores = 0
    
    for query in test_queries:
        # Search both tables and relationships
        table_results = vector_store.search_tables(query, limit=3)
        rel_results = vector_store.search_relationships(query, limit=3)
        
        has_results = len(table_results) > 0 or len(rel_results) > 0
        if has_results:
            coverage_report["queries_with_results"] += 1
        
        # Get top score
        top_score = 0.0
        if table_results:
            top_score = max(top_score, table_results[0]["score"])
        if rel_results:
            top_score = max(top_score, rel_results[0]["score"])
            
        total_top_scores += top_score
        
        coverage_report["query_results"][query] = {
            "has_results": has_results,
            "top_score": top_score,
            "table_results": len(table_results),
            "relationship_results": len(rel_results)
        }
    
    coverage_report["average_top_score"] = total_top_scores / len(test_queries)
    
    return coverage_report

# Analyze documentation quality
report = analyze_documentation_coverage(vector_store)
print(f"Documentation Coverage Report:")
print(f"  Queries with results: {report['queries_with_results']}/{report['total_queries']}")
print(f"  Average top score: {report['average_top_score']:.3f}")
```

### 4. Custom Vector Index Implementation

Create custom vector indexes for specialized use cases:

```python
class CustomVectorIndex:
    """Custom vector index implementation for specialized requirements."""
    
    def __init__(self, collection_name):
        self.collection_name = collection_name
        self.documents = {}  # In-memory storage for demo
        
    def add(self, id, vector, metadata=None):
        """Add document to custom index."""
        self.documents[id] = {
            "vector": vector,
            "metadata": metadata or {}
        }
        
    def search(self, vector, k=5):
        """Custom search implementation."""
        import numpy as np
        
        results = []
        for doc_id, doc_data in self.documents.items():
            # Calculate cosine similarity
            similarity = np.dot(vector, doc_data["vector"]) / (
                np.linalg.norm(vector) * np.linalg.norm(doc_data["vector"])
            )
            
            results.append({
                "id": doc_id,
                "metadata": doc_data["metadata"],
                "score": similarity
            })
        
        # Sort by similarity and return top k
        results.sort(key=lambda x: x["score"], reverse=True)
        return results[:k]
    
    def save(self):
        """Save index state."""
        pass

# Use custom index factory
def custom_index_factory(path):
    collection_name = os.path.basename(path).replace('.db', '')
    return CustomVectorIndex(collection_name)

# Create vector store with custom index
custom_vector_store = SQLVectorStore(
    base_path="__bin__/data/custom_indexes",
    vector_index_factory=custom_index_factory
)
```

## üîç Integration with Other Components

The Vector Store provides the foundation for semantic search capabilities:

- **Indexer Agent**: Uses vector store for document storage and retrieval
- **Entity Recognition Agent**: Leverages vector search for entity discovery
- **Search Tools**: Provides semantic search functionality for documentation
- **Embeddings Client**: Generates vectors for storage and search operations

## üéñÔ∏è Advanced Features

### ChromaDB Integration

- **Persistent Storage**: Local ChromaDB storage with SQLite backend
- **Collection Management**: Separate collections for tables and relationships  
- **Metadata Support**: Rich metadata storage alongside embeddings
- **Query Optimization**: Efficient similarity search with distance metrics

### Vector Operations

- **Embedding Generation**: OpenAI text-embedding-3-small (3072 dimensions)
- **Similarity Calculation**: Cosine distance with similarity score conversion
- **Result Ranking**: Automatic sorting by relevance scores
- **Batch Processing**: Efficient bulk operations for large datasets

### Document Management

- **Text Preparation**: Intelligent text preprocessing for embeddings
- **Metadata Enrichment**: Automatic metadata generation and formatting
- **Update Operations**: Support for updating existing documents
- **Removal Operations**: Clean removal of documents from indexes

### Performance Optimization

- **Local Storage**: No external dependencies for vector operations
- **Memory Efficiency**: Optimized memory usage for large document sets
- **Query Caching**: Efficient caching of frequently accessed embeddings
- **Concurrent Access**: Thread-safe operations for multi-user scenarios

## üìà Performance Characteristics

- **Storage**: Handles 10,000+ documents efficiently with ChromaDB
- **Search Speed**: Sub-second response times for typical queries
- **Memory Usage**: Minimal memory footprint with lazy loading
- **Scalability**: Linear scaling with document count
- **Accuracy**: High-quality semantic search with OpenAI embeddings
- **Persistence**: Durable storage with automatic recovery

## üö¶ Prerequisites

1. **OpenAI API Access**: Valid API key for embedding generation
2. **ChromaDB**: ChromaDB package for vector storage (installed via requirements.txt)
3. **File System Access**: Write permissions for vector index storage
4. **Disk Space**: Storage space for embeddings (typically 1-10MB per 1000 documents)
5. **Dependencies**: All required packages from requirements.txt

## üîß Error Handling

### Common Error Scenarios

1. **ChromaDB Initialization**: Collection creation or connection failures
2. **OpenAI API Issues**: Rate limiting, authentication, or network problems
3. **Storage Issues**: Disk space, permissions, or file system problems
4. **Invalid Documents**: Malformed document data or missing required fields
5. **Search Failures**: Query processing or result retrieval errors

### Error Recovery

```python
try:
    # Initialize vector store
    vector_store = SQLVectorStore()
    vector_store.create_table_index()
    vector_store.create_relationship_index()
    
    print("Vector store initialized successfully")
    
except Exception as e:
    print(f"Vector store initialization failed: {e}")
    
    if "chromadb" in str(e).lower():
        print("ChromaDB initialization issue:")
        print("  - Check if ChromaDB is properly installed")
        print("  - Verify write permissions to storage directory")
        print("  - Ensure sufficient disk space")
    elif "openai" in str(e).lower():
        print("OpenAI API issue:")
        print("  - Check OPENAI_API_KEY environment variable")
        print("  - Verify API key has embeddings access")
        print("  - Check network connectivity")
    else:
        print("General initialization error:")
        print("  - Check file system permissions")
        print("  - Verify all dependencies are installed")

# Test basic operations
try:
    # Test document addition
    test_doc = {
        "name": "test_table",
        "business_purpose": "Test table for verification",
        "schema": {"columns": ["id"]},
        "type": "table"
    }
    
    vector_store.add_table_document("test_table", test_doc)
    
    # Test search
    results = vector_store.search_tables("test", limit=1)
    
    if results and results[0]["content"]["name"] == "test_table":
        print("‚úì Vector store working correctly")
    else:
        print("‚ö† Vector store may have issues")
        
except Exception as e:
    print(f"Vector store operation failed: {e}")
```

### Recovery Strategies

```python
def repair_vector_store(base_path):
    """Attempt to repair corrupted vector store."""
    try:
        import shutil
        
        # Backup existing data
        backup_path = f"{base_path}_backup"
        if os.path.exists(base_path):
            shutil.copytree(base_path, backup_path)
            print(f"Backed up existing data to {backup_path}")
        
        # Reinitialize vector store
        if os.path.exists(base_path):
            shutil.rmtree(base_path)
            
        vector_store = SQLVectorStore(base_path=base_path)
        vector_store.create_table_index()
        vector_store.create_relationship_index()
        
        print("Vector store reinitialized successfully")
        return vector_store
        
    except Exception as e:
        print(f"Vector store repair failed: {e}")
        return None

def check_vector_store_health(vector_store):
    """Check vector store health and performance."""
    health_report = {
        "status": "unknown",
        "issues": [],
        "recommendations": []
    }
    
    try:
        # Test basic operations
        test_doc = {"name": "health_check", "type": "table", "business_purpose": "test"}
        vector_store.add_table_document("health_check", test_doc)
        
        results = vector_store.search_tables("health_check", limit=1)
        
        if results and len(results) > 0:
            health_report["status"] = "healthy"
        else:
            health_report["status"] = "degraded"
            health_report["issues"].append("Search not returning expected results")
            
    except Exception as e:
        health_report["status"] = "unhealthy"
        health_report["issues"].append(f"Basic operations failing: {e}")
        health_report["recommendations"].append("Consider reinitializing vector store")
    
    return health_report
```



================================================
FILE: smol-sql-agents/API_DOCUMENTATION.md
================================================
# Smol-SQL-Agents API Documentation

## Overview

The Smol-SQL-Agents backend provides a RESTful API for natural language to SQL conversion, database schema exploration, and AI-powered documentation search. All endpoints are prefixed with `/api` and return JSON responses.

**Base URL**: `http://localhost:5000`

---

## üìã Table of Contents

- [Health & Status Endpoints](#health--status-endpoints)
- [Query Processing Endpoints](#query-processing-endpoints)
- [Documentation & Schema Endpoints](#documentation--schema-endpoints)
- [Debug Endpoints](#debug-endpoints)
- [Error Handling](#error-handling)

---

## üè• Health & Status Endpoints

### GET `/api/message`

**Health check endpoint**

**Input**: None

**Output**:

```json
{
  "message": "Hello from the Smol-SQL-Agents backend! üëã"
}
```

**Status Codes**: 200

---

### GET `/api/status`

**Get SQL Agents status and system information**

**Input**: None

**Output**:

```json
{
  "sql_agents_available": true,
  "initialized": true,
  "environment": {
    "database_url_set": true,
    "openai_key_set": true
  },
  "agents": ["main_agent", "indexer_agent", "entity_agent", "business_agent"],
  "initialization_time": "2024-01-15T10:30:00",
  "factory_initialized": true
}
```

**Error Response** (500):

```json
{
  "error": "Error message",
  "sql_agents_available": false,
  "initialized": false
}
```

---

## üîç Query Processing Endpoints

### POST `/api/query`

**Execute natural language query and return SQL + results**

**Input**:

```json
{
  "query": "Show me the top 10 customers by total spending"
}
```

**Output** (Success):

```json
{
  "sql": "SELECT c.name, SUM(o.total) as total_spending FROM customers c JOIN orders o ON c.id = o.customer_id GROUP BY c.id, c.name ORDER BY total_spending DESC LIMIT 10;",
  "results": [
    {
      "name": "John Doe",
      "total_spending": 1500.00
    }
  ],
  "query": "Show me the top 10 customers by total spending",
  "timestamp": "2024-01-15T10:30:00",
  "success": true,
  "pipeline_results": {
    "entity_recognition": {...},
    "business_context": {...},
    "sql_generation": {...}
  }
}
```

**Error Response** (400/500):

```json
{
  "sql": "",
  "results": [],
  "query": "query text",
  "timestamp": "2024-01-15T10:30:00",
  "success": false,
  "error": "Error message"
}
```

---

### POST `/api/recognize-entities`

**Recognize applicable database entities for a user query**

**Input**:

```json
{
  "query": "Find customers who made purchases",
  "intent": "customer_analysis",
  "max_entities": 5
}
```

**Output** (Success):

```json
{
  "success": true,
  "applicable_entities": [
    {
      "name": "customers",
      "confidence": 0.95,
      "reason": "Directly mentioned in query"
    },
    {
      "name": "orders",
      "confidence": 0.88,
      "reason": "Related to purchases"
    }
  ]
}
```

**Error Response** (400/500):

```json
{
  "success": false,
  "error": "Error message",
  "applicable_entities": []
}
```

---

### POST `/api/business-context`

**Gather business context for a user query**

**Input**:

```json
{
  "query": "Analyze customer spending patterns",
  "intent": "business_analytics"
}
```

**Output** (Success):

```json
{
  "success": true,
  "matched_concepts": [
    {
      "name": "customer_analytics",
      "similarity": 0.92
    },
    {
      "name": "revenue_tracking",
      "similarity": 0.88
    }
  ],
  "business_instructions": [
    {
      "concept": "customer_analytics",
      "instructions": "Focus on high-value customers and spending trends"
    }
  ]
}
```

**Error Response** (400/500):

```json
{
  "success": false,
  "error": "Error message",
  "matched_concepts": [],
  "business_instructions": []
}
```

---

### POST `/api/generate-sql`

**Generate SQL from natural language using the complete pipeline**

**Input**:

```json
{
  "query": "Find customers who spent more than $1000",
  "intent": "customer_segmentation"
}
```

**Output** (Success):

```json
{
  "success": true,
  "generated_sql": "SELECT * FROM customers WHERE total_spending > 1000",
  "validation": {
    "syntax_valid": true,
    "business_compliant": true,
    "security_valid": true,
    "performance_issues": []
  },
  "optimization_suggestions": [
    {
      "type": "Index Optimization",
      "priority": "medium",
      "impact": "20% performance improvement",
      "message": "Consider adding index on total_spending"
    }
  ]
}
```

**Error Response** (400/500):

```json
{
  "success": false,
  "error": "Error message",
  "generated_sql": "",
  "validation": {},
  "optimization_suggestions": []
}
```

---

## üìö Documentation & Schema Endpoints

### POST `/api/search`

**Search documentation using text or vector search**

**Input**:

```json
{
  "query": "customer information",
  "type": "text"
}
```

**Alternative Input** (Vector Search):

```json
{
  "query": "How to find customer data and their purchase history",
  "type": "vector"
}
```

**Output** (Success):

```json
{
  "results": {
    "tables": [
      {
        "name": "customers",
        "description": "Customer information table",
        "columns": ["id", "name", "email", "created_at"],
        "relevance_score": 0.95
      }
    ],
    "relationships": [
      {
        "from_table": "customers",
        "to_table": "orders",
        "relationship_type": "one-to-many",
        "description": "Customer can have multiple orders"
      }
    ]
  },
  "query": "customer information",
  "type": "text",
  "total": 2
}
```

**Error Response** (400/500):

```json
{
  "results": [],
  "query": "search query",
  "type": "text",
  "total": 0,
  "error": "Error message"
}
```

---

### GET `/api/schema`

**Get database schema information**

**Input**: None

**Output** (Success):

```json
{
  "tables": [
    {
      "name": "customers",
      "columns": [
        {
          "name": "id",
          "type": "INTEGER",
          "nullable": false,
          "primary_key": true
        },
        {
          "name": "name",
          "type": "VARCHAR(255)",
          "nullable": false,
          "primary_key": false
        }
      ]
    }
  ],
  "success": true
}
```

**Error Response** (500):

```json
{
  "tables": [],
  "success": false,
  "error": "Error message"
}
```

---

## üêõ Debug Endpoints

### GET `/api/debug/objects`

**Debug endpoint to show object creation status**

**Input**: None

**Output** (Success):

```json
{
  "agent_manager_id": 140234567890,
  "factory_initialized": true,
  "initialization_time": "2024-01-15T10:30:00",
  "shared_components": {
    "llm_model": true,
    "database_tools": true,
    "instances_count": 5,
    "shared_components_count": 2
  },
  "agent_instances": {
    "main_agent": 140234567891,
    "indexer_agent": 140234567892,
    "entity_agent": 140234567893
  },
  "shared_components": {
    "concept_loader": 140234567894,
    "concept_matcher": 140234567895
  }
}
```

**Error Response** (500):

```json
{
  "error": "Error message",
  "objects": {}
}
```

---

### GET `/api/debug/database`

**Debug endpoint to show database performance metrics**

**Input**: None

**Output** (Success):

```json
{
  "database_inspector_id": 140234567896,
  "cache_stats": {
    "table_names_cached": true,
    "table_schemas_cached": 15,
    "relationships_cached": true,
    "initialization_time": 2.5
  },
  "connection_pool": {
    "pool_size": 5,
    "checked_in": 3,
    "checked_out": 2,
    "overflow": 0
  },
  "engine_config": {
    "echo": false,
    "pool_pre_ping": true,
    "pool_recycle": 3600
  }
}
```

**Error Response** (500):

```json
{
  "error": "Error message",
  "database_stats": {}
}
```

---

## üè† Root Endpoint

### GET `/`

**Root endpoint with API information**

**Input**: None

**Output**:

```json
{
  "message": "Smol-SQL-Agents Backend API",
  "version": "1.0.0",
  "endpoints": {
    "health": "/api/message",
    "status": "/api/status",
    "query": "/api/query",
    "schema": "/api/schema"
  }
}
```

---

## ‚ö†Ô∏è Error Handling

### Standard Error Response Format

All endpoints return consistent error responses with the following structure:

```json
{
  "success": false,
  "error": "Error description",
  "message": "Additional error details"
}
```

### HTTP Status Codes

| Status Code | Description |
|-------------|-------------|
| 200 | Success |
| 400 | Bad Request - Invalid input data |
| 404 | Not Found - Endpoint doesn't exist |
| 500 | Internal Server Error - Server-side error |

### Common Error Scenarios

#### Agent Manager Not Available

```json
{
  "success": false,
  "error": "Agent manager not available"
}
```

#### No JSON Data Provided

```json
{
  "success": false,
  "error": "No JSON data provided"
}
```

#### Query Cannot Be Empty

```json
{
  "success": false,
  "error": "Query cannot be empty"
}
```

---

## üîß Environment Variables

The API requires the following environment variables:

| Variable | Description | Required |
|----------|-------------|----------|
| `DATABASE_URL` | Database connection string | Yes |
| `OPENAI_API_KEY` | OpenAI API key for LLM operations | Yes |
| `SECRET_KEY` | Flask secret key | No (defaults to 'dev-secret-key') |
| `PORT` | Server port | No (defaults to 5000) |
| `FLASK_ENV` | Flask environment | No (defaults to production) |

---

## üìù Usage Examples

### cURL Examples

**Health Check**:

```bash
curl http://localhost:5000/api/message
```

**Execute Query**:

```bash
curl -X POST http://localhost:5000/api/query \
  -H "Content-Type: application/json" \
  -d '{"query": "Show me the top 10 customers"}'
```

**Search Documentation**:

```bash
curl -X POST http://localhost:5000/api/search \
  -H "Content-Type: application/json" \
  -d '{"query": "customer data", "type": "text"}'
```

**Get Schema**:

```bash
curl http://localhost:5000/api/schema
```

### JavaScript/Fetch Examples

**Execute Query**:

```javascript
const response = await fetch('http://localhost:5000/api/query', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    query: 'Show me the top 10 customers'
  })
});

const data = await response.json();
console.log(data.sql); // Generated SQL
console.log(data.results); // Query results
```

**Search Documentation**:

```javascript
const response = await fetch('http://localhost:5000/api/search', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    query: 'customer information',
    type: 'vector'
  })
});

const data = await response.json();
console.log(data.results.tables); // Found tables
console.log(data.results.relationships); // Found relationships
```

---

## üöÄ Getting Started

1. **Set Environment Variables**:

   ```bash
   export DATABASE_URL="your_database_connection_string"
   export OPENAI_API_KEY="your_openai_api_key"
   ```

2. **Start the Server**:

   ```bash
   cd smol-sql-agents/backend
   python app.py
   ```

3. **Test Health Check**:

   ```bash
   curl http://localhost:5000/api/message
   ```

4. **Check Status**:

   ```bash
   curl http://localhost:5000/api/status
   ```

---

## üìä API Status Monitoring

Use the `/api/status` endpoint to monitor:

- SQL Agents availability
- System initialization status
- Environment configuration
- Available agents list
- Initialization timing

---

*Last updated: January 2024*



================================================
FILE: smol-sql-agents/di.py
================================================
import os
import logging
import sys
import asyncio
import concurrent.futures
from pathlib import Path
from dotenv import load_dotenv
from typing import List, Dict, Any, Optional
import time
from contextlib import contextmanager

# FIXED: Consistent relative imports from sql_agents package
from src.database.inspector import DatabaseInspector
from src.agents.core import PersistentDocumentationAgent
from src.agents.entity_recognition import EntityRecognitionAgent
from src.agents.business import BusinessContextAgent
from src.agents.nl2sql import NL2SQLAgent
from src.agents.integration import SQLAgentPipeline
from src.agents.tools.factory import DatabaseToolsFactory
from src.agents.concepts.loader import ConceptLoader
from src.agents.concepts.matcher import ConceptMatcher
from src.output.formatters import DocumentationFormatter
from src.agents.batch_manager import BatchIndexingManager
from .di import SharedInstanceManager

# Global shared instance manager
shared_manager = SharedInstanceManager()

load_dotenv()


class SharedInstanceManager:
    """Manages shared instances to avoid repeated instantiation costs."""
    
    def __init__(self):
        self._main_agent = None
        self._database_tools = None
        self._shared_llm_model = None
        self._entity_agent = None
        self._business_agent = None
        self._nl2sql_agent = None
        self._concept_loader = None
        self._concept_matcher = None
        self._initialized = False
        self.api_routes = None
    
    def initialize(self):
        """Initialize all shared instances."""
        if self._initialized:
            return
        
        logger = logging.getLogger(__name__)
        logger.info("Initializing shared instances...")
        
        try:
            self.api_routes = ApiRoutes()
            api_bp = self.api_routes.api_bp
                
            # Initialize shared LLM model first
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEY environment variable is not set")
            
            from smolagents.models import OpenAIModel
            self._shared_llm_model = OpenAIModel(model_id="gpt-4o-mini", api_key=api_key)
            
            # Initialize main agent (contains indexer_agent) with shared LLM model
            self._main_agent = PersistentDocumentationAgent(shared_llm_model=self._shared_llm_model)
            
            # Initialize database tools using unified factory
            database_inspector = DatabaseInspector()
            self._database_tools = DatabaseToolsFactory.create_database_tools(database_inspector)
            
            # Initialize concept components
            concepts_dir = "src/agents/concepts/examples"
            self._concept_loader = ConceptLoader(concepts_dir)
            self._concept_matcher = ConceptMatcher(self._main_agent.indexer_agent)
            
            # Initialize agents with shared components
            self._entity_agent = EntityRecognitionAgent(
                self._main_agent.indexer_agent,
                shared_llm_model=self._shared_llm_model
            )
            
            self._business_agent = BusinessContextAgent(
                indexer_agent=self._main_agent.indexer_agent,
                concepts_dir=concepts_dir,
                shared_llm_model=self._shared_llm_model,
                shared_concept_loader=self._concept_loader,
                shared_concept_matcher=self._concept_matcher
            )
            
            self._nl2sql_agent = NL2SQLAgent(
                self._database_tools,
                shared_llm_model=self._shared_llm_model
            )
            
            self._initialized = True
            logger.info("Shared instances initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize shared instances: {e}")
            raise
    
    @property
    def main_agent(self):
        """Get the main documentation agent."""
        if not self._initialized:
            self.initialize()
        return self._main_agent
    
    @property
    def database_tools(self):
        """Get the database tools instance."""
        if not self._initialized:
            self.initialize()
        return self._database_tools
    
    @property
    def entity_agent(self):
        """Get the entity recognition agent."""
        if not self._initialized:
            self.initialize()
        return self._entity_agent
    
    @property
    def business_agent(self):
        """Get the business context agent."""
        if not self._initialized:
            self.initialize()
        return self._business_agent
    
    @property
    def nl2sql_agent(self):
        """Get the NL2SQL agent."""
        if not self._initialized:
            self.initialize()
        return self._nl2sql_agent
    
    @property
    def indexer_agent(self):
        """Get the indexer agent from main agent."""
        if not self._initialized:
            self.initialize()
        return self._main_agent.indexer_agent
    
    def reset(self):
        """Reset all shared instances (useful for testing)."""
        self._main_agent = None
        self._database_tools = None
        self._shared_llm_model = None
        self._entity_agent = None
        self._business_agent = None
        self._nl2sql_agent = None
        self._concept_loader = None
        self._concept_matcher = None
        self._initialized = False



================================================
FILE: smol-sql-agents/requirements.txt
================================================
# requirements.txt

# AI Agent Framework
smolagents

# LLM Abstraction Layer
litellm

# Database Toolkit
SQLAlchemy

# Database-specific driver (example for PostgreSQL)
psycopg2-binary

# Environment variable management
python-dotenv

# Additional for persistence and formatting
jinja2                        # Template engine for output formatting

# Vector operations and embeddings
chromadb                      # Vector database for local storage
openai                       # OpenAI API client for embeddings
numpy                        # Vector operations
tiktoken                     # Token counting for OpenAI
tenacity                     # Retry mechanism for API calls
pyodbc                       # ODBC driver for SQL Server

# SQL Parsing
sqlparse

# Environment variables
dotenv

# Testing
pytest                       # Testing framework
pytest-cov                   # Test coverage reporting


================================================
FILE: smol-sql-agents/run_tests.py
================================================
#!/usr/bin/env python3
"""
Simple test script to verify the SQL agents structure.
Run with: python -m run_tests
"""

import os
import sys
import logging

# Add the src directory to the path
current_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.join(current_dir, 'src')
sys.path.insert(0, src_dir)

def test_basic_imports():
    """Test basic imports without relative imports."""
    print("Testing basic imports...")
    
    try:
        # Test direct imports
        import yaml
        print("‚úì yaml import successful")
        
        import sqlparse
        print("‚úì sqlparse import successful")
        
        # Test our modules
        from agents.concepts.loader import ConceptLoader
        print("‚úì ConceptLoader import successful")
        
        from validation.business_validator import BusinessValidator
        print("‚úì BusinessValidator import successful")
        
        from validation.tsql_validator import TSQLValidator
        print("‚úì TSQLValidator import successful")
        
        from validation.query_optimizer import QueryOptimizer
        print("‚úì QueryOptimizer import successful")
        
        return True
        
    except ImportError as e:
        print(f"‚úó Import error: {e}")
        return False
    except Exception as e:
        print(f"‚úó Unexpected error: {e}")
        return False

def test_concept_loader():
    """Test the concept loader functionality."""
    print("\nTesting concept loader...")
    
    try:
        from agents.concepts.loader import ConceptLoader
        
        # Create a concept loader
        concepts_dir = os.path.join(src_dir, "agents", "concepts")
        loader = ConceptLoader(concepts_dir)
        
        # Test getting all concepts
        all_concepts = loader.get_all_concepts()
        print(f"‚úì Concept loader initialized, found {len(all_concepts)} concepts")
        
        return True
        
    except Exception as e:
        print(f"‚úó Concept loader test failed: {e}")
        return False

def test_validators():
    """Test the validators."""
    print("\nTesting validators...")
    
    try:
        from validation.business_validator import BusinessValidator
        from validation.tsql_validator import TSQLValidator
        from validation.query_optimizer import QueryOptimizer
        
        # Test business validator
        business_validator = BusinessValidator()
        print("‚úì Business validator initialized")
        
        # Test T-SQL validator
        tsql_validator = TSQLValidator()
        print("‚úì T-SQL validator initialized")
        
        # Test query optimizer
        query_optimizer = QueryOptimizer()
        print("‚úì Query optimizer initialized")
        
        # Test basic validation
        test_query = "SELECT * FROM customers"
        syntax_result = tsql_validator.validate_syntax(test_query)
        print(f"‚úì T-SQL validation test: {syntax_result.get('valid', False)}")
        
        return True
        
    except Exception as e:
        print(f"‚úó Validator test failed: {e}")
        return False

def main():
    """Run all tests."""
    print("SQL Agents Structure Test")
    print("=" * 40)
    
    # Setup logging
    logging.basicConfig(level=logging.INFO)
    
    tests = [
        test_basic_imports,
        test_concept_loader,
        test_validators
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        if test():
            passed += 1
    
    print("\n" + "=" * 40)
    print(f"Test Results: {passed}/{total} tests passed")
    
    if passed == total:
        print("‚úì All tests passed! The SQL agents structure is working correctly.")
        return 0
    else:
        print("‚úó Some tests failed. Please check the errors above.")
        return 1

if __name__ == "__main__":
    exit(main()) 


================================================
FILE: smol-sql-agents/setup.py
================================================
from setuptools import setup, find_packages

setup(
    name="sql-doc-agent",
    version="0.1",
    packages=find_packages(),
    install_requires=[
        "openai>=1.12.0",
        "numpy>=1.26.4",
        "tiktoken>=0.6.0",
        "vectordb>=0.0.21",
        "pytest>=8.0.0",
        "pytest-mock>=3.12.0",
        "tenacity>=8.0.0"
    ],
    python_requires=">=3.8"
)

from setuptools import setup, find_packages

setup(
    name="sql-doc-agent",
    version="0.1",
    packages=find_packages(),
    install_requires=[
        "openai>=1.12.0",
        "numpy>=1.26.4",
        "tiktoken>=0.6.0",
        "vectordb>=1.0.0",
        "pytest>=8.0.0",
        "pytest-mock>=3.12.0",
        "tenacity>=8.0.0"
    ],
    python_requires=">=3.8",
)



================================================
FILE: smol-sql-agents/simple_test.py
================================================
#!/usr/bin/env python3
"""
Simple test script that avoids relative imports to test the basic structure.
"""

import os
import sys
import logging

# Add the src directory to the path
current_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.join(current_dir, 'src')
sys.path.insert(0, src_dir)

def test_direct_imports():
    """Test direct imports without relative imports."""
    print("Testing direct imports...")
    
    try:
        # Test external libraries
        import yaml
        print("‚úì yaml import successful")
        
        import sqlparse
        print("‚úì sqlparse import successful")
        
        # Test our validation modules directly
        import validation.business_validator
        print("‚úì business_validator module import successful")
        
        import validation.tsql_validator
        print("‚úì tsql_validator module import successful")
        
        import validation.query_optimizer
        print("‚úì query_optimizer module import successful")
        
        return True
        
    except ImportError as e:
        print(f"‚úó Import error: {e}")
        return False
    except Exception as e:
        print(f"‚úó Unexpected error: {e}")
        return False

def test_validation_classes():
    """Test validation class instantiation."""
    print("\nTesting validation classes...")
    
    try:
        from validation.business_validator import BusinessValidator
        from validation.tsql_validator import TSQLValidator
        from validation.query_optimizer import QueryOptimizer
        
        # Test business validator
        business_validator = BusinessValidator()
        print("‚úì Business validator instantiated")
        
        # Test T-SQL validator
        tsql_validator = TSQLValidator()
        print("‚úì T-SQL validator instantiated")
        
        # Test query optimizer
        query_optimizer = QueryOptimizer()
        print("‚úì Query optimizer instantiated")
        
        return True
        
    except Exception as e:
        print(f"‚úó Validation class test failed: {e}")
        return False

def test_validation_functionality():
    """Test basic validation functionality."""
    print("\nTesting validation functionality...")
    
    try:
        from validation.tsql_validator import TSQLValidator
        
        tsql_validator = TSQLValidator()
        
        # Test syntax validation
        test_query = "SELECT * FROM customers"
        result = tsql_validator.validate_syntax(test_query)
        print(f"‚úì Syntax validation test: {result.get('valid', False)}")
        
        # Test performance patterns
        performance_issues = tsql_validator.check_performance_patterns(test_query)
        print(f"‚úì Performance pattern check: {len(performance_issues)} issues found")
        
        # Test security validation
        security_result = tsql_validator.validate_security(test_query)
        print(f"‚úì Security validation test: {security_result.get('valid', False)}")
        
        return True
        
    except Exception as e:
        print(f"‚úó Validation functionality test failed: {e}")
        return False

def test_query_optimizer():
    """Test query optimizer functionality."""
    print("\nTesting query optimizer...")
    
    try:
        from validation.query_optimizer import QueryOptimizer
        
        optimizer = QueryOptimizer()
        
        # Test performance analysis
        test_query = "SELECT * FROM customers"
        analysis = optimizer.analyze_performance(test_query)
        print(f"‚úì Performance analysis: complexity score {analysis.get('complexity_score', 0)}")
        
        # Test optimization suggestions
        suggestions = optimizer._get_optimization_suggestions(test_query)
        print(f"‚úì Optimization suggestions: {len(suggestions)} suggestions")
        
        return True
        
    except Exception as e:
        print(f"‚úó Query optimizer test failed: {e}")
        return False

def main():
    """Run all tests."""
    print("SQL Agents Structure Test (Simple)")
    print("=" * 40)
    
    # Setup logging
    logging.basicConfig(level=logging.INFO)
    
    tests = [
        test_direct_imports,
        test_validation_classes,
        test_validation_functionality,
        test_query_optimizer
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        if test():
            passed += 1
    
    print("\n" + "=" * 40)
    print(f"Test Results: {passed}/{total} tests passed")
    
    if passed == total:
        print("‚úì All tests passed! The SQL agents structure is working correctly.")
        return 0
    else:
        print("‚úó Some tests failed. Please check the errors above.")
        return 1

if __name__ == "__main__":
    exit(main()) 


================================================
FILE: smol-sql-agents/test_db_performance.py
================================================
#!/usr/bin/env python3
"""
Database Performance Test Script
Tests the performance improvements in DatabaseInspector
"""

import time
import os
import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from database.inspector import DatabaseInspector, get_database_inspector

def test_initialization_performance():
    """Test the performance difference between old and new initialization."""
    print("üîç Testing DatabaseInspector Performance")
    print("=" * 50)
    
    # Test 1: Old style (eager loading)
    print("\n1. Testing eager loading (old style):")
    start_time = time.time()
    try:
        inspector1 = DatabaseInspector(lazy_load=False)
        eager_time = time.time() - start_time
        print(f"   ‚úÖ Eager loading took: {eager_time:.2f}s")
    except Exception as e:
        print(f"   ‚ùå Eager loading failed: {e}")
        eager_time = None
    
    # Test 2: Lazy loading (new style)
    print("\n2. Testing lazy loading (new style):")
    start_time = time.time()
    try:
        inspector2 = DatabaseInspector(lazy_load=True)
        lazy_time = time.time() - start_time
        print(f"   ‚úÖ Lazy loading took: {lazy_time:.2f}s")
    except Exception as e:
        print(f"   ‚ùå Lazy loading failed: {e}")
        lazy_time = None
    
    # Test 3: Singleton pattern
    print("\n3. Testing singleton pattern:")
    start_time = time.time()
    try:
        inspector3 = get_database_inspector(lazy_load=True)
        singleton_time = time.time() - start_time
        print(f"   ‚úÖ Singleton creation took: {singleton_time:.2f}s")
    except Exception as e:
        print(f"   ‚ùå Singleton creation failed: {e}")
        singleton_time = None
    
    # Test 4: Cache warming
    print("\n4. Testing cache warming:")
    if inspector2:
        start_time = time.time()
        try:
            inspector2.warm_up_cache()
            warm_time = time.time() - start_time
            print(f"   ‚úÖ Cache warming took: {warm_time:.2f}s")
        except Exception as e:
            print(f"   ‚ùå Cache warming failed: {e}")
            warm_time = None
    
    # Test 5: Cached operations
    print("\n5. Testing cached operations:")
    if inspector2:
        start_time = time.time()
        try:
            tables = inspector2.get_all_table_names()
            cached_time = time.time() - start_time
            print(f"   ‚úÖ Cached table names took: {cached_time:.4f}s")
            print(f"   üìä Found {len(tables)} tables")
        except Exception as e:
            print(f"   ‚ùå Cached operations failed: {e}")
    
    # Performance comparison
    print("\n" + "=" * 50)
    print("üìä Performance Summary:")
    if eager_time and lazy_time:
        improvement = ((eager_time - lazy_time) / eager_time) * 100
        print(f"   Lazy loading is {improvement:.1f}% faster than eager loading")
    
    if singleton_time:
        print(f"   Singleton creation: {singleton_time:.2f}s")
    
    # Cache stats
    if inspector2:
        stats = inspector2.get_cache_stats()
        print(f"   Cache stats: {stats}")

def test_connection_pool():
    """Test connection pool performance."""
    print("\nüîç Testing Connection Pool")
    print("=" * 30)
    
    try:
        inspector = get_database_inspector()
        
        # Test multiple operations
        operations = [
            ("get_all_table_names", lambda: inspector.get_all_table_names()),
            ("get_table_schema (first table)", lambda: inspector.get_table_schema("users") if "users" in inspector.get_all_table_names() else None),
            ("get_foreign_key_relationships", lambda: inspector.get_all_foreign_key_relationships())
        ]
        
        for name, operation in operations:
            start_time = time.time()
            try:
                result = operation()
                duration = time.time() - start_time
                print(f"   ‚úÖ {name}: {duration:.4f}s")
            except Exception as e:
                print(f"   ‚ùå {name}: Failed - {e}")
        
        # Show pool stats
        pool = inspector.engine.pool
        print(f"\n   üìä Connection Pool Stats:")
        print(f"      Pool size: {pool.size()}")
        print(f"      Checked in: {pool.checkedin()}")
        print(f"      Checked out: {pool.checkedout()}")
        print(f"      Overflow: {pool.overflow()}")
        
    except Exception as e:
        print(f"   ‚ùå Connection pool test failed: {e}")

if __name__ == "__main__":
    # Check if DATABASE_URL is set
    if not os.getenv("DATABASE_URL"):
        print("‚ùå DATABASE_URL environment variable not set")
        print("Please set DATABASE_URL before running this test")
        sys.exit(1)
    
    test_initialization_performance()
    test_connection_pool()
    
    print("\n‚úÖ Performance test completed!") 


================================================
FILE: smol-sql-agents/test_structure.py
================================================
#!/usr/bin/env python3
"""
Test script to verify the SQL agents structure is working correctly.
This script tests imports and basic initialization of all components.
"""

import os
import sys
import logging

# Add the src directory to the path
current_dir = os.path.dirname(os.path.abspath(__file__))
src_dir = os.path.join(current_dir, 'src')
sys.path.insert(0, src_dir)

def test_imports():
    """Test that all components can be imported successfully."""
    print("Testing imports...")
    
    try:
        # Test agent imports
        from agents import (
            BusinessContextAgent, 
            NL2SQLAgent, 
            SQLAgentPipeline,
            EntityRecognitionAgent,
            SQLIndexerAgent
        )
        print("‚úì Agent imports successful")
        
        # Test concept imports
        from agents.concepts import ConceptLoader, ConceptMatcher, BusinessConcept
        print("‚úì Concept imports successful")
        
        # Test validation imports
        from validation import BusinessValidator, TSQLValidator, QueryOptimizer
        print("‚úì Validation imports successful")
        
        # Test database tools import
        from database.tools import DatabaseTools
        print("‚úì Database tools import successful")
        
        return True
        
    except ImportError as e:
        print(f"‚úó Import error: {e}")
        return False
    except Exception as e:
        print(f"‚úó Unexpected error during imports: {e}")
        return False

def test_concept_loader():
    """Test the concept loader functionality."""
    print("\nTesting concept loader...")
    
    try:
        from agents.concepts import ConceptLoader
        
        # Create a concept loader with a test directory
        concepts_dir = os.path.join(src_dir, "agents", "concepts")
        loader = ConceptLoader(concepts_dir)
        
        # Test getting all concepts
        all_concepts = loader.get_all_concepts()
        print(f"‚úì Concept loader initialized, found {len(all_concepts)} concepts")
        
        return True
        
    except Exception as e:
        print(f"‚úó Concept loader test failed: {e}")
        return False

def test_business_validator():
    """Test the business validator functionality."""
    print("\nTesting business validator...")
    
    try:
        from validation import BusinessValidator
        
        validator = BusinessValidator()
        print("‚úì Business validator initialized")
        
        # Test basic validation
        test_query = "SELECT * FROM customers"
        validation_result = validator.validate_against_concepts(test_query, [])
        print(f"‚úì Business validation test completed: {validation_result.get('valid', False)}")
        
        return True
        
    except Exception as e:
        print(f"‚úó Business validator test failed: {e}")
        return False

def test_tsql_validator():
    """Test the T-SQL validator functionality."""
    print("\nTesting T-SQL validator...")
    
    try:
        from validation import TSQLValidator
        
        validator = TSQLValidator()
        print("‚úì T-SQL validator initialized")
        
        # Test basic validation
        test_query = "SELECT * FROM customers"
        validation_result = validator.validate_syntax(test_query)
        print(f"‚úì T-SQL validation test completed: {validation_result.get('valid', False)}")
        
        return True
        
    except Exception as e:
        print(f"‚úó T-SQL validator test failed: {e}")
        return False

def test_query_optimizer():
    """Test the query optimizer functionality."""
    print("\nTesting query optimizer...")
    
    try:
        from validation import QueryOptimizer
        
        optimizer = QueryOptimizer()
        print("‚úì Query optimizer initialized")
        
        # Test basic optimization
        test_query = "SELECT * FROM customers"
        optimization_result = optimizer.analyze_performance(test_query)
        print(f"‚úì Query optimization test completed: {optimization_result.get('complexity_score', 0)}")
        
        return True
        
    except Exception as e:
        print(f"‚úó Query optimizer test failed: {e}")
        return False

def main():
    """Run all tests."""
    print("SQL Agents Structure Test")
    print("=" * 40)
    
    # Setup logging
    logging.basicConfig(level=logging.INFO)
    
    tests = [
        test_imports,
        test_concept_loader,
        test_business_validator,
        test_tsql_validator,
        test_query_optimizer
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        if test():
            passed += 1
    
    print("\n" + "=" * 40)
    print(f"Test Results: {passed}/{total} tests passed")
    
    if passed == total:
        print("‚úì All tests passed! The SQL agents structure is working correctly.")
        return 0
    else:
        print("‚úó Some tests failed. Please check the errors above.")
        return 1

if __name__ == "__main__":
    exit(main()) 


================================================
FILE: smol-sql-agents/backend/__init__.py
================================================
# Backend package initialization
# Import key modules from src to make them available when importing from backend

from src.database.inspector import DatabaseInspector
from src.agents.core import PersistentDocumentationAgent
from src.agents.entity_recognition import EntityRecognitionAgent
from src.agents.business import BusinessContextAgent
from src.agents.nl2sql import NL2SQLAgent
from src.agents.integration import SQLAgentPipeline
from src.agents.tools.factory import DatabaseToolsFactory
from src.agents.concepts.loader import ConceptLoader
from src.agents.concepts.matcher import ConceptMatcher
from src.output.formatters import DocumentationFormatter
from src.agents.batch_manager import BatchIndexingManager

# Make these available for import
__all__ = [
    'DatabaseInspector',
    'PersistentDocumentationAgent',
    'EntityRecognitionAgent',
    'BusinessContextAgent',
    'NL2SQLAgent',
    'SQLAgentPipeline',
    'DatabaseToolsFactory',
    'ConceptLoader',
    'ConceptMatcher',
    'DocumentationFormatter',
    'BatchIndexingManager',
]



================================================
FILE: smol-sql-agents/backend/app.py
================================================
# Standard library imports
from datetime import datetime
import os
import logging
import sys
import time
import concurrent.futures
from pathlib import Path
from typing import List, Dict, Any, Optional
import time
from contextlib import contextmanager

# Third-party imports
from dotenv import load_dotenv
from flask import Blueprint, current_app, jsonify, request, Flask
from flask_cors import CORS

# Add parent directory to Python path to find src module
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

# Local imports
from src.database.inspector import DatabaseInspector
from src.agents.core import PersistentDocumentationAgent
from src.agents.entity_recognition import EntityRecognitionAgent
from src.agents.business import BusinessContextAgent
from src.agents.nl2sql import NL2SQLAgent
from src.agents.integration import SQLAgentPipeline
from src.agents.tools.factory import DatabaseToolsFactory
from src.agents.concepts.loader import ConceptLoader
from src.agents.concepts.matcher import ConceptMatcher
from src.output.formatters import DocumentationFormatter
from src.agents.batch_manager import BatchIndexingManager

logger = logging.getLogger(__name__)

SQL_AGENTS_AVAILABLE = True
print("‚úÖ SQL Agents package imported successfully")

class ApiRoutes:
    def __init__(self):
        
        self.api_bp = Blueprint('api', __name__)
        self.documentation_store = None
        self.register_routes()
        self.register_error_handlers()

    def get_documentation_store(self):
        """Get the documentation store instance."""
        if self.documentation_store is None:
            from src.database.persistence import DocumentationStore
            self.documentation_store = DocumentationStore()
        return self.documentation_store

    @staticmethod
    def get_agent_manager():
        """Get the agent manager from current app."""
        agent_manager = current_app.agent_manager
        if agent_manager and hasattr(agent_manager, 'is_initialized'):
            return agent_manager if agent_manager.is_initialized() else None
        return agent_manager

    def register_routes(self):
        self.api_bp.add_url_rule('/message', view_func=self.get_message)
        self.api_bp.add_url_rule('/status', view_func=self.get_status)
        self.api_bp.add_url_rule('/query', view_func=self.execute_query, methods=['POST'])
        self.api_bp.add_url_rule('/recognize-entities', view_func=self.recognize_entities, methods=['POST'])
        self.api_bp.add_url_rule('/business-context', view_func=self.gather_business_context, methods=['POST'])
        self.api_bp.add_url_rule('/generate-sql', view_func=self.generate_sql, methods=['POST'])
        self.api_bp.add_url_rule('/search', view_func=self.search_documentation, methods=['POST'])
        self.api_bp.add_url_rule('/schema', view_func=self.get_schema)
        self.api_bp.add_url_rule('/debug/objects', view_func=self.debug_objects)
        self.api_bp.add_url_rule('/debug/database', view_func=self.debug_database)
        
        # Documentation endpoints
        self.api_bp.add_url_rule('/documentation/summaries', view_func=self.get_all_summaries)
        self.api_bp.add_url_rule('/documentation/summaries/<item_id>', view_func=self.get_summary_by_id)
        self.api_bp.add_url_rule('/documentation/tables', view_func=self.get_all_table_documentation)
        self.api_bp.add_url_rule('/documentation/tables/<table_name>', view_func=self.get_table_documentation)
        self.api_bp.add_url_rule('/documentation/relationships', view_func=self.get_all_relationship_documentation)
        self.api_bp.add_url_rule('/documentation/relationships/<relationship_id>', view_func=self.get_relationship_documentation)

    def register_error_handlers(self):
        self.api_bp.register_error_handler(400, self.bad_request)
        self.api_bp.register_error_handler(404, self.not_found)
        self.api_bp.register_error_handler(500, self.internal_server_error)

    def get_message(self):
        """Health check endpoint."""
        return jsonify({"message": "Hello from the Smol-SQL-Agents backend! üëã"})


    def get_status(self):
        """Get the status of SQL Agents and available features, with verbose logging and execution time tracking."""
        start_time = time.perf_counter()
        logger.info("Status endpoint called: checking SQL Agents status and environment variables.")
        try:
            agent_manager = self.get_agent_manager()
            agents_available = agent_manager is not None

            env_vars = current_app.config.get('ENV_VARS', {})
            logger.debug(f"Environment variables loaded: {env_vars}")

            status = {
                "sql_agents_available": agents_available,
                "initialized": agents_available,
                "environment": {
                    "database_url_set": bool(env_vars.get('DATABASE_URL')),
                    "openai_key_set": bool(env_vars.get('OPENAI_API_KEY'))
                },
                "agents": list(agent_manager.get_all_agents().keys()) if agent_manager else [],
                "initialization_time": getattr(agent_manager, '_initialization_time', None) if agent_manager else None,
                "factory_initialized": getattr(agent_manager, 'is_initialized', lambda: False)() if agent_manager else False
            }

            logger.info(f"Status response: {status}")
            elapsed = time.perf_counter() - start_time
            logger.info(f"get_status execution time: {elapsed:.4f} seconds")
            return jsonify(status)

        except Exception as e:
            elapsed = time.perf_counter() - start_time
            logger.error(f"Status check failed after {elapsed:.4f} seconds: {e}", exc_info=True)
            return jsonify({
                "error": str(e),
                "sql_agents_available": False,
                "initialized": False
            }), 500

    def execute_query(self):
        """Execute a natural language query and return SQL + results, with verbose logging and execution time tracking."""
        start_time = time.perf_counter()
        logger.info("execute_query endpoint called.")
        try:
            data = request.get_json()
            logger.debug(f"Received data: {data}")
            if not data:
                logger.warning("No JSON data provided in request.")
                return jsonify({"success": False, "error": "No JSON data provided"}), 400

            query = data.get("query", "").strip()
            if not query:
                logger.warning("Query is empty in request.")
                return jsonify({"success": False, "error": "Query cannot be empty"}), 400

            agent_manager = self.get_agent_manager()
            if not agent_manager:
                logger.error("Agent manager not available.")
                return jsonify({
                    "sql": "",
                    "results": [],
                    "query": query,
                    "timestamp": datetime.now().isoformat(),
                    "success": False,
                    "error": "Agent manager not available"
                })

            pipeline = agent_manager.get_sql_pipeline()
            if not pipeline:
                logger.error("SQL pipeline not available.")
                return jsonify({
                    "sql": "",
                    "results": [],
                    "query": query,
                    "timestamp": datetime.now().isoformat(),
                    "success": False,
                    "error": "Pipeline not available"
                })

            logger.info(f"Processing user query: {query}")
            process_start = time.perf_counter()
            results = pipeline.process_user_query(query)
            process_elapsed = time.perf_counter() - process_start
            logger.info(f"pipeline.process_user_query execution time: {process_elapsed:.4f} seconds")

            if results.get("success"):
                sql_generation = results.get("sql_generation", {})
                generated_sql = sql_generation.get("generated_sql", "")
                query_execution = sql_generation.get("query_execution", {})

                sample_data = query_execution.get("sample_data", {})
                results_data = []

                if sample_data and sample_data.get("sample_rows"):
                    results_data = sample_data.get("sample_rows", [])

                logger.info(f"Query executed successfully. SQL: {generated_sql}")
                elapsed = time.perf_counter() - start_time
                logger.info(f"execute_query total execution time: {elapsed:.4f} seconds")
                return jsonify({
                    "sql": generated_sql,
                    "results": results_data,
                    "query": query,
                    "timestamp": datetime.now().isoformat(),
                    "success": True,
                    "pipeline_results": results
                })
            else:
                logger.warning(f"Query execution failed: {results.get('error', 'Unknown error')}")
                elapsed = time.perf_counter() - start_time
                logger.info(f"execute_query total execution time: {elapsed:.4f} seconds")
                return jsonify({
                    "sql": "",
                    "results": [],
                    "query": query,
                    "timestamp": datetime.now().isoformat(),
                    "success": False,
                    "error": results.get("error", "Unknown error")
                })

        except Exception as e:
            elapsed = time.perf_counter() - start_time
            logger.error(f"Query execution failed after {elapsed:.4f} seconds: {e}", exc_info=True)
            return jsonify({
                "sql": "",
                "results": [],
                "query": data.get("query", "") if 'data' in locals() else "",
                "timestamp": datetime.now().isoformat(),
                "success": False,
                "error": str(e)
            }), 500

    def recognize_entities(self):
        """Recognize applicable database entities for a user query, with verbose logging and execution time tracking."""
        start_time = time.perf_counter()
        logger.info("recognize_entities endpoint called.")
        try:
            data = request.get_json()
            logger.debug(f"Received data: {data}")
            if not data:
                logger.warning("No JSON data provided in request.")
                return jsonify({"success": False, "error": "No JSON data provided"}), 400

            query = data.get("query", "")
            intent = data.get("intent", None)
            max_entities = data.get("max_entities", 5)
            logger.debug(f"Recognize entities for query: '{query}', intent: '{intent}', max_entities: {max_entities}")

            agent_manager = self.get_agent_manager()
            if not agent_manager:
                logger.error("Agent manager not available.")
                return jsonify({
                    "success": False,
                    "error": "Agent manager not available",
                    "applicable_entities": []
                })

            entity_agent = agent_manager.get_entity_agent()
            if not entity_agent:
                logger.error("Entity agent not available.")
                return jsonify({
                    "success": False,
                    "error": "Entity agent not available",
                    "applicable_entities": []
                })

            process_start = time.perf_counter()
            results = entity_agent.recognize_entities(query, intent, max_entities)
            process_elapsed = time.perf_counter() - process_start
            logger.info(f"entity_agent.recognize_entities execution time: {process_elapsed:.4f} seconds")
            logger.info(f"Entities recognized: {results.get('applicable_entities', [])}")
            elapsed = time.perf_counter() - start_time
            logger.info(f"recognize_entities total execution time: {elapsed:.4f} seconds")
            return jsonify(results)

        except Exception as e:
            elapsed = time.perf_counter() - start_time
            logger.error(f"Entity recognition failed after {elapsed:.4f} seconds: {e}", exc_info=True)
            return jsonify({
                "success": False,
                "error": str(e),
                "applicable_entities": []
            }), 500

    def gather_business_context(self):
        """Gather business context for a user query, with verbose logging and execution time tracking."""
        start_time = time.perf_counter()
        logger.info("gather_business_context endpoint called.")
        try:
            data = request.get_json()
            logger.debug(f"Received data: {data}")
            if not data:
                logger.warning("No JSON data provided in request.")
                return jsonify({"success": False, "error": "No JSON data provided"}), 400

            query = data.get("query", "")
            intent = data.get("intent", None)
            logger.debug(f"Gather business context for query: '{query}', intent: '{intent}'")

            agent_manager = self.get_agent_manager()
            if not agent_manager:
                logger.error("Agent manager not available.")
                return jsonify({
                    "success": False,
                    "error": "Agent manager not available",
                    "matched_concepts": [],
                    "business_instructions": []
                })

            business_agent = agent_manager.get_business_agent()
            if not business_agent:
                logger.error("Business agent not available.")
                return jsonify({
                    "success": False,
                    "error": "Business agent not available",
                    "matched_concepts": [],
                    "business_instructions": []
                })

            applicable_entities = ["customers", "accounts", "transactions", "branches", "employees", "loans", "cards"]
            logger.debug(f"Default applicable entities for business context: {applicable_entities}")

            process_start = time.perf_counter()
            results = business_agent.gather_business_context(query, applicable_entities)
            process_elapsed = time.perf_counter() - process_start
            logger.info(f"business_agent.gather_business_context execution time: {process_elapsed:.4f} seconds")
            logger.info(f"Business context gathered: {results}")
            elapsed = time.perf_counter() - start_time
            logger.info(f"gather_business_context total execution time: {elapsed:.4f} seconds")
            return jsonify(results)

        except Exception as e:
            elapsed = time.perf_counter() - start_time
            logger.error(f"Business context gathering failed after {elapsed:.4f} seconds: {e}", exc_info=True)
            return jsonify({
                "success": False,
                "error": str(e),
                "matched_concepts": [],
                "business_instructions": []
            }), 500

    def generate_sql(self):
        """Generate SQL from natural language using the complete pipeline, with verbose logging and execution time tracking."""
        start_time = time.perf_counter()
        logger.info("generate_sql endpoint called.")
        try:
            data = request.get_json()
            logger.debug(f"Received data: {data}")
            if not data:
                logger.warning("No JSON data provided in request.")
                return jsonify({"success": False, "error": "No JSON data provided"}), 400

            query = data.get("query", "")
            intent = data.get("intent", None)
            logger.debug(f"Generate SQL for query: '{query}', intent: '{intent}'")

            agent_manager = self.get_agent_manager()
            if not agent_manager:
                logger.error("Agent manager not available.")
                return jsonify({
                    "success": False,
                    "error": "Agent manager not available",
                    "generated_sql": "",
                    "validation": {},
                    "optimization_suggestions": []
                })

            pipeline = agent_manager.get_sql_pipeline()
            if not pipeline:
                logger.error("Pipeline not available.")
                return jsonify({
                    "success": False,
                    "error": "Pipeline not available",
                    "generated_sql": "",
                    "validation": {},
                    "optimization_suggestions": []
                })

            process_start = time.perf_counter()
            results = pipeline.process_user_query(query, intent)
            process_elapsed = time.perf_counter() - process_start
            logger.info(f"pipeline.process_user_query execution time: {process_elapsed:.4f} seconds")
            logger.info(f"SQL generation results: {results}")
            elapsed = time.perf_counter() - start_time
            logger.info(f"generate_sql total execution time: {elapsed:.4f} seconds")
            return jsonify(results)

        except Exception as e:
            elapsed = time.perf_counter() - start_time
            logger.error(f"SQL generation failed after {elapsed:.4f} seconds: {e}", exc_info=True)
            return jsonify({
                "success": False,
                "error": str(e),
                "generated_sql": "",
                "validation": {},
                "optimization_suggestions": []
            }), 500

    def search_documentation(self):
        """Search documentation using text or vector search, with verbose logging and execution time tracking."""
        start_time = time.perf_counter()
        logger.info("search_documentation endpoint called.")
        try:
            data = request.get_json()
            logger.debug(f"Received data: {data}")
            if not data:
                logger.warning("No JSON data provided in request.")
                return jsonify({"success": False, "error": "No JSON data provided"}), 400

            search_type = data.get("type", "text")
            query = data.get("query", "")
            logger.debug(f"Search documentation for query: '{query}', type: '{search_type}'")

            agent_manager = self.get_agent_manager()
            if not agent_manager:
                logger.error("Agent manager not available.")
                return jsonify({
                    "results": [],
                    "query": query,
                    "type": search_type,
                    "total": 0,
                    "error": "Agent manager not available"
                })

            main_agent = agent_manager.get_main_agent()
            indexer_agent = agent_manager.get_indexer_agent()

            if (main_agent and indexer_agent and
                hasattr(main_agent, 'vector_indexing_available') and
                main_agent.vector_indexing_available):

                logger.info("Using indexer agent to search documentation.")
                process_start = time.perf_counter()
                results = indexer_agent.search_documentation(query, search_type)
                process_elapsed = time.perf_counter() - process_start
                logger.info(f"indexer_agent.search_documentation execution time: {process_elapsed:.4f} seconds")
                total_results = len(results.get("tables", []) + results.get("relationships", []))
                logger.info(f"Search results: {total_results} items found.")
                elapsed = time.perf_counter() - start_time
                logger.info(f"search_documentation total execution time: {elapsed:.4f} seconds")
                return jsonify({
                    "results": results,
                    "query": query,
                    "type": search_type,
                    "total": total_results
                })
            else:
                logger.warning("Search not available: main_agent or indexer_agent missing or vector indexing not available.")
                elapsed = time.perf_counter() - start_time
                logger.info(f"search_documentation total execution time: {elapsed:.4f} seconds")
                return jsonify({
                    "results": [],
                    "query": query,
                    "type": search_type,
                    "total": 0,
                    "error": "Search not available"
                })

        except Exception as e:
            elapsed = time.perf_counter() - start_time
            logger.error(f"Search failed after {elapsed:.4f} seconds: {e}", exc_info=True)
            return jsonify({
                "results": [],
                "query": data.get("query", "") if 'data' in locals() else "",
                "type": data.get("type", "text") if 'data' in locals() else "text",
                "total": 0,
                "error": str(e)
            }), 500

    def get_schema(self):
        """Get database schema information, with verbose logging and execution time tracking."""
        start_time = time.perf_counter()
        logger.info("get_schema endpoint called.")
        try:
            agent_manager = self.get_agent_manager()
            if not agent_manager:
                logger.error("Agent manager not available.")
                return jsonify({
                    "tables": [],
                    "success": False,
                    "error": "Agent manager not available"
                })

            database_tools = agent_manager.get_unified_database_tools()
            logger.debug(f"Database tools: {database_tools}")

            if database_tools and hasattr(database_tools, 'get_detailed_schema_unified'):
                logger.info("Using get_detailed_schema_unified to retrieve schema.")
                process_start = time.perf_counter()
                schema_result = database_tools.get_detailed_schema_unified()
                process_elapsed = time.perf_counter() - process_start
                logger.info(f"database_tools.get_detailed_schema_unified execution time: {process_elapsed:.4f} seconds")
                if schema_result.get("success"):
                    logger.info("Detailed schema retrieval successful.")
                    elapsed = time.perf_counter() - start_time
                    logger.info(f"get_schema total execution time: {elapsed:.4f} seconds")
                    return jsonify({
                        "tables": schema_result.get("tables", []),
                        "relationships": schema_result.get("relationships", []),
                        "count": schema_result.get("count", 0),
                        "success": True
                    })
                else:
                    logger.error(f"Detailed schema retrieval failed: {schema_result.get('error', 'Schema retrieval failed')}")
                    elapsed = time.perf_counter() - start_time
                    logger.info(f"get_schema total execution time: {elapsed:.4f} seconds")
                    return jsonify({
                        "tables": [],
                        "relationships": [],
                        "success": False,
                        "error": schema_result.get("error", "Schema retrieval failed")
                    }), 500

            if database_tools and hasattr(database_tools, 'get_all_tables_unified'):
                logger.info("Using get_all_tables_unified to retrieve schema.")
                process_start = time.perf_counter()
                schema_result = database_tools.get_all_tables_unified()
                process_elapsed = time.perf_counter() - process_start
                logger.info(f"database_tools.get_all_tables_unified execution time: {process_elapsed:.4f} seconds")
                if schema_result.get("success"):
                    tables = []
                    for table_name in schema_result.get("tables", []):
                        tables.append({
                            "name": table_name,
                            "schema": None,
                            "columns": [],
                            "column_count": 0,
                            "primary_key_columns": [],
                            "foreign_key_columns": [],
                            "nullable_columns": [],
                            "not_null_columns": []
                        })
                    logger.info(f"Simple schema retrieval successful. Tables: {len(tables)}")
                    elapsed = time.perf_counter() - start_time
                    logger.info(f"get_schema total execution time: {elapsed:.4f} seconds")
                    return jsonify({
                        "tables": tables,
                        "relationships": [],
                        "count": len(tables),
                        "success": True
                    })

            logger.warning("Schema not available from database tools.")
            elapsed = time.perf_counter() - start_time
            logger.info(f"get_schema total execution time: {elapsed:.4f} seconds")
            return jsonify({
                "tables": [],
                "relationships": [],
                "success": False,
                "error": "Schema not available"
            })

        except Exception as e:
            elapsed = time.perf_counter() - start_time
            logger.error(f"Schema retrieval failed after {elapsed:.4f} seconds: {e}", exc_info=True)
            return jsonify({
                "tables": [],
                "relationships": [],
                "success": False,
                "error": str(e)
            }), 500

    def debug_objects(self):
        """Debug endpoint to show object creation status, with verbose logging and execution time tracking."""
        start_time = time.perf_counter()
        logger.info("debug_objects endpoint called.")
        try:
            agent_manager = self.get_agent_manager()
            if not agent_manager:
                logger.error("Agent manager not available.")
                return jsonify({
                    "error": "Agent manager not available",
                    "objects": {}
                })

            debug_info = {
                "agent_manager_id": id(agent_manager),
                "factory_initialized": getattr(agent_manager, 'is_initialized', lambda: False)(),
                "initialization_time": getattr(agent_manager, '_initialization_time', None),
                "shared_components": {
                    "llm_model": agent_manager._shared_llm_model is not None,
                    "database_tools": agent_manager._unified_database_tools is not None,
                    "instances_count": len(agent_manager._instances),
                    "shared_components_count": len(agent_manager._shared_components)
                },
                "agent_instances": {
                    name: id(instance) for name, instance in agent_manager._instances.items()
                },
                "shared_components": {
                    name: id(component) for name, component in agent_manager._shared_components.items()
                }
            }

            logger.info(f"Debug objects info: {debug_info}")
            elapsed = time.perf_counter() - start_time
            logger.info(f"debug_objects total execution time: {elapsed:.4f} seconds")
            return jsonify(debug_info)

        except Exception as e:
            elapsed = time.perf_counter() - start_time
            logger.error(f"Debug objects failed after {elapsed:.4f} seconds: {e}", exc_info=True)
            return jsonify({
                "error": str(e),
                "objects": {}
            }), 500

    def debug_database(self):
        """Debug endpoint to show database performance metrics, with verbose logging and execution time tracking."""
        start_time = time.perf_counter()
        logger.info("debug_database endpoint called.")
        try:
            agent_manager = self.get_agent_manager()
            if not agent_manager:
                logger.error("Agent manager not available.")
                return jsonify({
                    "error": "Agent manager not available",
                    "database_stats": {}
                })

            db_tools = agent_manager.get_unified_database_tools()
            if not hasattr(db_tools, 'inspector'):
                logger.error("Database inspector not available.")
                return jsonify({
                    "error": "Database inspector not available",
                    "database_stats": {}
                })

            inspector = db_tools.inspector
            logger.debug(f"Inspector object: {inspector}")
            process_start = time.perf_counter()
            cache_stats = inspector.get_cache_stats()
            process_elapsed = time.perf_counter() - process_start
            logger.info(f"inspector.get_cache_stats execution time: {process_elapsed:.4f} seconds")
            
            debug_info = {
                "database_inspector_id": id(inspector),
                "cache_stats": cache_stats,
                "connection_pool": {
                    "pool_size": inspector.engine.pool.size(),
                    "checked_in": inspector.engine.pool.checkedin(),
                    "checked_out": inspector.engine.pool.checkedout(),
                    "overflow": inspector.engine.pool.overflow()
                },
                "engine_config": {
                    "echo": inspector.engine.echo,
                    "pool_pre_ping": inspector.engine.pool._pre_ping,
                    "pool_recycle": inspector.engine.pool._recycle
                }
            }

            logger.info(f"Debug database info: {debug_info}")
            elapsed = time.perf_counter() - start_time
            logger.info(f"debug_database total execution time: {elapsed:.4f} seconds")
            return jsonify(debug_info)

        except Exception as e:
            elapsed = time.perf_counter() - start_time
            logger.error(f"Debug database failed after {elapsed:.4f} seconds: {e}", exc_info=True)
            return jsonify({
                "error": str(e),
                "database_stats": {}
            }), 500

    def get_all_summaries(self):
        """Get all AI-generated business purposes and summaries."""
        start_time = time.perf_counter()
        logger.info("get_all_summaries endpoint called.")
        
        try:
            store = self.get_documentation_store()
            
            # Get all tables and their documentation
            all_tables = store.get_all_tables()
            table_summaries = {}
            
            for table_name in all_tables:
                table_info = store.get_table_info(table_name)
                if table_info:
                    table_summaries[table_name] = {
                        "id": f"table_{table_name}",
                        "type": "table",
                        "name": table_name,
                        "business_purpose": table_info["business_purpose"],
                        "documentation": table_info["documentation"],
                        "status": table_info["status"],
                        "processed_at": table_info.get("processed_at")
                    }
            
            # Get all relationships and their documentation
            all_relationships = store.get_all_relationships()
            relationship_summaries = {}
            
            for relationship in all_relationships:
                rel_info = store.get_relationship_info(str(relationship['id']))
                if rel_info:
                    relationship_summaries[str(relationship['id'])] = {
                        "id": f"relationship_{relationship['id']}",
                        "type": "relationship",
                        "name": f"{relationship['constrained_table']}_to_{relationship['referred_table']}",
                        "relationship_type": rel_info["relationship_type"],
                        "documentation": rel_info["documentation"],
                        "status": rel_info["status"],
                        "constrained_table": relationship["constrained_table"],
                        "referred_table": relationship["referred_table"]
                    }
            
            # Combine all summaries
            all_summaries = {**table_summaries, **relationship_summaries}
            
            # Calculate statistics
            statistics = {
                "total_items": len(all_summaries),
                "tables": {
                    "total": len(table_summaries),
                    "completed": len([t for t in table_summaries.values() if t["status"] == "completed"])
                },
                "relationships": {
                    "total": len(relationship_summaries),
                    "completed": len([r for r in relationship_summaries.values() if r["status"] == "completed"])
                }
            }
            
            elapsed = time.perf_counter() - start_time
            logger.info(f"get_all_summaries total execution time: {elapsed:.4f} seconds")
            
            return jsonify({
                "success": True,
                "summaries": all_summaries,
                "statistics": statistics
            })
            
        except Exception as e:
            elapsed = time.perf_counter() - start_time
            logger.error(f"get_all_summaries failed after {elapsed:.4f} seconds: {e}", exc_info=True)
            return jsonify({
                "success": False,
                "error": str(e),
                "summaries": {},
                "statistics": {}
            }), 500

    def get_summary_by_id(self, item_id):
        """Get a specific summary by ID."""
        start_time = time.perf_counter()
        logger.info(f"get_summary_by_id endpoint called with id: {item_id}")
        
        try:
            store = self.get_documentation_store()
            
            # Parse item_id to determine type and actual ID
            if item_id.startswith("table_"):
                table_name = item_id[6:]  # Remove "table_" prefix
                table_info = store.get_table_info(table_name)
                if table_info:
                    summary = {
                        "id": item_id,
                        "type": "table",
                        "name": table_name,
                        "business_purpose": table_info["business_purpose"],
                        "documentation": table_info["documentation"],
                        "schema_data": table_info["schema_data"],
                        "status": table_info["status"],
                        "processed_at": table_info.get("processed_at")
                    }
                    
                    elapsed = time.perf_counter() - start_time
                    logger.info(f"get_summary_by_id total execution time: {elapsed:.4f} seconds")
                    return jsonify({
                        "success": True,
                        "summary": summary
                    })
                else:
                    return jsonify({
                        "success": False,
                        "error": f"Table '{table_name}' not found"
                    }), 404
                    
            elif item_id.startswith("relationship_"):
                relationship_id = item_id[13:]  # Remove "relationship_" prefix
                all_relationships = store.get_all_relationships()
                
                # Find the relationship by ID
                target_relationship = None
                for rel in all_relationships:
                    if str(rel['id']) == relationship_id:
                        target_relationship = rel
                        break
                
                if target_relationship:
                    rel_info = store.get_relationship_info(relationship_id)
                    if rel_info:
                        summary = {
                            "id": item_id,
                            "type": "relationship",
                            "name": f"{target_relationship['constrained_table']}_to_{target_relationship['referred_table']}",
                            "relationship_type": rel_info["relationship_type"],
                            "documentation": rel_info["documentation"],
                            "status": rel_info["status"],
                            "constrained_table": target_relationship["constrained_table"],
                            "referred_table": target_relationship["referred_table"],
                            "constrained_columns": target_relationship["constrained_columns"],
                            "referred_columns": target_relationship["referred_columns"]
                        }
                        
                        elapsed = time.perf_counter() - start_time
                        logger.info(f"get_summary_by_id total execution time: {elapsed:.4f} seconds")
                        return jsonify({
                            "success": True,
                            "summary": summary
                        })
                
                return jsonify({
                    "success": False,
                    "error": f"Relationship '{relationship_id}' not found"
                }), 404
            else:
                return jsonify({
                    "success": False,
                    "error": f"Invalid item ID format: {item_id}"
                }), 400
                
        except Exception as e:
            elapsed = time.perf_counter() - start_time
            logger.error(f"get_summary_by_id failed after {elapsed:.4f} seconds: {e}", exc_info=True)
            return jsonify({
                "success": False,
                "error": str(e)
            }), 500

    def get_all_table_documentation(self):
        """Get all table documentation."""
        start_time = time.perf_counter()
        logger.info("get_all_table_documentation endpoint called.")
        
        try:
            store = self.get_documentation_store()
            all_tables = store.get_all_tables()
            
            table_documentation = {}
            for table_name in all_tables:
                table_info = store.get_table_info(table_name)
                if table_info:
                    table_documentation[table_name] = {
                        "table_name": table_name,
                        "business_purpose": table_info["business_purpose"],
                        "documentation": table_info["documentation"],
                        "schema_data": table_info["schema_data"],
                        "status": table_info["status"],
                        "processed_at": table_info.get("processed_at")
                    }
            
            elapsed = time.perf_counter() - start_time
            logger.info(f"get_all_table_documentation total execution time: {elapsed:.4f} seconds")
            
            return jsonify({
                "success": True,
                "tables": table_documentation,
                "total_tables": len(table_documentation)
            })
            
        except Exception as e:
            elapsed = time.perf_counter() - start_time
            logger.error(f"get_all_table_documentation failed after {elapsed:.4f} seconds: {e}", exc_info=True)
            return jsonify({
                "success": False,
                "error": str(e),
                "tables": {},
                "total_tables": 0
            }), 500

    def get_table_documentation(self, table_name):
        """Get documentation for a specific table."""
        start_time = time.perf_counter()
        logger.info(f"get_table_documentation endpoint called for table: {table_name}")
        
        try:
            store = self.get_documentation_store()
            table_info = store.get_table_info(table_name)
            
            if table_info:
                elapsed = time.perf_counter() - start_time
                logger.info(f"get_table_documentation total execution time: {elapsed:.4f} seconds")
                
                return jsonify({
                    "success": True,
                    "table": {
                        "table_name": table_name,
                        "business_purpose": table_info["business_purpose"],
                        "documentation": table_info["documentation"],
                        "schema_data": table_info["schema_data"],
                        "status": table_info["status"],
                        "processed_at": table_info.get("processed_at")
                    }
                })
            else:
                return jsonify({
                    "success": False,
                    "error": f"Table '{table_name}' not found"
                }), 404
                
        except Exception as e:
            elapsed = time.perf_counter() - start_time
            logger.error(f"get_table_documentation failed after {elapsed:.4f} seconds: {e}", exc_info=True)
            return jsonify({
                "success": False,
                "error": str(e)
            }), 500

    def get_all_relationship_documentation(self):
        """Get all relationship documentation."""
        start_time = time.perf_counter()
        logger.info("get_all_relationship_documentation endpoint called.")
        
        try:
            store = self.get_documentation_store()
            all_relationships = store.get_all_relationships()
            
            relationship_documentation = {}
            for relationship in all_relationships:
                rel_info = store.get_relationship_info(str(relationship['id']))
                if rel_info:
                    relationship_documentation[str(relationship['id'])] = {
                        "id": relationship['id'],
                        "constrained_table": relationship["constrained_table"],
                        "referred_table": relationship["referred_table"],
                        "constrained_columns": relationship["constrained_columns"],
                        "referred_columns": relationship["referred_columns"],
                        "relationship_type": rel_info["relationship_type"],
                        "documentation": rel_info["documentation"],
                        "status": rel_info["status"],
                        "processed_at": rel_info.get("processed_at")
                    }
            
            elapsed = time.perf_counter() - start_time
            logger.info(f"get_all_relationship_documentation total execution time: {elapsed:.4f} seconds")
            
            return jsonify({
                "success": True,
                "relationships": relationship_documentation,
                "total_relationships": len(relationship_documentation)
            })
            
        except Exception as e:
            elapsed = time.perf_counter() - start_time
            logger.error(f"get_all_relationship_documentation failed after {elapsed:.4f} seconds: {e}", exc_info=True)
            return jsonify({
                "success": False,
                "error": str(e),
                "relationships": {},
                "total_relationships": 0
            }), 500

    def get_relationship_documentation(self, relationship_id):
        """Get documentation for a specific relationship."""
        start_time = time.perf_counter()
        logger.info(f"get_relationship_documentation endpoint called for relationship: {relationship_id}")
        
        try:
            store = self.get_documentation_store()
            all_relationships = store.get_all_relationships()
            
            # Find the target relationship
            target_relationship = None
            for rel in all_relationships:
                if str(rel['id']) == relationship_id:
                    target_relationship = rel
                    break
            
            if target_relationship:
                rel_info = store.get_relationship_info(relationship_id)
                if rel_info:
                    elapsed = time.perf_counter() - start_time
                    logger.info(f"get_relationship_documentation total execution time: {elapsed:.4f} seconds")
                    
                    return jsonify({
                        "success": True,
                        "relationship": {
                            "id": relationship_id,
                            "constrained_table": target_relationship["constrained_table"],
                            "referred_table": target_relationship["referred_table"],
                            "constrained_columns": target_relationship["constrained_columns"],
                            "referred_columns": target_relationship["referred_columns"],
                            "relationship_type": rel_info["relationship_type"],
                            "documentation": rel_info["documentation"],
                            "status": rel_info["status"],
                            "processed_at": rel_info.get("processed_at")
                        }
                    })
            
            return jsonify({
                "success": False,
                "error": f"Relationship '{relationship_id}' not found"
            }), 404
            
        except Exception as e:
            elapsed = time.perf_counter() - start_time
            logger.error(f"get_relationship_documentation failed after {elapsed:.4f} seconds: {e}", exc_info=True)
            return jsonify({
                "success": False,
                "error": str(e)
            }), 500

    def bad_request(self, error):
        """Handle bad request errors, with verbose logging."""
        logger.warning(f"Bad request: {error}")
        return jsonify({
            "success": False,
            "error": "Bad request",
            "message": str(error.description)
        }), 400

    def not_found(self, error):
        """Handle not found errors."""
        return jsonify({
            "success": False,
            "error": "Endpoint not found",
            "message": "The requested API endpoint does not exist"
        }), 404

    def internal_server_error(self, error):
        """Handle internal server errors."""
        logger.error(f"Internal server error: {error}")
        return jsonify({
            "success": False,
            "error": "Internal server error",
            "message": "An unexpected error occurred"
        }), 500

# Instantiate and expose the blueprint for use in the Flask app
api_routes = ApiRoutes()
api_bp = api_routes.api_bp

def create_app():
    """Create and configure the Flask application."""
    app = Flask(__name__)
    
    # Load environment variables
    load_dotenv()
    
    # Configure CORS
    CORS(app, origins=["http://localhost:3000", "http://127.0.0.1:3000"])
    
    # Basic configuration
    app.config['SECRET_KEY'] = os.environ.get('SECRET_KEY', 'dev-secret-key')
    app.config['ENV_VARS'] = {
        'DATABASE_URL': os.environ.get('DATABASE_URL'),
        'OPENAI_API_KEY': os.environ.get('OPENAI_API_KEY')
    }
    
    # Register blueprints
    app.register_blueprint(api_bp, url_prefix='/api')
    
    # Initialize agent manager at app startup
    app.agent_manager = None
    
    def initialize_agents():
        """Initialize agents once at startup."""
        if app.agent_manager is not None:
            return app.agent_manager
            
        try:
            from src.agents.factory import agent_factory
            # Initialize the factory
            agent_factory.initialize()
            app.agent_manager = agent_factory
            logger.info("Agent manager initialized successfully at startup")
            return app.agent_manager
        except Exception as e:
            logger.error(f"Failed to initialize agent manager: {e}")
            app.agent_manager = None
            return None
    
    # Initialize agents immediately
    logger.info("Starting SQL Agents initialization...")
    initialize_agents()
    if app.agent_manager:
        logger.info("‚úÖ SQL Agents initialized successfully")
        logger.info(f"Available agents: {list(app.agent_manager.get_all_agents().keys())}")
    else:
        logger.warning("‚ö†Ô∏è SQL Agents initialization failed")
    
    @app.route('/')
    def index():
        """Root endpoint."""
        return jsonify({
            "message": "Smol-SQL-Agents Backend API",
            "version": "1.0.0",
            "endpoints": {
                "health": "/api/message",
                "status": "/api/status",
                "query": "/api/query",
                "recognize_entities": "/api/recognize-entities",
                "business_context": "/api/business-context",
                "generate_sql": "/api/generate-sql",
                "search": "/api/search",
                "schema": "/api/schema",
                "debug_objects": "/api/debug/objects",
                "debug_database": "/api/debug/database",
                "documentation": {
                    "all_summaries": "/api/documentation/summaries",
                    "summary_by_id": "/api/documentation/summaries/{id}",
                    "all_tables": "/api/documentation/tables",
                    "table_by_name": "/api/documentation/tables/{table_name}",
                    "all_relationships": "/api/documentation/relationships",
                    "relationship_by_id": "/api/documentation/relationships/{relationship_id}"
                }
            }
        })
    
    return app

# Create the Flask app
app = create_app()

if __name__ == '__main__':
    # Get port from environment or use default
    port = int(os.environ.get('PORT', 5000))
    debug = os.environ.get('FLASK_ENV') == 'development'
    
    print(f"üöÄ Starting Smol-SQL-Agents Flask server on port {port}")
    print(f"üìä API endpoints available at http://localhost:{port}/api/")
    print(f"üîç Health check: http://localhost:{port}/api/message")
    
    app.run(
        host='0.0.0.0',
        port=port,
        debug=debug,
        threaded=True
    )


================================================
FILE: smol-sql-agents/backend/generate.py
================================================
import os
import logging
import sys
import asyncio
import concurrent.futures
from pathlib import Path
from dotenv import load_dotenv
from typing import List, Dict, Any, Optional
import time
from contextlib import contextmanager

# Add parent directory to Python path to find src module
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

# FIXED: Consistent relative imports from sql_agents package
from src.database.inspector import DatabaseInspector
from src.agents.core import PersistentDocumentationAgent
from src.agents.entity_recognition import EntityRecognitionAgent
from src.agents.business import BusinessContextAgent
from src.agents.nl2sql import NL2SQLAgent
from src.agents.integration import SQLAgentPipeline
from src.agents.tools.factory import DatabaseToolsFactory
from src.agents.concepts.loader import ConceptLoader
from src.agents.concepts.matcher import ConceptMatcher
from src.output.formatters import DocumentationFormatter
from src.agents.batch_manager import BatchIndexingManager




def setup_logging():
    """Configure logging for the application."""
    log_level = os.getenv('LOG_LEVEL', 'INFO')
    log_file = os.getenv('LOG_FILE', 'agent.log')
    
    # Create logs directory
    logs_dir = Path('__bin__/logs')
    logs_dir.mkdir(exist_ok=True)
    
    logging.basicConfig(
        level=getattr(logging, log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(logs_dir / log_file),
            logging.StreamHandler()
        ]
    )

def generate_documentation(resume: bool = False, batch_indexing: bool = True):
    """Enhanced main function with OpenAI-powered vector indexing and batch processing."""
    setup_logging()
    logger = logging.getLogger(__name__)
    
    logger.info("Starting Autonomous SQL Knowledgebase Agent")
    
    try:
        agent = PersistentDocumentationAgent()
        
        if not resume:
            logger.info("Starting fresh documentation generation")
            # Initial data collection
            inspector = DatabaseInspector()
            tables = inspector.get_all_table_names()
            relationships = inspector.get_all_foreign_key_relationships()
            
            # Initialize session
            session_id = agent.store.start_generation_session(
                os.getenv('DATABASE_URL'), tables, relationships
            )
            logger.info(f"Started session {session_id}")
        else:
            logger.info("Resuming previous documentation generation")
        
        if batch_indexing and agent.vector_indexing_available and agent.indexer_agent:
            # Use batch processing for efficiency
            logger.info("Using batch processing for vector indexing")
            batch_manager = BatchIndexingManager(agent.indexer_agent)
            
            # Get processing statistics
            stats = batch_manager.get_processing_stats(agent.store)
            logger.info(f"Processing stats: {stats}")
            
            # Process tables in batches
            logger.info("Processing tables with batch indexing...")
            table_results = batch_manager.batch_process_pending_tables(agent.store)
            successful_tables = sum(1 for success in table_results.values() if success)
            logger.info(f"Table processing completed: {successful_tables}/{len(table_results)} successful")
            
            # Process relationships in batches
            logger.info("Processing relationships with batch indexing...")
            rel_results = batch_manager.batch_process_pending_relationships(agent.store)
            successful_rels = sum(1 for success in rel_results.values() if success)
            logger.info(f"Relationship processing completed: {successful_rels}/{len(rel_results)} successful")
        elif batch_indexing and not agent.vector_indexing_available:
            logger.warning("Batch indexing requested but vector indexing is not available")
            logger.info("Falling back to individual processing mode")
            batch_indexing = False
            
        else:
            # Process individually (existing logic)
            logger.info("Using individual processing mode")
            
            # Process pending tables
            pending_tables = agent.store.get_pending_tables()
            logger.info(f"Found {len(pending_tables)} pending tables")
            
            processed_tables = set()  # Track tables processed in this session
            for table in pending_tables:
                if table in processed_tables:
                    logger.warning(f"Skipping already processed table in this session: {table}")
                    continue
                    
                if agent.store.is_table_processed(table):
                    logger.warning(f"Skipping previously processed table: {table}")
                    continue
                
                try:
                    agent.process_table_documentation(table)
                    processed_tables.add(table)
                    
                    # Log progress
                    progress = agent.store.get_generation_progress()
                    logger.info(f"Progress - Tables: {progress['tables']}")
                except Exception as e:
                    logger.error(f"Failed to process table {table}: {e}")
                    if not resume:
                        raise
            
            # Process pending relationships
            pending_relationships = agent.store.get_pending_relationships()
            logger.info(f"Found {len(pending_relationships)} pending relationships")
            
            processed_relationships = set()  # Track relationships processed in this session
            for relationship in pending_relationships:
                rel_id = relationship['id']
                if rel_id in processed_relationships:
                    logger.warning(f"Skipping already processed relationship in this session: {rel_id}")
                    continue
                    
                if agent.store.is_relationship_processed(relationship):
                    logger.warning(f"Skipping previously processed relationship: {relationship['constrained_table']} -> {relationship['referred_table']}")
                    continue
                
                try:
                    agent.process_relationship_documentation(relationship)
                    processed_relationships.add(rel_id)
                    
                    # Log progress
                    progress = agent.store.get_generation_progress()
                    logger.info(f"Progress - Relationships: {progress['relationships']}")
                except Exception as e:
                    logger.error(f"Failed to process relationship {rel_id}: {e}")
                    if not resume:
                        raise
        
        # Generate final documentation
        formatter = DocumentationFormatter()
        
        # Generate multiple formats
        formats = ['markdown', 'html']
        for fmt in formats:
            output_path = f"__bin__/output/database_docs.{fmt}"
            documentation = formatter.generate_documentation(fmt)
            
            Path('__bin__/output').mkdir(exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(documentation)
            
            logger.info(f"Documentation generated: {output_path}")
        
        logger.info("Documentation generation completed successfully")
        
        # Index already processed documents if vector indexing is available
        if agent.vector_indexing_available and agent.indexer_agent:
            logger.info("Indexing already processed documents...")
            agent.index_processed_documents()
        
    except Exception as e:
        logger.error(f"Documentation generation failed: {e}")
        raise

generate_documentation(resume=True)


================================================
FILE: smol-sql-agents/backend/requirements.txt
================================================
Flask==2.3.3
Flask-CORS==4.0.0

# SQL Agents dependencies
smolagents
litellm
SQLAlchemy
psycopg2-binary
python-dotenv
jinja2
chromadb
openai
numpy
tiktoken
tenacity
pyodbc
sqlparse
flask
flask-cors


================================================
FILE: smol-sql-agents/src/__init__.py
================================================
# src/__init__.py
# Main source package for SQL Documentation Agent 


================================================
FILE: smol-sql-agents/src/agents/__init__.py
================================================
# src/agents/__init__.py
# Agent implementations for SQL documentation

# Base classes and utilities
from .base import BaseAgent, CachingMixin, ValidationMixin
from .factory import AgentFactory, agent_factory

# Core agents
from .core import PersistentDocumentationAgent
from .indexer import SQLIndexerAgent
from .entity_recognition import EntityRecognitionAgent
from .batch_manager import BatchIndexingManager
from .business import BusinessContextAgent
from .nl2sql import NL2SQLAgent
from .integration import SQLAgentPipeline

__all__ = [
    # Base classes and utilities
    'BaseAgent',
    'CachingMixin', 
    'ValidationMixin',
    'AgentFactory',
    'agent_factory',
    
    # Core agents
    'PersistentDocumentationAgent',
    'SQLIndexerAgent', 
    'EntityRecognitionAgent',
    'BatchIndexingManager',
    'BusinessContextAgent',
    'NL2SQLAgent',
    'SQLAgentPipeline'
]


================================================
FILE: smol-sql-agents/src/agents/base.py
================================================
import os
import logging
from typing import Dict, List, Optional, Any
from abc import ABC, abstractmethod

# Import smolagents components
from smolagents.agents import CodeAgent
from smolagents.models import OpenAIModel
from smolagents.tools import tool

logger = logging.getLogger(__name__)

class BaseAgent(ABC):
    """Base class for all agents to eliminate code duplication."""
    
    def __init__(self, shared_llm_model=None, additional_imports=None, agent_name="Base Agent", database_tools=None):
        """Initialize base agent with common functionality.
        
        Args:
            shared_llm_model: Optional shared LLM model instance
            additional_imports: List of additional imports for CodeAgent
            agent_name: Name of the agent for logging
            database_tools: Optional unified database tools instance
        """
        self.agent_name = agent_name
        self.database_tools = database_tools
        
        # Initialize LLM model (shared or new)
        if shared_llm_model:
            self.llm_model = shared_llm_model
        else:
            self._initialize_llm_model()
        
        # Setup agent-specific components
        self._setup_agent_components()
        
        # Setup tools - this will be overridden by subclasses
        self.tools = []
        self._setup_tools()
        
        # Integrate unified database tools if provided
        if self.database_tools:
            self._integrate_database_tools()
        
        # Validate that all tools are properly decorated
        self._validate_tools()
        
        # Initialize CodeAgent
        self.agent = CodeAgent(
            model=self.llm_model,
            tools=self.tools,
            additional_authorized_imports=additional_imports or []
        )
        
        logger.info(f"{self.agent_name} initialized")
    
    def _initialize_llm_model(self):
        """Initialize OpenAI model for the agent."""
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set")
        
        self.llm_model = OpenAIModel(model_id="gpt-4o-mini", api_key=api_key)
    
    def _validate_tools(self):
        """Validate that all tools are properly decorated."""
        from smolagents.tools import Tool
        
        for i, tool_func in enumerate(self.tools):
            if not isinstance(tool_func, Tool):
                logger.error(f"Tool at index {i} is not properly decorated with @tool")
                logger.error(f"Tool: {tool_func}")
                raise ValueError(f"All tools must be instances of Tool (or decorated with @tool). Found: {type(tool_func)}")
    
    @abstractmethod
    def _setup_agent_components(self):
        """Setup agent-specific components. Override in subclasses."""
        pass
    
    @abstractmethod
    def _setup_tools(self):
        """Setup agent tools. Override in subclasses."""
        pass

    def _integrate_database_tools(self):
        """Integrate unified database tools into agent tools list."""
        if hasattr(self.database_tools, 'create_tools'):
            try:
                database_tools_list = self.database_tools.create_tools()
                self.tools.extend(database_tools_list)
                logger.info(f"Integrated {len(database_tools_list)} unified database tools into {self.agent_name}")
            except Exception as e:
                logger.error(f"Failed to integrate database tools into {self.agent_name}: {e}")
        else:
            logger.warning(f"Database tools provided to {self.agent_name} but no create_tools method found")

class CachingMixin:
    """Mixin for agents that need caching functionality."""
    
    def __init__(self, cache_size: int = 50):
        self._cache = {}
        self._cache_size = cache_size
    
    def _get_cache_key(self, key_string: str) -> str:
        """Generate a cache key from a string."""
        import hashlib
        return hashlib.md5(key_string.lower().strip().encode()).hexdigest()
    
    def _get_cached_result(self, cache_key: str) -> Optional[Dict]:
        """Get cached result if available."""
        return self._cache.get(cache_key)
    
    def _cache_result(self, cache_key: str, result: Dict):
        """Cache a result with size management."""
        self._cache[cache_key] = result
        
        # Limit cache size to prevent memory issues
        if len(self._cache) > self._cache_size:
            # Remove oldest entries
            oldest_keys = list(self._cache.keys())[:10]
            for key in oldest_keys:
                del self._cache[key]
    
    def clear_cache(self):
        """Clear the cache."""
        self._cache.clear()

class ValidationMixin:
    """Mixin for agents that need validation functionality."""
    
    def __init__(self):
        self.validators = {}
    
    def add_validator(self, name: str, validator):
        """Add a validator function."""
        self.validators[name] = validator
    
    def validate(self, data: Any, validator_name: str) -> bool:
        """Validate data using the specified validator."""
        if validator_name not in self.validators:
            logger.warning(f"Validator '{validator_name}' not found")
            return True  # Default to valid if no validator
        
        try:
            return self.validators[validator_name](data)
        except Exception as e:
            logger.error(f"Validation error for '{validator_name}': {e}")
            return False 


================================================
FILE: smol-sql-agents/src/agents/batch_manager.py
================================================
"""Batch processing manager for efficient OpenAI embeddings generation."""

import os
import logging
from typing import List, Dict, Optional
from .indexer import SQLIndexerAgent
from ..database.persistence import DocumentationStore

logger = logging.getLogger(__name__)

class BatchIndexingManager:
    """Manages efficient batch processing for OpenAI embeddings."""
    
    def __init__(self, indexer_agent: SQLIndexerAgent):
        """Initialize the batch indexing manager.
        
        Args:
            indexer_agent: The SQLIndexerAgent instance to use for indexing
        """
        self.indexer = indexer_agent
        self.batch_size = int(os.getenv("EMBEDDING_BATCH_SIZE", "100"))
        self.max_retries = int(os.getenv("EMBEDDING_MAX_RETRIES", "3"))
        
    def batch_process_pending_tables(self, doc_store: DocumentationStore) -> Dict[str, bool]:
        """Process multiple tables in batches to optimize OpenAI API usage.
        
        Args:
            doc_store: Documentation store to get pending tables from
            
        Returns:
            Dict[str, bool]: Mapping of table names to processing success status
        """
        pending_tables = doc_store.get_pending_tables()
        if not pending_tables:
            logger.info("No pending tables to process")
            return {}
            
        logger.info(f"Processing {len(pending_tables)} tables in batches of {self.batch_size}")
        
        # Group tables into batches
        table_batches = self._group_into_batches(pending_tables, self.batch_size)
        
        results = {}
        for i, batch in enumerate(table_batches):
            logger.info(f"Processing batch {i+1}/{len(table_batches)} ({len(batch)} tables)")
            
            # Prepare table data for batch processing
            tables_data = []
            for table_name in batch:
                try:
                    # Get table schema and documentation
                    table_info = doc_store.get_table_info(table_name)
                    if table_info:
                        tables_data.append({
                            "name": table_name,
                            "schema": table_info.get("schema_data", {}),
                            "business_purpose": table_info.get("business_purpose", ""),
                            "documentation": table_info.get("documentation", "")
                        })
                except Exception as e:
                    logger.error(f"Failed to prepare table data for {table_name}: {e}")
                    results[table_name] = False
                    continue
            
            # Process batch
            if tables_data:
                batch_results = self.indexer.batch_index_tables(tables_data)
                results.update(batch_results)
                
                # Log batch progress
                successful = sum(1 for success in batch_results.values() if success)
                logger.info(f"Batch {i+1} completed: {successful}/{len(batch_results)} successful")
        
        return results
    
    def batch_process_pending_relationships(self, doc_store: DocumentationStore) -> Dict[str, bool]:
        """Process multiple relationships in batches to optimize OpenAI API usage.
        
        Args:
            doc_store: Documentation store to get pending relationships from
            
        Returns:
            Dict[str, bool]: Mapping of relationship IDs to processing success status
        """
        pending_relationships = doc_store.get_pending_relationships()
        if not pending_relationships:
            logger.info("No pending relationships to process")
            return {}
            
        logger.info(f"Processing {len(pending_relationships)} relationships in batches of {self.batch_size}")
        
        # Group relationships into batches
        rel_batches = self._group_into_batches(pending_relationships, self.batch_size)
        
        results = {}
        for i, batch in enumerate(rel_batches):
            logger.info(f"Processing batch {i+1}/{len(rel_batches)} ({len(batch)} relationships)")
            
            # Prepare relationship data for batch processing
            relationships_data = []
            for relationship in batch:
                try:
                    rel_id = relationship.get("id", "unknown")
                    # Get relationship documentation
                    rel_info = doc_store.get_relationship_info(rel_id)
                    if rel_info:
                        relationships_data.append({
                            "id": rel_id,
                            "name": rel_id,
                            "type": rel_info.get("relationship_type", ""),
                            "documentation": rel_info.get("documentation", ""),
                            "tables": [relationship.get("constrained_table"), relationship.get("referred_table")]
                        })
                except Exception as e:
                    logger.error(f"Failed to prepare relationship data for {rel_id}: {e}")
                    results[rel_id] = False
                    continue
            
            # Process batch
            if relationships_data:
                batch_results = self.indexer.batch_index_relationships(relationships_data)
                results.update(batch_results)
                
                # Log batch progress
                successful = sum(1 for success in batch_results.values() if success)
                logger.info(f"Batch {i+1} completed: {successful}/{len(batch_results)} successful")
        
        return results
    
    def estimate_embedding_costs(self, texts: List[str]) -> Dict[str, float]:
        """Estimate OpenAI API costs for embedding generation request.
        
        Args:
            texts: List of texts to estimate costs for
            
        Returns:
            Dict[str, float]: Cost estimation details
        """
        try:
            # Rough estimation based on OpenAI pricing
            # text-embedding-3-small: $0.00002 per 1K tokens
            # Average tokens per text (rough estimate)
            total_tokens = sum(len(text.split()) * 1.3 for text in texts)  # Rough token estimation
            cost_per_1k_tokens = 0.00002
            
            estimated_cost = (total_tokens / 1000) * cost_per_1k_tokens
            
            return {
                "total_texts": len(texts),
                "estimated_tokens": int(total_tokens),
                "estimated_cost_usd": round(estimated_cost, 6),
                "cost_per_text": round(estimated_cost / len(texts), 6) if texts else 0
            }
        except Exception as e:
            logger.error(f"Failed to estimate embedding costs: {e}")
            return {
                "total_texts": len(texts),
                "estimated_tokens": 0,
                "estimated_cost_usd": 0,
                "cost_per_text": 0,
                "error": str(e)
            }
    
    def _group_into_batches(self, items: List, batch_size: int) -> List[List]:
        """Group items into optimal batch sizes.
        
        Args:
            items: List of items to group
            batch_size: Maximum size of each batch
            
        Returns:
            List[List]: List of batches
        """
        if not items:
            return []
        
        batches = []
        for i in range(0, len(items), batch_size):
            batch = items[i:i + batch_size]
            batches.append(batch)
        
        return batches
    
    def get_processing_stats(self, doc_store: DocumentationStore) -> Dict[str, any]:
        """Get statistics about pending processing tasks.
        
        Args:
            doc_store: Documentation store to analyze
            
        Returns:
            Dict[str, any]: Processing statistics
        """
        pending_tables = doc_store.get_pending_tables()
        pending_relationships = doc_store.get_pending_relationships()
        
        # Estimate costs for pending items
        table_texts = [f"Table: {table}" for table in pending_tables]
        rel_texts = [f"Relationship: {rel.get('id', 'unknown')}" for rel in pending_relationships]
        
        table_cost_estimate = self.estimate_embedding_costs(table_texts)
        rel_cost_estimate = self.estimate_embedding_costs(rel_texts)
        
        return {
            "pending_tables": len(pending_tables),
            "pending_relationships": len(pending_relationships),
            "total_pending": len(pending_tables) + len(pending_relationships),
            "estimated_table_cost": table_cost_estimate["estimated_cost_usd"],
            "estimated_relationship_cost": rel_cost_estimate["estimated_cost_usd"],
            "total_estimated_cost": table_cost_estimate["estimated_cost_usd"] + rel_cost_estimate["estimated_cost_usd"],
            "batch_size": self.batch_size,
            "estimated_batches": (len(pending_tables) + len(pending_relationships)) // self.batch_size + 1
        } 


================================================
FILE: smol-sql-agents/src/agents/business.py
================================================
import os
import logging
from typing import Dict, List, Any, Tuple

# Import smolagents components
from smolagents.tools import tool

# Import base classes
from .base import BaseAgent

# Import concept components
from .concepts.loader import BusinessConcept, ConceptLoader
from .concepts.matcher import ConceptMatcher

logger = logging.getLogger(__name__)

class BusinessContextAgent(BaseAgent):
    """Streamlined business context agent with consistent dictionary returns."""
    
    def __init__(self, indexer_agent=None, concepts_dir: str = "src/agents/concepts", 
                 shared_llm_model=None, shared_concept_loader=None, shared_concept_matcher=None,
                 database_tools=None):
        self.indexer_agent = indexer_agent
        
        # Use shared components if provided
        self.concept_loader = shared_concept_loader or ConceptLoader(concepts_dir)
        self.concept_matcher = shared_concept_matcher or ConceptMatcher(indexer_agent)
        
        # Initialize base agent with unified database tools
        super().__init__(
            shared_llm_model=shared_llm_model,
            additional_imports=['yaml', 'json'],
            agent_name="Business Context Agent",
            database_tools=database_tools
        )
        
        logger.info("Business Context Agent initialized")

    def _setup_agent_components(self):
        """Setup agent-specific components."""
        # Concept loader and matcher are already initialized in __init__
        pass

    def _setup_tools(self):
        """Setup essential tools for the business context agent."""
        self.tools = []
        
        @tool
        def load_concepts_for_entities(entity_names: List[str]) -> List[Dict]:
            """Load concepts for specified entities.
            
            Args:
                entity_names: List of entity names to load concepts for.
                
            Returns:
                List of concept dictionaries for the specified entities.
            """
            try:
                concepts = self.concept_loader.get_concepts_for_entities(entity_names)
                return [concept.__dict__ for concept in concepts]
            except Exception as e:
                logger.error(f"Error loading concepts: {e}")
                return []

        @tool
        def match_concepts_to_query(user_query: str, available_concepts: List[Dict]) -> List[Dict]:
            """Match concepts to user query.
            
            Args:
                user_query: The user's query to match concepts against.
                available_concepts: List of available concept dictionaries.
                
            Returns:
                List of matched concepts with similarity scores.
            """
            try:
                concepts = [BusinessConcept(**concept) for concept in available_concepts]
                matches = self.concept_matcher.match_concepts_to_query(user_query, concepts)
                return [{"concept": match[0].__dict__, "similarity": match[1]} for match in matches]
            except Exception as e:
                logger.error(f"Error matching concepts: {e}")
                return []

        @tool
        def get_concept_examples(concept_name: str, user_query: str, max_examples: int = 3) -> List[Dict]:
            """Retrieve similar examples for a concept.
            
            Args:
                concept_name: Name of the concept to get examples for.
                user_query: The user's query to find similar examples.
                max_examples: Maximum number of examples to retrieve.
                
            Returns:
                List of similar examples for the concept.
            """
            try:
                concept = self.concept_loader.get_concept_by_name(concept_name)
                if concept:
                    return self.concept_matcher.find_similar_examples(concept, user_query, max_examples)
                return []
            except Exception as e:
                logger.error(f"Error getting examples: {e}")
                return []

        # Removed @tool function - converted to private method:
        # validate_required_joins -> _validate_required_joins

        self.tools.extend([
            load_concepts_for_entities,
            match_concepts_to_query,
            get_concept_examples
        ])

    def gather_business_context(self, user_query: str, applicable_entities: List[str]) -> Dict[str, Any]:
        """Main method to gather business context."""
        try:
            logger.info(f"Gathering business context for query: {user_query}")
            logger.info(f"Applicable entities: {applicable_entities}")
            
            if not applicable_entities:
                return self._empty_business_context()
            
            # Load concepts for entities
            concepts = self.concept_loader.get_concepts_for_entities(applicable_entities)
            logger.info(f"Found {len(concepts)} concepts for entities")
            
            if not concepts:
                return self._empty_business_context()
            
            # Match concepts to user query
            matched_concepts = self.concept_matcher.match_concepts_to_query(user_query, concepts)
            logger.info(f"Matched {len(matched_concepts)} concepts to query")
            
            # Get relevant examples
            relevant_examples = []
            for concept, similarity in matched_concepts:
                examples = self.concept_matcher.find_similar_examples(concept, user_query)
                for example, example_similarity in examples:
                    relevant_examples.append({
                        "example": example,
                        "similarity": example_similarity,
                        "concept_name": concept.name
                    })
            
            # Extract business instructions
            business_instructions = [
                {
                    "concept": concept.name,
                    "instructions": concept.instructions,
                    "similarity": similarity
                }
                for concept, similarity in matched_concepts
            ]
            
            # Validate joins
            join_validation = {}
            for concept, similarity in matched_concepts:
                validation = self._validate_required_joins(applicable_entities, concept.required_joins)
                join_validation[concept.name] = validation
            
            # Calculate entity coverage
            entities_with_concepts = len(set([concept.name for concept, _ in matched_concepts]))
            entity_coverage = {
                "total_entities": len(applicable_entities),
                "entities_with_concepts": entities_with_concepts
            }
            
            # Format response
            return {
                "success": True,
                "matched_concepts": [
                    {
                        "name": concept.name,
                        "description": concept.description,
                        "target_entities": concept.target,
                        "required_joins": concept.required_joins,
                        "similarity": similarity
                    }
                    for concept, similarity in matched_concepts
                ],
                "business_instructions": business_instructions,
                "relevant_examples": relevant_examples,
                "join_validation": join_validation,
                "entity_coverage": entity_coverage
            }
            
        except Exception as e:
            logger.error(f"Error gathering business context: {e}")
            return {
                "success": False,
                "error": str(e),
                "matched_concepts": [],
                "business_instructions": [],
                "relevant_examples": [],
                "join_validation": {},
                "entity_coverage": {"total_entities": 0, "entities_with_concepts": 0}
            }

    def _validate_required_joins(self, entities: List[str], required_joins: List[str]) -> Dict:
        """Validate that required joins can be satisfied."""
        try:
            validation_result = {
                "valid": True,
                "missing_entities": [],
                "satisfied_joins": [],
                "unsatisfied_joins": []
            }
            
            # Extract entity names from join conditions
            required_entities = set()
            for join in required_joins:
                join_lower = join.lower()
                if "=" in join_lower:
                    parts = join_lower.split("=")
                    for part in parts:
                        if "." in part:
                            entity = part.split(".")[0].strip()
                            required_entities.add(entity)
            
            # Check availability
            available_entities = set(entities)
            missing_entities = required_entities - available_entities
            
            if missing_entities:
                validation_result["valid"] = False
                validation_result["missing_entities"] = list(missing_entities)
            
            # Check join satisfaction
            for join in required_joins:
                can_satisfy = all(entity in available_entities for entity in required_entities)
                if can_satisfy:
                    validation_result["satisfied_joins"].append(join)
                else:
                    validation_result["unsatisfied_joins"].append(join)
            
            return validation_result
            
        except Exception as e:
            logger.error(f"Error validating joins: {e}")
            return {"valid": False, "error": str(e)}

    def _empty_business_context(self) -> Dict[str, Any]:
        """Return empty business context with proper structure."""
        return {
            "success": True,
            "matched_concepts": [],
            "business_instructions": [],
            "relevant_examples": [],
            "join_validation": {},
            "entity_coverage": {"total_entities": 0, "entities_with_concepts": 0}
        }


================================================
FILE: smol-sql-agents/src/agents/core.py
================================================
import json
import logging
from typing import Dict, List, Any
from smolagents.tools import tool

# Import base classes
from .base import BaseAgent
from ..database.inspector import DatabaseInspector
from ..database.persistence import DocumentationStore
from ..agents.indexer import SQLIndexerAgent
from ..vector.store import SQLVectorStore
from .tools.factory import DatabaseToolsFactory

logger = logging.getLogger(__name__)

class PersistentDocumentationAgent(BaseAgent):
    """Streamlined core documentation agent with consistent dictionary returns."""
    
    def __init__(self, shared_llm_model=None):
        # Initialize agent-specific components
        self.db_inspector = DatabaseInspector()
        self.store = DocumentationStore()
        
        # Initialize unified database tools
        self.database_tools = DatabaseToolsFactory.create_database_tools(self.db_inspector)
        
        # Initialize vector store with error handling
        try:
            self.indexer_agent = SQLIndexerAgent(SQLVectorStore(), shared_llm_model=shared_llm_model)
            self.vector_indexing_available = True
            logger.info("Vector indexing initialized successfully")
        except Exception as e:
            logger.warning(f"Vector indexing not available: {e}")
            self.indexer_agent = None
            self.vector_indexing_available = False
        
        # Initialize base agent with unified database tools
        super().__init__(
            shared_llm_model=shared_llm_model,
            additional_imports=['json'],
            agent_name="Core Documentation Agent",
            database_tools=self.database_tools
        )
    
    def _setup_agent_components(self):
        """Setup agent-specific components."""
        pass
    
    def _setup_tools(self):
        """Setup essential documentation tools."""
        self.tools = []
        
        # Database tools will be integrated automatically by BaseAgent
        # No need to manually add them here
    
    def process_table_documentation(self, table_name: str):
        """Process and index documentation for a single table."""
        logger.info(f"Processing table: {table_name}")
        
        prompt = f"""
        Generate documentation for database table: {table_name}
        
        Steps:
        1. Call get_table_schema_unified_tool("{table_name}") to get table schema
        2. Analyze table name and columns to infer business purpose
        3. Return JSON with business purpose and schema data
        
        Return JSON format:
        {{
            "business_purpose": "Clear description of table's purpose",
            "schema_data": {{ 
                "table_name": "name",
                "columns": [...]
            }}
        }}

        Use Python syntax: True/False (not true/false).
        Return valid JSON only.
        """
        
        try:
            result = self.agent.run(prompt)
            
            # Parse result
            if isinstance(result, str):
                try:
                    result = json.loads(result)
                except json.JSONDecodeError:
                    logger.error(f"Failed to parse JSON for table {table_name}")
                    raise ValueError(f"Invalid JSON response for table {table_name}")
            
            if not isinstance(result, dict):
                logger.error(f"Expected dict for table {table_name}, got {type(result)}")
                raise ValueError(f"Invalid response type for table {table_name}")
            
            # Validate required fields
            if "business_purpose" not in result or "schema_data" not in result:
                logger.error(f"Missing required fields for table {table_name}")
                raise ValueError(f"Missing required fields for table {table_name}")
            
            # Ensure proper types
            if not isinstance(result["business_purpose"], str):
                result["business_purpose"] = str(result["business_purpose"])
            if not isinstance(result["schema_data"], dict):
                raise ValueError(f"schema_data must be dict for table {table_name}")
            
            business_purpose = result["business_purpose"]
            schema_data = result["schema_data"]
            documentation = f"## {table_name}\n\n{business_purpose}"
            
            # Save to documentation store
            self.store.save_table_documentation(
                table_name, schema_data, business_purpose, documentation
            )
            
            # Index with vector store if available
            if self.vector_indexing_available and self.indexer_agent:
                try:
                    self._index_processed_table(table_name, result)
                except Exception as e:
                    logger.error(f"Vector indexing failed for table {table_name}: {e}")
            else:
                logger.info(f"Skipping vector indexing for table {table_name}")
            
            logger.info(f"Completed processing table: {table_name}")
            
        except Exception as e:
            logger.error(f"Failed to process table {table_name}: {e}")
            raise
    
    def process_relationship_documentation(self, relationship: dict):
        """Process and index documentation for a single relationship."""
        rel_id = relationship['id']
        logger.info(f"Processing relationship: {rel_id}")
        
        prompt = f"""
        Analyze this database relationship and generate documentation:
        
        From: {relationship['constrained_table']}.{relationship['constrained_columns']}
        To: {relationship['referred_table']}.{relationship['referred_columns']}
        
        Return JSON format:
        {{
            "relationship_type": "one-to-one|one-to-many|many-to-many",
            "documentation": "Clear explanation of business relationship"
        }}

        Use Python syntax: True/False (not true/false).
        Return valid JSON only.
        """
        
        try:
            result = self.agent.run(prompt)
            
            # Parse result
            if isinstance(result, str):
                try:
                    result = json.loads(result)
                except json.JSONDecodeError:
                    logger.error(f"Failed to parse JSON for relationship {rel_id}")
                    raise ValueError(f"Invalid JSON response for relationship {rel_id}")
            
            if not isinstance(result, dict):
                logger.error(f"Expected dict for relationship {rel_id}, got {type(result)}")
                raise ValueError(f"Invalid response type for relationship {rel_id}")
            
            # Validate required fields
            if "relationship_type" not in result or "documentation" not in result:
                logger.error(f"Missing required fields for relationship {rel_id}")
                raise ValueError(f"Missing required fields for relationship {rel_id}")
            
            # Ensure proper types
            if not isinstance(result["relationship_type"], str):
                result["relationship_type"] = str(result["relationship_type"])
            if not isinstance(result["documentation"], str):
                result["documentation"] = str(result["documentation"])
            
            relationship_type = result["relationship_type"]
            documentation = result["documentation"]
            
            # Save to documentation store
            self.store.save_relationship_documentation(
                rel_id, relationship_type, documentation
            )
            
            # Index with vector store if available
            if self.vector_indexing_available and self.indexer_agent:
                try:
                    self._index_processed_relationship(relationship, result)
                except Exception as e:
                    logger.error(f"Vector indexing failed for relationship {rel_id}: {e}")
            else:
                logger.info(f"Skipping vector indexing for relationship {rel_id}")
            
            logger.info(f"Completed processing relationship: {rel_id}")
            
        except Exception as e:
            logger.error(f"Failed to process relationship {rel_id}: {e}")
            raise
            
    def _index_processed_table(self, table_name: str, data: dict):
        """Index table documentation using vector store."""
        try:
            table_data = {
                "name": table_name,
                "business_purpose": data["business_purpose"],
                "schema": data["schema_data"],
                "type": "table"
            }
            
            success = self.indexer_agent.index_table_documentation(table_data)
            if not success:
                raise ValueError(f"Failed to index table documentation for {table_name}")
                
        except Exception as e:
            logger.error(f"Failed to index table documentation for {table_name}: {e}")
            raise
            
    def _index_processed_relationship(self, relationship: dict, data: dict):
        """Index relationship documentation using vector store."""
        try:
            rel_id = str(relationship["id"])
            rel_name = f"{relationship['constrained_table']}_{relationship['referred_table']}_rel"
            
            rel_data = {
                "name": rel_name,
                "type": data["relationship_type"],
                "documentation": data["documentation"],
                "tables": [relationship["constrained_table"], relationship["referred_table"]],
                "doc_type": "relationship"
            }
            
            success = self.indexer_agent.index_relationship_documentation(rel_data)
            if not success:
                raise ValueError(f"Failed to index relationship documentation for {rel_name}")
                
        except Exception as e:
            logger.error(f"Failed to index relationship documentation for {relationship['id']}: {e}")
            raise
    
    def index_processed_documents(self):
        """Index all processed documents that haven't been indexed yet."""
        if not self.vector_indexing_available or not self.indexer_agent:
            logger.warning("Vector indexing not available")
            return
        
        logger.info("Indexing processed documents...")
        
        # Get all processed tables and relationships
        all_tables = self.store.get_all_tables()
        all_relationships = self.store.get_all_relationships()
        
        logger.info(f"Found {len(all_tables)} tables and {len(all_relationships)} relationships")
        
        # Index tables
        indexed_tables = 0
        for table_name in all_tables:
            try:
                table_info = self.store.get_table_info(table_name)
                if table_info:
                    table_data = {
                        "name": table_name,
                        "business_purpose": table_info.get("business_purpose", ""),
                        "schema": table_info.get("schema_data", {}),
                        "type": "table"
                    }
                    
                    success = self.indexer_agent.index_table_documentation(table_data)
                    if success:
                        indexed_tables += 1
                        
            except Exception as e:
                logger.error(f"Error indexing table {table_name}: {e}")
        
        # Index relationships
        indexed_relationships = 0
        for relationship in all_relationships:
            try:
                rel_id = relationship.get("id", "unknown")
                rel_info = self.store.get_relationship_info(rel_id)
                
                if rel_info:
                    rel_data = {
                        "id": rel_id,
                        "name": rel_id,
                        "type": rel_info.get("relationship_type", ""),
                        "documentation": rel_info.get("documentation", ""),
                        "tables": [relationship.get("constrained_table"), relationship.get("referred_table")],
                        "doc_type": "relationship"
                    }
                    
                    success = self.indexer_agent.index_relationship_documentation(rel_data)
                    if success:
                        indexed_relationships += 1
                        
            except Exception as e:
                logger.error(f"Error indexing relationship {rel_id}: {e}")
        
        logger.info(f"Indexing completed: {indexed_tables} tables, {indexed_relationships} relationships")
    
    def retry_vector_indexing_initialization(self):
        """Retry initializing vector indexing if previously unavailable."""
        if self.vector_indexing_available and self.indexer_agent:
            logger.info("Vector indexing already available")
            return True
        
        try:
            logger.info("Attempting to initialize vector indexing...")
            self.indexer_agent = SQLIndexerAgent(SQLVectorStore(), shared_llm_model=self.llm_model)
            self.vector_indexing_available = True
            logger.info("Vector indexing initialized successfully")
            return True
        except Exception as e:
            logger.warning(f"Vector indexing initialization failed: {e}")
            self.indexer_agent = None
            self.vector_indexing_available = False
            return False


================================================
FILE: smol-sql-agents/src/agents/entity_recognition.py
================================================
import json
import logging
import concurrent.futures
from typing import Dict, List, Optional, Any

# Import smolagents tools
from smolagents.tools import tool

# Import base classes
from .base import BaseAgent
from .indexer import SQLIndexerAgent

logger = logging.getLogger(__name__)

class EntityRecognitionAgent(BaseAgent):
    """Streamlined entity recognition agent with consistent dictionary returns."""
    
    def __init__(self, indexer_agent: SQLIndexerAgent, shared_llm_model=None, database_tools=None):
        self.indexer_agent = indexer_agent
        
        # Caching for performance
        self._embedding_cache = {}
        self._result_cache = {}
        
        # Initialize base agent with unified database tools
        super().__init__(
            shared_llm_model=shared_llm_model,
            additional_imports=['json'],
            agent_name="Entity Recognition Agent",
            database_tools=database_tools
        )
    
    def _setup_agent_components(self):
        """Setup agent-specific components."""
        pass
    
    def _setup_tools(self):
        """Setup essential entity recognition tools."""
        self.tools = []
        
        @tool
        def search_table_entities(query: str, max_results: int = 10) -> Dict:
            """Search for table entities relevant to a user query.
            
            Args:
                query: The user query to search for relevant table entities.
                max_results: Maximum number of results to return.
                
            Returns:
                Dictionary with search results and table entities found.
            """
            try:
                if not query or not query.strip():
                    return {"success": False, "error": "Query cannot be empty"}
                
                search_results = self.indexer_agent.search_documentation(
                    query=query.strip(),
                    doc_type="table"
                )
                
                if search_results.get("tables") and len(search_results["tables"]) > max_results:
                    search_results["tables"] = search_results["tables"][:max_results]
                
                return {
                    "success": True,
                    "query": query,
                    "tables": search_results.get("tables", []),
                    "total_found": len(search_results.get("tables", []))
                }
                
            except Exception as e:
                logger.error(f"Failed to search table entities: {e}")
                return {"success": False, "error": str(e)}

        # Removed @tool functions - converted to private methods:
        # analyze_entity_relevance -> _analyze_entity_relevance
        # get_entity_recommendations -> _get_entity_recommendations
        
        self.tools.extend([
            search_table_entities
        ])
    
    def recognize_entities_optimized(self, user_query: str, user_intent: str = None, max_entities: int = 5) -> Dict:
        """Optimized entity recognition with caching."""
        try:
            if not user_query or not user_query.strip():
                return self._empty_entity_result("Query cannot be empty")
            
            intent = user_intent or user_query
            
            # Check cache
            cache_key = self._get_cache_key(user_query, intent)
            cached_result = self._get_cached_result(cache_key)
            if cached_result:
                logger.info("Using cached entity recognition result")
                return cached_result
            
            # Search and analyze with parallel processing
            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                search_future = executor.submit(
                    self.indexer_agent.search_documentation,
                    query=user_query.strip(),
                    doc_type="table"
                )
                
                direct_future = executor.submit(
                    self._analyze_entities_direct,
                    user_query,
                    intent
                )
                
                search_results = search_future.result()
                direct_analysis = direct_future.result()
                
                # Use direct analysis if high confidence
                if direct_analysis.get("confidence", 0.0) > 0.55:  # Lowered threshold
                    result = {
                        "success": True,
                        "applicable_entities": direct_analysis.get("applicable_entities", []),
                        "recommendations": direct_analysis.get("recommendations", []),
                        "analysis": direct_analysis.get("analysis", ""),
                        "confidence": direct_analysis.get("confidence", 0.0)
                    }
                    self._cache_result(cache_key, result)
                    return result
                
                # Process search results
                tables = search_results.get("tables", [])
                if not tables:
                    result = self._empty_entity_result(f"No relevant tables found for: {user_query}")
                    self._cache_result(cache_key, result)
                    return result
                
                # Analyze entities
                entity_analysis = []
                for table_result in tables[:max_entities * 2]:
                    table_content = table_result.get("content", {})
                    table_name = table_content.get("name", "unknown")
                    business_purpose = table_content.get("business_purpose", "")
                    similarity_score = table_result.get("score", 0.0)
                    
                    relevance_factors = {
                        "semantic_similarity": similarity_score,
                        "business_purpose_match": self._calculate_purpose_match_cached(business_purpose, intent),
                        "table_name_relevance": self._calculate_name_relevance_cached(table_name, intent)
                    }
                    
                    overall_relevance = (
                        relevance_factors["semantic_similarity"] * 0.5 +
                        relevance_factors["business_purpose_match"] * 0.3 +
                        relevance_factors["table_name_relevance"] * 0.2
                    )
                    
                    entity_analysis.append({
                        "table_name": table_name,
                        "business_purpose": business_purpose,
                        "relevance_score": round(overall_relevance, 3),
                        "relevance_factors": relevance_factors,
                        "recommendation": self._get_relevance_recommendation(overall_relevance)
                    })
                
                # Sort and filter
                entity_analysis.sort(key=lambda x: x["relevance_score"], reverse=True)
                applicable_entities = [e for e in entity_analysis if e["relevance_score"] > 0.3][:max_entities]
                
                # Generate recommendations
                recommendations = [
                    {
                        "priority": i + 1,
                        "table_name": entity.get("table_name", "unknown"),
                        "relevance_score": entity.get("relevance_score", 0.0),
                        "business_purpose": entity.get("business_purpose", ""),
                        "recommendation": entity.get("recommendation", "")
                    }
                    for i, entity in enumerate(applicable_entities)
                ]
                
                # Calculate confidence
                confidence = 0.0
                if applicable_entities:
                    avg_relevance = sum(e["relevance_score"] for e in applicable_entities) / len(applicable_entities)
                    confidence = min(avg_relevance * 1.2, 1.0)
                
                result = {
                    "success": True,
                    "applicable_entities": applicable_entities,
                    "recommendations": recommendations,
                    "analysis": self._generate_analysis_summary(applicable_entities, intent),
                    "confidence": round(confidence, 3)
                }
                
                self._cache_result(cache_key, result)
                return result
                
        except Exception as e:
            error_msg = str(e)
            logger.error(f"Failed to recognize entities: {error_msg}")
            return self._empty_entity_result(error_msg)
    
    def recognize_entities(self, user_query: str, user_intent: str = None, max_entities: int = 5) -> Dict:
        """Main entity recognition method - delegates to optimized version."""
        # Use the optimized method instead of LLM-based approach
        return self.recognize_entities_optimized(user_query, user_intent, max_entities)
    
    def _empty_entity_result(self, error_msg: str = "") -> Dict[str, Any]:
        """Return empty entity recognition result."""
        return {
            "success": False,
            "error": error_msg,
            "applicable_entities": [],
            "recommendations": [],
            "analysis": "",
            "confidence": 0.0
        }
    
    def _analyze_entities_direct(self, user_query: str, user_intent: str) -> Dict:
        """Direct analysis for early termination with more realistic scoring."""
        try:
            query_lower = user_query.lower()
            intent_lower = user_intent.lower()
            
            table_patterns = {
                "customer": ["customers", "customer", "client"],
                "account": ["accounts", "account", "banking"],
                "transaction": ["transactions", "transaction", "payment"],
                "employee": ["employees", "employee", "staff"],
                "branch": ["branches", "branch", "location"],
                "loan": ["loans", "loan", "credit"],
                "card": ["cards", "card", "credit_card"]
            }
            
            applicable_entities = []
            total_score = 0.0
            
            for pattern, tables in table_patterns.items():
                if pattern in query_lower or pattern in intent_lower:
                    for table in tables:
                        relevance = 0.0
                        
                        # More nuanced scoring
                        if pattern in query_lower:
                            relevance += 0.4  # Reduced from 0.6
                        if pattern in intent_lower:
                            relevance += 0.2  # Reduced from 0.4
                        
                        # Add some randomness to avoid perfect scores
                        import random
                        relevance += random.uniform(-0.1, 0.1)
                        
                        # Cap at 0.8 to avoid perfect scores
                        relevance = min(relevance, 0.8)
                        
                        if relevance > 0.3:
                            applicable_entities.append({
                                "table_name": table,
                                "business_purpose": f"Contains {pattern} related data",
                                "relevance_score": round(relevance, 3),
                                "recommendation": f"Highly relevant for {pattern} queries"
                            })
                            total_score += relevance
            
            confidence = total_score / max(len(applicable_entities), 1)
            # Cap confidence to avoid triggering early termination
            confidence = min(confidence, 0.7)
            
            return {
                "applicable_entities": applicable_entities,
                "confidence": round(confidence, 3),
                "analysis": f"Direct analysis found {len(applicable_entities)} relevant entities"
            }
            
        except Exception as e:
            logger.error(f"Direct analysis failed: {e}")
            return {"applicable_entities": [], "confidence": 0.0, "analysis": "Direct analysis failed"}
    
    def _get_cache_key(self, query: str, intent: str = None) -> str:
        """Generate cache key."""
        import hashlib
        key_string = f"{query}:{intent or ''}"
        return hashlib.md5(key_string.lower().strip().encode()).hexdigest()
    
    def _get_cached_result(self, cache_key: str) -> Optional[Dict]:
        """Get cached result."""
        return self._result_cache.get(cache_key)
    
    def _cache_result(self, cache_key: str, result: Dict):
        """Cache result with size management."""
        self._result_cache[cache_key] = result
        if len(self._result_cache) > 100:
            oldest_keys = list(self._result_cache.keys())[:10]
            for key in oldest_keys:
                del self._result_cache[key]
    
    def _calculate_purpose_match_cached(self, business_purpose: str, user_intent: str) -> float:
        """Cached purpose match calculation."""
        cache_key = f"purpose:{business_purpose}:{user_intent}"
        if cache_key in self._embedding_cache:
            return self._embedding_cache[cache_key]
        
        result = self._calculate_purpose_match(business_purpose, user_intent)
        self._embedding_cache[cache_key] = result
        return result
    
    def _calculate_name_relevance_cached(self, table_name: str, user_intent: str) -> float:
        """Cached name relevance calculation."""
        cache_key = f"name:{table_name}:{user_intent}"
        if cache_key in self._embedding_cache:
            return self._embedding_cache[cache_key]
        
        result = self._calculate_name_relevance(table_name, user_intent)
        self._embedding_cache[cache_key] = result
        return result
    
    def _calculate_purpose_match(self, business_purpose: str, user_intent: str) -> float:
        """Calculate business purpose match."""
        if not business_purpose or not user_intent:
            return 0.0
        
        purpose_words = set(business_purpose.lower().split())
        intent_words = set(user_intent.lower().split())
        
        if not purpose_words or not intent_words:
            return 0.0
        
        common_words = purpose_words.intersection(intent_words)
        return len(common_words) / max(len(intent_words), 1)
    
    def _calculate_name_relevance(self, table_name: str, user_intent: str) -> float:
        """Calculate table name relevance."""
        if not table_name or not user_intent:
            return 0.0
        
        table_name_lower = table_name.lower()
        intent_lower = user_intent.lower()
        
        if table_name_lower in intent_lower:
            return 1.0
        
        intent_words = intent_lower.split()
        for word in intent_words:
            if word in table_name_lower or table_name_lower in word:
                return 0.7
        
        return 0.0
    
    def _get_relevance_recommendation(self, relevance_score: float) -> str:
        """Get recommendation based on relevance score."""
        if relevance_score >= 0.8:
            return "Highly relevant - strongly recommended"
        elif relevance_score >= 0.6:
            return "Relevant - good match"
        elif relevance_score >= 0.4:
            return "Moderately relevant"
        elif relevance_score >= 0.2:
            return "Low relevance"
        else:
            return "Not relevant"
    
    def _generate_analysis_summary(self, applicable_entities: List[Dict], user_intent: str) -> str:
        """Generate analysis summary."""
        if not applicable_entities:
            return f"No highly relevant entities found for: '{user_intent}'"
        
        entity_count = len(applicable_entities)
        top_entity = applicable_entities[0]["table_name"]
        avg_score = sum(e["relevance_score"] for e in applicable_entities) / entity_count
        
        return f"Found {entity_count} applicable entities for '{user_intent}'. Top match: '{top_entity}' with average relevance: {avg_score:.2f}"
    
    def _analyze_entity_relevance(self, search_results: Dict, user_intent: str) -> Dict:
        """Analyze search results to determine entity relevance.
        
        Args:
            search_results: Dictionary containing search results with tables and scores.
            user_intent: The user's intent or context for the query.
            
        Returns:
            Dictionary with analyzed entity relevance and recommendations.
        """
        try:
            if not search_results.get("success"):
                return {
                    "success": False,
                    "error": "Invalid search results",
                    "applicable_entities": [],
                    "analysis": "Search results invalid",
                    "confidence": 0.0
                }
            
            tables = search_results.get("tables", [])
            if not tables:
                return {
                    "success": True,
                    "applicable_entities": [],
                    "analysis": "No relevant table entities found",
                    "confidence": 0.0
                }
            
            # Analyze each table for relevance
            entity_analysis = []
            for table_result in tables:
                table_content = table_result.get("content", {})
                table_name = table_content.get("name", "unknown")
                business_purpose = table_content.get("business_purpose", "")
                similarity_score = table_result.get("score", 0.0)
                
                # Calculate relevance
                relevance_factors = {
                    "semantic_similarity": similarity_score,
                    "business_purpose_match": self._calculate_purpose_match(business_purpose, user_intent),
                    "table_name_relevance": self._calculate_name_relevance(table_name, user_intent)
                }
                
                overall_relevance = (
                    relevance_factors["semantic_similarity"] * 0.5 +
                    relevance_factors["business_purpose_match"] * 0.3 +
                    relevance_factors["table_name_relevance"] * 0.2
                )
                
                entity_analysis.append({
                    "table_name": table_name,
                    "business_purpose": business_purpose,
                    "relevance_score": round(overall_relevance, 3),
                    "relevance_factors": relevance_factors,
                    "recommendation": self._get_relevance_recommendation(overall_relevance)
                })
            
            # Sort by relevance and filter
            entity_analysis.sort(key=lambda x: x["relevance_score"], reverse=True)
            applicable_entities = [e for e in entity_analysis if e["relevance_score"] > 0.3]
            
            # Calculate confidence
            confidence = 0.0
            if applicable_entities:
                avg_relevance = sum(e["relevance_score"] for e in applicable_entities) / len(applicable_entities)
                confidence = min(avg_relevance * 1.2, 1.0)
            
            return {
                "success": True,
                "applicable_entities": applicable_entities,
                "all_analyzed_entities": entity_analysis,
                "analysis": self._generate_analysis_summary(applicable_entities, user_intent),
                "confidence": round(confidence, 3),
                "total_entities_analyzed": len(entity_analysis)
            }
            
        except Exception as e:
            logger.error(f"Failed to analyze entity relevance: {e}")
            return {"success": False, "error": str(e)}
    
    def _get_entity_recommendations(self, applicable_entities: List[Dict], max_recommendations: int = 5) -> Dict:
        """Generate entity recommendations.
        
        Args:
            applicable_entities: List of applicable entities with relevance scores.
            max_recommendations: Maximum number of recommendations to generate.
            
        Returns:
            Dictionary with entity recommendations and priority rankings.
        """
        try:
            if not applicable_entities:
                return {
                    "success": True,
                    "recommendations": [],
                    "message": "No applicable entities found"
                }
            
            recommendations = []
            for i, entity in enumerate(applicable_entities[:max_recommendations]):
                recommendations.append({
                    "priority": i + 1,
                    "table_name": entity.get("table_name", "unknown"),
                    "relevance_score": entity.get("relevance_score", 0.0),
                    "business_purpose": entity.get("business_purpose", ""),
                    "recommendation": entity.get("recommendation", "")
                })
            
            return {
                "success": True,
                "recommendations": recommendations,
                "total_recommendations": len(recommendations),
                "message": f"Generated {len(recommendations)} recommendations"
            }
            
        except Exception as e:
            logger.error(f"Failed to generate recommendations: {e}")
            return {"success": False, "error": str(e)}


================================================
FILE: smol-sql-agents/src/agents/factory.py
================================================
import logging
from typing import Dict, Any, Optional
from pathlib import Path

# Import all agents
from .core import PersistentDocumentationAgent
from .indexer import SQLIndexerAgent
from .entity_recognition import EntityRecognitionAgent
from .business import BusinessContextAgent
from .nl2sql import NL2SQLAgent
from .batch_manager import BatchIndexingManager
from .integration import SQLAgentPipeline

# Import base classes
from .base import BaseAgent, CachingMixin, ValidationMixin

# Import unified database tools
from .tools.factory import DatabaseToolsFactory
from ..database.inspector import DatabaseInspector

logger = logging.getLogger(__name__)

class AgentFactory:
    """Factory for creating and managing agent instances."""
    
    def __init__(self):
        self._instances = {}
        self._shared_llm_model = None
        self._shared_components = {}
        self._unified_database_tools = None
    
    def get_shared_llm_model(self):
        """Get or create shared LLM model."""
        if not self._shared_llm_model:
            from smolagents.models import OpenAIModel
            import os
            
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEY environment variable is not set")
            
            self._shared_llm_model = OpenAIModel(model_id="gpt-4o-mini", api_key=api_key)
            logger.info("Shared LLM model created")
        
        return self._shared_llm_model
    
    def get_unified_database_tools(self):
        """Get or create unified database tools."""
        if not self._unified_database_tools:
            database_inspector = DatabaseInspector()
            self._unified_database_tools = DatabaseToolsFactory.create_database_tools(database_inspector)
            logger.info("Unified database tools created")
        
        return self._unified_database_tools
    
    def get_main_agent(self) -> PersistentDocumentationAgent:
        """Get or create main documentation agent."""
        if "main_agent" not in self._instances:
            self._instances["main_agent"] = PersistentDocumentationAgent()
            logger.info("Main agent created")
        
        return self._instances["main_agent"]
    
    def get_indexer_agent(self) -> SQLIndexerAgent:
        """Get or create indexer agent."""
        if "indexer_agent" not in self._instances:
            from ..vector.store import SQLVectorStore
            
            vector_store = SQLVectorStore()
            self._instances["indexer_agent"] = SQLIndexerAgent(
                vector_store, 
                shared_llm_model=self.get_shared_llm_model()
            )
            logger.info("Indexer agent created")
        
        return self._instances["indexer_agent"]
    
    def get_entity_agent(self) -> EntityRecognitionAgent:
        """Get or create entity recognition agent."""
        if "entity_agent" not in self._instances:
            self._instances["entity_agent"] = EntityRecognitionAgent(
                self.get_indexer_agent(),
                shared_llm_model=self.get_shared_llm_model(),
                database_tools=self.get_unified_database_tools()
            )
            logger.info("Entity recognition agent created")
        
        return self._instances["entity_agent"]
    
    def get_business_agent(self, concepts_dir: str = "src/agents/concepts") -> BusinessContextAgent:
        """Get or create business context agent."""
        if "business_agent" not in self._instances:
            # Get shared components
            shared_concept_loader = self._get_shared_component("concept_loader", concepts_dir)
            shared_concept_matcher = self._get_shared_component("concept_matcher", self.get_indexer_agent())
            
            self._instances["business_agent"] = BusinessContextAgent(
                indexer_agent=self.get_indexer_agent(),
                concepts_dir=concepts_dir,
                shared_llm_model=self.get_shared_llm_model(),
                shared_concept_loader=shared_concept_loader,
                shared_concept_matcher=shared_concept_matcher,
                database_tools=self.get_unified_database_tools()
            )
            logger.info("Business context agent created")
        
        return self._instances["business_agent"]
    
    def get_nl2sql_agent(self, database_tools=None) -> NL2SQLAgent:
        """Get or create NL2SQL agent."""
        if "nl2sql_agent" not in self._instances:
            # Use unified database tools if no specific tools provided
            if database_tools is None:
                database_tools = self.get_unified_database_tools()
            
            self._instances["nl2sql_agent"] = NL2SQLAgent(
                database_tools,
                shared_llm_model=self.get_shared_llm_model()
            )
            logger.info("NL2SQL agent created")
        
        return self._instances["nl2sql_agent"]
    
    def get_batch_manager(self) -> BatchIndexingManager:
        """Get or create batch manager."""
        if "batch_manager" not in self._instances:
            self._instances["batch_manager"] = BatchIndexingManager(
                self.get_indexer_agent()
            )
            logger.info("Batch manager created")
        
        return self._instances["batch_manager"]
    
    def get_sql_pipeline(self, database_tools=None) -> SQLAgentPipeline:
        """Get or create SQL agent pipeline."""
        if "sql_pipeline" not in self._instances:
            # Use unified database tools if no specific tools provided
            if database_tools is None:
                database_tools = self.get_unified_database_tools()
            
            self._instances["sql_pipeline"] = SQLAgentPipeline(
                indexer_agent=self.get_indexer_agent(),
                database_tools=database_tools,
                shared_entity_agent=self.get_entity_agent(),
                shared_business_agent=self.get_business_agent(),
                shared_nl2sql_agent=self.get_nl2sql_agent(database_tools)
            )
            logger.info("SQL agent pipeline created")
        
        return self._instances["sql_pipeline"]
    
    def _get_shared_component(self, component_name: str, *args, **kwargs):
        """Get or create shared component."""
        if component_name not in self._shared_components:
            if component_name == "concept_loader":
                from .concepts.loader import ConceptLoader
                self._shared_components[component_name] = ConceptLoader(*args, **kwargs)
            elif component_name == "concept_matcher":
                from .concepts.matcher import ConceptMatcher
                self._shared_components[component_name] = ConceptMatcher(*args, **kwargs)
            else:
                raise ValueError(f"Unknown shared component: {component_name}")
        
        return self._shared_components[component_name]
    
    def get_all_agents(self) -> Dict[str, Any]:
        """Get all agent instances."""
        return {
            "main_agent": self.get_main_agent(),
            "indexer_agent": self.get_indexer_agent(),
            "entity_agent": self.get_entity_agent(),
            "business_agent": self.get_business_agent(),
            "batch_manager": self.get_batch_manager()
        }
    
    def reset(self):
        """Reset all instances (useful for testing)."""
        self._instances.clear()
        self._shared_components.clear()
        self._shared_llm_model = None
        self._unified_database_tools = None
        logger.info("All agent instances reset")

# Global factory instance
agent_factory = AgentFactory() 


================================================
FILE: smol-sql-agents/src/agents/indexer.py
================================================
import json
import logging
import os
from typing import Dict, List, Optional, Any

# Import smolagents components
from smolagents.agents import CodeAgent
from smolagents.models import OpenAIModel
from smolagents.tools import tool

# Import vector components
from ..vector.store import SQLVectorStore

logger = logging.getLogger(__name__)

class SQLIndexerAgent:
    """Streamlined vector indexing agent with consistent dictionary returns."""
    
    def __init__(self, vector_store: SQLVectorStore, shared_llm_model=None):
        self.vector_store = vector_store
        self.embeddings_client = vector_store.embeddings_client
        
        # Use shared LLM model if provided
        if shared_llm_model:
            self.llm_model = shared_llm_model
        else:
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEY environment variable is not set")
            self.llm_model = OpenAIModel(model_id="gpt-4o-mini", api_key=api_key)
        
        # Initialize vector indexes
        try:
            self.vector_store.create_table_index()
            self.vector_store.create_relationship_index()
        except Exception as e:
            logger.error(f"Failed to initialize vector indexes: {e}")
            raise
        
        # Create tools
        self._setup_tools()
        
        # Initialize CodeAgent
        self.agent = CodeAgent(
            model=self.llm_model,
            tools=self.tools,
            additional_authorized_imports=['json']
        )
        
        logger.info("SQL indexer agent initialized")
    
    def _setup_tools(self):
        """Setup essential indexing tools."""
        
        @tool
        def index_table_documentation(table_data: Dict) -> Dict:
            """Index table documentation with OpenAI embeddings.
            
            Args:
                table_data: Dictionary containing table documentation data including name, business_purpose, schema, and type.
                
            Returns:
                Dictionary with success status and result information.
            """
            try:
                if not self._validate_table_data(table_data):
                    return {
                        "success": False,
                        "error": "Invalid table documentation format"
                    }
                
                table_name = table_data.get("name")
                if not table_name:
                    return {"success": False, "error": "Table name is required"}
                
                self.vector_store.add_table_document(table_name, table_data)
                
                return {
                    "success": True,
                    "message": f"Successfully indexed table: {table_name}",
                    "table_name": table_name
                }
                
            except Exception as e:
                logger.error(f"Failed to index table documentation: {e}")
                return {"success": False, "error": str(e)}

        @tool
        def index_relationship_documentation(relationship_data: Dict) -> Dict:
            """Index relationship documentation with OpenAI embeddings.
            
            Args:
                relationship_data: Dictionary containing relationship documentation data including id, name, type, documentation, tables, and doc_type.
                
            Returns:
                Dictionary with success status and result information.
            """
            try:
                if not self._validate_relationship_data(relationship_data):
                    return {
                        "success": False,
                        "error": "Invalid relationship documentation format"
                    }
                
                relationship_id = relationship_data.get("id") or f"{relationship_data.get('name')}_rel"
                self.vector_store.add_relationship_document(relationship_id, relationship_data)
                
                return {
                    "success": True,
                    "message": f"Successfully indexed relationship: {relationship_id}",
                    "relationship_id": relationship_id
                }
                
            except Exception as e:
                logger.error(f"Failed to index relationship documentation: {e}")
                return {"success": False, "error": str(e)}

        @tool
        def search_documentation(query: str, doc_type: str = "all") -> Dict:
            """Search indexed documentation using OpenAI embeddings.
            
            Args:
                query: The search query to find relevant documentation.
                doc_type: Type of documentation to search ("all", "table", or "relationship").
                
            Returns:
                Dictionary with search results including tables and relationships.
            """
            try:
                if doc_type not in ["all", "table", "relationship"]:
                    return {
                        "success": False,
                        "error": f"Invalid doc_type: {doc_type}"
                    }
                
                results = []
                rel_results = []
                
                if doc_type in ["all", "table"]:
                    results = self.vector_store.search_tables(query)
                
                if doc_type in ["all", "relationship"]:
                    rel_results = self.vector_store.search_relationships(query)
                
                return {
                    "success": True,
                    "query": query,
                    "doc_type": doc_type,
                    "tables": results,
                    "relationships": rel_results,
                    "total_results": len(results) + len(rel_results)
                }
                
            except Exception as e:
                logger.error(f"Failed to search documentation: {e}")
                return {
                    "success": False,
                    "error": str(e),
                    "tables": [],
                    "relationships": [],
                    "total_results": 0
                }

        @tool
        def get_indexing_status() -> Dict:
            """Get the current status of vector indexes."""
            try:
                table_count = self.vector_store.table_index.collection.count() if self.vector_store.table_index else 0
                rel_count = self.vector_store.relationship_index.collection.count() if self.vector_store.relationship_index else 0
                
                return {
                    "success": True,
                    "table_index_count": table_count,
                    "relationship_index_count": rel_count,
                    "total_indexed_documents": table_count + rel_count
                }
                
            except Exception as e:
                logger.error(f"Failed to get indexing status: {e}")
                return {"success": False, "error": str(e)}

        self.tools = [
            index_table_documentation,
            index_relationship_documentation,
            search_documentation,
            get_indexing_status
        ]
    
    def process_indexing_instruction(self, instruction: str) -> Dict:
        """Process natural language indexing instructions."""
        try:
            prompt = f"""
            Process this indexing instruction: {instruction}
            
            Available operations:
            1. Index table documentation
            2. Index relationship documentation  
            3. Search existing documentation
            4. Get indexing status
            
            Use Python syntax: True/False (not true/false).
            Return a JSON response with the operation results.
            """
            
            result = self.agent.run(prompt)
            
            # Ensure result is a dictionary
            if isinstance(result, str):
                try:
                    result = json.loads(result)
                except json.JSONDecodeError:
                    logger.error(f"Failed to parse JSON response: {result}")
                    return {"success": False, "error": "Invalid JSON response"}
            
            if not isinstance(result, dict):
                logger.error(f"Expected dict, got {type(result)}")
                return {"success": False, "error": "Invalid response type"}
            
            # Ensure success field exists
            if "success" not in result:
                result["success"] = True
            
            return result
                
        except Exception as e:
            logger.error(f"Failed to process indexing instruction: {e}")
            return {"success": False, "error": str(e)}
    
    def index_table_documentation(self, table_data: Dict) -> bool:
        """Index table documentation."""
        result = self.process_indexing_instruction(
            f"Index table documentation: {json.dumps(table_data)}"
        )
        return result.get("success", False)
    
    def index_relationship_documentation(self, relationship_data: Dict) -> bool:
        """Index relationship documentation."""
        result = self.process_indexing_instruction(
            f"Index relationship documentation: {json.dumps(relationship_data)}"
        )
        return result.get("success", False)
    
    def search_documentation(self, query: str, doc_type: str = "all") -> Dict:
        """Search documentation using OpenAI embeddings."""
        try:
            if doc_type not in ["all", "table", "relationship"]:
                return {
                    "tables": [],
                    "relationships": [],
                    "total_results": 0,
                    "error": f"Invalid doc_type: {doc_type}"
                }
            
            results = {"tables": [], "relationships": [], "total_results": 0}
            
            if doc_type in ["all", "table"]:
                table_results = self.vector_store.search_tables(query)
                results["tables"] = table_results
                results["total_results"] += len(table_results)
                
            if doc_type in ["all", "relationship"]:
                rel_results = self.vector_store.search_relationships(query)
                results["relationships"] = rel_results
                results["total_results"] += len(rel_results)
                
            return results
            
        except Exception as e:
            logger.error(f"Search failed: {e}")
            return {"tables": [], "relationships": [], "total_results": 0, "error": str(e)}
    
    def batch_index_tables(self, tables_data: List[Dict]) -> Dict[str, bool]:
        """Efficiently index multiple tables."""
        results = {}
        for table_data in tables_data:
            table_name = table_data.get("name", "unknown")
            results[table_name] = self.index_table_documentation(table_data)
        return results
    
    def batch_index_relationships(self, relationships_data: List[Dict]) -> Dict[str, bool]:
        """Efficiently index multiple relationships."""
        results = {}
        for rel_data in relationships_data:
            rel_id = rel_data.get("id") or f"{rel_data.get('name', 'unknown')}_rel"
            results[rel_id] = self.index_relationship_documentation(rel_data)
        return results
    
    def _validate_table_data(self, table_data: Dict) -> bool:
        """Validate table documentation data structure."""
        required_fields = ["name", "business_purpose", "schema", "type"]
        return all(field in table_data for field in required_fields)
    
    def _validate_relationship_data(self, relationship_data: Dict) -> bool:
        """Validate relationship documentation data structure."""
        required_fields = ["name", "type", "documentation", "tables"]
        return all(field in relationship_data for field in required_fields)


================================================
FILE: smol-sql-agents/src/agents/integration.py
================================================
import logging
from typing import Dict, List, Any

# Import smolagents tools
from smolagents.tools import tool

# Import base classes
from .base import BaseAgent

logger = logging.getLogger(__name__)

class SQLAgentPipeline(BaseAgent):
    """Streamlined SQL agent pipeline with consistent dictionary structures."""
    
    def __init__(self, indexer_agent=None, database_tools=None, 
                 shared_entity_agent=None, shared_business_agent=None, shared_nl2sql_agent=None):
        # Store dependencies
        self.indexer_agent = indexer_agent
        self.database_tools = database_tools
        
        # Initialize base agent with unified database tools
        super().__init__(
            additional_imports=['json'],
            agent_name="SQL Agent Pipeline",
            database_tools=self.database_tools
        )
        
        # Initialize agents
        self._initialize_agents(shared_entity_agent, shared_business_agent, shared_nl2sql_agent)
    
    def _setup_agent_components(self):
        """Setup agent-specific components."""
        pass
    
    def _setup_tools(self):
        """Setup essential pipeline tools."""
        self.tools = []
        
        # Removed @tool functions - converted to private methods:
        # execute_entity_recognition -> _execute_entity_recognition
        # gather_business_context -> _gather_business_context  
        # generate_sql -> _generate_sql
        # format_final_response -> _format_final_response
    
    def _initialize_agents(self, shared_entity_agent, shared_business_agent, shared_nl2sql_agent):
        """Initialize pipeline agents."""
        try:
            from .entity_recognition import EntityRecognitionAgent
            from .business import BusinessContextAgent
            from .nl2sql import NL2SQLAgent
        except ImportError as e:
            logger.error(f"Failed to import required agents: {e}")
            raise
        
        # Use shared agents or create new ones
        self.entity_agent = shared_entity_agent or EntityRecognitionAgent(self.indexer_agent)
        self.business_agent = shared_business_agent or BusinessContextAgent(self.indexer_agent)
        self.nl2sql_agent = shared_nl2sql_agent or NL2SQLAgent(self.database_tools)
    
    def _execute_entity_recognition(self, user_query: str, user_intent: str) -> Dict[str, Any]:
        """Execute entity recognition step."""
        logger.info("Executing entity recognition...")
        
        entity_results = self.entity_agent.recognize_entities_optimized(user_query, user_intent, max_entities=10)
        
        if not isinstance(entity_results, dict):
            logger.error(f"Entity recognition returned {type(entity_results)}, expected dict")
            return {
                "success": False,
                "error": "Entity recognition returned invalid type",
                "applicable_entities": [],
                "entities": []
            }
        
        if entity_results.get("success", False):
            applicable_entities = entity_results.get("applicable_entities", [])
            entities = [
                entity.get("table_name") if isinstance(entity, dict) else entity
                for entity in applicable_entities
                if entity
            ]
            entity_results["entities"] = entities
            logger.info(f"Found entities: {entities}")
        else:
            logger.warning("Entity recognition returned no results")
            entity_results["entities"] = []
        
        return entity_results
    
    def _gather_business_context(self, user_query: str, entity_results: Dict) -> Dict[str, Any]:
        """Gather business context step."""
        logger.info("Gathering business context...")
        
        if not isinstance(entity_results, dict):
            logger.error(f"Entity results must be a dictionary, got {type(entity_results)}")
            return self._default_business_context()
        
        # Extract entities directly - no fallbacks or nested structure handling
        applicable_entities = entity_results.get("applicable_entities", [])
        entities_list = entity_results.get("entities", [])
        
        # Extract entity names
        entities = []
        for entity in applicable_entities:
            if isinstance(entity, dict):
                table_name = entity.get("table_name")
                if table_name:
                    entities.append(table_name)
            elif isinstance(entity, str):
                entities.append(entity)
        
        # Use entities_list as fallback only if no applicable_entities found
        if not entities and entities_list:
            entities = [entity for entity in entities_list if isinstance(entity, str)]
        
        if not entities:
            logger.warning("No entities found for business context gathering")
            return self._default_business_context()
        
        logger.info(f"Using entities for business context: {entities}")
        business_context = self.business_agent.gather_business_context(user_query, entities)
        
        if not isinstance(business_context, dict):
            logger.error(f"Business context agent returned {type(business_context)}, expected dict")
            return self._default_business_context()
        
        return business_context
    
    def _generate_sql(self, user_query: str, business_context: Dict, entity_context: Dict) -> Dict[str, Any]:
        """Generate SQL step."""
        logger.info("Generating SQL query...")
        
        if not isinstance(business_context, dict) or not isinstance(entity_context, dict):
            logger.error("Business context and entity context must be dictionaries")
            return {
                "success": False,
                "error": "Invalid context types",
                "generated_sql": "",
                "is_valid": False
            }
        
        entity_context_for_sql = {
            "entities": entity_context.get("entities", []),
            "entity_descriptions": entity_context.get("entity_descriptions", {}),
            "confidence_scores": entity_context.get("confidence_scores", {})
        }
        
        sql_results = self.nl2sql_agent.generate_sql_optimized(user_query, business_context, entity_context_for_sql)
        
        if not isinstance(sql_results, dict):
            logger.error(f"SQL generation returned {type(sql_results)}, expected dict")
            return {
                "success": False,
                "error": "SQL generation returned invalid type",
                "generated_sql": "",
                "is_valid": False
            }
        
        return sql_results
    
    def _format_final_response(self, entity_results: Dict, business_context: Dict, sql_results: Dict) -> Dict[str, Any]:
        """Format final response step."""
        return {
            "success": True,
            "pipeline_summary": {
                "entity_recognition_success": entity_results.get("success", False),
                "business_context_success": business_context.get("success", False),
                "sql_generation_success": sql_results.get("success", False),
                "sql_validation_success": sql_results.get("is_valid", False)
            },
            "entity_recognition": {
                "entities": entity_results.get("entities", []),
                "confidence": entity_results.get("confidence", 0.0)
            },
            "business_context": {
                "matched_concepts": business_context.get("matched_concepts", []),
                "business_instructions": business_context.get("business_instructions", [])
            },
            "sql_generation": {
                "generated_sql": sql_results.get("generated_sql"),
                "is_valid": sql_results.get("is_valid", False),
                "validation": sql_results.get("validation", {}),
                "query_execution": sql_results.get("query_execution", {})
            }
        }
    
    def _default_business_context(self) -> Dict[str, Any]:
        """Return default business context structure."""
        return {
            "success": True,
            "matched_concepts": [],
            "business_instructions": [],
            "join_validation": {},
            "relevant_examples": [],
            "entity_coverage": {"entities_with_concepts": 0, "total_entities": 0}
        }
    
    def _build_entity_context(self, entity_results: Dict) -> Dict[str, Any]:
        """Build entity context from entity recognition results."""
        applicable_entities = entity_results.get("applicable_entities", [])
        entities_list = entity_results.get("entities", [])
        
        entities = []
        entity_descriptions = {}
        confidence_scores = {}
        
        # Process applicable_entities
        for entity in applicable_entities:
            if isinstance(entity, dict):
                table_name = entity.get("table_name")
                if table_name:
                    entities.append(table_name)
                    entity_descriptions[table_name] = entity.get("business_purpose", "")
                    confidence_scores[table_name] = entity.get("relevance_score", 0.0)
        
        # Process entities_list as fallback
        if not entities and entities_list:
            for entity in entities_list:
                if isinstance(entity, str):
                    entities.append(entity)
                    entity_descriptions[entity] = f"Table {entity}"
                    confidence_scores[entity] = 0.5
        
        return {
            "entities": entities,
            "entity_descriptions": entity_descriptions,
            "confidence_scores": confidence_scores
        }
    
    def process_user_query(self, user_query: str, user_intent: str = None) -> Dict[str, Any]:
        """Complete pipeline from user query to validated SQL."""
        logger.info(f"Starting pipeline for query: {user_query}")
        
        # Step 1: Entity Recognition
        entity_results = self._execute_entity_recognition(user_query, user_intent or user_query)
        if not entity_results.get("success", False):
            return {"success": False, "error": "Entity recognition failed", "step": "entity_recognition"}
        
        # Step 2: Business Context Gathering
        business_context = self._gather_business_context(user_query, entity_results)
        if not business_context.get("success", False):
            return {"success": False, "error": "Business context gathering failed", "step": "business_context"}
        
        # Step 3: SQL Generation
        entity_context = self._build_entity_context(entity_results)
        sql_results = self._generate_sql(user_query, business_context, entity_context)
        if not sql_results.get("success", False):
            return {"success": False, "error": "SQL generation failed", "step": "sql_generation"}
        
        # Step 4: Format Final Response
        final_response = self._format_final_response(entity_results, business_context, sql_results)
        
        logger.info("Pipeline completed successfully")
        return final_response


================================================
FILE: smol-sql-agents/src/agents/nl2sql.py
================================================
import logging
import concurrent.futures
from typing import Dict, List, Optional, Any

# Import smolagents tools
from smolagents.tools import tool

# Import base classes
from .base import BaseAgent, CachingMixin, ValidationMixin
from .tools.shared import DatabaseTools
from ..validation.business_validator import BusinessValidator
from ..validation.tsql_validator import TSQLValidator

logger = logging.getLogger(__name__)

class NL2SQLAgent(BaseAgent, CachingMixin, ValidationMixin):
    """Streamlined NL2SQL Agent with consistent dictionary returns."""
    
    def __init__(self, database_tools: DatabaseTools, shared_llm_model=None):
        # Initialize mixins
        CachingMixin.__init__(self, cache_size=50)
        ValidationMixin.__init__(self)
        
        self.database_tools = database_tools
        
        # Initialize base agent with unified database tools
        super().__init__(
            shared_llm_model=shared_llm_model,
            additional_imports=['json'],
            agent_name="NL2SQL Agent",
            database_tools=self.database_tools
        )
    
    def _setup_agent_components(self):
        """Setup agent-specific components."""
        self.business_validator = BusinessValidator()
        self.tsql_validator = TSQLValidator()
        
        # Add validators to mixin
        self.add_validator("syntax", self.tsql_validator.validate_syntax)
        self.add_validator("security", self.tsql_validator.validate_security)
        self.add_validator("performance", self.tsql_validator.check_performance_patterns)
    
    def _setup_tools(self):
        """Setup essential NL2SQL tools."""
        self.tools = []
        
        # Database tools will be integrated automatically by BaseAgent
        # Unified database tools include: get_table_schema_unified_tool, get_all_tables_unified_tool, get_relationships_unified_tool
        
        @tool
        def execute_query_and_return_results(query: str, max_rows: int = 100) -> Dict:
            """Execute query and return results.
            
            Args:
                query: The SQL query to execute.
                max_rows: Maximum number of rows to return.
                
            Returns:
                Dictionary with query execution results and sample data.
            """
            try:
                result = self.database_tools.execute_query_safe(query, max_rows)
                
                if not result.get("success"):
                    return result
                
                rows = result.get("rows", [])
                columns = result.get("columns", [])
                
                return {
                    "success": True,
                    "total_rows": len(rows),
                    "returned_rows": len(rows),
                    "truncated": len(rows) >= max_rows,
                    "sample_data": self._create_sample_summary(rows, columns)
                }
                
            except Exception as e:
                logger.error(f"Query execution failed: {e}")
                return {"success": False, "error": str(e)}
        
        # Add final answer tool for SQL response
        @tool
        def final_answer(sql_query: str) -> Dict[str, Any]:
            """Return the final SQL answer.
            
            Args:
                sql_query: The final SQL query to return.
                
            Returns:
                Dictionary with the final SQL query and success status.
            """
            try:
                return {
                    "success": True,
                    "final_sql": sql_query.strip(),
                    "message": "Final SQL query generated"
                }
            except Exception as e:
                return {"success": False, "error": str(e)}
        
        self.tools.extend([
            execute_query_and_return_results,
            final_answer
        ])
    
    def generate_sql_optimized(self, user_query: str, business_context: Dict, entity_context: Dict) -> Dict[str, Any]:
        """Optimized SQL generation with parallel validation."""
        logger.info(f"Starting SQL generation for query: {user_query}")
        try:
            if not isinstance(business_context, dict) or not isinstance(entity_context, dict):
                logger.error("Business context and entity context must be dictionaries")
                return {
                    "success": False,
                    "error": "Business context and entity context must be dictionaries",
                    "generated_sql": "",
                    "is_valid": False
                }
            
            # Build prompt
            prompt = self._build_query_prompt(user_query, business_context, entity_context)
            logger.info(f"Built prompt for SQL generation")
            
            # Generate SQL
            response = self.agent.run(prompt)
            generated_sql = self._extract_sql_from_response(response)
            logger.info(f"Extracted SQL: {generated_sql[:100]}...")
            
            if not generated_sql:
                logger.error("No valid SQL generated")
                return {
                    "success": False,
                    "error": "No valid SQL generated",
                    "generated_sql": "",
                    "is_valid": False
                }
            
            # Check cache
            cache_key = self._get_cache_key(f"{generated_sql}:{hash(str(business_context))}")
            cached_validation = self._get_cached_result(cache_key)
            
            if cached_validation:
                logger.info("Using cached validation results")
                return self._format_response_with_cache(generated_sql, cached_validation)
            
            # Parallel validation and execution
            logger.info("Starting parallel validation")
            return self._execute_parallel_validation(generated_sql, business_context)
            
        except Exception as e:
            logger.error(f"SQL generation failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "generated_sql": "",
                "is_valid": False
            }
    
    def _execute_parallel_validation(self, sql: str, business_context: Dict) -> Dict[str, Any]:
        """Execute parallel validation and query execution."""
        logger.info(f"Starting parallel validation for SQL: {sql[:100]}...")
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            # Submit validation tasks
            futures = {
                "syntax": executor.submit(self.validate, sql, "syntax"),
                "business": executor.submit(self._check_business_compliance, sql, business_context),
                "security": executor.submit(self.validate, sql, "security"),
                "performance": executor.submit(self.validate, sql, "performance"),
                "execution": executor.submit(self._execute_query_impl, sql, 100)
            }
            
            # Collect results
            results = {name: future.result() for name, future in futures.items()}
            
            # Debug logging
            logger.info(f"Validation results: {results}")
            
            # Cache results
            cache_key = self._get_cache_key(f"{sql}:{hash(str(business_context))}")
            self._cache_result(cache_key, results)
            
            return self._format_validation_response(sql, results)
    
    def _format_validation_response(self, sql: str, results: Dict) -> Dict[str, Any]:
        """Format validation response."""
        # Handle both boolean and dictionary validation results
        def get_validation_result(result, key, default=False):
            if isinstance(result, bool):
                return result
            elif isinstance(result, dict):
                return result.get(key, default)
            else:
                return default
        
        validation = {
            "syntax_valid": get_validation_result(results["syntax"], "valid", False),
            "business_compliant": get_validation_result(results["business"], "valid", False),
            "security_valid": get_validation_result(results["security"], "valid", False),
            "performance_issues": results["performance"].get("issues", []) if isinstance(results["performance"], dict) else []
        }
        
        # Debug logging
        logger.info(f"Formatted validation: {validation}")
        
        final_result = {
            "success": True,
            "generated_sql": sql,
            "validation": validation,
            "query_execution": results["execution"],
            "is_valid": all([
                validation["syntax_valid"],
                validation["business_compliant"],
                validation["security_valid"]
            ])
        }
        
        logger.info(f"Final SQL generation result: {final_result}")
        return final_result
    
    def _format_response_with_cache(self, sql: str, cached_results: Dict) -> Dict[str, Any]:
        """Format response using cached validation results."""
        return {
            "success": True,
            "generated_sql": sql,
            "validation": cached_results,
            "cached": True,
            "is_valid": cached_results.get("syntax_valid", False)
        }
    
    def _build_query_prompt(self, user_query: str, business_context: Dict, entity_context: Dict) -> str:
        """Build query prompt."""
        schema_info = self._format_schema_info(entity_context.get("table_schemas", {}))
        business_instructions = business_context.get("business_instructions", [])
        
        business_context_str = ""
        if business_instructions:
            business_context_str = "Business context:\n"
            for instruction in business_instructions[:3]:  # Limit to top 3
                business_context_str += f"- {instruction.get('instructions', '')}\n"
        
        return f"""
        Generate T-SQL for the following request: {user_query}
        
        Available schema information:
        {schema_info}
        
        {business_context_str}
        
        Instructions:
        1. Use get_table_schema_unified_tool() to verify column names and table structure
        2. Test your query using execute_query_and_return_results() 
        3. Return the final SQL using final_answer()
        
        Generate clean, efficient T-SQL that answers the user's request.
        """
    
    def _format_schema_info(self, table_schemas: Dict) -> str:
        """Format schema information for prompt."""
        if not table_schemas:
            return "No schema information available"
        
        schema_lines = []
        for table_name, schema in table_schemas.items():
            columns = schema.get("columns", [])
            column_names = [col.get("name", "") for col in columns if col.get("name")]
            if column_names:
                schema_lines.append(f"{table_name}: {', '.join(column_names)}")
        
        return "\n".join(schema_lines) if schema_lines else "No valid schema information"
    
    def _extract_sql_from_response(self, response) -> Optional[str]:
        """Extract SQL from agent response."""
        if hasattr(response, 'text'):
            response = response.text
        
        # Handle dictionary response (from final_answer tool)
        if isinstance(response, dict):
            if 'final_sql' in response:
                return response['final_sql']
            elif 'sql' in response:
                return response['sql']
            elif 'query' in response:
                return response['query']
        
        if isinstance(response, str):
            # Extract SQL from code blocks
            import re
            sql_pattern = r'```sql\s*(.*?)\s*```'
            match = re.search(sql_pattern, response, re.DOTALL)
            if match:
                return match.group(1).strip()
            
            # Look for final_answer in response
            final_answer_pattern = r'final_answer\s*\(\s*["\']([^"\']*)["\']'
            match = re.search(final_answer_pattern, response)
            if match:
                return match.group(1).strip()
            
            # Look for get_accurate_schema in response (legacy support)
            get_accurate_schema_pattern = r'get_accurate_schema\s*\(\s*["\']([^"\']*)["\']'
            match = re.search(get_accurate_schema_pattern, response)
            if match:
                return match.group(1).strip()
            
            # Fallback: extract SQL-like content
            lines = response.split('\n')
            sql_lines = []
            for line in lines:
                if any(keyword in line.upper() for keyword in ['SELECT', 'FROM', 'WHERE', 'JOIN', 'GROUP', 'ORDER']):
                    sql_lines.append(line.strip())
            
            if sql_lines:
                return '\n'.join(sql_lines)
        
        logger.warning(f"Could not extract SQL from response: {type(response)} - {response}")
        return None
    
    def _check_business_compliance(self, query: str, business_context: Dict) -> Dict:
        """Check business compliance of query."""
        matched_concepts = business_context.get("matched_concepts", [])
        try:
            return self.business_validator.validate_against_concepts(query, matched_concepts)
        except Exception as e:
            logger.error(f"Business compliance check failed: {e}")
            return {"success": False, "error": str(e)}
    
    def _execute_query_impl(self, query: str, max_rows: int = 100) -> Dict:
        """Implementation of query execution."""
        try:
            result = self.database_tools.execute_query_safe(query, max_rows)
            
            if not result.get("success"):
                return result
            
            rows = result.get("rows", [])
            columns = result.get("columns", [])
            
            return {
                "success": True,
                "total_rows": len(rows),
                "returned_rows": len(rows),
                "truncated": len(rows) >= max_rows,
                "sample_data": self._create_sample_summary(rows, columns)
            }
            
        except Exception as e:
            logger.error(f"Query execution failed: {e}")
            return {"success": False, "error": str(e)}
    
    def _create_sample_summary(self, rows: List[Dict], columns: List[str]) -> Dict[str, Any]:
        """Create summary of sample data."""
        if not rows:
            return {"message": "No data returned"}
        
        sample_rows = rows[:5]  # First 5 rows
        
        # Calculate numeric statistics
        numeric_stats = {}
        for col in columns:
            numeric_values = [
                row.get(col) for row in rows 
                if row.get(col) is not None and isinstance(row.get(col), (int, float))
            ]
            if numeric_values:
                numeric_stats[col] = {
                    "min": min(numeric_values),
                    "max": max(numeric_values),
                    "avg": round(sum(numeric_values) / len(numeric_values), 2)
                }
        
        return {
            "sample_rows": sample_rows,
            "columns": columns,
            "numeric_stats": numeric_stats
        }
    
    # Legacy method for compatibility
    def generate_sql(self, user_query: str, business_context: Dict, entity_context: Dict) -> Dict[str, Any]:
        """Legacy SQL generation method."""
        return self.generate_sql_optimized(user_query, business_context, entity_context)
    
    def _format_final_sql_response(self, sql_query: str) -> Dict:
        """Return the final SQL answer.
        
        Args:
            sql_query: The final SQL query to return.
            
        Returns:
            Dictionary with the final SQL query and success status.
        """
        try:
            return {
                "success": True,
                "final_sql": sql_query.strip(),
                "message": "Final SQL query generated"
            }
        except Exception as e:
            return {"success": False, "error": str(e)}


================================================
FILE: smol-sql-agents/src/agents/concepts/__init__.py
================================================
"""Business concepts module for SQL agents."""

# Try to import components, but make them optional for testing
try:
    from .loader import ConceptLoader, BusinessConcept
    from .matcher import ConceptMatcher
    CONCEPTS_AVAILABLE = True
except ImportError:
    CONCEPTS_AVAILABLE = False
    # Create dummy classes for testing
    class ConceptLoader:
        def __init__(self, *args, **kwargs):
            pass
        def get_concepts_for_entities(self, *args, **kwargs):
            return []
        def get_all_concepts(self):
            return []
    class BusinessConcept:
        def __init__(self, **kwargs):
            for key, value in kwargs.items():
                setattr(self, key, value)
    class ConceptMatcher:
        def __init__(self, *args, **kwargs):
            pass
        def match_concepts_to_query(self, *args, **kwargs):
            return []

__all__ = ['ConceptLoader', 'BusinessConcept', 'ConceptMatcher'] 


================================================
FILE: smol-sql-agents/src/agents/concepts/loader.py
================================================
import yaml
import logging
from typing import Dict, List, Optional
from pathlib import Path
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class BusinessConcept:
    """Data class representing a business concept."""
    name: str
    description: str
    target: List[str]
    instructions: str
    required_joins: List[str]
    examples: List[Dict]
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'BusinessConcept':
        """Create BusinessConcept from dictionary."""
        return cls(
            name=data.get('name', ''),
            description=data.get('description', ''),
            target=data.get('target', []),
            instructions=data.get('instructions', ''),
            required_joins=data.get('required_joins', []),
            examples=data.get('examples', [])
        )

class ConceptLoader:
    """Loads and manages business concepts from YAML files."""
    
    def __init__(self, concepts_dir: str):
        self.concepts_dir = Path(concepts_dir)
        self._concepts_cache = {}
        self._load_all_concepts()

    def _load_all_concepts(self):
        """Load all concept files from the concepts directory and subdirectories."""
        try:
            if not self.concepts_dir.exists():
                logger.warning(f"Concepts directory {self.concepts_dir} does not exist. Creating it.")
                self.concepts_dir.mkdir(parents=True, exist_ok=True)
                return
            
            # Search recursively in all subdirectories for concept files
            concept_files = list(self.concepts_dir.rglob("*.yaml")) + list(self.concepts_dir.rglob("*.yml"))
            
            if not concept_files:
                logger.warning(f"No concept files found in {self.concepts_dir} or its subdirectories")
                return
            
            for file_path in concept_files:
                try:
                    concepts = self._load_concept_file(file_path)
                    for concept in concepts:
                        self._concepts_cache[concept.name] = concept
                    logger.info(f"Loaded {len(concepts)} concepts from {file_path}")
                except Exception as e:
                    logger.error(f"Error loading concepts from {file_path}: {e}")
                    
        except Exception as e:
            logger.error(f"Error loading concepts: {e}")

    def _load_concept_file(self, file_path: Path) -> List[BusinessConcept]:
        """Load concepts from a single YAML file."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
            
            if not data or 'concepts' not in data:
                logger.warning(f"No 'concepts' key found in {file_path}")
                return []
            
            concepts = []
            for concept_data in data['concepts']:
                if self._validate_concept_structure(concept_data):
                    concept = BusinessConcept.from_dict(concept_data)
                    concepts.append(concept)
                else:
                    logger.warning(f"Invalid concept structure in {file_path}: {concept_data.get('name', 'unknown')}")
            
            return concepts
            
        except Exception as e:
            logger.error(f"Error loading concept file {file_path}: {e}")
            return []

    def _validate_concept_structure(self, concept_data: Dict) -> bool:
        """Validate that concept has required fields."""
        required_fields = ['name', 'description', 'target', 'instructions']
        
        for field in required_fields:
            if field not in concept_data:
                return False
        
        # Ensure target is a list
        if not isinstance(concept_data.get('target'), list):
            return False
        
        # Ensure required_joins is a list (optional field)
        if 'required_joins' in concept_data and not isinstance(concept_data['required_joins'], list):
            return False
        
        # Ensure examples is a list (optional field)
        if 'examples' in concept_data and not isinstance(concept_data['examples'], list):
            return False
        
        return True

    def get_concepts_for_entities(self, entity_names: List[str]) -> List[BusinessConcept]:
        """Get all concepts that target any of the specified entities."""
        try:
            applicable_concepts = []
            entity_set = set(entity_names)
            
            for concept in self._concepts_cache.values():
                # Check if any of the concept's target entities match the provided entities
                concept_targets = concept.target
                matching_entities = [entity for entity in concept_targets if entity in entity_set]
                
                if matching_entities:
                    applicable_concepts.append(concept)
            
            return applicable_concepts
            
        except Exception as e:
            logger.error(f"Error getting concepts for entities: {e}")
            return []

    def get_concept_by_name(self, concept_name: str) -> Optional[BusinessConcept]:
        """Retrieve a specific concept by name."""
        try:
            return self._concepts_cache.get(concept_name)
        except Exception as e:
            logger.error(f"Error getting concept by name: {e}")
            return None

    def get_all_concepts(self) -> List[BusinessConcept]:
        """Get all loaded concepts."""
        try:
            return list(self._concepts_cache.values())
        except Exception as e:
            logger.error(f"Error getting all concepts: {e}")
            return []

    def reload_concepts(self):
        """Reload all concepts from disk."""
        try:
            self._concepts_cache.clear()
            self._load_all_concepts()
            logger.info("Concepts reloaded successfully")
        except Exception as e:
            logger.error(f"Error reloading concepts: {e}") 


================================================
FILE: smol-sql-agents/src/agents/concepts/matcher.py
================================================
import logging
from typing import Dict, List, Tuple
from .loader import BusinessConcept
from ...vector.embeddings import OpenAIEmbeddingsClient

logger = logging.getLogger(__name__)

class ConceptMatcher:
    """Matches business concepts to user queries using semantic similarity."""
    
    def __init__(self, indexer_agent=None):
        self.indexer_agent = indexer_agent
        self.embeddings_client = OpenAIEmbeddingsClient()

    def match_concepts_to_query(self, user_query: str, concepts: List[BusinessConcept], 
                               threshold: float = 0.5) -> List[Tuple[BusinessConcept, float]]:  # ADJUSTED TO REASONABLE LEVEL
        """Match concepts to user query based on semantic similarity of descriptions."""
        try:
            matches = []
            
            for concept in concepts:
                similarity = self._calculate_concept_similarity(user_query, concept.description)
                
                if similarity >= threshold:
                    matches.append((concept, similarity))
            
            # Sort by similarity score (highest first)
            matches.sort(key=lambda x: x[1], reverse=True)
            
            return matches
            
        except Exception as e:
            logger.error(f"Error matching concepts to query: {e}")
            return []

    def find_similar_examples(self, concept: BusinessConcept, user_query: str, 
                             max_examples: int = 3) -> List[Dict]:
        """Find most similar examples within a concept using embeddings."""
        try:
            if not concept.examples:
                return []
            
            # Rank examples by similarity to user query
            ranked_examples = self._rank_examples_by_similarity(user_query, concept.examples)
            
            # Return top examples
            return ranked_examples[:max_examples]
            
        except Exception as e:
            logger.error(f"Error finding similar examples: {e}")
            return []

    def _calculate_concept_similarity(self, user_query: str, concept_description: str) -> float:
        """Calculate semantic similarity between user query and concept description."""
        try:
            # Use embeddings client directly for similarity calculation
            embedding1 = self.embeddings_client.generate_embedding(user_query)
            embedding2 = self.embeddings_client.generate_embedding(concept_description)
            similarity = self._cosine_similarity(embedding1, embedding2)
            
            return similarity
                
        except Exception as e:
            logger.error(f"Error calculating concept similarity: {e}")
            return 0.0

    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """Calculate cosine similarity between two vectors."""
        try:
            import numpy as np
            
            v1 = np.array(vec1)
            v2 = np.array(vec2)
            
            dot_product = np.dot(v1, v2)
            norm1 = np.linalg.norm(v1)
            norm2 = np.linalg.norm(v2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            similarity = dot_product / (norm1 * norm2)
            return max(0.0, min(1.0, similarity))
            
        except Exception as e:
            logger.error(f"Error calculating cosine similarity: {e}")
            return 0.0

    def _rank_examples_by_similarity(self, user_query: str, examples: List[Dict]) -> List[Tuple[Dict, float]]:
        """Rank concept examples by similarity to user query."""
        try:
            ranked_examples = []
            
            for example in examples:
                example_query = example.get("query", "")
                similarity = self._calculate_concept_similarity(user_query, example_query)
                
                ranked_examples.append((example, similarity))
            
            # Sort by similarity score (highest first)
            ranked_examples.sort(key=lambda x: x[1], reverse=True)
            
            return ranked_examples
            
        except Exception as e:
            logger.error(f"Error ranking examples by similarity: {e}")
            return []

    def _simple_similarity(self, query: str, description: str) -> float:
        """Simple keyword-based similarity as fallback."""
        try:
            query_words = set(query.lower().split())
            desc_words = set(description.lower().split())
            
            if not query_words or not desc_words:
                return 0.0
            
            intersection = query_words.intersection(desc_words)
            union = query_words.union(desc_words)
            
            return len(intersection) / len(union) if union else 0.0
            
        except Exception as e:
            logger.error(f"Error calculating simple similarity: {e}")
            return 0.0 


================================================
FILE: smol-sql-agents/src/agents/concepts/examples/test_concepts.yaml
================================================
concepts:
  - name: "customer_analysis"
    description: "Analyze customer data including account activity, balances, and demographics"
    target: ["customers", "accounts", "transactions", "loans"]
    instructions: |
      - Include customer identification and contact information
      - Calculate total account balances and transaction frequency
      - Consider customer segmentation and demographics
      - Apply appropriate date filters for analysis periods
      - Group results by customer categories when relevant
    required_joins:
      - "customers.customer_id = accounts.customer_id"
      - "accounts.account_id = transactions.account_id"
      - "customers.customer_id = loans.customer_id"
    examples:
      - query: "List all customers with their total account balances"
        context: "Customer financial overview"
        business_logic: "Join customers and accounts, sum balances per customer"
        expected_tables: ["customers", "accounts"]
      - query: "Show transaction history for each customer"
        context: "Customer transaction analysis"
        business_logic: "Join customers, accounts, and transactions; list transactions per customer"
        expected_tables: ["customers", "accounts", "transactions"]
      - query: "Which customers have active loans?"
        context: "Loan customer identification"
        business_logic: "Join customers and loans, filter for active status"
        expected_tables: ["customers", "loans"]
      - query: "Find customers who opened accounts in the last year"
        context: "Recent customer acquisition"
        business_logic: "Filter accounts by open_date, join with customers"
        expected_tables: ["customers", "accounts"]

  - name: "account_activity"
    description: "Analyze account balances, types, and recent activity"
    target: ["accounts", "transactions", "customers"]
    instructions: |
      - Calculate current and average balances
      - Segment by account type and status
      - Include recent transaction activity
      - Apply time-based filters for recent activity
      - Group by branch or customer as needed
    required_joins:
      - "accounts.account_id = transactions.account_id"
      - "accounts.customer_id = customers.customer_id"
    examples:
      - query: "Show all active accounts with their current balances"
        context: "Account status and balances"
        business_logic: "Filter accounts by status, display balances"
        expected_tables: ["accounts"]
      - query: "List accounts with the highest number of transactions in the past month"
        context: "Active account identification"
        business_logic: "Join accounts and transactions, count transactions per account, filter by date"
        expected_tables: ["accounts", "transactions"]
      - query: "Find savings accounts with balances over $10,000"
        context: "High-value savings accounts"
        business_logic: "Filter accounts by type and balance"
        expected_tables: ["accounts"]

  - name: "branch_performance"
    description: "Evaluate branch activity, employee count, and customer engagement"
    target: ["branches", "employees", "accounts", "customers"]
    instructions: |
      - Count number of employees per branch
      - Calculate total accounts and balances per branch
      - Analyze customer distribution by branch
      - Include branch contact and location information
    required_joins:
      - "branches.branch_id = employees.branch_id"
      - "branches.branch_id = accounts.branch_id"
      - "accounts.customer_id = customers.customer_id"
    examples:
      - query: "List all branches with number of employees and total account balances"
        context: "Branch resource and financial overview"
        business_logic: "Join branches, employees, and accounts; aggregate counts and balances"
        expected_tables: ["branches", "employees", "accounts"]
      - query: "Show customer count per branch"
        context: "Branch customer engagement"
        business_logic: "Join branches, accounts, and customers; count unique customers per branch"
        expected_tables: ["branches", "accounts", "customers"]

  - name: "loan_analysis"
    description: "Analyze loan distribution, status, and customer loan activity"
    target: ["loans", "customers"]
    instructions: |
      - Calculate total and average loan amounts
      - Segment by loan type and status
      - Identify customers with multiple loans
      - Apply filters for active, paid off, or defaulted loans
    required_joins:
      - "loans.customer_id = customers.customer_id"
    examples:
      - query: "Show all active loans with customer names"
        context: "Active loan listing"
        business_logic: "Join loans and customers, filter for active status"
        expected_tables: ["loans", "customers"]
      - query: "Find customers with more than one loan"
        context: "Multiple loan customers"
        business_logic: "Group loans by customer, count, filter for count > 1"
        expected_tables: ["loans", "customers"]
      - query: "Calculate average loan amount by loan type"
        context: "Loan type analysis"
        business_logic: "Group loans by type, calculate average principal"
        expected_tables: ["loans"]

  - name: "card_activity"
    description: "Monitor card issuance, status, and account linkage"
    target: ["cards", "accounts", "customers"]
    instructions: |
      - List all active cards and their types
      - Link cards to account and customer information
      - Identify expired or lost cards
      - Analyze card distribution by account type
    required_joins:
      - "cards.account_id = accounts.account_id"
      - "accounts.customer_id = customers.customer_id"
    examples:
      - query: "List all active debit and credit cards with account and customer info"
        context: "Card issuance overview"
        business_logic: "Join cards, accounts, and customers; filter for active status"
        expected_tables: ["cards", "accounts", "customers"]
      - query: "Show cards that are expired or lost"
        context: "Inactive card identification"
        business_logic: "Filter cards by status"
        expected_tables: ["cards"]
      - query: "Count number of cards per account type"
        context: "Card distribution analysis"
        business_logic: "Join cards and accounts, group by account type, count cards"
        expected_tables: ["cards", "accounts"]


================================================
FILE: smol-sql-agents/src/agents/tools/__init__.py
================================================
"""
Shared tools framework for SQL agents.
"""

from .shared import DatabaseTools, ValidationTools, CachingTools, UtilityTools

__all__ = [
    'DatabaseTools',
    'ValidationTools', 
    'CachingTools',
    'UtilityTools'
] 


================================================
FILE: smol-sql-agents/src/agents/tools/factory.py
================================================
"""
Database tools factory for creating unified database tool instances.
"""

from .shared import DatabaseTools

class DatabaseToolsFactory:
    """Factory for creating database tools instances."""
    
    @staticmethod
    def create_database_tools(database_inspector):
        """Create a database tools instance.
        
        Args:
            database_inspector: Database inspector instance
            
        Returns:
            DatabaseTools: Configured database tools instance
        """
        return DatabaseTools(database_inspector) 


================================================
FILE: smol-sql-agents/src/agents/tools/shared.py
================================================
"""
Shared tools framework for SQL agents.
Provides unified tools for database operations, validation, caching, and utilities.
"""

import logging
import json
from typing import Dict, List, Any, Optional
from smolagents.tools import tool

logger = logging.getLogger(__name__)

class DatabaseTools:
    """Unified database operations for all agents."""
    
    def __init__(self, database_inspector=None):
        """Initialize database tools.
        
        Args:
            database_inspector: Database inspector instance
        """
        self.database_inspector = database_inspector
    
    def get_table_schema_unified(self, table_name: str) -> Dict[str, Any]:
        """Get unified table schema information.
        
        Args:
            table_name: Name of the table to get schema for
            
        Returns:
            Dict: Table schema information with standardized format
        """
        try:
            if not self.database_inspector:
                return {
                    "success": False,
                    "error": "Database inspector not available",
                    "details": "This tool requires a database inspector to be provided"
                }
            
            schema = self.database_inspector.get_table_schema(table_name)
            if not schema:
                return {
                    "success": False,
                    "error": f"Table '{table_name}' not found",
                    "details": "The specified table does not exist in the database"
                }
            
            return {
                "success": True,
                "table_name": table_name,
                "schema": schema,
                "columns": schema.get("columns", []),
                "primary_key": schema.get("primary_key"),
                "foreign_keys": schema.get("foreign_keys", []),
                "description": schema.get("description", f"Table {table_name}")
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "details": "Exception occurred while getting table schema"
            }
    
    def get_all_tables_unified(self) -> Dict[str, Any]:
        """Get all table names in unified format.
        
        Returns:
            Dict: List of all table names with standardized format
        """
        try:
            if not self.database_inspector:
                return {
                    "success": False,
                    "error": "Database inspector not available",
                    "details": "This tool requires a database inspector to be provided"
                }
            
            table_names = self.database_inspector.get_all_table_names()
            if not table_names:
                return {
                    "success": True,
                    "tables": [],
                    "count": 0,
                    "message": "No tables found in database"
                }
            
            return {
                "success": True,
                "tables": table_names,
                "count": len(table_names),
                "message": f"Found {len(table_names)} tables"
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "details": "Exception occurred while getting table names"
            }
    
    def get_relationships_unified(self) -> Dict[str, Any]:
        """Get all foreign key relationships in unified format.
        
        Returns:
            Dict: List of all relationships with standardized format
        """
        try:
            if not self.database_inspector:
                return {
                    "success": False,
                    "error": "Database inspector not available",
                    "details": "This tool requires a database inspector to be provided"
                }
            
            relationships = self.database_inspector.get_all_foreign_key_relationships()
            if not relationships:
                return {
                    "success": True,
                    "relationships": [],
                    "count": 0,
                    "message": "No foreign key relationships found"
                }
            
            return {
                "success": True,
                "relationships": relationships,
                "count": len(relationships),
                "message": f"Found {len(relationships)} foreign key relationships"
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "details": "Exception occurred while getting relationships"
            }
    
    def execute_query_safe(self, query: str, max_rows: int = 100) -> Dict[str, Any]:
        """Execute query safely with unified format.
        
        Args:
            query: SQL query to execute
            max_rows: Maximum number of rows to return
            
        Returns:
            Dict: Query execution results with standardized format
        """
        try:
            if not self.database_inspector:
                return {
                    "success": False,
                    "error": "Database inspector not available",
                    "details": "This tool requires a database inspector to be provided"
                }
            
            # Use the database inspector's engine to execute the query
            from sqlalchemy import text
            with self.database_inspector.engine.connect() as connection:
                # Add row limiting for safety
                if "TOP" not in query.upper() and "SELECT" in query.upper():
                    # For SQL Server, add TOP clause
                    select_index = query.upper().find("SELECT")
                    if select_index != -1:
                        after_select = query[select_index + 6:].lstrip()
                        query = query[:select_index + 6] + f" TOP {max_rows} " + after_select
                
                result = connection.execute(text(query))
                rows = result.fetchall()
                columns = result.keys()
                
                # Convert to list of dictionaries
                row_dicts = [dict(zip(columns, row)) for row in rows]
                
                return {
                    "success": True,
                    "rows": row_dicts,
                    "columns": list(columns),
                    "total_rows": len(row_dicts),
                    "returned_rows": len(row_dicts),
                    "truncated": len(row_dicts) >= max_rows
                }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "details": "Exception occurred while executing query"
            }
    
    def create_tools(self):
        """Create tool functions for this database tools instance."""
        database_tools_instance = self
        
        @tool
        def get_table_schema_unified_tool(table_name: str) -> Dict[str, Any]:
            """Get unified table schema information.
            
            Args:
                table_name: Name of the table to get schema for
                
            Returns:
                Dict: Table schema information with standardized format
            """
            return database_tools_instance.get_table_schema_unified(table_name)
        
        @tool
        def get_all_tables_unified_tool() -> Dict[str, Any]:
            """Get all table names in unified format.
            
            Returns:
                Dict: List of all table names with standardized format
            """
            return database_tools_instance.get_all_tables_unified()
        
        @tool
        def get_relationships_unified_tool() -> Dict[str, Any]:
            """Get all foreign key relationships in unified format.
            
            Returns:
                Dict: List of all relationships with standardized format
            """
            return database_tools_instance.get_relationships_unified()
        
        return [
            get_table_schema_unified_tool,
            get_all_tables_unified_tool,
            get_relationships_unified_tool
        ]

class ValidationTools:
    """Unified validation tools for all agents."""
    
    @staticmethod
    @tool
    def validate_input_unified(input_data: Any, expected_type: str) -> Dict[str, Any]:
        """Validate input data against expected type.
        
        Args:
            input_data: Data to validate
            expected_type: Expected type (str, list, dict, etc.)
            
        Returns:
            Dict: Validation result with standardized format
        """
        try:
            if expected_type == "str" and isinstance(input_data, str):
                return {"valid": True, "message": "Input is valid string"}
            elif expected_type == "list" and isinstance(input_data, list):
                return {"valid": True, "message": "Input is valid list"}
            elif expected_type == "dict" and isinstance(input_data, dict):
                return {"valid": True, "message": "Input is valid dictionary"}
            else:
                return {"valid": False, "error": f"Expected {expected_type}, got {type(input_data).__name__}"}
        except Exception as e:
            return {"valid": False, "error": str(e)}
    
    @staticmethod
    @tool
    def validate_query_safety(query: str) -> Dict[str, Any]:
        """Validate SQL query for safety concerns.
        
        Args:
            query: SQL query to validate
            
        Returns:
            Dict: Safety validation result
        """
        try:
            # Basic SQL injection prevention checks
            dangerous_keywords = [
                "DROP", "DELETE", "TRUNCATE", "ALTER", "CREATE", "INSERT", "UPDATE"
            ]
            
            query_upper = query.upper()
            issues = []
            
            for keyword in dangerous_keywords:
                if keyword in query_upper:
                    issues.append(f"Contains potentially dangerous keyword: {keyword}")
            
            if issues:
                return {
                    "safe": False,
                    "issues": issues,
                    "recommendation": "Review query for security concerns"
                }
            else:
                return {
                    "safe": True,
                    "message": "Query appears safe for execution"
                }
        except Exception as e:
            return {"safe": False, "error": str(e)}
    
    @staticmethod
    @tool
    def validate_required_params(params: Dict, required_keys: List[str]) -> Dict[str, Any]:
        """Validate that required parameters are present.
        
        Args:
            params: Parameters to validate
            required_keys: List of required keys
            
        Returns:
            Dict: Validation result
        """
        try:
            missing_keys = [key for key in required_keys if key not in params]
            if missing_keys:
                return {"valid": False, "error": f"Missing required parameters: {missing_keys}"}
            return {"valid": True, "message": "All required parameters present"}
        except Exception as e:
            return {"valid": False, "error": str(e)}

class CachingTools:
    """Unified caching tools for all agents."""
    
    def __init__(self, cache_size: int = 100):
        """Initialize caching tools.
        
        Args:
            cache_size: Maximum number of cached items
        """
        self._cache = {}
        self._cache_size = cache_size
    
    def get_cached_result(self, cache_key: str) -> Dict[str, Any]:
        """Get cached result if available.
        
        Args:
            cache_key: Cache key to look up
            
        Returns:
            Dict: Cached result or None if not found
        """
        try:
            result = self._cache.get(cache_key)
            if result:
                return {"success": True, "cached": True, "data": result}
            else:
                return {"success": True, "cached": False, "data": None}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def cache_result(self, cache_key: str, data: Any, ttl: int = 1800) -> Dict[str, Any]:
        """Cache a result with TTL.
        
        Args:
            cache_key: Cache key
            data: Data to cache
            ttl: Time to live in seconds
            
        Returns:
            Dict: Caching result
        """
        try:
            import time
            self._cache[cache_key] = {
                "data": data,
                "timestamp": time.time(),
                "ttl": ttl
            }
            
            # Limit cache size
            if len(self._cache) > self._cache_size:
                # Remove oldest entries
                oldest_keys = list(self._cache.keys())[:10]
                for key in oldest_keys:
                    del self._cache[key]
            
            return {"success": True, "message": "Result cached successfully"}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def clear_cache(self) -> Dict[str, Any]:
        """Clear the cache.
        
        Returns:
            Dict: Clear operation result
        """
        try:
            self._cache.clear()
            return {"success": True, "message": "Cache cleared successfully"}
        except Exception as e:
            return {"success": False, "error": str(e)}

class UtilityTools:
    """Unified utility tools for all agents."""
    
    @staticmethod
    @tool
    def format_response_unified(data: Any, format_type: str = "json") -> Dict[str, Any]:
        """Format response data in specified format.
        
        Args:
            data: Data to format
            format_type: Format type (json, text, etc.)
            
        Returns:
            Dict: Formatted response
        """
        try:
            if format_type == "json":
                return {"success": True, "data": data, "format": "json"}
            elif format_type == "text":
                return {"success": True, "data": str(data), "format": "text"}
            else:
                return {"success": False, "error": f"Unsupported format: {format_type}"}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    @staticmethod
    @tool
    def log_operation_unified(operation: str, details: Dict = None) -> Dict[str, Any]:
        """Log an operation for debugging and monitoring.
        
        Args:
            operation: Name of the operation
            details: Additional details to log
            
        Returns:
            Dict: Logging result
        """
        try:
            log_message = f"Operation: {operation}"
            if details:
                log_message += f" | Details: {details}"
            
            logger.info(log_message)
            return {"success": True, "message": "Operation logged successfully"}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    @staticmethod
    @tool
    def safe_execute_unified(func_name: str, func_args: Dict = None) -> Dict[str, Any]:
        """Safely execute a function with error handling.
        
        Args:
            func_name: Name of the function to execute
            func_args: Arguments for the function
            
        Returns:
            Dict: Execution result
        """
        try:
            # This is a placeholder - actual implementation will depend on the agent
            return {
                "success": False,
                "error": "Function execution not implemented",
                "details": "This tool requires agent-specific implementation"
            }
        except Exception as e:
            return {"success": False, "error": str(e)} 


================================================
FILE: smol-sql-agents/src/database/__init__.py
================================================
# src/database/__init__.py
# Database inspection and persistence utilities 


================================================
FILE: smol-sql-agents/src/database/inspector.py
================================================
import os
import logging
from sqlalchemy import create_engine, inspect, MetaData
from dotenv import load_dotenv

load_dotenv()
logger = logging.getLogger(__name__)

class DatabaseInspector:
    """A toolkit for inspecting a SQL database schema using SQLAlchemy."""

    def __init__(self):
        """Initializes the database engine and inspector."""
        db_url = os.getenv("DATABASE_URL")
        if not db_url:
            raise ValueError("DATABASE_URL environment variable not set.")
        
        logger.info("Initializing database connection")
        self.engine = create_engine(db_url)
        self.inspector = inspect(self.engine)
        self.metadata = MetaData()
        self.metadata.reflect(bind=self.engine)
        logger.info("Database inspector initialized successfully")

    def get_all_table_names(self) -> list[str]:
        """Retrieves a list of all user-defined table names in the public schema, excluding system tables."""
        try:
            tables = self.inspector.get_table_names()
            # Exclude system tables (e.g., those starting with 'pg_' or 'sql_')
            user_tables = [t for t in tables if not (t.startswith('pg_') or t.startswith('spt_') or t.startswith('MSreplication_'))]
            logger.info(f"Found {len(user_tables)} user tables in database")
            return user_tables
        except Exception as e:
            logger.error(f"Failed to retrieve table names: {e}")
            raise

    def get_table_schema(self, table_name: str) -> dict:
        """Retrieves the detailed schema for a specific table.
        
        Args:
            table_name: The name of the table to retrieve schema for.
            
        Returns:
            A dictionary containing the table schema with columns and constraints.
        """
        try:
            columns = self.inspector.get_columns(table_name)
            pk_constraint = self.inspector.get_pk_constraint(table_name)
            pk_columns = pk_constraint.get('constrained_columns', [])
            
            schema = {
                'table_name': table_name,
                'columns': []
            }
            
            for col in columns:
                schema['columns'].append({
                    'name': col['name'],
                    'type': str(col['type']),
                    'nullable': col['nullable'],
                    'primary_key': col['name'] in pk_columns,
                    'default': col.get('default')
                })
            
            logger.info(f"Retrieved schema for table: {table_name}")
            return schema
            
        except Exception as e:
            logger.error(f"Failed to retrieve schema for table {table_name}: {e}")
            raise

    def get_all_foreign_key_relationships(self) -> list[dict]:
        """Retrieves all foreign key relationships across the entire database."""
        try:
            relationships = []
            tables = self.get_all_table_names()
            
            for table in tables:
                fks = self.inspector.get_foreign_keys(table)
                for fk in fks:
                    relationships.append({
                        'constrained_table': table,
                        'constrained_columns': fk['constrained_columns'],
                        'referred_table': fk['referred_table'],
                        'referred_columns': fk['referred_columns']
                    })
            
            logger.info(f"Found {len(relationships)} foreign key relationships")
            return relationships
            
        except Exception as e:
            logger.error(f"Failed to retrieve foreign key relationships: {e}")
            raise



================================================
FILE: smol-sql-agents/src/database/persistence.py
================================================
import sqlite3
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

logger = logging.getLogger(__name__)

class DocumentationStore:
    """SQLite-based persistence layer for documentation generation."""
    
    def __init__(self, db_path: str = "__bin__/data/documentation.db"):
        """Initialize the documentation store."""
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(exist_ok=True)
        self._init_database()
        logger.info(f"Documentation store initialized at {db_path}")
    
    def _init_database(self):
        """Create the necessary tables for documentation storage."""
        with sqlite3.connect(self.db_path) as conn:
            conn.executescript("""
                CREATE TABLE IF NOT EXISTS processing_state (
                    id INTEGER PRIMARY KEY,
                    phase TEXT NOT NULL,
                    status TEXT NOT NULL,  -- 'pending', 'completed', 'failed'
                    started_at TIMESTAMP,
                    completed_at TIMESTAMP,
                    error_message TEXT
                );
                
                CREATE TABLE IF NOT EXISTS table_metadata (
                    table_name TEXT PRIMARY KEY,
                    schema_data TEXT NOT NULL,  -- JSON
                    business_purpose TEXT,
                    documentation TEXT,         -- Generated markdown section
                    processed_at TIMESTAMP,
                    status TEXT DEFAULT 'pending'
                );
                
                CREATE TABLE IF NOT EXISTS relationship_metadata (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    constrained_table TEXT NOT NULL,
                    constrained_columns TEXT NOT NULL,  -- JSON array
                    referred_table TEXT NOT NULL,
                    referred_columns TEXT NOT NULL,     -- JSON array
                    relationship_type TEXT,             -- inferred type
                    documentation TEXT,                 -- Generated markdown section
                    processed_at TIMESTAMP,
                    status TEXT DEFAULT 'pending'
                );
                
                CREATE TABLE IF NOT EXISTS generation_metadata (
                    id INTEGER PRIMARY KEY,
                    source_database_url TEXT NOT NULL,
                    started_at TIMESTAMP NOT NULL,
                    completed_at TIMESTAMP,
                    total_tables INTEGER,
                    total_relationships INTEGER,
                    status TEXT DEFAULT 'in_progress'
                );
            """)
            logger.info("Database schema initialized")
    
    def start_generation_session(self, db_url: str, tables: List[str], 
                                relationships: List[Dict]) -> int:
        """Start a new documentation generation session."""
        with sqlite3.connect(self.db_path) as conn:
            # Get already completed tables
            cursor = conn.execute("""
                SELECT table_name FROM table_metadata 
                WHERE status = 'completed'
            """)
            completed_tables = {row[0] for row in cursor.fetchall()}
            
            # Get already completed relationships
            cursor = conn.execute("""
                SELECT id FROM relationship_metadata 
                WHERE status = 'completed'
            """)
            completed_relationships = {row[0] for row in cursor.fetchall()}
            
            # Start new session
            cursor = conn.execute("""
                INSERT INTO generation_metadata 
                (source_database_url, started_at, total_tables, total_relationships)
                VALUES (?, ?, ?, ?)
            """, (db_url, datetime.now(), len(tables), len(relationships)))
            
            session_id = cursor.lastrowid
            
            # Initialize table processing states - only for non-completed tables
            for table in tables:
                if table not in completed_tables:
                    conn.execute("""
                        INSERT OR REPLACE INTO table_metadata (table_name, schema_data, status)
                        VALUES (?, ?, 'pending')
                    """, (table, "{}"))
            
            # Initialize relationship processing states - only for non-completed relationships
            for rel in relationships:
                if rel.get('id') not in completed_relationships:
                    conn.execute("""
                        INSERT OR REPLACE INTO relationship_metadata 
                        (constrained_table, constrained_columns, referred_table, referred_columns, status)
                        VALUES (?, ?, ?, ?, 'pending')
                    """, (rel["constrained_table"], 
                         json.dumps(rel["constrained_columns"]),
                         rel["referred_table"],
                         json.dumps(rel["referred_columns"])))
            
            logger.info(f"Started generation session {session_id} with {len(tables)} tables and {len(relationships)} relationships")
            return session_id
    
    def save_table_documentation(self, table_name: str, schema_data: Dict, 
                                business_purpose: str, documentation: str):
        """Save processed table documentation."""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                UPDATE table_metadata 
                SET schema_data = ?, business_purpose = ?, documentation = ?,
                    processed_at = ?, status = 'completed'
                WHERE table_name = ?
            """, (json.dumps(schema_data), business_purpose, documentation,
                  datetime.now(), table_name))
            logger.info(f"Saved documentation for table: {table_name}")
    
    def save_relationship_documentation(self, relationship_id: int, 
                                      relationship_type: str, documentation: str):
        """Save processed relationship documentation."""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                UPDATE relationship_metadata
                SET relationship_type = ?, documentation = ?, 
                    processed_at = ?, status = 'completed'
                WHERE id = ?
            """, (relationship_type, documentation, datetime.now(), relationship_id))
            logger.info(f"Saved documentation for relationship: {relationship_id}")
    
    def get_pending_tables(self) -> List[str]:
        """Get list of tables that still need processing."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT table_name FROM table_metadata 
                WHERE status = 'pending' 
                AND table_name NOT IN (
                    SELECT table_name FROM table_metadata 
                    WHERE status = 'completed'
                )
            """)
            return [row[0] for row in cursor.fetchall()]
    
    def get_pending_relationships(self) -> List[Dict]:
        """Get list of relationships that still need processing."""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT id, constrained_table, constrained_columns, 
                       referred_table, referred_columns
                FROM relationship_metadata 
                WHERE status = 'pending'
                AND id NOT IN (
                    SELECT id FROM relationship_metadata 
                    WHERE status = 'completed'
                )
            """)
            return [{
                'id': row[0],
                'constrained_table': row[1],
                'constrained_columns': json.loads(row[2]),
                'referred_table': row[3], 
                'referred_columns': json.loads(row[4])
            } for row in cursor.fetchall()]
    
    def get_generation_progress(self) -> Dict:
        """Get current progress statistics."""
        with sqlite3.connect(self.db_path) as conn:
            # Get table progress
            cursor = conn.execute("""
                SELECT status, COUNT(*) FROM table_metadata GROUP BY status
            """)
            table_stats = dict(cursor.fetchall())
            
            # Get relationship progress
            cursor = conn.execute("""
                SELECT status, COUNT(*) FROM relationship_metadata GROUP BY status  
            """)
            rel_stats = dict(cursor.fetchall())
            
            return {
                'tables': table_stats,
                'relationships': rel_stats
            }
            
    def is_table_processed(self, table_name: str) -> bool:
        """Check if a table has already been processed.
        
        Args:
            table_name: Name of the table to check
            
        Returns:
            bool: True if the table has been processed, False otherwise
        """
        with sqlite3.connect(self.db_path) as conn:
            # First check what records exist for this table
            cursor = conn.execute("""
                SELECT table_name, status FROM table_metadata 
                WHERE table_name = ?
            """, (table_name,))
            all_records = cursor.fetchall()
            logger.debug(f"Found records for table {table_name}: {all_records}")
            
            # Now check for completed status
            cursor = conn.execute("""
                SELECT status FROM table_metadata 
                WHERE table_name = ? AND status = 'completed'
            """, (table_name,))
            result = cursor.fetchone()
            is_processed = result is not None
            logger.debug(f"Table {table_name} processed status: {is_processed}, result: {result}")
            return is_processed
    
    def is_relationship_processed(self, relationship: Dict) -> bool:
        """Check if a relationship has already been processed.
        
        Args:
            relationship: Dictionary containing relationship info with constrained_table,
                        constrained_columns, referred_table, and referred_columns
            
        Returns:
            bool: True if the relationship has been processed, False otherwise
        """
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT status FROM relationship_metadata 
                WHERE constrained_table = ?
                AND constrained_columns = ?
                AND referred_table = ?
                AND referred_columns = ?
                AND status = 'completed'
            """, (
                relationship['constrained_table'],
                json.dumps(relationship['constrained_columns']),
                relationship['referred_table'],
                json.dumps(relationship['referred_columns'])
            ))
            result = cursor.fetchone()
            logger.debug(f"Relationship {relationship['constrained_table']} -> {relationship['referred_table']} processed status: {result is not None}")
            return result is not None
    
    def get_table_info(self, table_name: str) -> Optional[Dict]:
        """Get complete information for a table including schema, business purpose, and documentation.
        
        Args:
            table_name: Name of the table to retrieve information for
            
        Returns:
            Optional[Dict]: Table information or None if not found
        """
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT schema_data, business_purpose, documentation, status
                FROM table_metadata 
                WHERE table_name = ?
            """, (table_name,))
            result = cursor.fetchone()
            
            if result:
                return {
                    "table_name": table_name,
                    "schema_data": json.loads(result[0]) if result[0] else {},
                    "business_purpose": result[1] or "",
                    "documentation": result[2] or "",
                    "status": result[3]
                }
            return None
    
    def get_relationship_info(self, relationship_id: str) -> Optional[Dict]:
        """Get complete information for a relationship including type and documentation.
        
        Args:
            relationship_id: ID of the relationship to retrieve information for
            
        Returns:
            Optional[Dict]: Relationship information or None if not found
        """
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT relationship_type, documentation, status
                FROM relationship_metadata 
                WHERE id = ?
            """, (relationship_id,))
            result = cursor.fetchone()
            
            if result:
                return {
                    "id": relationship_id,
                    "relationship_type": result[0] or "",
                    "documentation": result[1] or "",
                    "status": result[2]
                }
            return None
    
    def get_all_tables(self) -> List[str]:
        """Get all processed tables from the database.
        
        Returns:
            List[str]: List of all table names that have been processed
        """
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT DISTINCT table_name FROM table_metadata 
                WHERE status = 'completed'
            """)
            return [row[0] for row in cursor.fetchall()]
    
    def get_all_relationships(self) -> List[Dict]:
        """Get all processed relationships from the database.
        
        Returns:
            List[Dict]: List of all processed relationships
        """
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT id, constrained_table, constrained_columns, 
                       referred_table, referred_columns
                FROM relationship_metadata 
                WHERE status = 'completed'
            """)
            return [{
                'id': row[0],
                'constrained_table': row[1],
                'constrained_columns': json.loads(row[2]),
                'referred_table': row[3], 
                'referred_columns': json.loads(row[4])
            } for row in cursor.fetchall()]



================================================
FILE: smol-sql-agents/src/output/__init__.py
================================================
# src/__bin__/output/__init__.py
# Output formatting and documentation generation 


================================================
FILE: smol-sql-agents/src/output/formatters.py
================================================

import sqlite3
import json
import logging
from typing import Dict
from jinja2 import Environment, BaseLoader

logger = logging.getLogger(__name__)

class DocumentationFormatter:
    """Generates formatted documentation from stored data."""
    
    def __init__(self, db_path: str = "__bin__/data/documentation.db"):
        self.db_path = db_path
        self.templates = {
            'markdown': self._markdown_template(),
            'html': self._html_template()
        }
    
    def generate_documentation(self, format_type: str = 'markdown') -> str:
        """Generate complete documentation in specified format."""
        logger.info(f"Generating documentation in {format_type} format")
        
        data = self._load_documentation_data()
        template = Environment(loader=BaseLoader()).from_string(
            self.templates.get(format_type, self.templates['markdown'])
        )
        
        result = template.render(**data)
        logger.info(f"Documentation generated successfully ({len(result)} characters)")
        return result
    
    def _load_documentation_data(self) -> Dict:
        """Load all documentation data from SQLite."""
        with sqlite3.connect(self.db_path) as conn:
            # Load table documentation
            cursor = conn.execute("""
                SELECT DISTINCT table_name, schema_data, business_purpose, documentation
                FROM table_metadata 
                WHERE status = 'completed'
                AND table_name NOT IN (
                    SELECT t2.table_name 
                    FROM table_metadata t2 
                    WHERE t2.table_name = table_metadata.table_name 
                    AND t2.rowid > table_metadata.rowid
                )
                ORDER BY table_name
            """)
            
            tables = []
            for row in cursor.fetchall():
                tables.append({
                    'name': row[0],
                    'schema': json.loads(row[1]),
                    'purpose': row[2],
                    'documentation': row[3]
                })
            
            # Load relationship documentation  
            cursor = conn.execute("""
                SELECT DISTINCT constrained_table, constrained_columns, referred_table, 
                       referred_columns, relationship_type, documentation
                FROM relationship_metadata
                WHERE status = 'completed'
                AND id NOT IN (
                    SELECT r2.id 
                    FROM relationship_metadata r2 
                    WHERE r2.constrained_table = relationship_metadata.constrained_table
                    AND r2.referred_table = relationship_metadata.referred_table
                    AND r2.id > relationship_metadata.id
                )
                ORDER BY constrained_table, referred_table
            """)
            
            relationships = []
            for row in cursor.fetchall():
                relationships.append({
                    'constrained_table': row[0],
                    'constrained_columns': json.loads(row[1]),
                    'referred_table': row[2],
                    'referred_columns': json.loads(row[3]),
                    'type': row[4],
                    'documentation': row[5]
                })
            
            # Load generation metadata
            cursor = conn.execute("""
                SELECT started_at, completed_at, total_tables, total_relationships
                FROM generation_metadata
                ORDER BY id DESC LIMIT 1
            """)
            
            metadata = cursor.fetchone()
            
            return {
                'tables': tables,
                'relationships': relationships,
                'metadata': {
                    'started_at': metadata[0] if metadata else None,
                    'completed_at': metadata[1] if metadata else None,
                    'total_tables': metadata[2] if metadata else 0,
                    'total_relationships': metadata[3] if metadata else 0
                }
            }
    
    def _markdown_template(self) -> str:
        """Markdown template for documentation generation."""
        return """# Database Knowledge Base

Generated on: {{ metadata.completed_at or 'In Progress' }}
Total Tables: {{ metadata.total_tables }}
Total Relationships: {{ metadata.total_relationships }}

# Tables

{% for table in tables %}
## {{ table.name }}

{{ table.purpose }}

| Column | Type | Primary Key | Nullable |
|--------|------|-------------|----------|
{% for column in table.schema.columns -%}
| {{ column.name }} | {{ column.type }} | {{ 'Yes' if column.primary_key else 'No' }} | {{ 'Yes' if column.nullable else 'No' }} |
{% endfor %}

{{ table.documentation }}

{% endfor %}

# Relationships

{% for rel in relationships %}
### {{ rel.constrained_table }}.{{ rel.constrained_columns|join(',') }} ‚Üí {{ rel.referred_table }}.{{ rel.referred_columns|join(',') }}

{{ rel.documentation }}

{% endfor %}
"""
    
    def _html_template(self) -> str:
        """HTML template for documentation generation."""
        return """<!DOCTYPE html>
<html>
<head>
    <title>Database Knowledge Base</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        table { border-collapse: collapse; width: 100%; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        .metadata { background-color: #f9f9f9; padding: 10px; margin-bottom: 20px; }
    </style>
</head>
<body>
    <h1>Database Knowledge Base</h1>
    
    <div class="metadata">
        <strong>Generated:</strong> {{ metadata.completed_at or 'In Progress' }}<br>
        <strong>Tables:</strong> {{ metadata.total_tables }}<br>
        <strong>Relationships:</strong> {{ metadata.total_relationships }}
    </div>
    
    <h1>Tables</h1>
    {% for table in tables %}
    <h2>{{ table.name }}</h2>
    <p>{{ table.purpose }}</p>
    
    <table>
        <tr><th>Column</th><th>Type</th><th>Primary Key</th><th>Nullable</th></tr>
        {% for column in table.schema.columns %}
        <tr>
            <td>{{ column.name }}</td>
            <td>{{ column.type }}</td>
            <td>{{ 'Yes' if column.primary_key else 'No' }}</td>
            <td>{{ 'Yes' if column.nullable else 'No' }}</td>
        </tr>
        {% endfor %}
    </table>
    
    <div>{{ table.documentation }}</div>
    {% endfor %}
    
    <h1>Relationships</h1>
    {% for rel in relationships %}
    <h3>{{ rel.constrained_table }}.{{ rel.constrained_columns|join(',') }} ‚Üí {{ rel.referred_table }}.{{ rel.referred_columns|join(',') }}</h3>
    <p>{{ rel.documentation }}</p>
    {% endfor %}
</body>
</html>"""



================================================
FILE: smol-sql-agents/src/utils/__init__.py
================================================
# src/utils/__init__.py
# Utility functions and helpers 


================================================
FILE: smol-sql-agents/src/validation/__init__.py
================================================
"""Validation framework for SQL agents."""

from .business_validator import BusinessValidator
from .tsql_validator import TSQLValidator
from .query_optimizer import QueryOptimizer
 
__all__ = ['BusinessValidator', 'TSQLValidator', 'QueryOptimizer'] 


================================================
FILE: smol-sql-agents/src/validation/business_validator.py
================================================
import logging
import sqlparse
from typing import Dict, List, Any
from ..agents.concepts.loader import BusinessConcept

logger = logging.getLogger(__name__)

class BusinessValidator:
    """Validates generated queries against business rules and concepts."""
    
    def __init__(self):
        self.validation_rules = self._load_validation_rules()

    def validate_against_concepts(self, query: str, applicable_concepts: List[BusinessConcept]) -> Dict[str, Any]:
        """Validate query against business concept requirements."""
        try:
            validation_result = {
                "valid": True,
                "issues": [],
                "warnings": [],
                "concept_compliance": {}
            }
            
            for concept in applicable_concepts:
                concept_validation = self._validate_single_concept(query, concept)
                validation_result["concept_compliance"][concept.name] = concept_validation
                
                if not concept_validation["valid"]:
                    validation_result["valid"] = False
                    validation_result["issues"].extend(concept_validation["issues"])
                
                if concept_validation["warnings"]:
                    validation_result["warnings"].extend(concept_validation["warnings"])
            
            return validation_result
            
        except Exception as e:
            logger.error(f"Error validating against concepts: {e}")
            return {
                "valid": False,
                "error": str(e),
                "issues": [],
                "warnings": []
            }

    def check_required_joins(self, query: str, required_joins: List[str]) -> Dict[str, bool]:
        """Verify that required joins are present in query."""
        try:
            parsed_query = sqlparse.parse(query)[0]
            query_joins = self._extract_joins_from_query(parsed_query)
            
            missing_joins = []
            for required_join in required_joins:
                if not self._join_exists_in_query(required_join, query_joins):
                    missing_joins.append(required_join)
            
            return {
                "valid": len(missing_joins) == 0,
                "missing_joins": missing_joins,
                "found_joins": query_joins
            }
            
        except Exception as e:
            logger.error(f"Error checking required joins: {e}")
            return {"valid": False, "error": str(e)}

    def validate_business_logic(self, query: str, business_instructions: List[str]) -> Dict[str, Any]:
        """Check if query follows business logic instructions."""
        try:
            validation_result = {
                "valid": True,
                "issues": [],
                "warnings": []
            }
            
            for instruction in business_instructions:
                instruction_check = self._check_instruction_compliance(query, instruction)
                if not instruction_check["compliant"]:
                    validation_result["valid"] = False
                    validation_result["issues"].append(instruction_check["issue"])
                elif instruction_check["warning"]:
                    validation_result["warnings"].append(instruction_check["warning"])
            
            return validation_result
            
        except Exception as e:
            logger.error(f"Error validating business logic: {e}")
            return {"valid": False, "error": str(e)}

    def check_data_privacy_compliance(self, query: str) -> Dict[str, Any]:
        """Ensure query doesn't expose sensitive data inappropriately."""
        try:
            privacy_issues = []
            
            # Check for potential sensitive data patterns
            sensitive_patterns = [
                "password", "ssn", "credit_card", "social_security",
                "phone", "email", "address", "birth_date"
            ]
            
            query_lower = query.lower()
            for pattern in sensitive_patterns:
                if pattern in query_lower:
                    privacy_issues.append(f"Query may expose sensitive data: {pattern}")
            
            # Check for SELECT * usage which might expose unnecessary data
            if "select *" in query_lower:
                privacy_issues.append("SELECT * may expose unnecessary sensitive data")
            
            return {
                "compliant": len(privacy_issues) == 0,
                "issues": privacy_issues
            }
            
        except Exception as e:
            logger.error(f"Error checking data privacy compliance: {e}")
            return {"compliant": False, "error": str(e)}

    def _load_validation_rules(self) -> Dict[str, Any]:
        """Load business validation rules configuration."""
        return {
            "required_patterns": {
                "customer_lifetime_value": ["SUM", "COUNT", "customer"],
                "sales_performance_analysis": ["SUM", "AVG", "sales"],
                "inventory_analysis": ["COUNT", "SUM", "inventory"]
            },
            "forbidden_patterns": {
                "customer_lifetime_value": ["DELETE", "UPDATE", "DROP"],
                "sales_performance_analysis": ["DELETE", "UPDATE", "DROP"],
                "inventory_analysis": ["DELETE", "UPDATE", "DROP"]
            }
        }

    def _validate_single_concept(self, query: str, concept: BusinessConcept) -> Dict[str, Any]:
        """Validate query against a single business concept."""
        try:
            validation_result = {
                "valid": True,
                "issues": [],
                "warnings": []
            }
            
            # Check required joins
            if concept.required_joins:
                join_validation = self.check_required_joins(query, concept.required_joins)
                if not join_validation["valid"]:
                    validation_result["valid"] = False
                    validation_result["issues"].append(f"Missing required joins: {join_validation['missing_joins']}")
            
            # Check business logic compliance
            if concept.instructions:
                logic_validation = self.validate_business_logic(query, [concept.instructions])
                if not logic_validation["valid"]:
                    validation_result["valid"] = False
                    validation_result["issues"].extend(logic_validation["issues"])
                
                if logic_validation["warnings"]:
                    validation_result["warnings"].extend(logic_validation["warnings"])
            
            # Check for required aggregation patterns
            if hasattr(concept, 'name') and concept.name in self.validation_rules["required_patterns"]:
                required_patterns = self.validation_rules["required_patterns"][concept.name]
                query_lower = query.lower()
                
                for pattern in required_patterns:
                    if pattern.lower() not in query_lower:
                        validation_result["warnings"].append(f"Expected pattern '{pattern}' not found in query")
            
            return validation_result
            
        except Exception as e:
            logger.error(f"Error validating single concept: {e}")
            return {"valid": False, "error": str(e)}

    def _extract_joins_from_query(self, parsed_query) -> List[str]:
        """Extract JOIN clauses from parsed SQL."""
        try:
            joins = []
            
            def extract_joins_from_token(token):
                if token.ttype is sqlparse.tokens.Keyword and token.value.upper() == 'JOIN':
                    # Get the join condition from surrounding tokens
                    join_condition = ""
                    for sibling in token.parent.tokens:
                        if sibling.ttype is sqlparse.tokens.Whitespace:
                            continue
                        if sibling.ttype is sqlparse.tokens.Keyword and sibling.value.upper() == 'ON':
                            # Get the condition after ON
                            for next_token in sibling.parent.tokens:
                                if next_token.ttype is sqlparse.tokens.Whitespace:
                                    continue
                                if next_token.ttype is sqlparse.tokens.Keyword and next_token.value.upper() == 'ON':
                                    continue
                                join_condition += str(next_token)
                            break
                        join_condition += str(sibling)
                    
                    if join_condition.strip():
                        joins.append(join_condition.strip())
                
                # Recursively check child tokens
                for child in token.tokens:
                    extract_joins_from_token(child)
            
            extract_joins_from_token(parsed_query)
            return joins
            
        except Exception as e:
            logger.error(f"Error extracting joins from query: {e}")
            return []

    def _join_exists_in_query(self, required_join: str, query_joins: List[str]) -> bool:
        """Check if a required join exists in the query joins."""
        try:
            # Simple string matching for join conditions
            required_join_normalized = required_join.replace(" ", "").lower()
            
            for query_join in query_joins:
                query_join_normalized = query_join.replace(" ", "").lower()
                if required_join_normalized in query_join_normalized:
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"Error checking if join exists: {e}")
            return False

    def _check_instruction_compliance(self, query: str, instruction: str) -> Dict[str, Any]:
        """Check if query complies with a business instruction."""
        try:
            instruction_lower = instruction.lower()
            query_lower = query.lower()
            
            # Check for time-based analysis instructions
            if "time" in instruction_lower or "date" in instruction_lower:
                if not any(word in query_lower for word in ["date", "time", "year", "month", "day"]):
                    return {
                        "compliant": False,
                        "issue": "Time-based analysis required but no date/time filtering found",
                        "warning": None
                    }
            
            # Check for aggregation instructions
            if "calculate" in instruction_lower or "sum" in instruction_lower:
                if not any(func in query_lower for func in ["sum(", "count(", "avg(", "max(", "min("]):
                    return {
                        "compliant": False,
                        "issue": "Aggregation required but no aggregation functions found",
                        "warning": None
                    }
            
            # Check for grouping instructions
            if "group" in instruction_lower:
                if "group by" not in query_lower:
                    return {
                        "compliant": False,
                        "issue": "Grouping required but no GROUP BY clause found",
                        "warning": None
                    }
            
            return {
                "compliant": True,
                "issue": None,
                "warning": None
            }
            
        except Exception as e:
            logger.error(f"Error checking instruction compliance: {e}")
            return {"compliant": False, "error": str(e)}

    def _check_aggregation_compliance(self, query: str, concept_instructions: str) -> bool:
        """Verify aggregations follow business concept guidelines."""
        try:
            # This is a simplified check - in production you'd have more sophisticated logic
            query_lower = query.lower()
            instructions_lower = concept_instructions.lower()
            
            # Check if aggregation is required but missing
            if any(word in instructions_lower for word in ["calculate", "sum", "total", "average"]):
                if not any(func in query_lower for func in ["sum(", "count(", "avg(", "max(", "min("]):
                    return False
            
            return True
            
        except Exception as e:
            logger.error(f"Error checking aggregation compliance: {e}")
            return False 


================================================
FILE: smol-sql-agents/src/validation/query_optimizer.py
================================================
import logging
import sqlparse
from typing import Dict, List, Any, Optional

logger = logging.getLogger(__name__)

class QueryOptimizer:
    """Provides query optimization suggestions and improvements."""
    
    def __init__(self):
        self.optimization_rules = self._load_optimization_rules()

    def analyze_performance(self, query: str, execution_plan: Dict = None) -> Dict[str, Any]:
        """Analyze query performance and suggest optimizations."""
        try:
            analysis_result = {
                "complexity_score": 0,
                "optimization_suggestions": [],
                "performance_issues": [],
                "estimated_impact": "low"
            }
            
            # Calculate complexity score
            complexity_score = self._calculate_complexity_score(query)
            analysis_result["complexity_score"] = complexity_score
            
            # Get optimization suggestions
            suggestions = self._get_optimization_suggestions(query)
            analysis_result["optimization_suggestions"] = suggestions
            
            # Identify performance issues
            performance_issues = self._identify_performance_issues(query)
            analysis_result["performance_issues"] = performance_issues
            
            # Estimate impact based on issues and suggestions
            impact_level = self._estimate_optimization_impact(suggestions, performance_issues)
            analysis_result["estimated_impact"] = impact_level
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"Error analyzing performance: {e}")
            return {"error": str(e)}

    def suggest_index_usage(self, query: str, table_stats: Dict) -> List[Dict[str, str]]:
        """Suggest optimal index usage for query."""
        try:
            suggestions = []
            query_lower = query.lower()
            
            # Extract table names from query
            tables = self._extract_table_names(query)
            
            for table in tables:
                if table in table_stats and table_stats[table].get("exists", False):
                    row_count = table_stats[table].get("row_count", 0)
                    
                    # Suggest indexes based on query patterns
                    if "where" in query_lower and row_count > 10000:
                        suggestions.append({
                            "table": table,
                            "type": "index",
                            "message": f"Consider adding indexes on WHERE clause columns for table {table} ({row_count} rows)"
                        })
                    
                    if "join" in query_lower and row_count > 5000:
                        suggestions.append({
                            "table": table,
                            "type": "index",
                            "message": f"Consider adding indexes on JOIN columns for table {table}"
                        })
                    
                    if "order by" in query_lower and row_count > 1000:
                        suggestions.append({
                            "table": table,
                            "type": "index",
                            "message": f"Consider adding indexes on ORDER BY columns for table {table}"
                        })
            
            return suggestions
            
        except Exception as e:
            logger.error(f"Error suggesting index usage: {e}")
            return [{"error": str(e)}]

    def optimize_joins(self, query: str) -> Dict[str, Any]:
        """Suggest JOIN optimization strategies."""
        try:
            optimization_result = {
                "suggestions": [],
                "join_order": [],
                "estimated_improvement": "low"
            }
            
            # Analyze JOIN structure
            join_analysis = self._analyze_join_structure(query)
            
            if join_analysis["join_count"] > 2:
                optimization_result["suggestions"].append({
                    "type": "join_order",
                    "message": "Multiple JOINs detected - consider optimizing join order for better performance"
                })
            
            if join_analysis["has_cross_join"]:
                optimization_result["suggestions"].append({
                    "type": "join_type",
                    "message": "CROSS JOIN detected - consider using INNER JOIN with proper conditions"
                })
            
            if join_analysis["has_subquery_joins"]:
                optimization_result["suggestions"].append({
                    "type": "join_style",
                    "message": "Subquery in JOIN detected - consider using EXISTS or IN for better performance"
                })
            
            # Suggest join order optimization
            if join_analysis["join_count"] > 1:
                optimization_result["join_order"] = self._suggest_join_order(query)
                optimization_result["estimated_improvement"] = "medium"
            
            return optimization_result
            
        except Exception as e:
            logger.error(f"Error optimizing joins: {e}")
            return {"error": str(e)}

    def suggest_query_rewrite(self, query: str) -> Optional[str]:
        """Suggest alternative query structure for better performance."""
        try:
            query_lower = query.lower()
            rewritten_query = query
            
            # Replace SELECT * with specific columns if possible
            if "select *" in query_lower:
                # This is a simplified example - in practice you'd need schema information
                rewritten_query = rewritten_query.replace("SELECT *", "SELECT id, name, created_date")
            
            # Optimize WHERE clauses
            if "where" in query_lower and "like '%" in query_lower:
                # Suggest using full-text search or prefix matching
                rewritten_query = rewritten_query.replace("LIKE '%", "LIKE '")
            
            # Optimize subqueries
            if "in (" in query_lower and "select" in query_lower:
                # Suggest using EXISTS instead of IN with subquery
                # This is a simplified example
                pass
            
            # Only return if there are actual changes
            if rewritten_query != query:
                return rewritten_query
            
            return None
            
        except Exception as e:
            logger.error(f"Error suggesting query rewrite: {e}")
            return None

    def _load_optimization_rules(self) -> Dict[str, Any]:
        """Load query optimization rule definitions."""
        return {
            "complexity_thresholds": {
                "low": 10,
                "medium": 25,
                "high": 50
            },
            "performance_patterns": {
                "select_star": {"impact": "medium", "suggestion": "Specify only needed columns"},
                "missing_where": {"impact": "high", "suggestion": "Add WHERE clause for large tables"},
                "inefficient_joins": {"impact": "medium", "suggestion": "Optimize JOIN order and conditions"},
                "subquery_in_select": {"impact": "medium", "suggestion": "Consider using JOINs instead"}
            }
        }

    def _calculate_complexity_score(self, query: str) -> int:
        """Calculate query complexity score."""
        try:
            score = 0
            query_lower = query.lower()
            
            # Count various complexity factors
            score += query_lower.count("join") * 5
            score += query_lower.count("select") * 2
            score += query_lower.count("where") * 3
            score += query_lower.count("group by") * 4
            score += query_lower.count("order by") * 3
            score += query_lower.count("having") * 4
            score += query_lower.count("union") * 6
            score += query_lower.count("subquery") * 5
            
            # Add complexity for nested structures
            score += (query.count('(') - query.count(')')) * 2
            
            return score
            
        except Exception as e:
            logger.error(f"Error calculating complexity score: {e}")
            return 0

    def _get_optimization_suggestions(self, query: str) -> List[Dict[str, str]]:
        """Get specific optimization suggestions for the query."""
        try:
            suggestions = []
            query_lower = query.lower()
            
            # Check for SELECT *
            if "select *" in query_lower:
                suggestions.append({
                    "type": "performance",
                    "priority": "high",
                    "message": "Replace SELECT * with specific column names",
                    "impact": "medium"
                })
            
            # Check for missing WHERE clause
            if "from" in query_lower and "where" not in query_lower:
                suggestions.append({
                    "type": "performance",
                    "priority": "medium",
                    "message": "Consider adding WHERE clause for large tables",
                    "impact": "high"
                })
            
            # Check for inefficient JOINs
            if query_lower.count("join") > 2:
                suggestions.append({
                    "type": "performance",
                    "priority": "medium",
                    "message": "Multiple JOINs detected - consider optimizing join order",
                    "impact": "medium"
                })
            
            # Check for subqueries in SELECT
            if "select" in query_lower and "(" in query_lower:
                suggestions.append({
                    "type": "performance",
                    "priority": "low",
                    "message": "Consider using JOINs instead of subqueries in SELECT",
                    "impact": "medium"
                })
            
            return suggestions
            
        except Exception as e:
            logger.error(f"Error getting optimization suggestions: {e}")
            return [{"error": str(e)}]

    def _identify_performance_issues(self, query: str) -> List[Dict[str, str]]:
        """Identify specific performance issues in the query."""
        try:
            issues = []
            query_lower = query.lower()
            
            # Check for common anti-patterns
            if "select *" in query_lower:
                issues.append({
                    "type": "anti_pattern",
                    "severity": "warning",
                    "description": "SELECT * usage may return unnecessary data"
                })
            
            if "order by" in query_lower and "limit" not in query_lower:
                issues.append({
                    "type": "performance",
                    "severity": "info",
                    "description": "ORDER BY without LIMIT may process large result sets"
                })
            
            if query_lower.count("select") > 3:
                issues.append({
                    "type": "complexity",
                    "severity": "warning",
                    "description": "Multiple SELECT statements may indicate inefficient query structure"
                })
            
            return issues
            
        except Exception as e:
            logger.error(f"Error identifying performance issues: {e}")
            return [{"error": str(e)}]

    def _estimate_optimization_impact(self, suggestions: List[Dict], issues: List[Dict]) -> str:
        """Estimate the impact of applying optimizations."""
        try:
            high_priority_count = sum(1 for s in suggestions if s.get("priority") == "high")
            high_impact_count = sum(1 for s in suggestions if s.get("impact") == "high")
            
            if high_priority_count > 0 or high_impact_count > 0:
                return "high"
            elif len(suggestions) > 2 or len(issues) > 2:
                return "medium"
            else:
                return "low"
                
        except Exception as e:
            logger.error(f"Error estimating optimization impact: {e}")
            return "low"

    def _extract_table_names(self, query: str) -> List[str]:
        """Extract table names from query."""
        try:
            tables = []
            query_lower = query.lower()
            
            # Simple extraction - in production you'd use proper SQL parsing
            words = query_lower.split()
            for i, word in enumerate(words):
                if word == "from" and i + 1 < len(words):
                    table_name = words[i + 1].strip(";,")
                    if table_name and not table_name.startswith("("):
                        tables.append(table_name)
                elif word == "join" and i + 1 < len(words):
                    table_name = words[i + 1].strip(";,")
                    if table_name and not table_name.startswith("("):
                        tables.append(table_name)
            
            return list(set(tables))  # Remove duplicates
            
        except Exception as e:
            logger.error(f"Error extracting table names: {e}")
            return []

    def _analyze_join_structure(self, query: str) -> Dict[str, Any]:
        """Analyze the JOIN structure of the query."""
        try:
            analysis = {
                "join_count": 0,
                "has_cross_join": False,
                "has_subquery_joins": False,
                "join_types": []
            }
            
            query_lower = query.lower()
            
            # Count JOINs
            analysis["join_count"] = query_lower.count("join")
            
            # Check for CROSS JOIN
            if "cross join" in query_lower:
                analysis["has_cross_join"] = True
                analysis["join_types"].append("CROSS")
            
            # Check for subquery JOINs
            if "join" in query_lower and "(" in query_lower:
                analysis["has_subquery_joins"] = True
            
            # Identify other join types
            if "inner join" in query_lower:
                analysis["join_types"].append("INNER")
            if "left join" in query_lower:
                analysis["join_types"].append("LEFT")
            if "right join" in query_lower:
                analysis["join_types"].append("RIGHT")
            
            return analysis
            
        except Exception as e:
            logger.error(f"Error analyzing join structure: {e}")
            return {"join_count": 0, "has_cross_join": False, "has_subquery_joins": False, "join_types": []}

    def _suggest_join_order(self, query: str) -> List[str]:
        """Suggest optimal join order based on table sizes and relationships."""
        try:
            # This is a simplified suggestion - in practice you'd need table statistics
            tables = self._extract_table_names(query)
            
            if len(tables) <= 2:
                return tables
            
            # Simple heuristic: put smaller tables first
            # In practice, you'd use actual table statistics
            return sorted(tables, key=lambda x: len(x))  # Sort by table name length as proxy for size
            
        except Exception as e:
            logger.error(f"Error suggesting join order: {e}")
            return []

    def _analyze_join_order(self, parsed_query) -> List[str]:
        """Analyze JOIN order for optimization opportunities."""
        try:
            issues = []
            
            # This would analyze the actual JOIN order in the parsed query
            # For now, return empty list as placeholder
            return issues
            
        except Exception as e:
            logger.error(f"Error analyzing join order: {e}")
            return []

    def _check_subquery_optimization(self, parsed_query) -> List[str]:
        """Check if subqueries can be optimized or rewritten."""
        try:
            issues = []
            
            # This would analyze subqueries in the parsed query
            # For now, return empty list as placeholder
            return issues
            
        except Exception as e:
            logger.error(f"Error checking subquery optimization: {e}")
            return [] 


================================================
FILE: smol-sql-agents/src/validation/tsql_validator.py
================================================
import logging
import sqlparse
from typing import Dict, List, Any

logger = logging.getLogger(__name__)

class TSQLValidator:
    """Validates T-SQL syntax, best practices, and performance considerations."""
    
    def __init__(self):
        self.forbidden_keywords = ['DROP', 'DELETE', 'UPDATE', 'INSERT', 'TRUNCATE', 'ALTER']
        self.performance_patterns = self._load_performance_patterns()

    def validate_syntax(self, query: str) -> Dict[str, Any]:
        """Validate T-SQL syntax and structure."""
        try:
            validation_result = {
                "valid": True,
                "errors": [],
                "warnings": []
            }
            
            # Parse the query
            try:
                parsed_statements = sqlparse.parse(query)
                if not parsed_statements:
                    validation_result["valid"] = False
                    validation_result["errors"].append("Empty query")
                    return validation_result
                
                # Check each statement
                for statement in parsed_statements:
                    statement_validation = self._validate_statement(statement)
                    if not statement_validation["valid"]:
                        validation_result["valid"] = False
                        validation_result["errors"].extend(statement_validation["errors"])
                    
                    if statement_validation["warnings"]:
                        validation_result["warnings"].extend(statement_validation["warnings"])
                
            except Exception as e:
                validation_result["valid"] = False
                validation_result["errors"].append(f"Syntax parsing error: {str(e)}")
            
            return validation_result
            
        except Exception as e:
            logger.error(f"Error validating syntax: {e}")
            return {"valid": False, "error": str(e)}

    def check_performance_patterns(self, query: str) -> List[Dict[str, str]]:
        """Check for common performance anti-patterns."""
        try:
            issues = []
            query_lower = query.lower()
            
            # Check for SELECT *
            if self._check_select_star_usage(query_lower):
                issues.append({
                    "type": "performance",
                    "severity": "warning",
                    "message": "SELECT * usage detected - consider specifying only needed columns"
                })
            
            # Check for missing WHERE clause in large table queries
            if "from" in query_lower and "where" not in query_lower:
                issues.append({
                    "type": "performance",
                    "severity": "warning",
                    "message": "No WHERE clause detected - consider adding filters for large tables"
                })
            
            # Check for potential N+1 query patterns
            if query_lower.count("select") > 3:
                issues.append({
                    "type": "performance",
                    "severity": "info",
                    "message": "Multiple SELECT statements detected - consider using JOINs or subqueries"
                })
            
            # Check for missing indexes hints
            if "join" in query_lower and "index" not in query_lower:
                issues.append({
                    "type": "performance",
                    "severity": "info",
                    "message": "JOIN detected without index hints - verify proper indexing"
                })
            
            return issues
            
        except Exception as e:
            logger.error(f"Error checking performance patterns: {e}")
            return [{"type": "error", "severity": "error", "message": str(e)}]

    def validate_security(self, query: str) -> Dict[str, Any]:
        """Check for SQL injection risks and forbidden operations."""
        try:
            security_result = {
                "valid": True,
                "risks": [],
                "forbidden_operations": []
            }
            
            query_upper = query.upper()
            
            # Check for forbidden keywords
            for keyword in self.forbidden_keywords:
                if keyword in query_upper:
                    security_result["valid"] = False
                    security_result["forbidden_operations"].append(keyword)
            
            # Check for potential SQL injection patterns
            injection_patterns = [
                "exec(", "execute(", "sp_", "xp_", "openrowset", "opendatasource"
            ]
            
            for pattern in injection_patterns:
                if pattern in query_upper:
                    security_result["risks"].append(f"Potential SQL injection risk: {pattern}")
            
            # Check for dynamic SQL patterns
            if "exec" in query_upper or "execute" in query_upper:
                security_result["risks"].append("Dynamic SQL execution detected")
            
            # Check for system table access
            system_tables = ["sys.", "information_schema.", "master.", "tempdb."]
            for table in system_tables:
                if table in query_upper:
                    security_result["risks"].append(f"System table access: {table}")
            
            return security_result
            
        except Exception as e:
            logger.error(f"Error validating security: {e}")
            return {"valid": False, "error": str(e)}

    def suggest_improvements(self, query: str) -> List[Dict[str, str]]:
        """Suggest query improvements for readability and performance."""
        try:
            suggestions = []
            query_lower = query.lower()
            
            # Check for proper aliasing
            if "from" in query_lower and "as" not in query_lower:
                suggestions.append({
                    "type": "readability",
                    "message": "Consider using table aliases for better readability"
                })
            
            # Check for proper column aliasing
            if "select" in query_lower and "as" not in query_lower:
                suggestions.append({
                    "type": "readability",
                    "message": "Consider aliasing calculated columns for clarity"
                })
            
            # Check for proper indentation
            if query.count('\n') < 3:
                suggestions.append({
                    "type": "readability",
                    "message": "Consider proper query formatting and indentation"
                })
            
            # Check for ORDER BY without LIMIT
            if "order by" in query_lower and "limit" not in query_lower:
                suggestions.append({
                    "type": "performance",
                    "message": "Consider adding LIMIT clause when using ORDER BY"
                })
            
            # Check for proper JOIN syntax
            if "join" in query_lower and "on" not in query_lower:
                suggestions.append({
                    "type": "syntax",
                    "message": "JOIN detected without ON clause - verify join conditions"
                })
            
            return suggestions
            
        except Exception as e:
            logger.error(f"Error suggesting improvements: {e}")
            return [{"type": "error", "message": str(e)}]

    def _load_performance_patterns(self) -> Dict[str, str]:
        """Load performance anti-pattern definitions."""
        return {
            "select_star": "SELECT * usage without specific columns",
            "missing_where": "No WHERE clause on large table queries",
            "multiple_selects": "Multiple SELECT statements instead of JOINs",
            "missing_indexes": "JOINs without proper index considerations",
            "no_limit": "ORDER BY without LIMIT clause"
        }

    def _check_select_star_usage(self, query_lower: str) -> bool:
        """Check for inefficient SELECT * usage."""
        try:
            # Look for SELECT * pattern
            if "select *" in query_lower:
                # Check if it's in a subquery or main query
                select_parts = query_lower.split("select")
                for part in select_parts[1:]:  # Skip first empty part
                    if part.strip().startswith("*"):
                        return True
            
            return False
            
        except Exception as e:
            logger.error(f"Error checking SELECT * usage: {e}")
            return False

    def _analyze_where_clause(self, parsed_query) -> List[str]:
        """Analyze WHERE clause for optimization opportunities."""
        try:
            issues = []
            
            def analyze_where_tokens(tokens):
                for token in tokens:
                    if token.ttype is sqlparse.tokens.Keyword and token.value.upper() == 'WHERE':
                        # Analyze the WHERE clause
                        where_conditions = []
                        for sibling in token.parent.tokens:
                            if sibling.ttype is sqlparse.tokens.Whitespace:
                                continue
                            if sibling.ttype is sqlparse.tokens.Keyword and sibling.value.upper() == 'WHERE':
                                continue
                            where_conditions.append(str(sibling))
                        
                        where_text = ' '.join(where_conditions).lower()
                        
                        # Check for common issues
                        if "like '%" in where_text:
                            issues.append("LIKE with leading wildcard may not use indexes efficiently")
                        
                        if "or" in where_text and "and" in where_text:
                            issues.append("Complex OR/AND conditions may benefit from query restructuring")
                        
                        if "is null" in where_text:
                            issues.append("NULL checks may benefit from proper indexing")
                    
                    # Recursively check child tokens
                    for child in token.tokens:
                        analyze_where_tokens(child)
            
            analyze_where_tokens(parsed_query)
            return issues
            
        except Exception as e:
            logger.error(f"Error analyzing WHERE clause: {e}")
            return []

    def _validate_statement(self, statement) -> Dict[str, Any]:
        """Validate a single SQL statement."""
        try:
            validation_result = {
                "valid": True,
                "errors": [],
                "warnings": []
            }
            
            # Check for basic structure
            statement_text = str(statement).strip()
            if not statement_text:
                validation_result["valid"] = False
                validation_result["errors"].append("Empty statement")
                return validation_result
            
            # Check for required keywords
            if not any(keyword in statement_text.upper() for keyword in ["SELECT", "FROM"]):
                validation_result["valid"] = False
                validation_result["errors"].append("Missing required SELECT and FROM clauses")
            
            # Check for balanced parentheses
            if statement_text.count('(') != statement_text.count(')'):
                validation_result["valid"] = False
                validation_result["errors"].append("Unbalanced parentheses")
            
            # Check for proper semicolon usage
            if statement_text.endswith(';'):
                validation_result["warnings"].append("Semicolon at end of statement")
            
            return validation_result
            
        except Exception as e:
            logger.error(f"Error validating statement: {e}")
            return {"valid": False, "error": str(e)} 


================================================
FILE: smol-sql-agents/src/vector/__init__.py
================================================
# src/vector/__init__.py
# Vector storage and search functionality 


================================================
FILE: smol-sql-agents/src/vector/embeddings.py
================================================
"""OpenAI embeddings wrapper for SQL documentation."""

from typing import List, Callable, Any
import os
import re
import openai
import tiktoken
from tenacity import retry, stop_after_attempt, wait_exponential

class OpenAIEmbeddingsClient:
    """Handles OpenAI embeddings generation with error handling and batching."""
    
    def __init__(self):
        self.client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.model = os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-3-small")
        self.batch_size = int(os.getenv("EMBEDDING_BATCH_SIZE", "100"))
        self.max_retries = int(os.getenv("EMBEDDING_MAX_RETRIES", "3"))
        self._encoder = tiktoken.encoding_for_model(self.model)

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    def generate_embedding(self, text: str) -> List[float]:
        """Generate single embedding for text.
        
        Args:
            text: The text to generate embedding for
            
        Returns:
            List[float]: The embedding vector
            
        Raises:
            Exception: If the API call fails after retries
        """
        prepared_text = self._prepare_text_for_embedding(text)
        response = self.client.embeddings.create(
            input=prepared_text,
            model=self.model
        )
        return response.data[0].embedding

    def generate_embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for multiple texts efficiently.
        
        Args:
            texts: List of texts to generate embeddings for
            
        Returns:
            List[List[float]]: List of embedding vectors
            
        Raises:
            Exception: If the API call fails after retries
        """
        prepared_texts = [self._prepare_text_for_embedding(text) for text in texts]
        embeddings = []
        
        # Process in batches
        for i in range(0, len(prepared_texts), self.batch_size):
            batch = prepared_texts[i:i + self.batch_size]
            response = self._retry_with_backoff(
                self.client.embeddings.create,
                input=batch,
                model=self.model
            )
            batch_embeddings = [data.embedding for data in response.data]
            embeddings.extend(batch_embeddings)
        
        return embeddings

    def _prepare_text_for_embedding(self, text: str) -> str:
        """Clean and prepare text for embedding generation.
        
        Args:
            text: Raw text to prepare
            
        Returns:
            str: Cleaned and prepared text
        """
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text.strip())
        
        # Truncate if needed
        if self._count_tokens(text) > 8000:
            text = self._truncate_text(text)
            
        return text

    def _count_tokens(self, text: str) -> int:
        """Count tokens to ensure we don't exceed OpenAI limits.
        
        Args:
            text: Text to count tokens for
            
        Returns:
            int: Number of tokens
        """
        return len(self._encoder.encode(text))

    def _truncate_text(self, text: str, max_tokens: int = 8000) -> str:
        """Truncate text to fit within token limits.
        
        Args:
            text: Text to truncate
            max_tokens: Maximum number of tokens allowed
            
        Returns:
            str: Truncated text
        """
        tokens = self._encoder.encode(text)
        if len(tokens) <= max_tokens:
            return text
            
        truncated_tokens = tokens[:max_tokens]
        return self._encoder.decode(truncated_tokens)

    def _retry_with_backoff(self, func: Callable, *args: Any, **kwargs: Any) -> Any:
        """Retry failed requests with exponential backoff.
        
        Args:
            func: Function to retry
            *args: Positional arguments for the function
            **kwargs: Keyword arguments for the function
            
        Returns:
            Any: Result from the function
            
        Raises:
            Exception: If all retries fail
        """
        @retry(
            stop=stop_after_attempt(self.max_retries),
            wait=wait_exponential(multiplier=1, min=4, max=10)
        )
        def _wrapped_func():
            return func(*args, **kwargs)
            
        return _wrapped_func()



================================================
FILE: smol-sql-agents/src/vector/search.py
================================================
"""Enhanced search tools using OpenAI embeddings for semantic similarity."""

from typing import List, Dict
from ..agents.core import PersistentDocumentationAgent
from ..agents.indexer import SQLIndexerAgent
from .store import SQLVectorStore

def search_table_documentation(query: str, limit: int = 5) -> List[Dict]:
    """Search table documentation using OpenAI embeddings similarity.
    
    Args:
        query: Natural language search query
        limit: Maximum number of results to return
        
    Returns:
        list[dict]: Relevant table documentation matches with similarity scores
    """
    agent = PersistentDocumentationAgent()
    results = agent.indexer_agent.search_documentation(query, doc_type="tables", limit=limit)
    
    # Format results for consistent output
    formatted_results = []
    for result in results:
        formatted_results.append({
            "table_name": result["metadata"]["table_name"],
            "business_purpose": result["metadata"]["business_purpose"],
            "similarity_score": result["similarity"],
            "schema": result["metadata"].get("schema", {}),
        })
    
    return formatted_results

def search_relationship_documentation(query: str, limit: int = 5) -> List[Dict]:
    """Search relationship documentation using OpenAI embeddings similarity.
    
    Args:
        query: Natural language search query
        limit: Maximum number of results to return
        
    Returns:
        list[dict]: Relevant relationship documentation matches with similarity scores
    """
    agent = PersistentDocumentationAgent()
    results = agent.indexer_agent.search_documentation(query, doc_type="relationships", limit=limit)
    
    # Format results for consistent output
    formatted_results = []
    for result in results:
        formatted_results.append({
            "relationship_id": result["metadata"]["relationship_id"],
            "relationship_type": result["metadata"]["relationship_type"],
            "documentation": result["metadata"]["documentation"],
            "tables_involved": result["metadata"]["tables"],
            "similarity_score": result["similarity"]
        })
    
    return formatted_results

def semantic_search_all_documentation(query: str, limit: int = 10) -> Dict:
    """Search across all documentation using OpenAI embeddings.
    
    Args:
        query: Natural language search query
        limit: Maximum total results to return
        
    Returns:
        dict: Combined results from tables and relationships with similarity scores
    """
    # Handle invalid limits
    if limit <= 0:
        return {
            "tables": [],
            "relationships": [],
            "total_results": 0
        }
    
    # Calculate per-category limits ensuring total doesn't exceed overall limit
    per_category_limit = limit // 2
    remaining_limit = limit % 2
    
    # Search both categories
    table_results = search_table_documentation(query, per_category_limit + remaining_limit)
    relationship_results = search_relationship_documentation(query, per_category_limit)
    
    # Combine and sort all results by similarity score
    all_results = []
    for result in table_results:
        all_results.append({
            "type": "table",
            "name": result["table_name"],
            "similarity": result["similarity_score"],
            "content": result
        })
    
    for result in relationship_results:
        all_results.append({
            "type": "relationship",
            "name": result["relationship_id"],
            "similarity": result["similarity_score"],
            "content": result
        })
    
    # Sort combined results by similarity score
    all_results.sort(key=lambda x: x["similarity"], reverse=True)
    
    # Take top results up to limit
    all_results = all_results[:limit]
    
    # Format final response
    return {
        "tables": [r["content"] for r in all_results if r["type"] == "table"],
        "relationships": [r["content"] for r in all_results if r["type"] == "relationship"],
        "total_results": len(all_results)
    }



================================================
FILE: smol-sql-agents/src/vector/store.py
================================================
"""Vector database wrapper for SQL documentation using ChromaDB."""

from typing import Dict, List, Optional, Protocol, Any
import os
import json
import logging
import chromadb
from pathlib import Path
from .embeddings import OpenAIEmbeddingsClient

logger = logging.getLogger(__name__)

class VectorIndex(Protocol):
    """Protocol for vector index implementations."""
    
    def add(self, id: str, vector: List[float], metadata: Optional[Dict] = None) -> None:
        """Add vector to index."""
        ...
        
    def search(self, vector: List[float], k: int = 5) -> List[Dict]:
        """Search for similar vectors."""
        ...
        
    def save(self) -> None:
        """Save the index."""
        ...

class ChromaDBIndex:
    """ChromaDB-based vector index implementation."""
    
    def __init__(self, collection_name: str, persist_directory: str = None):
        """Initialize ChromaDB index.
        
        Args:
            collection_name: Name of the collection
            persist_directory: Directory to persist the database
        """
        self.collection_name = collection_name
        self.persist_directory = persist_directory or "__bin__/data/vector_indexes"
        
        # Ensure directory exists
        Path(self.persist_directory).mkdir(parents=True, exist_ok=True)
        
        # Initialize ChromaDB client
        self.client = chromadb.PersistentClient(path=self.persist_directory)
        
        # Get or create collection
        try:
            self.collection = self.client.get_collection(name=collection_name)
            logger.info(f"Loaded existing ChromaDB collection: {collection_name}")
        except Exception:
            self.collection = self.client.create_collection(
                name=collection_name,
                metadata={"description": f"Vector index for {collection_name}"}
            )
            logger.info(f"Created new ChromaDB collection: {collection_name}")
        
    def add(self, id: str, vector: List[float], metadata: Optional[Dict] = None) -> None:
        """Add vector to ChromaDB collection."""
        try:
            # Prepare metadata for ChromaDB (convert lists to strings)
            chroma_metadata = self._prepare_metadata_for_chroma(metadata or {})
            
            # Add the document ID to metadata for retrieval
            chroma_metadata["id"] = id
            
            # Add to collection
            self.collection.add(
                embeddings=[vector],
                documents=[self._create_document_text(metadata)],
                metadatas=[chroma_metadata],
                ids=[id]
            )
            
            logger.debug(f"Added vector to ChromaDB collection: {id}")
            
        except Exception as e:
            logger.error(f"Failed to add vector to ChromaDB: {e}")
            raise
        
    def search(self, vector: List[float], k: int = 5) -> List[Dict]:
        """Search for similar vectors using ChromaDB."""
        try:
            # Search in ChromaDB
            results = self.collection.query(
                query_embeddings=[vector],
                n_results=k,
                include=["metadatas", "distances"]
            )
            
            # Process results
            search_results = []
            
            if results["metadatas"] and results["metadatas"][0]:
                for i, metadata in enumerate(results["metadatas"][0]):
                    # Get distance from ChromaDB
                    distance = results["distances"][0][i] if results["distances"] and results["distances"][0] else 0.0
                    
                    # ChromaDB uses cosine distance (0 = identical, 2 = completely opposite)
                    # Convert to similarity score (1 = identical, 0 = completely opposite)
                    similarity = 1.0 - (distance / 2.0)
                    
                    # Ensure similarity is between 0 and 1
                    similarity = max(0.0, min(1.0, similarity))
                    
                    logger.debug(f"ChromaDB distance: {distance}, calculated similarity: {similarity}")
                    
                    search_results.append({
                        "id": metadata.get("id", f"result_{i}"),
                        "metadata": metadata,
                        "score": similarity
                    })
            
            return search_results
            
        except Exception as e:
            logger.error(f"Failed to search ChromaDB collection: {e}")
            return []
        
    def save(self) -> None:
        """Save index - ChromaDB handles persistence automatically."""
        # ChromaDB automatically persists data
        pass
    
    def _create_document_text(self, metadata: Optional[Dict]) -> str:
        """Create document text for ChromaDB storage."""
        if not metadata:
            return ""
        
        # Create a text representation of the metadata
        parts = []
        
        if metadata.get("type") == "table":
            parts.extend([
                f"Table: {metadata.get('name', '')}",
                f"Description: {metadata.get('description', '')}",
                f"Columns: {', '.join(metadata.get('columns', []))}",
                f"Business Purpose: {metadata.get('business_purpose', '')}"
            ])
        else:
            parts.extend([
                f"Relationship: {metadata.get('name', '')}",
                f"Type: {metadata.get('relationship_type', '')}",
                f"Description: {metadata.get('description', '')}",
                f"Tables: {', '.join(metadata.get('tables', []))}"
            ])
        
        return " | ".join(filter(None, parts))

    def _prepare_metadata_for_chroma(self, metadata: Dict) -> Dict:
        """Prepare metadata for ChromaDB by converting lists to strings."""
        chroma_metadata = {}
        
        for key, value in metadata.items():
            if isinstance(value, list):
                # Convert lists to comma-separated strings
                chroma_metadata[key] = ", ".join(str(item) for item in value)
            elif isinstance(value, dict):
                # Convert dictionaries to JSON strings
                chroma_metadata[key] = json.dumps(value)
            else:
                # Keep primitive types as-is
                chroma_metadata[key] = value
                
        return chroma_metadata

class SQLVectorStore:
    """Manages vector indexes using OpenAI embeddings with ChromaDB persistence."""
    
    def __init__(self, base_path: str = "__bin__/data/vector_indexes", vector_index_factory: Any = None):
        """Initialize vector store with base path for indexes.
        
        Args:
            base_path (str): Base directory for storing vector indexes
            vector_index_factory: Factory function to create vector indexes
        """
        self.base_path = base_path
        self.embeddings_client = OpenAIEmbeddingsClient()
        self.vector_index_factory = vector_index_factory or self._default_index_factory
        self.table_index = None     # vector index instance for tables
        self.relationship_index = None  # vector index instance for relationships
        self._ensure_directories()
        
    def _default_index_factory(self, path: str) -> VectorIndex:
        """Create default vector index implementation using ChromaDB."""
        # Extract collection name from path
        collection_name = os.path.basename(path).replace('.db', '')
        return ChromaDBIndex(collection_name=collection_name, persist_directory=self.base_path)
        
    def create_table_index(self, index_name: str = "tables"):
        """Create vector index for table documentation.
        
        Args:
            index_name: Name of the index
            
        Returns:
            None
        """
        index_path = os.path.join(self.base_path, "tables", f"{index_name}.db")
        self.table_index = self.vector_index_factory(index_path)
        
    def create_relationship_index(self, index_name: str = "relationships"):
        """Create vector index for relationship documentation.
        
        Args:
            index_name: Name of the index
            
        Returns:
            None
        """
        index_path = os.path.join(self.base_path, "relationships", f"{index_name}.db")
        self.relationship_index = self.vector_index_factory(index_path)
        
    def add_table_document(self, table_name: str, content: Dict):
        """Add table documentation with OpenAI-generated embedding.
        
        Args:
            table_name: Name of the table
            content: Dictionary containing table documentation
            
        Returns:
            None
            
        Raises:
            ValueError: If table index hasn't been created
        """
        if not self.table_index:
            raise ValueError("Table index not initialized. Call create_table_index first.")
            
        doc_text = self._prepare_document_text(content, "table")
        embedding = self.embeddings_client.generate_embedding(doc_text)
        metadata = self._create_document_metadata(content, "table")
        metadata["id"] = table_name  # Add ID for search results
        
        self.table_index.add(
            id=table_name,
            vector=embedding,
            metadata=metadata
        )
        self.table_index.save()
        
    def add_relationship_document(self, relationship_id: str, content: Dict):
        """Add relationship documentation with OpenAI-generated embedding.
        
        Args:
            relationship_id: Unique identifier for the relationship
            content: Dictionary containing relationship documentation
            
        Returns:
            None
            
        Raises:
            ValueError: If relationship index hasn't been created
        """
        if not self.relationship_index:
            raise ValueError("Relationship index not initialized. Call create_relationship_index first.")
            
        doc_text = self._prepare_document_text(content, "relationship")
        embedding = self.embeddings_client.generate_embedding(doc_text)
        metadata = self._create_document_metadata(content, "relationship")
        metadata["id"] = relationship_id  # Add ID for search results
        
        self.relationship_index.add(
            id=relationship_id,
            vector=embedding,
            metadata=metadata
        )
        self.relationship_index.save()
        
    def search_tables(self, query: str, limit: int = 5) -> List[Dict]:
        """Search table documentation using OpenAI query embedding.
        
        Args:
            query: Search query
            limit: Maximum number of results to return
            
        Returns:
            List[Dict]: List of matching table documents with scores
        """
        if not self.table_index:
            raise ValueError("Table index not initialized. Call create_table_index first.")
            
        query_embedding = self.embeddings_client.generate_embedding(query)
        results = self.table_index.search(
            vector=query_embedding,
            k=limit
        )
        
        # Process results based on the VectorIndex protocol
        return [
            {
                "id": result["id"],
                "content": result["metadata"],  # metadata is already the dictionary
                "score": result.get("score", 1.0)  # default score if not provided
            }
            for result in results
        ]
        
    def search_relationships(self, query: str, limit: int = 5) -> List[Dict]:
        """Search relationship documentation using OpenAI query embedding.
        
        Args:
            query: Search query
            limit: Maximum number of results to return
            
        Returns:
            List[Dict]: List of matching relationship documents with scores
        """
        if not self.relationship_index:
            raise ValueError("Relationship index not initialized. Call create_relationship_index first.")
            
        query_embedding = self.embeddings_client.generate_embedding(query)
        results = self.relationship_index.search(
            vector=query_embedding,
            k=limit
        )
        
        # Process results based on the VectorIndex protocol
        return [
            {
                "id": result["id"],
                "content": result["metadata"],  # metadata is already the dictionary
                "score": result.get("score", 1.0)  # default score if not provided
            }
            for result in results
        ]
        
    def _prepare_document_text(self, content: Dict, doc_type: str) -> str:
        """Prepare document content for embedding generation.
        
        Args:
            content: Document content dictionary
            doc_type: Type of document ('table' or 'relationship')
            
        Returns:
            str: Prepared text for embedding
        """
        if doc_type == "table":
            # Combine relevant table information
            parts = [
                f"Table: {content.get('name', '')}",
                f"Description: {content.get('description', '')}",
                f"Columns: {', '.join(content.get('columns', []))}"
            ]
        else:
            # Combine relevant relationship information
            parts = [
                f"Relationship: {content.get('name', '')}",
                f"Type: {content.get('type', '')}",
                f"Description: {content.get('description', '')}",
                f"Related Tables: {', '.join(content.get('tables', []))}"
            ]
            
        return " | ".join(filter(None, parts))
        
    def _create_document_metadata(self, content: Dict, doc_type: str) -> Dict:
        """Create metadata to store alongside vectors.
        
        Args:
            content: Document content dictionary
            doc_type: Type of document ('table' or 'relationship')
            
        Returns:
            Dict: Metadata dictionary
        """
        metadata = {
            "type": doc_type,
            "name": content.get("name", ""),
            "description": content.get("description", "")
        }
        
        if doc_type == "table":
            columns = content.get("columns", [])
            metadata.update({
                "columns": columns,
                "column_count": len(columns),
                "business_purpose": content.get("business_purpose", ""),
                "schema_data": content.get("schema_data", {})
            })
        else:
            tables = content.get("tables", [])
            metadata.update({
                "relationship_type": content.get("type", ""),
                "tables": tables,
                "table_count": len(tables),
                "constrained_table": content.get("constrained_table", ""),
                "referred_table": content.get("referred_table", ""),
                "documentation": content.get("documentation", "")
            })
            
        return metadata
        
    def _ensure_directories(self):
        """Create necessary directories if they don't exist."""
        Path(os.path.join(self.base_path, "tables")).mkdir(parents=True, exist_ok=True)
        Path(os.path.join(self.base_path, "relationships")).mkdir(parents=True, exist_ok=True)



================================================
FILE: smol-sql-agents/tests/__init__.py
================================================
"""Test package initialization."""



================================================
FILE: smol-sql-agents/tests/test_agent_core.py
================================================
"""Tests for the PersistentDocumentationAgent with vector indexing."""

import json
import pytest
from unittest.mock import Mock, patch, ANY

from src.agents.core import PersistentDocumentationAgent
from src.database.inspector import DatabaseInspector
from src.database.persistence import DocumentationStore
from src.agents.indexer import SQLIndexerAgent
from src.vector.store import SQLVectorStore

@pytest.fixture
def mock_llm_model():
    """Create a mock LLM model."""
    model = Mock()
    return model

@pytest.fixture
def mock_db_inspector():
    """Create a mock database inspector."""
    inspector = Mock(spec=DatabaseInspector)
    inspector.get_all_table_names.return_value = ["users", "orders"]
    inspector.get_table_schema.return_value = {
        "name": "users",
        "columns": [
            {"name": "id", "type": "integer", "primary_key": True},
            {"name": "username", "type": "varchar"},
            {"name": "email", "type": "varchar"}
        ]
    }
    inspector.get_all_foreign_key_relationships.return_value = [
        {
            "id": "users_orders_fk",
            "constrained_table": "orders",
            "constrained_columns": ["user_id"],
            "referred_table": "users",
            "referred_columns": ["id"]
        }
    ]
    return inspector

@pytest.fixture
def mock_doc_store():
    """Create a mock documentation store."""
    store = Mock(spec=DocumentationStore)
    return store

@pytest.fixture
def mock_indexer_agent():
    """Create a mock indexer agent."""
    agent = Mock(spec=SQLIndexerAgent)
    agent.index_table_documentation.return_value = True
    agent.index_relationship_documentation.return_value = True
    return agent

@pytest.fixture
def mock_code_agent():
    """Create a mock code agent."""
    agent = Mock()
    return agent

@pytest.fixture
def doc_agent(mock_llm_model, mock_db_inspector, mock_doc_store, mock_code_agent):
    """Create a documentation agent with mocked dependencies."""
    def mock_getenv(key, default=None):
        if key == "OPENAI_API_KEY":
            return "dummy-api-key"
        elif key == "OPENAI_EMBEDDING_MODEL":
            return "text-embedding-3-small"
        elif key == "EMBEDDING_BATCH_SIZE":
            return "100"
        else:
            return default
    
    # Mock the vector store and indexer agent
    mock_vector_store = Mock(spec=SQLVectorStore)
    mock_indexer_agent = Mock(spec=SQLIndexerAgent)
    
    with patch('src.agents.core.OpenAIModel', return_value=mock_llm_model), \
         patch('src.agents.core.DatabaseInspector', return_value=mock_db_inspector), \
         patch('src.agents.core.DocumentationStore', return_value=mock_doc_store), \
         patch('src.agents.core.CodeAgent', return_value=mock_code_agent), \
         patch('src.agents.core.SQLVectorStore', return_value=mock_vector_store), \
         patch('src.agents.core.SQLIndexerAgent', return_value=mock_indexer_agent), \
         patch('src.agents.core.os.getenv', side_effect=mock_getenv), \
         patch('src.vector.embeddings.os.getenv', side_effect=mock_getenv), \
         patch('src.vector.store.os.getenv', side_effect=mock_getenv):
        agent = PersistentDocumentationAgent()
        return agent

@pytest.fixture
def doc_agent_with_indexing(mock_llm_model, mock_db_inspector, mock_doc_store, mock_code_agent, mock_indexer_agent):
    """Create a documentation agent with vector indexing capabilities."""
    def mock_getenv(key, default=None):
        if key == "OPENAI_API_KEY":
            return "dummy-api-key"
        elif key == "OPENAI_EMBEDDING_MODEL":
            return "text-embedding-3-small"
        elif key == "EMBEDDING_BATCH_SIZE":
            return "100"
        else:
            return default
    
    with patch('agent.agent_core.OpenAIModel', return_value=mock_llm_model), \
         patch('agent.agent_core.DatabaseInspector', return_value=mock_db_inspector), \
         patch('agent.agent_core.DocumentationStore', return_value=mock_doc_store), \
         patch('agent.agent_core.CodeAgent', return_value=mock_code_agent), \
         patch('agent.agent_core.SQLIndexerAgent', return_value=mock_indexer_agent), \
         patch('agent.agent_core.SQLVectorStore'), \
         patch('agent.agent_core.os.getenv', side_effect=mock_getenv), \
         patch('agent.embeddings_client.os.getenv', side_effect=mock_getenv), \
         patch('agent.vector_store.os.getenv', side_effect=mock_getenv):
        agent = PersistentDocumentationAgent()
        return agent

@pytest.fixture
def mock_vector_store():
    """Create a mock vector store."""
    store = Mock(spec=SQLVectorStore)
    return store

@pytest.fixture
def doc_agent_with_vector(doc_agent, mock_indexer_agent, mock_vector_store):
    """Create a documentation agent with vector indexing capabilities."""
    doc_agent.indexing_agent = mock_indexer_agent
    doc_agent.vector_store = mock_vector_store
    return doc_agent

# ============================================================================
# Basic Agent Functionality Tests
# ============================================================================

def test_agent_initialization_success(doc_agent, mock_db_inspector, mock_doc_store):
    """Test successful agent initialization."""
    assert doc_agent.db_inspector == mock_db_inspector
    assert doc_agent.store == mock_doc_store

def test_agent_initialization_missing_api_key():
    """Test agent initialization with missing API key."""
    with patch('agent.agent_core.os.getenv', return_value=None), \
         pytest.raises(ValueError) as exc_info:
        PersistentDocumentationAgent()
    assert "OPENAI_API_KEY environment variable is not set" in str(exc_info.value)

# ============================================================================
# Table Documentation Tests
# ============================================================================

def test_process_table_documentation_success(doc_agent, mock_code_agent):
    """Test successful processing of table documentation."""
    # Mock LLM response
    mock_response = {
        "business_purpose": "Stores user account information",
        "schema_data": {
            "table_name": "users",
            "columns": [
                {"name": "id", "type": "integer"},
                {"name": "username", "type": "varchar"}
            ]
        }
    }
    mock_code_agent.run.return_value = json.dumps(mock_response)
    
    # Process table
    doc_agent.process_table_documentation("users")
    
    # Verify interactions
    mock_code_agent.run.assert_called_once()
    doc_agent.store.save_table_documentation.assert_called_once_with(
        "users",
        mock_response["schema_data"],
        mock_response["business_purpose"],
        ANY  # We don't need to verify the exact markdown format
    )

def test_process_table_documentation_invalid_json(doc_agent, mock_code_agent):
    """Test handling of invalid JSON response for table documentation."""
    mock_code_agent.run.return_value = "Invalid JSON"
    
    with pytest.raises(json.JSONDecodeError):
        doc_agent.process_table_documentation("users")

def test_process_table_documentation_missing_fields(doc_agent, mock_code_agent):
    """Test handling of missing fields in table documentation response."""
    mock_response = {
        "business_purpose": "Stores user account information"
        # Missing schema_data
    }
    mock_code_agent.run.return_value = json.dumps(mock_response)
    
    with pytest.raises(ValueError) as exc_info:
        doc_agent.process_table_documentation("users")
    assert "Missing required fields" in str(exc_info.value)

# ============================================================================
# Relationship Documentation Tests
# ============================================================================

def test_process_relationship_documentation_success(doc_agent, mock_code_agent):
    """Test successful processing of relationship documentation."""
    relationship = {
        "id": "users_orders_fk",
        "constrained_table": "orders",
        "constrained_columns": ["user_id"],
        "referred_table": "users",
        "referred_columns": ["id"]
    }
    
    mock_response = {
        "relationship_type": "one-to-many",
        "documentation": "Each user can have multiple orders"
    }
    mock_code_agent.run.return_value = json.dumps(mock_response)
    
    doc_agent.process_relationship_documentation(relationship)
    
    mock_code_agent.run.assert_called_once()
    doc_agent.store.save_relationship_documentation.assert_called_once_with(
        "users_orders_fk",
        mock_response["relationship_type"],
        mock_response["documentation"]
    )

def test_process_relationship_documentation_invalid_json(doc_agent, mock_code_agent):
    """Test handling of invalid JSON response for relationship documentation."""
    relationship = {
        "id": "users_orders_fk",
        "constrained_table": "orders",
        "constrained_columns": ["user_id"],
        "referred_table": "users",
        "referred_columns": ["id"]
    }
    mock_code_agent.run.return_value = "Invalid JSON"
    
    with pytest.raises(json.JSONDecodeError):
        doc_agent.process_relationship_documentation(relationship)

def test_process_relationship_documentation_missing_fields(doc_agent, mock_code_agent):
    """Test handling of missing fields in relationship documentation response."""
    relationship = {
        "id": "users_orders_fk",
        "constrained_table": "orders",
        "constrained_columns": ["user_id"],
        "referred_table": "users",
        "referred_columns": ["id"]
    }
    mock_response = {
        "relationship_type": "one-to-many"
        # Missing documentation
    }
    mock_code_agent.run.return_value = json.dumps(mock_response)
    
    with pytest.raises(ValueError) as exc_info:
        doc_agent.process_relationship_documentation(relationship)
    assert "Missing required fields" in str(exc_info.value)

def test_process_relationship_documentation_agent_error(doc_agent, mock_code_agent):
    """Test handling of agent errors during relationship documentation."""
    relationship = {
        "id": "users_orders_fk",
        "constrained_table": "orders",
        "constrained_columns": ["user_id"],
        "referred_table": "users",
        "referred_columns": ["id"]
    }
    mock_code_agent.run.side_effect = Exception("Agent error")
    
    with pytest.raises(Exception) as exc_info:
        doc_agent.process_relationship_documentation(relationship)
    assert "Agent error" in str(exc_info.value)

# ============================================================================
# Vector Indexing Tests
# ============================================================================

def test_process_table_documentation_with_indexing(doc_agent_with_indexing, mock_code_agent, mock_indexer_agent):
    """Test table documentation processing with vector indexing."""
    # Mock LLM response
    table_response = {
        "business_purpose": "Stores user account information",
        "schema_data": {
            "table_name": "users",
            "columns": [
                {"name": "id", "type": "integer"},
                {"name": "username", "type": "varchar"}
            ]
        }
    }
    mock_code_agent.run.return_value = json.dumps(table_response)
    
    # Process table
    doc_agent_with_indexing.process_table_documentation("users")
    
    # Verify regular documentation was saved
    doc_agent_with_indexing.store.save_table_documentation.assert_called_once_with(
        "users",
        table_response["schema_data"],
        table_response["business_purpose"],
        ANY
    )
    
    # Verify vector indexing was performed
    mock_indexer_agent.index_table_documentation.assert_called_once()
    table_data = mock_indexer_agent.index_table_documentation.call_args[0][0]
    assert table_data["name"] == "users"
    assert table_data["business_purpose"] == table_response["business_purpose"]
    assert table_data["schema"] == table_response["schema_data"]

def test_process_relationship_documentation_with_indexing(doc_agent_with_indexing, mock_code_agent, mock_indexer_agent):
    """Test relationship documentation processing with vector indexing."""
    relationship = {
        "id": "users_orders_fk",
        "constrained_table": "orders",
        "constrained_columns": ["user_id"],
        "referred_table": "users",
        "referred_columns": ["id"]
    }
    
    # Mock LLM response
    rel_response = {
        "relationship_type": "one-to-many",
        "documentation": "Each user can have multiple orders"
    }
    mock_code_agent.run.return_value = json.dumps(rel_response)
    
    # Process relationship
    doc_agent_with_indexing.process_relationship_documentation(relationship)
    
    # Verify regular documentation was saved
    doc_agent_with_indexing.store.save_relationship_documentation.assert_called_once_with(
        "users_orders_fk",
        rel_response["relationship_type"],
        rel_response["documentation"]
    )
    
    # Verify vector indexing was performed
    mock_indexer_agent.index_relationship_documentation.assert_called_once()
    rel_data = mock_indexer_agent.index_relationship_documentation.call_args[0][0]
    assert rel_data["name"] == "users_orders_fk"
    assert rel_data["type"] == rel_response["relationship_type"]
    assert rel_data["documentation"] == rel_response["documentation"]
    assert rel_data["tables"] == ["orders", "users"]

def test_indexing_error_handling(doc_agent_with_indexing, mock_code_agent, mock_indexer_agent):
    """Test handling of indexing errors."""
    # Mock LLM response
    table_response = {
        "business_purpose": "Stores user account information",
        "schema_data": {
            "table_name": "users",
            "columns": [{"name": "id", "type": "integer"}]
        }
    }
    mock_code_agent.run.return_value = json.dumps(table_response)
    
    # Simulate indexing failure
    mock_indexer_agent.index_table_documentation.return_value = False
    
    # Process should complete but log the error
    with pytest.raises(ValueError) as exc_info:
        doc_agent_with_indexing.process_table_documentation("users")
    assert "Failed to index table documentation" in str(exc_info.value)
    
    # Regular documentation should still be saved
    doc_agent_with_indexing.store.save_table_documentation.assert_called_once()

# ============================================================================
# Vector Store Tests
# ============================================================================

# Note: These tests were removed because they test functionality that doesn't exist
# in the current implementation. The agent_core.py doesn't have index_documentation()
# or search_documentation() methods, and the DocumentationStore doesn't have
# get_all_table_documentation() method.



================================================
FILE: smol-sql-agents/tests/test_batch_manager.py
================================================
"""Tests for the BatchIndexingManager."""

import pytest
from unittest.mock import Mock, patch
from src.agents.batch_manager import BatchIndexingManager
from src.agents.indexer import SQLIndexerAgent
from src.database.persistence import DocumentationStore

@pytest.fixture
def mock_indexer_agent():
    """Create a mock indexer agent."""
    agent = Mock(spec=SQLIndexerAgent)
    agent.batch_index_tables.return_value = {"table1": True, "table2": False}
    agent.batch_index_relationships.return_value = {"rel1": True, "rel2": True}
    return agent

@pytest.fixture
def mock_doc_store():
    """Create a mock documentation store."""
    store = Mock(spec=DocumentationStore)
    store.get_pending_tables.return_value = ["table1", "table2", "table3"]
    store.get_pending_relationships.return_value = [
        {"id": "rel1", "constrained_table": "table1", "referred_table": "table2"},
        {"id": "rel2", "constrained_table": "table2", "referred_table": "table3"}
    ]
    store.get_table_info.return_value = {
        "table_name": "test_table",
        "schema_data": {"columns": []},
        "business_purpose": "Test table",
        "documentation": "Test documentation"
    }
    store.get_relationship_info.return_value = {
        "id": "test_rel",
        "relationship_type": "one-to-many",
        "documentation": "Test relationship"
    }
    return store

@pytest.fixture
def batch_manager(mock_indexer_agent):
    """Create a batch manager with mocked dependencies."""
    def mock_getenv(key, default=None):
        if key == "EMBEDDING_BATCH_SIZE":
            return "100"
        elif key == "EMBEDDING_MAX_RETRIES":
            return "3"
        else:
            return default
    
    with patch('src.agents.batch_manager.os.getenv', side_effect=mock_getenv):
        manager = BatchIndexingManager(mock_indexer_agent)
        return manager

def test_batch_manager_initialization(batch_manager):
    """Test batch manager initialization."""
    assert batch_manager.indexer == batch_manager.indexer
    assert batch_manager.batch_size == 100
    assert batch_manager.max_retries == 3

def test_group_into_batches(batch_manager):
    """Test grouping items into batches."""
    items = list(range(10))
    batches = batch_manager._group_into_batches(items, 3)
    
    assert len(batches) == 4
    assert batches[0] == [0, 1, 2]
    assert batches[1] == [3, 4, 5]
    assert batches[2] == [6, 7, 8]
    assert batches[3] == [9]

def test_group_into_batches_empty(batch_manager):
    """Test grouping empty list into batches."""
    batches = batch_manager._group_into_batches([], 5)
    assert batches == []

def test_estimate_embedding_costs(batch_manager):
    """Test cost estimation for embeddings."""
    # Use longer texts to ensure we get a meaningful cost estimate
    texts = [
        "This is a much longer test text that should generate enough tokens to create a meaningful cost estimate for OpenAI embeddings",
        "Another longer test text with more words to ensure we have sufficient token count for cost calculation",
        "Third longer test text with additional content to make sure the cost estimation works properly"
    ]
    costs = batch_manager.estimate_embedding_costs(texts)
    
    assert costs["total_texts"] == 3
    assert costs["estimated_tokens"] > 0
    # OpenAI embedding costs are very small, so we just check that the calculation works
    assert costs["estimated_cost_usd"] >= 0
    assert costs["cost_per_text"] >= 0

def test_estimate_embedding_costs_empty(batch_manager):
    """Test cost estimation for empty text list."""
    costs = batch_manager.estimate_embedding_costs([])
    
    assert costs["total_texts"] == 0
    assert costs["estimated_cost_usd"] == 0
    assert costs["cost_per_text"] == 0

def test_get_processing_stats(batch_manager, mock_doc_store):
    """Test getting processing statistics."""
    stats = batch_manager.get_processing_stats(mock_doc_store)
    
    assert stats["pending_tables"] == 3
    assert stats["pending_relationships"] == 2
    assert stats["total_pending"] == 5
    assert stats["batch_size"] == 100
    assert stats["estimated_batches"] == 1  # 5 items / 100 batch size = 1 batch

def test_batch_process_pending_tables(batch_manager, mock_doc_store):
    """Test batch processing of pending tables."""
    results = batch_manager.batch_process_pending_tables(mock_doc_store)
    
    # Verify the indexer was called
    mock_doc_store.get_pending_tables.assert_called_once()
    batch_manager.indexer.batch_index_tables.assert_called_once()
    
    # Verify results
    assert "table1" in results
    assert "table2" in results
    assert results["table1"] is True
    assert results["table2"] is False

def test_batch_process_pending_tables_empty(batch_manager, mock_doc_store):
    """Test batch processing when no pending tables."""
    mock_doc_store.get_pending_tables.return_value = []
    
    results = batch_manager.batch_process_pending_tables(mock_doc_store)
    
    assert results == {}
    batch_manager.indexer.batch_index_tables.assert_not_called()

def test_batch_process_pending_relationships(batch_manager, mock_doc_store):
    """Test batch processing of pending relationships."""
    results = batch_manager.batch_process_pending_relationships(mock_doc_store)
    
    # Verify the indexer was called
    mock_doc_store.get_pending_relationships.assert_called_once()
    batch_manager.indexer.batch_index_relationships.assert_called_once()
    
    # Verify results
    assert "rel1" in results
    assert "rel2" in results
    assert results["rel1"] is True
    assert results["rel2"] is True

def test_batch_process_pending_relationships_empty(batch_manager, mock_doc_store):
    """Test batch processing when no pending relationships."""
    mock_doc_store.get_pending_relationships.return_value = []
    
    results = batch_manager.batch_process_pending_relationships(mock_doc_store)
    
    assert results == {}
    batch_manager.indexer.batch_index_relationships.assert_not_called()

def test_batch_process_with_exception(batch_manager, mock_doc_store):
    """Test batch processing when exceptions occur."""
    # Mock an exception when getting table info
    mock_doc_store.get_table_info.side_effect = Exception("Database error")
    
    results = batch_manager.batch_process_pending_tables(mock_doc_store)
    
    # Should handle the exception gracefully
    assert "table1" in results
    assert results["table1"] is False 


================================================
FILE: smol-sql-agents/tests/test_index_processed_documents.py
================================================
"""Tests for indexing already processed documents."""

import pytest
from unittest.mock import Mock, patch
from src.agents.core import PersistentDocumentationAgent

def test_index_processed_documents_with_vector_indexing():
    """Test indexing processed documents when vector indexing is available."""
    with patch('src.agents.core.SQLVectorStore') as mock_vector_store, \
         patch('src.agents.core.DatabaseInspector') as mock_db_inspector, \
         patch('src.agents.core.OpenAIModel') as mock_llm_model, \
         patch('src.agents.core.DocumentationStore') as mock_doc_store, \
         patch('src.agents.core.CodeAgent') as mock_code_agent, \
         patch('src.agents.core.os.getenv', return_value='dummy-api-key'):
        
        # Mock the documentation store methods
        mock_doc_store.return_value.get_all_tables.return_value = ["table1", "table2"]
        mock_doc_store.return_value.get_all_relationships.return_value = [
            {"id": "rel1", "constrained_table": "table1", "referred_table": "table2"}
        ]
        mock_doc_store.return_value.get_table_info.return_value = {
            "table_name": "table1",
            "business_purpose": "Test table",
            "schema_data": {"columns": []},
            "documentation": "Test documentation"
        }
        mock_doc_store.return_value.get_relationship_info.return_value = {
            "id": "rel1",
            "relationship_type": "one-to-many",
            "documentation": "Test relationship"
        }
        
        # Mock the indexer agent
        mock_indexer = Mock()
        mock_indexer.index_table_documentation.return_value = True
        mock_indexer.index_relationship_documentation.return_value = True
        
        # Create agent with mocked indexer
        agent = PersistentDocumentationAgent()
        agent.indexer_agent = mock_indexer
        agent.vector_indexing_available = True
        
        # Call the method
        agent.index_processed_documents()
        
        # Verify that the indexer was called for both tables and relationships
        assert mock_indexer.index_table_documentation.call_count == 2
        assert mock_indexer.index_relationship_documentation.call_count == 1

def test_index_processed_documents_without_vector_indexing():
    """Test indexing processed documents when vector indexing is not available."""
    with patch('src.agents.core.SQLVectorStore') as mock_vector_store, \
         patch('src.agents.core.DatabaseInspector') as mock_db_inspector, \
         patch('src.agents.core.OpenAIModel') as mock_llm_model, \
         patch('src.agents.core.DocumentationStore') as mock_doc_store, \
         patch('src.agents.core.CodeAgent') as mock_code_agent, \
         patch('src.agents.core.os.getenv', return_value='dummy-api-key'):
        
        # Create agent without vector indexing
        agent = PersistentDocumentationAgent()
        agent.indexer_agent = None
        agent.vector_indexing_available = False
        
        # Call the method - should not raise an exception
        agent.index_processed_documents()
        
        # Verify that no indexing was attempted
        # (The method should just log a warning and return)

def test_index_processed_documents_with_indexing_failures():
    """Test indexing processed documents when some indexing operations fail."""
    with patch('src.agents.core.SQLVectorStore') as mock_vector_store, \
         patch('src.agents.core.DatabaseInspector') as mock_db_inspector, \
         patch('src.agents.core.OpenAIModel') as mock_llm_model, \
         patch('src.agents.core.DocumentationStore') as mock_doc_store, \
         patch('src.agents.core.CodeAgent') as mock_code_agent, \
         patch('src.agents.core.os.getenv', return_value='dummy-api-key'):
        
        # Mock the documentation store methods
        mock_doc_store.return_value.get_all_tables.return_value = ["table1", "table2"]
        mock_doc_store.return_value.get_all_relationships.return_value = [
            {"id": "rel1", "constrained_table": "table1", "referred_table": "table2"}
        ]
        mock_doc_store.return_value.get_table_info.return_value = {
            "table_name": "table1",
            "business_purpose": "Test table",
            "schema_data": {"columns": []},
            "documentation": "Test documentation"
        }
        mock_doc_store.return_value.get_relationship_info.return_value = {
            "id": "rel1",
            "relationship_type": "one-to-many",
            "documentation": "Test relationship"
        }
        
        # Mock the indexer agent with some failures
        mock_indexer = Mock()
        mock_indexer.index_table_documentation.side_effect = [True, False]  # Second table fails
        mock_indexer.index_relationship_documentation.return_value = True
        
        # Create agent with mocked indexer
        agent = PersistentDocumentationAgent()
        agent.indexer_agent = mock_indexer
        agent.vector_indexing_available = True
        
        # Call the method - should handle failures gracefully
        agent.index_processed_documents()
        
        # Verify that the indexer was called for both tables and relationships
        assert mock_indexer.index_table_documentation.call_count == 2
        assert mock_indexer.index_relationship_documentation.call_count == 1 


================================================
FILE: smol-sql-agents/tests/test_indexer_agent.py
================================================
"""Tests for the SQL Indexer Agent and related components."""

import sys
import os
import pytest
from unittest.mock import Mock, patch

# Add the parent directory to the Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.agents.indexer import SQLIndexerAgent
from src.vector.store import SQLVectorStore
from src.vector.embeddings import OpenAIEmbeddingsClient

@pytest.fixture
def mock_embedding():
    """Mock embedding vector."""
    return [0.1] * 3072  # OpenAI embeddings are 3072-dimensional

@pytest.fixture
def mock_embeddings_client():
    """Create a mock OpenAI embeddings client."""
    client = Mock(spec=OpenAIEmbeddingsClient)
    client.generate_embedding.return_value = [0.1] * 3072
    client.generate_embeddings_batch.return_value = [[0.1] * 3072]
    return client

@pytest.fixture
def mock_vector_index():
    """Create a mock vector index."""
    index = Mock()
    index.docs = []
    
    def mock_add(id, vector, metadata):
        index.docs.append({"id": id, "metadata": metadata})
    index.add = Mock(side_effect=mock_add)
    
    def mock_delete(ids):
        if not any(doc["id"] in ids for doc in index.docs):
            raise ValueError("Document not found")
        index.docs = [doc for doc in index.docs if doc["id"] not in ids]
        return True
    index.delete = Mock(side_effect=mock_delete)
    
    def mock_save():
        return True
    index.save = Mock(side_effect=mock_save)
    
    def mock_search(vector, k=5):
        if not index.docs:
            return []
        return [
            {
                "id": doc["id"],
                "metadata": doc["metadata"],
                "score": 0.95
            }
            for doc in index.docs[:k]
        ]
    index.search = Mock(side_effect=mock_search)
    return index

@pytest.fixture
def mock_vector_index_factory(mock_vector_index):
    """Create a mock vector index factory."""
    def factory(path):
        return mock_vector_index
    return factory

@pytest.fixture
def mock_vector_store(mock_embeddings_client, mock_vector_index_factory, tmp_path):
    """Create a mock vector store with test indexes."""
    store = SQLVectorStore(
        base_path=str(tmp_path),
        vector_index_factory=mock_vector_index_factory
    )
    store.embeddings_client = mock_embeddings_client
    store.create_table_index()
    store.create_relationship_index()
    return store

@pytest.fixture
def indexer_agent(mock_vector_store):
    """Create an indexer agent with mocked dependencies."""
    return SQLIndexerAgent(mock_vector_store)

def test_vector_store_init_failure(mock_embeddings_client):
    """Test handling of vector store initialization failure."""
    failing_store = Mock(spec=SQLVectorStore)
    failing_store.embeddings_client = mock_embeddings_client
    failing_store.create_table_index.side_effect = Exception("Failed to create index")
    
    with pytest.raises(Exception) as exc_info:
        SQLIndexerAgent(failing_store)
    assert "Failed to create index" in str(exc_info.value)

def test_index_table_documentation(indexer_agent):
    """Test indexing a single table document."""
    table_data = {
        "name": "users",
        "columns": ["id", "username", "email"],
        "description": "User account information"
    }
    
    result = indexer_agent.index_table_documentation(table_data)
    assert result == True

def test_index_table_documentation_invalid_data(indexer_agent):
    """Test indexing with invalid table data."""
    invalid_table_data = {
        "description": "Missing required fields"
    }
    
    result = indexer_agent.index_table_documentation(invalid_table_data)
    assert result == False

def test_index_relationship_documentation(indexer_agent):
    """Test indexing a single relationship document."""
    relationship_data = {
        "name": "user_orders",
        "type": "one_to_many",
        "tables": ["users", "orders"],
        "description": "User's order history"
    }
    
    result = indexer_agent.index_relationship_documentation(relationship_data)
    assert result == True

def test_search_documentation_all(indexer_agent, mock_embedding):
    """Test searching across all documentation types."""
    # First add some test data
    table_data = {
        "name": "products",
        "columns": ["id", "name", "price"],
        "description": "Product catalog"
    }
    relationship_data = {
        "name": "product_categories",
        "type": "many_to_many",
        "tables": ["products", "categories"],
        "description": "Product categorization"
    }
    
    indexer_agent.index_table_documentation(table_data)
    indexer_agent.index_relationship_documentation(relationship_data)
    
    results = indexer_agent.search_documentation("product")
    assert "tables" in results
    assert "relationships" in results
    assert "total_results" in results
    assert results["total_results"] > 0

def test_batch_index_tables(indexer_agent):
    """Test batch indexing of multiple tables."""
    tables_data = [
        {
            "name": "customers",
            "columns": ["id", "name", "email"],
            "description": "Customer information"
        },
        {
            "name": "orders",
            "columns": ["id", "customer_id", "total"],
            "description": "Order details"
        }
    ]
    
    results = indexer_agent.batch_index_tables(tables_data)
    assert len(results) == 2
    assert all(results.values())

def test_update_table_index(indexer_agent):
    """Test updating an existing table document."""
    original_data = {
        "name": "products",
        "columns": ["id", "name"],
        "description": "Basic product info"
    }
    updated_data = {
        "name": "products",
        "columns": ["id", "name", "price"],
        "description": "Enhanced product info"
    }
    
    # First add the original
    indexer_agent.index_table_documentation(original_data)
    
    # Then update it
    result = indexer_agent.update_table_index("products", updated_data)
    assert result == True

def test_remove_from_index(indexer_agent):
    """Test removing a document from the index."""
    table_data = {
        "name": "temp_table",
        "columns": ["id"],
        "description": "Temporary table"
    }
    
    # First add the table
    indexer_agent.index_table_documentation(table_data)
    
    # Then remove it
    result = indexer_agent.remove_from_index("temp_table", "table")
    assert result == True

def test_search_with_empty_indexes(indexer_agent):
    """Test search behavior with empty indexes."""
    results = indexer_agent.search_documentation("test query")
    assert results["total_results"] == 0
    assert len(results["tables"]) == 0
    assert len(results["relationships"]) == 0

def test_search_specific_type(indexer_agent):
    """Test searching only tables or only relationships."""
    table_data = {
        "name": "inventory",
        "columns": ["id", "quantity"],
        "description": "Stock levels"
    }
    
    indexer_agent.index_table_documentation(table_data)
    
    # Search only tables
    table_results = indexer_agent.search_documentation("inventory", doc_type="table")
    assert "relationships" in table_results
    assert len(table_results["relationships"]) == 0
    assert len(table_results["tables"]) > 0

def test_batch_index_relationships(indexer_agent):
    """Test batch indexing of multiple relationships."""
    relationships_data = [
        {
            "name": "user_addresses",
            "type": "one_to_many",
            "tables": ["users", "addresses"],
            "description": "User's addresses"
        },
        {
            "name": "order_items",
            "type": "one_to_many",
            "tables": ["orders", "items"],
            "description": "Items in an order"
        }
    ]
    
    results = indexer_agent.batch_index_relationships(relationships_data)
    assert len(results) == 2
    assert all(results.values())

def test_invalid_search_type(indexer_agent):
    """Test search with invalid doc_type."""
    results = indexer_agent.search_documentation("test", doc_type="invalid_type")
    assert results["total_results"] == 0
    assert "error" in results

def test_update_nonexistent_table(indexer_agent):
    """Test updating a table that doesn't exist."""
    updated_data = {
        "name": "nonexistent",
        "columns": ["id"],
        "description": "This table doesn't exist"
    }
    result = indexer_agent.update_table_index("nonexistent", updated_data)
    assert result == False

def test_remove_nonexistent_document(indexer_agent):
    """Test removing a document that doesn't exist."""
    result = indexer_agent.remove_from_index("nonexistent", "table")
    assert result == False

def test_special_characters_handling(indexer_agent):
    """Test handling of special characters in names."""
    table_data = {
        "name": "user$data#2023",
        "columns": ["id", "data"],
        "description": "Table with special characters in name"
    }
    result = indexer_agent.index_table_documentation(table_data)
    assert result == True
    
    # Try searching for it
    search_results = indexer_agent.search_documentation("user$data")
    assert search_results["total_results"] > 0

if __name__ == "__main__":
    pytest.main([__file__])



================================================
FILE: smol-sql-agents/tests/test_recognize_entities.py
================================================
"""Tests for the Entity Recognition Agent."""

import pytest
from unittest.mock import Mock, patch
from src.agents.entity_recognition import EntityRecognitionAgent
from src.agents.indexer import SQLIndexerAgent

@pytest.fixture
def mock_indexer_agent():
    """Create a mock indexer agent."""
    agent = Mock(spec=SQLIndexerAgent)
    
    # Mock search results for different scenarios
    def mock_search_documentation(query, doc_type):
        if "user" in query.lower():
            return {
                "tables": [
                    {
                        "id": "users",
                        "content": {
                            "name": "users",
                            "business_purpose": "Stores user account information and profile data",
                            "schema_data": {"columns": ["id", "username", "email", "created_at"]},
                            "type": "table"
                        },
                        "score": 0.92
                    },
                    {
                        "id": "user_profiles",
                        "content": {
                            "name": "user_profiles",
                            "business_purpose": "Extended user profile information",
                            "schema_data": {"columns": ["user_id", "first_name", "last_name", "phone"]},
                            "type": "table"
                        },
                        "score": 0.85
                    }
                ],
                "relationships": [],
                "total_results": 2
            }
        elif "order" in query.lower():
            return {
                "tables": [
                    {
                        "id": "orders",
                        "content": {
                            "name": "orders",
                            "business_purpose": "Customer order information and transaction data",
                            "schema_data": {"columns": ["id", "user_id", "total_amount", "order_date"]},
                            "type": "table"
                        },
                        "score": 0.88
                    }
                ],
                "relationships": [],
                "total_results": 1
            }
        else:
            return {
                "tables": [],
                "relationships": [],
                "total_results": 0
            }
    
    agent.search_documentation.side_effect = mock_search_documentation
    return agent

@pytest.fixture
def entity_agent(mock_indexer_agent):
    """Create an entity recognition agent with mocked dependencies."""
    def mock_getenv(key, default=None):
        if key == "OPENAI_API_KEY":
            return "dummy-api-key"
        else:
            return default
    
    with patch('src.agents.entity_recognition.OpenAIModel') as mock_model, \
         patch('src.agents.entity_recognition.CodeAgent') as mock_code_agent, \
         patch('src.agents.entity_recognition.os.getenv', side_effect=mock_getenv):
        agent = EntityRecognitionAgent(mock_indexer_agent)
        return agent

def test_entity_agent_initialization(mock_indexer_agent):
    """Test successful initialization of entity recognition agent."""
    def mock_getenv(key, default=None):
        if key == "OPENAI_API_KEY":
            return "dummy-api-key"
        else:
            return default
    
    with patch('src.agents.entity_recognition.OpenAIModel') as mock_model, \
         patch('src.agents.entity_recognition.CodeAgent') as mock_code_agent, \
         patch('src.agents.entity_recognition.os.getenv', side_effect=mock_getenv):
        agent = EntityRecognitionAgent(mock_indexer_agent)
        assert agent.indexer_agent == mock_indexer_agent
        assert agent.llm_model is not None
        assert agent.agent is not None

def test_entity_agent_initialization_missing_api_key(mock_indexer_agent):
    """Test agent initialization with missing API key."""
    with patch('src.agents.entity_recognition.os.getenv', return_value=None), \
         pytest.raises(ValueError) as exc_info:
        EntityRecognitionAgent(mock_indexer_agent)
    assert "OPENAI_API_KEY environment variable is not set" in str(exc_info.value)

def test_quick_entity_lookup_success(entity_agent, mock_indexer_agent):
    """Test successful quick entity lookup."""
    result = entity_agent.quick_entity_lookup("user data", threshold=0.8)
    
    # Should return tables that meet the threshold
    assert isinstance(result, list)
    assert "users" in result  # Score 0.92 > 0.8
    assert "user_profiles" in result  # Score 0.85 > 0.8
    
    mock_indexer_agent.search_documentation.assert_called_once_with(
        query="user data",
        doc_type="table"
    )

def test_quick_entity_lookup_high_threshold(entity_agent, mock_indexer_agent):
    """Test quick entity lookup with high threshold."""
    result = entity_agent.quick_entity_lookup("user data", threshold=0.95)
    
    # Only very high scoring results should be returned
    assert isinstance(result, list)
    # No tables should meet 0.95 threshold based on mock data (highest is 0.92)
    assert len(result) == 0 or "users" not in result

def test_quick_entity_lookup_no_results(entity_agent, mock_indexer_agent):
    """Test quick entity lookup when no entities are found."""
    result = entity_agent.quick_entity_lookup("nonexistent data", threshold=0.7)
    
    assert isinstance(result, list)
    assert len(result) == 0

def test_quick_entity_lookup_with_exception(entity_agent, mock_indexer_agent):
    """Test quick entity lookup when an exception occurs."""
    mock_indexer_agent.search_documentation.side_effect = Exception("Search failed")
    
    result = entity_agent.quick_entity_lookup("user data")
    
    assert isinstance(result, list)
    assert len(result) == 0

def test_get_entity_details_success(entity_agent, mock_indexer_agent):
    """Test successful retrieval of entity details."""
    # Mock specific table search
    def mock_specific_search(query, doc_type):
        if query.lower() == "users":
            return {
                "tables": [
                    {
                        "id": "users",
                        "content": {
                            "name": "users",
                            "business_purpose": "Stores user account information",
                            "schema_data": {"columns": ["id", "username", "email"]},
                            "type": "table"
                        },
                        "score": 1.0  # Exact match
                    }
                ]
            }
        return {"tables": []}
    
    mock_indexer_agent.search_documentation.side_effect = mock_specific_search
    
    result = entity_agent.get_entity_details(["users"])
    
    assert isinstance(result, dict)
    assert "users" in result
    assert result["users"]["name"] == "users"
    assert result["users"]["business_purpose"] == "Stores user account information"

def test_get_entity_details_no_match(entity_agent, mock_indexer_agent):
    """Test get entity details when no matching entities found."""
    mock_indexer_agent.search_documentation.return_value = {"tables": []}
    
    result = entity_agent.get_entity_details(["nonexistent_table"])
    
    assert isinstance(result, dict)
    assert len(result) == 0

def test_get_entity_details_with_exception(entity_agent, mock_indexer_agent):
    """Test get entity details when an exception occurs."""
    mock_indexer_agent.search_documentation.side_effect = Exception("Search failed")
    
    result = entity_agent.get_entity_details(["users"])
    
    assert isinstance(result, dict)
    assert len(result) == 0

def test_recognize_entities_empty_query(entity_agent):
    """Test entity recognition with empty query."""
    result = entity_agent.recognize_entities("")
    
    assert result["success"] is False
    assert "Query cannot be empty" in result["error"]
    assert result["applicable_entities"] == []
    assert result["recommendations"] == []

def test_recognize_entities_whitespace_query(entity_agent):
    """Test entity recognition with whitespace-only query."""
    result = entity_agent.recognize_entities("   ")
    
    assert result["success"] is False
    assert "Query cannot be empty" in result["error"]

def test_recognize_entities_success(entity_agent):
    """Test successful entity recognition."""
    # Mock the agent's run method to return a successful response
    mock_response = {
        "success": True,
        "applicable_entities": [
            {
                "table_name": "users",
                "business_purpose": "Stores user account information",
                "relevance_score": 0.92,
                "recommendation": "Highly relevant - strongly recommended for your query"
            }
        ],
        "recommendations": [
            {
                "priority": 1,
                "table_name": "users",
                "suggested_actions": ["Query users to retrieve relevant data"]
            }
        ],
        "confidence": 0.92
    }
    
    entity_agent.agent.run.return_value = mock_response
    
    result = entity_agent.recognize_entities("user information")
    
    assert result["success"] is True
    assert len(result["applicable_entities"]) > 0
    assert result["applicable_entities"][0]["table_name"] == "users"
    assert result["confidence"] == 0.92

def test_recognize_entities_json_response(entity_agent):
    """Test entity recognition with JSON string response."""
    import json
    
    mock_response = {
        "success": True,
        "applicable_entities": [
            {
                "table_name": "orders",
                "relevance_score": 0.88
            }
        ],
        "recommendations": []
    }
    
    entity_agent.agent.run.return_value = json.dumps(mock_response)
    
    result = entity_agent.recognize_entities("order data")
    
    assert result["success"] is True
    assert len(result["applicable_entities"]) > 0

def test_recognize_entities_invalid_json(entity_agent):
    """Test entity recognition with invalid JSON response."""
    entity_agent.agent.run.return_value = "Invalid JSON response"
    
    result = entity_agent.recognize_entities("user data")
    
    assert result["success"] is False
    assert "Invalid JSON response from agent" in result["error"]
    assert result["applicable_entities"] == []

def test_recognize_entities_unexpected_response(entity_agent):
    """Test entity recognition with unexpected response type."""
    entity_agent.agent.run.return_value = 12345  # Unexpected integer
    
    result = entity_agent.recognize_entities("user data")
    
    assert result["success"] is False
    assert "Unexpected response type from agent" in result["error"]

def test_recognize_entities_with_exception(entity_agent):
    """Test entity recognition when an exception occurs."""
    entity_agent.agent.run.side_effect = Exception("Agent execution failed")
    
    result = entity_agent.recognize_entities("user data")
    
    assert result["success"] is False
    assert "Agent execution failed" in result["error"]
    assert result["applicable_entities"] == []

def test_recognize_entities_with_intent(entity_agent):
    """Test entity recognition with specific user intent."""
    mock_response = {
        "success": True,
        "applicable_entities": [
            {
                "table_name": "users",
                "relevance_score": 0.95
            }
        ],
        "recommendations": []
    }
    
    entity_agent.agent.run.return_value = mock_response
    
    result = entity_agent.recognize_entities(
        user_query="show me data",
        user_intent="I want to analyze user demographics"
    )
    
    assert result["success"] is True
    entity_agent.agent.run.assert_called_once()
    
    # Verify that the prompt included both query and intent
    call_args = entity_agent.agent.run.call_args[0][0]
    assert "show me data" in call_args
    assert "I want to analyze user demographics" in call_args

def test_private_methods(entity_agent):
    """Test private helper methods."""
    # Test _calculate_purpose_match
    match_score = entity_agent._calculate_purpose_match(
        "Stores user account information",
        "user account data"
    )
    assert match_score > 0.0
    assert match_score <= 1.0
    
    # Test _calculate_name_relevance
    name_relevance = entity_agent._calculate_name_relevance("users", "user data")
    assert name_relevance > 0.0
    
    # Test _get_relevance_recommendation
    high_rec = entity_agent._get_relevance_recommendation(0.9)
    assert "Highly relevant" in high_rec
    
    low_rec = entity_agent._get_relevance_recommendation(0.1)
    assert "Not relevant" in low_rec
    
    # Test _generate_analysis_summary
    entities = [{"table_name": "users", "relevance_score": 0.9}]
    summary = entity_agent._generate_analysis_summary(entities, "user data")
    assert "Found 1 applicable entities" in summary
    
    empty_summary = entity_agent._generate_analysis_summary([], "nonexistent")
    assert "No highly relevant entities found" in empty_summary

if __name__ == "__main__":
    pytest.main([__file__])


================================================
FILE: smol-sql-agents/tests/test_search_tools.py
================================================
"""Tests for the enhanced search tools using OpenAI embeddings."""

import pytest
from unittest.mock import Mock, patch
from src.vector.search import (
    search_table_documentation,
    search_relationship_documentation,
    semantic_search_all_documentation
)

@pytest.fixture
def mock_agent():
    """Create a mock PersistentDocumentationAgent."""
    with patch('src.vector.search.PersistentDocumentationAgent') as mock:
        agent_instance = Mock()
        mock.return_value = agent_instance
        yield agent_instance

@pytest.fixture
def mock_search_results():
    """Create mock search results."""
    table_result = {
        "metadata": {
            "table_name": "users",
            "business_purpose": "Stores user information",
            "schema": {
                "columns": [
                    {"name": "id", "type": "integer"},
                    {"name": "username", "type": "varchar"}
                ]
            }
        },
        "similarity": 0.95
    }
    
    relationship_result = {
        "metadata": {
            "relationship_id": "users_orders_fk",
            "relationship_type": "one-to-many",
            "documentation": "Each user can have multiple orders",
            "tables": ["users", "orders"]
        },
        "similarity": 0.85
    }
    
    return {"table": table_result, "relationship": relationship_result}

def test_search_table_documentation(mock_agent, mock_search_results):
    """Test searching table documentation."""
    # Setup
    mock_agent.indexer_agent.search_documentation.return_value = [mock_search_results["table"]]
    
    # Execute
    results = search_table_documentation("user tables", limit=5)
    
    # Verify
    assert len(results) == 1
    assert results[0]["table_name"] == "users"
    assert results[0]["similarity_score"] == 0.95
    assert "schema" in results[0]
    
    mock_agent.indexer_agent.search_documentation.assert_called_once_with(
        "user tables", doc_type="tables", limit=5
    )

def test_search_relationship_documentation(mock_agent, mock_search_results):
    """Test searching relationship documentation."""
    # Setup
    mock_agent.indexer_agent.search_documentation.return_value = [mock_search_results["relationship"]]
    
    # Execute
    results = search_relationship_documentation("user relationships", limit=5)
    
    # Verify
    assert len(results) == 1
    assert results[0]["relationship_id"] == "users_orders_fk"
    assert results[0]["similarity_score"] == 0.85
    assert results[0]["tables_involved"] == ["users", "orders"]
    
    mock_agent.indexer_agent.search_documentation.assert_called_once_with(
        "user relationships", doc_type="relationships", limit=5
    )

def test_semantic_search_all_documentation(mock_agent, mock_search_results):
    """Test searching all documentation types."""
    # Setup
    def mock_search_side_effect(query, doc_type, limit):
        if doc_type == "tables":
            return [mock_search_results["table"]]
        else:
            return [mock_search_results["relationship"]]
    
    mock_agent.indexer_agent.search_documentation.side_effect = mock_search_side_effect
    
    # Execute
    results = semantic_search_all_documentation("user data", limit=10)
    
    # Verify
    assert "tables" in results
    assert "relationships" in results
    assert results["total_results"] > 0
    
    # Verify tables results
    assert len(results["tables"]) > 0
    assert results["tables"][0]["table_name"] == "users"
    
    # Verify relationship results
    assert len(results["relationships"]) > 0
    assert results["relationships"][0]["relationship_id"] == "users_orders_fk"

def test_semantic_search_empty_results(mock_agent):
    """Test searching with no results."""
    # Setup
    mock_agent.indexer_agent.search_documentation.return_value = []
    
    # Execute
    results = semantic_search_all_documentation("nonexistent data", limit=10)
    
    # Verify
    assert results["total_results"] == 0
    assert len(results["tables"]) == 0
    assert len(results["relationships"]) == 0

def test_search_with_invalid_limit(mock_agent, mock_search_results):
    """Test searching with invalid limit values."""
    # Setup
    def mock_search_side_effect(query, doc_type, limit):
        if doc_type == "tables":
            return [mock_search_results["table"]]
        else:
            return [mock_search_results["relationship"]]
    
    mock_agent.indexer_agent.search_documentation.side_effect = mock_search_side_effect
    
    # Execute with zero limit
    results = semantic_search_all_documentation("test", limit=0)
    assert results["total_results"] == 0
    
    # Execute with negative limit
    results = semantic_search_all_documentation("test", limit=-1)
    assert results["total_results"] == 0



================================================
FILE: smol-sql-agents/tests/test_vector_indexing_fallback.py
================================================
"""Tests for vector indexing fallback behavior."""

import pytest
from unittest.mock import Mock, patch
from src.agents.core import PersistentDocumentationAgent

def test_agent_initialization_with_vector_store_failure():
    """Test that agent initializes gracefully when vector store fails."""
    with patch('src.agents.core.SQLVectorStore') as mock_vector_store, \
         patch('src.agents.core.DatabaseInspector') as mock_db_inspector, \
         patch('src.agents.core.OpenAIModel') as mock_llm_model, \
         patch('src.agents.core.DocumentationStore') as mock_doc_store, \
         patch('src.agents.core.CodeAgent') as mock_code_agent, \
         patch('src.agents.core.os.getenv', return_value='dummy-api-key'):
        
        # Make the vector store initialization fail
        mock_vector_store.side_effect = RuntimeError("vectordb compatibility issue")
        
        # Agent should still initialize successfully
        agent = PersistentDocumentationAgent()
        
        # Check that vector indexing is marked as unavailable
        assert agent.vector_indexing_available is False
        assert agent.indexer_agent is None
        
        # Check that other components are still available
        assert agent.llm_model is not None
        assert agent.db_inspector is not None
        assert agent.store is not None

def test_table_processing_without_vector_indexing():
    """Test table processing when vector indexing is not available."""
    with patch('src.agents.core.SQLVectorStore') as mock_vector_store, \
         patch('src.agents.core.DatabaseInspector') as mock_db_inspector, \
         patch('src.agents.core.OpenAIModel') as mock_llm_model, \
         patch('src.agents.core.DocumentationStore') as mock_doc_store, \
         patch('src.agents.core.CodeAgent') as mock_code_agent, \
         patch('src.agents.core.os.getenv', return_value='dummy-api-key'):
        
        # Make the vector store initialization fail
        mock_vector_store.side_effect = RuntimeError("vectordb compatibility issue")
        
        agent = PersistentDocumentationAgent()
        
        # Mock the agent's run method to return valid JSON
        mock_response = {
            "business_purpose": "Test table for user data",
            "schema_data": {
                "table_name": "test_table",
                "columns": [{"name": "id", "type": "integer"}]
            }
        }
        agent.agent.run.return_value = '{"business_purpose": "Test table for user data", "schema_data": {"table_name": "test_table", "columns": [{"name": "id", "type": "integer"}]}}'
        
        # Mock the store methods
        agent.store.save_table_documentation = Mock()
        agent.store.get_table_schema = Mock(return_value={"name": "test_table", "columns": []})
        
        # Process table - should work without vector indexing
        agent.process_table_documentation("test_table")
        
        # Verify that documentation was saved
        agent.store.save_table_documentation.assert_called_once()
        
        # Verify that vector indexing was not attempted
        assert agent.vector_indexing_available is False

def test_relationship_processing_without_vector_indexing():
    """Test relationship processing when vector indexing is not available."""
    with patch('src.agents.core.SQLVectorStore') as mock_vector_store, \
         patch('src.agents.core.DatabaseInspector') as mock_db_inspector, \
         patch('src.agents.core.OpenAIModel') as mock_llm_model, \
         patch('src.agents.core.DocumentationStore') as mock_doc_store, \
         patch('src.agents.core.CodeAgent') as mock_code_agent, \
         patch('src.agents.core.os.getenv', return_value='dummy-api-key'):
        
        # Make the vector store initialization fail
        mock_vector_store.side_effect = RuntimeError("vectordb compatibility issue")
        
        agent = PersistentDocumentationAgent()
        
        # Mock the agent's run method to return valid JSON
        agent.agent.run.return_value = '{"relationship_type": "one-to-many", "documentation": "Test relationship"}'
        
        # Mock the store methods
        agent.store.save_relationship_documentation = Mock()
        
        # Test relationship data
        relationship = {
            "id": "test_rel",
            "constrained_table": "table1",
            "constrained_columns": ["id"],
            "referred_table": "table2",
            "referred_columns": ["id"]
        }
        
        # Process relationship - should work without vector indexing
        agent.process_relationship_documentation(relationship)
        
        # Verify that documentation was saved
        agent.store.save_relationship_documentation.assert_called_once()
        
        # Verify that vector indexing was not attempted
        assert agent.vector_indexing_available is False 


================================================
FILE: smol-sql-agents/tests/test_vector_indexing_retry.py
================================================
"""Tests for vector indexing retry functionality."""

import pytest
from unittest.mock import Mock, patch
from src.agents.core import PersistentDocumentationAgent

def test_retry_vector_indexing_success():
    """Test successful retry of vector indexing initialization."""
    with patch('src.agents.core.SQLVectorStore') as mock_vector_store, \
         patch('src.agents.core.DatabaseInspector') as mock_db_inspector, \
         patch('src.agents.core.OpenAIModel') as mock_llm_model, \
         patch('src.agents.core.DocumentationStore') as mock_doc_store, \
         patch('src.agents.core.CodeAgent') as mock_code_agent, \
         patch('src.agents.core.os.getenv', return_value='dummy-api-key'):
        
        # First, make the initial initialization fail
        mock_vector_store.side_effect = RuntimeError("Initial failure")
        
        # Create agent - should have vector indexing disabled
        agent = PersistentDocumentationAgent()
        assert agent.vector_indexing_available is False
        assert agent.indexer_agent is None
        
        # Now make the retry succeed
        mock_vector_store.side_effect = None
        mock_indexer = Mock()
        with patch('src.agents.core.SQLIndexerAgent', return_value=mock_indexer):
            success = agent.retry_vector_indexing_initialization()
            
            assert success is True
            assert agent.vector_indexing_available is True
            assert agent.indexer_agent is not None

def test_retry_vector_indexing_already_available():
    """Test retry when vector indexing is already available."""
    with patch('src.agents.core.SQLVectorStore') as mock_vector_store, \
         patch('src.agents.core.DatabaseInspector') as mock_db_inspector, \
         patch('src.agents.core.OpenAIModel') as mock_llm_model, \
         patch('src.agents.core.DocumentationStore') as mock_doc_store, \
         patch('src.agents.core.CodeAgent') as mock_code_agent, \
         patch('src.agents.core.os.getenv', return_value='dummy-api-key'):
        
        # Create agent with vector indexing available
        agent = PersistentDocumentationAgent()
        agent.vector_indexing_available = True
        agent.indexer_agent = Mock()
        
        # Retry should return True without re-initializing
        success = agent.retry_vector_indexing_initialization()
        
        assert success is True
        assert agent.vector_indexing_available is True

def test_retry_vector_indexing_failure():
    """Test retry when vector indexing initialization fails."""
    with patch('src.agents.core.SQLVectorStore') as mock_vector_store, \
         patch('src.agents.core.DatabaseInspector') as mock_db_inspector, \
         patch('src.agents.core.OpenAIModel') as mock_llm_model, \
         patch('src.agents.core.DocumentationStore') as mock_doc_store, \
         patch('src.agents.core.CodeAgent') as mock_code_agent, \
         patch('src.agents.core.os.getenv', return_value='dummy-api-key'):
        
        # Make the initial initialization fail
        mock_vector_store.side_effect = RuntimeError("Initial failure")
        
        # Create agent - should have vector indexing disabled
        agent = PersistentDocumentationAgent()
        assert agent.vector_indexing_available is False
        assert agent.indexer_agent is None
        
        # Make the retry also fail
        mock_vector_store.side_effect = RuntimeError("Retry failure")
        success = agent.retry_vector_indexing_initialization()
        
        assert success is False
        assert agent.vector_indexing_available is False
        assert agent.indexer_agent is None

def test_vector_indexing_status_check():
    """Test checking vector indexing status."""
    with patch('src.agents.core.SQLVectorStore') as mock_vector_store, \
         patch('src.agents.core.DatabaseInspector') as mock_db_inspector, \
         patch('src.agents.core.OpenAIModel') as mock_llm_model, \
         patch('src.agents.core.DocumentationStore') as mock_doc_store, \
         patch('src.agents.core.CodeAgent') as mock_code_agent, \
         patch('src.agents.core.os.getenv', return_value='dummy-api-key'):
        
        # Test with vector indexing available
        agent = PersistentDocumentationAgent()
        agent.vector_indexing_available = True
        agent.indexer_agent = Mock()
        
        # Status should be available
        assert agent.vector_indexing_available is True
        assert agent.indexer_agent is not None
        
        # Test with vector indexing unavailable
        agent.vector_indexing_available = False
        agent.indexer_agent = None
        
        # Status should be unavailable
        assert agent.vector_indexing_available is False
        assert agent.indexer_agent is None 


================================================
FILE: tutorial/00_TOC.md
================================================
# Building Intelligent SQL Documentation Agents

## A Hands-On Guide to Autonomous Database Analysis with AI

### By Montray Davis

---

## **Introduction**

## Welcome to the Future of Database Documentation

For decades, database documentation has been the bane of every developer's existence. We've all been there‚Äîstaring at a database with hundreds of tables, trying to understand what `USR_PRFL_X_REF` actually means, wondering why the `customers` table has a foreign key to something called `LEGACY_SYS_MAP`. Traditional documentation tools give us schema dumps‚Äîendless tables of column names and data types that tell us *what* exists but nothing about *why* it exists.

What if your database could explain itself? What if an AI agent could look at your schema and tell you, "This table manages user authentication sessions," or "This relationship tracks the many-to-many connection between orders and products"? What if you could ask your database, "Show me everything related to user management," and get intelligent, ranked results?

This isn't science fiction. It's what we're building together in this book.

## The Revolution of Autonomous AI Agents

We're living through a pivotal moment in software development. Large Language Models (LLMs) like GPT-4 have evolved beyond simple text generation‚Äîthey can reason, analyze patterns, and make intelligent inferences about complex systems. When we combine this intelligence with modern vector databases and semantic search, we can create systems that don't just store information‚Äîthey *understand* it.

This book will teach you to build a suite of autonomous AI agents that work together to analyze, document, and make sense of SQL databases. These aren't simple chatbots or basic automation scripts. These are intelligent agents that can:

- **Infer business logic** from table and column names
- **Understand relationships** between entities across your entire database
- **Generate human-readable documentation** that reads like it was written by a senior architect
- **Answer natural language questions** about your data structure
- **Recommend relevant tables** based on what you're trying to accomplish
- **Resume complex analysis tasks** even when dealing with massive enterprise databases

### **Who This Book Is For**

This book is designed for developers, data engineers, and database professionals who want to harness the power of AI for database analysis and documentation. You should be comfortable with:

- **Python programming** (intermediate level)
- **SQL databases** and basic schema concepts
- **Command-line interfaces** and development workflows
- **API integration** and asynchronous programming concepts

You don't need to be an AI expert or have deep machine learning knowledge. We'll teach you everything you need to know about working with LLMs, vector databases, and AI agent frameworks as we build the system together.

### **What Makes This Approach Different**

Most database documentation tools are passive‚Äîthey extract information and display it. The system we're building is *active* and *intelligent*:

**Traditional Approach:**

```markdown
Database ‚Üí Schema Extraction ‚Üí Static Documentation
```

**Our AI-Powered Approach:**

```markdown
Database ‚Üí AI Analysis ‚Üí Semantic Understanding ‚Üí Intelligent Documentation + Search
```

Instead of just telling you that `user_id` is an integer foreign key, our system will tell you: "This links user accounts to their authentication sessions, enabling the system to track multiple concurrent logins per user."

### **The Technical Journey Ahead**

We'll build this system using cutting-edge technologies:

- **OpenAI GPT-4** for intelligent analysis and reasoning
- **ChromaDB** for persistent vector storage and semantic search
- **smolagents** for building autonomous AI agents
- **SQLAlchemy** for cross-database compatibility
- **OpenAI Embeddings** for semantic understanding

But this isn't just a technology showcase. Every chapter builds toward a complete, production-ready system that you can deploy in your organization.

### **Real-World Impact**

The techniques you'll learn have immediate practical applications:

- **Onboarding new developers** becomes dramatically faster when they can ask natural language questions about your database
- **Legacy system understanding** transforms from archaeological expedition to guided tour
- **Database migrations and refactoring** become safer when you understand the true business purpose of every table
- **Compliance and auditing** become manageable when you can instantly find all tables related to customer data or financial transactions

### **How This Book Works**

Each chapter follows a hands-on approach:

1. **Concept Introduction** - We explain what we're building and why
2. **Architecture Design** - We plan the implementation together
3. **Step-by-Step Implementation** - We write every line of code together
4. **Testing and Validation** - We ensure everything works correctly
5. **Real-World Application** - We see how it fits into the larger system

You'll have working code at the end of every chapter, and by the final chapter, you'll have a complete, deployable system.

### **The Code Repository**

All code from this book is available at `github.com/montray-ai/smol-sql-agents`. Each chapter has its own branch, so you can follow along step-by-step or jump to any point in the implementation.

### **Prerequisites and Setup**

Before we begin, ensure you have:

- **Python 3.10+** installed
- **OpenAI API account** with billing set up
- **Access to a SQL database** for testing (we'll provide sample databases)
- **Basic development tools** (git, code editor, terminal)

We'll walk through the complete environment setup in Chapter 1.

### **A Note on AI Ethics and Costs**

Throughout this book, we'll be making calls to OpenAI's API, which incurs costs. We'll teach you cost estimation and optimization techniques, but be aware that experimenting with large databases can generate meaningful API charges. We'll always show you how to estimate costs before proceeding.

Additionally, we'll discuss the ethical implications of AI-powered database analysis, including data privacy considerations and the importance of human oversight in AI-generated documentation.

### **Let's Begin**

Database documentation doesn't have to be a chore. With the right AI agents working autonomously, it can become an intelligent, searchable knowledge base that grows smarter over time.

Let's build the future of database understanding, one agent at a time.

---

*Montray Davis is a software architect and AI researcher specializing in autonomous systems and database intelligence. He has spent over a decade building large-scale data systems and is passionate about making complex databases more accessible through AI.*

---

## Table of Contents

## Part I: Getting Started

### Chapter 1: Introduction to AI-Powered Database Documentation

1.1 The Evolution of Database Documentation

- Limitations of traditional approaches
- The AI revolution in data understanding
- Real-world applications and benefits

1.2 System Overview

- Core components and architecture
- Technology stack
- High-level workflow

1.3 Setting Expectations

- What this tutorial covers
- Prerequisites and requirements
- How to get the most from this guide

---

## Part II: Core Agent Infrastructure

### Chapter 2: Agent Framework

2.1 Core Concepts

- Agent architecture
- Message passing system
- Tool integration framework

2.2 Base Agent Implementation

- Abstract base class
- Lifecycle methods
- Tool registration and management

2.3 Agent Tools

- Tool architecture and design
- Built-in tools overview
- Custom tool development
- Tool execution and error handling

### Chapter 3: Agent Implementation

3.1 Core Analysis Agent

- Schema analysis
- Relationship mapping
- Business context extraction

3.2 Specialized Agents

- Entity recognition agent
- Business logic analyzer
- Batch processing agent

3.3 Agent Toolbox

- Database interaction tools
- Query execution tools
- Documentation generation tools
- Validation tools

3.4 Agent Communication

- Tool execution flow
- Message routing between tools
- Error handling and recovery

---

## Part III: Data Management

### Chapter 4: Database Interaction

4.1 Connection Management

- Connection pooling
- Configuration
- Error handling

4.2 Schema Analysis

- Table and column inspection
- Relationship discovery
- Metadata extraction

4.3 Data Persistence

- State management
- Caching strategies
- Performance considerations

### Chapter 5: Advanced Features

5.1 Concept Matching

- Pattern recognition
- Semantic analysis
- Custom concept definitions

5.2 Performance Optimization

- Query optimization
- Caching strategies
- Resource management

---

## Part IV: Processing & Validation

### Chapter 6: Query Processing Pipeline

6.1 Natural Language Understanding

- Intent recognition
- Entity extraction
- Context management

6.2 SQL Generation

- Query construction
- Parameter binding
- Query optimization

6.3 Execution and Results

- Query execution
- Result processing
- Error handling

### Chapter 7: Validation System

7.1 Query Validation

- Syntax checking
- Semantic validation
- Safety checks

7.2 Business Rule Validation

- Custom validation rules
- Constraint checking
- Data quality validation

7.3 Error Handling

- Error classification
- User-friendly messages
- Recovery strategies

---

## Part V: System Integration

### Chapter 8: System Architecture

8.1 Component Integration

- Service composition
- Message flow
- Error handling

8.2 API Design

- RESTful endpoints
- WebSocket support
- Authentication/Authorization

8.3 Monitoring & Logging

- Metrics collection
- Log aggregation
- Alerting

### Chapter 9: Output & Presentation

9.1 Document Generation

- Template system
- Format support (Markdown, HTML, JSON)
- Custom formatting

9.2 Interactive Features

- Query interface
- Visualization
- Documentation browsing

9.3 Export Capabilities

- Report generation
- Data export
- Integration with other tools

---

## Part VI: Advanced Topics

### Chapter 10: Scaling & Performance

10.1 Horizontal Scaling
    - Load balancing
    - Distributed processing
    - Caching strategies

10.2 Advanced Caching
    - Multi-level caching
    - Cache invalidation
    - Performance tuning

10.3 High Availability
    - Fault tolerance
    - Disaster recovery
    - Backup strategies

### Chapter 11: Security & Compliance

11.1 Data Protection
    - Encryption
    - Access control
    - Audit logging

11.2 Compliance
    - Data privacy regulations
    - Industry standards
    - Audit trails

11.3 Security Best Practices
    - Secure coding
    - Vulnerability scanning
    - Penetration testing

---

## Part VII: Deployment & Operations

### Chapter 12: Deployment Guide

12.1 Environment Setup
    - Prerequisites
    - Configuration
    - Dependencies

12.2 Deployment Options
    - On-premises
    - Cloud deployment
    - Containerization

12.3 Maintenance
    - Upgrades
    - Backup/restore
    - Monitoring

---

## Appendices

### Appendix A: Configuration Reference

- Environment variables
- Configuration files
- Common settings

### Appendix B: Troubleshooting

- Common issues
- Debugging techniques
- Getting help

### Appendix C: Extending the System

- Plugin architecture
- Custom integrations
- Contributing guidelines

### Appendix D: Performance Tuning

- Benchmarking
- Optimization techniques
- Best practices

### Appendix E: Case Studies

- Real-world implementations
- Success stories
- Lessons learned



================================================
FILE: tutorial/01_introduction.md
================================================
# Chapter 1: Introduction to AI-Powered Database Documentation

## 1.1 The Evolution of Database Documentation

### The Documentation Crisis

In today's data-driven world, understanding database schemas is more critical than ever. Yet, traditional documentation methods are failing us because they:

- **Become outdated quickly** as schemas evolve
- **Lack business context** about why tables and columns exist
- **Are expensive to maintain** and often neglected
- **Fail to explain relationships** between different data entities
- **Don't scale** with growing data complexity

### The AI Revolution in Data Understanding

Modern AI, particularly large language models (LLMs), has transformed how we can approach database documentation. Our solution leverages:

- **Semantic Understanding**: AI that comprehends the meaning behind schema elements
- **Automated Analysis**: Intelligent agents that continuously analyze database structures
- **Contextual Documentation**: Dynamic, always-accurate documentation that evolves with your schema
- **Natural Language Interface**: The ability to query your database using plain English

## 1.2 System Overview

### Core Components

Our AI-powered documentation system consists of:

1. **Agent Framework**
   - Autonomous agents for different documentation tasks
   - Tool integration for database interaction
   - Message passing system for agent communication

2. **Data Management Layer**
   - Database connection management
   - Schema analysis and relationship mapping
   - Vector database integration for semantic search

3. **Processing Pipeline**
   - Natural language understanding
   - SQL generation and execution
   - Result processing and formatting

4. **Validation System**
   - Query validation
   - Business rule enforcement
   - Error handling and recovery

### Technology Stack

- **AI/ML**: OpenAI GPT-4 for natural language understanding
- **Vector Database**: ChromaDB for semantic search and storage
- **Agent Framework**: smolagents for building autonomous agents
- **Database Access**: SQLAlchemy for cross-database compatibility
- **Backend**: Python 3.10+ for core functionality
- **Frontend**: Web interface for interactive exploration

## 1.3 What You'll Build

By completing this tutorial, you'll develop a comprehensive system that can:

- **Automatically document** any SQL database schema
- **Answer natural language questions** about your data
- **Generate and execute SQL queries** based on user intent
- **Maintain documentation** that stays in sync with your schema
- **Scale** from small projects to enterprise databases

### Key Features

- **Intelligent Schema Analysis**: Deep understanding of database structures
- **Natural Language Interface**: Query your data in plain English
- **Automated Documentation**: Always up-to-date technical documentation
- **Business Context**: Understand the "why" behind your data
- **Security & Compliance**: Built with enterprise-grade security in mind

## 1.4 Prerequisites

To get the most from this tutorial, you should have:

### Technical Requirements

- Python 3.10 or higher
- pip (Python package manager)
- Git (for version control)
- Access to a terminal/shell

### Knowledge Prerequisites

- Basic Python programming
- Fundamental SQL knowledge
- Familiarity with database concepts
- Basic command line experience

### API Access

- OpenAI API key (for GPT-4 access)
- (Optional) Cloud provider account for deployment

## 1.5 Setting Up Your Environment

Let's set up a clean development environment:

1. **Create a new project directory**:

   ```bash
   mkdir ai-database-docs
   cd ai-database-docs
   ```

2. **Set up a virtual environment**:

   ```bash
   # On Windows
   python -m venv venv
   .\venv\Scripts\activate
   
   # On macOS/Linux
   python3 -m venv venv
   source venv/bin/activate
   ```

3. **Install required packages**:

   ```bash
   pip install openai chromadb sqlalchemy smolagents python-dotenv
   ```

4. **Create a `.env` file** for your API keys:

   ```bash
   OPENAI_API_KEY=your_api_key_here
   DATABASE_URL=your_database_connection_string
   ```

## What's Next?

In the next chapter, we'll dive into setting up the core agent framework that will power our documentation system. You'll learn how to create autonomous agents that can understand and document database schemas with minimal human intervention.



================================================
FILE: tutorial/02_agent_framework.md
================================================
# Chapter 2: Agent Framework

In this chapter, we'll explore the foundation of our AI-powered documentation system - the agent framework. This framework provides the building blocks for creating intelligent agents that can understand, analyze, and document database schemas.

## 2.1 Core Concepts

### Agent Architecture

Our agent framework is built around a flexible, modular architecture that enables different types of agents to work together seamlessly. The architecture consists of:

- **Base Agent**: Abstract base class providing common functionality
- **Specialized Agents**: Task-specific agents inheriting from the base class
- **Mixins**: Reusable components providing additional capabilities
- **Tools**: Modular components that agents can use to perform specific tasks

### Message Passing System

The framework uses a robust message passing system for inter-agent communication:

- **Asynchronous Communication**: Non-blocking message passing between agents
- **Message Queues**: Reliable message delivery with retry mechanisms
- **Event-Driven Architecture**: Agents respond to events and messages
- **Error Handling**: Comprehensive error handling and recovery mechanisms

### Tool Integration Framework

Tools extend agent capabilities in a modular way:

- **Tool Registration**: Dynamic tool discovery and registration
- **Dependency Injection**: Tools can depend on other tools or services
- **Lifecycle Management**: Tools can initialize and clean up resources
- **Access Control**: Fine-grained permissions for tool usage

## 2.2 Base Agent Implementation

### Abstract Base Class

The `BaseAgent` class provides the foundation for all agents in the system:

```python
class BaseAgent(ABC):
    def __init__(self, llm: BaseLLM, tools: List[BaseTool] = None):
        self.llm = llm
        self.tools = {tool.name: tool for tool in (tools or [])}
        self.state = {}
    
    @abstractmethod
    async def process(self, input_data: Any) -> Any:
        """Process input and return result"""
        pass
```

### Lifecycle Methods

Agents implement a standard lifecycle:

1. **Initialization**: Set up resources and dependencies
2. **Processing**: Handle incoming requests and messages
3. **Cleanup**: Release resources when done

Key lifecycle methods:

- `initialize()`: Prepare the agent for processing
- `process()`: Handle the main logic
- `cleanup()`: Release resources
- `handle_error()`: Process and recover from errors

### Tool Registration and Management

Agents can dynamically manage tools:

```python
# Register a new tool
agent.register_tool(tool_instance)

# Get a tool by name
tool = agent.get_tool('tool_name')

# Execute a tool with parameters
result = await agent.execute_tool('tool_name', **params)
```

## 2.3 Agent Tools

### Tool Architecture and Design

Tools follow a consistent design pattern:

- **Input Validation**: Validate input parameters
- **Execution**: Perform the tool's function
- **Result Processing**: Format and return results
- **Error Handling**: Handle and report errors

### Built-in Tools

The framework includes several built-in tools:

1. **Schema Inspector**: Examines database schemas
2. **Document Generator**: Creates documentation from schema information
3. **Query Executor**: Runs database queries
4. **Vector Indexer**: Manages vector embeddings for semantic search

### Custom Tool Development

Creating a custom tool is straightforward:

```python
from typing import Dict, Any
from dataclasses import dataclass
from src.tools.base import BaseTool

@dataclass
class ExampleTool(BaseTool):
    name = "example_tool"
    description = "An example tool that does something useful"
    
    async def execute(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Execute the tool with the given parameters"""
        try:
            # Tool logic here
            result = {"status": "success", "data": "Tool result"}
            return result
        except Exception as e:
            return {"status": "error", "message": str(e)}
```

### Tool Execution and Error Handling

The framework provides robust error handling:

- **Input Validation**: Automatic parameter validation
- **Error Recovery**: Graceful degradation on failure
- **Retry Logic**: Automatic retries for transient failures
- **Logging**: Comprehensive logging of tool execution

## Next Steps

In the next chapter, we'll dive into implementing our first agent and explore how to combine multiple agents to create powerful documentation workflows.


================================================
FILE: tutorial/03_agent_implementation.md
================================================
# Chapter 3: Agent Implementation

In this chapter, we'll dive into the implementation details of our agent system, exploring the core analysis agent, specialized agents, and their interactions.

## 3.1 Core Analysis Agent

The Core Analysis Agent is the central component responsible for database schema analysis and documentation generation.

### Schema Analysis

The agent performs comprehensive schema analysis to understand the database structure:

```python
class SchemaAnalyzer:
    def __init__(self, db_connection):
        self.conn = db_connection
        
    def analyze_schema(self):
        """Analyze database schema and extract metadata."""
        return {
            'tables': self._extract_tables(),
            'relationships': self._extract_relationships(),
            'constraints': self._extract_constraints()
        }
```

Key capabilities:

- Table discovery and metadata extraction
- Relationship mapping between tables
- Constraint analysis
- Data type profiling
- Index analysis

### Relationship Mapping

The agent identifies and documents relationships between tables:

1. **Primary-Foreign Key Relationships**
   - Identifies explicit relationships through foreign key constraints
   - Infers potential relationships through naming conventions
   - Validates relationship integrity

2. **Many-to-Many Relationships**
   - Detects junction tables
   - Maps complex relationships
   - Documents relationship cardinality

### Business Context Extraction

Extracts and documents business context for database objects:

```python
class BusinessContextExtractor:
    def __init__(self, llm_provider):
        self.llm = llm_provider
        
    def extract_context(self, schema_metadata):
        """Extract business context using LLM analysis."""
        prompt = self._build_prompt(schema_metadata)
        return self.llm.generate(prompt)
```

## 3.2 Specialized Agents

### Entity Recognition Agent

Identifies and classifies database entities from natural language queries:

```python
class EntityRecognitionAgent:
    def __init__(self, indexer_agent):
        self.indexer = indexer_agent
        
    def recognize_entities(self, query, max_entities=5):
        """Identify relevant database entities from natural language."""
        # Perform semantic search against indexed documentation
        results = self.indexer.semantic_search(query, top_k=max_entities)
        return self._rank_entities(results, query)
```

### Business Logic Analyzer

Analyzes and documents business rules and logic:

- **Rule Extraction**
  - Identifies business rules from schema constraints
  - Extracts validation rules
  - Documents domain-specific business logic

- **Example Generation**
  - Creates sample data scenarios
  - Documents edge cases
  - Provides usage examples

### Batch Processing Agent

Manages efficient processing of large datasets:

```python
class BatchManager:
    def __init__(self, batch_size=50):
        self.batch_size = batch_size
        
    def process_batch(self, items, process_fn):
        """Process items in optimized batches."""
        results = []
        for i in range(0, len(items), self.batch_size):
            batch = items[i:i + self.batch_size]
            batch_results = process_fn(batch)
            results.extend(batch_results)
        return results
```

## 3.3 Agent Toolbox

### Database Interaction Tools

- **Schema Inspector**: Examines database structure
- **Query Executor**: Runs SQL queries safely
- **Connection Manager**: Handles database connections

### Query Execution Tools

```python
class QueryExecutor:
    def __init__(self, connection_string):
        self.conn = create_connection(connection_string)
        
    def execute_safe(self, query, params=None):
        """Execute query with safety checks."""
        self._validate_query(query)
        return self.conn.execute(query, params or ())
```

### Documentation Generation Tools

- **Markdown Generator**: Creates formatted documentation
- **Diagram Renderer**: Generates ERD diagrams
- **API Documentation**: Creates API specifications

### Validation Tools

```python
class QueryValidator:
    def __init__(self, schema_metadata):
        self.schema = schema_metadata
        
    def validate_query(self, query):
        """Validate query against schema."""
        return {
            'syntax_valid': self._check_syntax(query),
            'tables_exist': self._check_tables(query),
            'columns_exist': self._check_columns(query)
        }
```

## 3.4 Agent Communication

### Tool Execution Flow

1. **Request Handling**
   - Parse incoming request
   - Authenticate and authorize
   - Log request metadata

2. **Tool Selection**
   - Route to appropriate agent
   - Validate input parameters
   - Check permissions

3. **Execution**
   - Execute tool with parameters
   - Monitor execution
   - Handle timeouts and retries

### Message Routing

```python
class MessageRouter:
    def __init__(self, agents):
        self.agents = agents
        
    def route_message(self, message):
        """Route message to appropriate agent."""
        agent = self._select_agent(message)
        return agent.process(message)
```

### Error Handling and Recovery

- **Error Classification**
  - Input validation errors
  - Permission errors
  - Resource constraints
  - External service failures

- **Recovery Strategies**
  - Automatic retries with backoff
  - Circuit breaking
  - Fallback mechanisms
  - Graceful degradation

## Next Steps

In the next chapter, we'll explore how to implement a custom agent and integrate it with the existing system.


================================================
FILE: tutorial/04_vector_database.md
================================================
# Chapter 4: Vector Database Integration with ChromaDB

In this chapter, we'll implement ChromaDB to store and search vector embeddings of our database documentation, enabling semantic search capabilities.

## Understanding Vector Embeddings

Vector embeddings are numerical representations of text that capture semantic meaning. Similar content will have similar vector representations, allowing us to perform semantic searches.

## Setting Up ChromaDB

First, let's create a vector store manager:

```python
# src/vector/chroma_manager.py
import chromadb
from chromadb.config import Settings
from typing import List, Dict, Any, Optional
import os

class ChromaManager:
    def __init__(self, collection_name: str = "database_docs"):
        """Initialize ChromaDB client and collection."""
        self.client = chromadb.Client(Settings(
            chroma_db_impl="duckdb+parquet",
            persist_directory=".chroma_db"
        )
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}  # Optimize for cosine similarity
        )
    
    def add_documents(self, documents: List[Dict[str, Any]]) -> None:
        """Add documents to the vector store."""
        if not documents:
            return
            
        ids = [str(hash(doc["content"])) for doc in documents]
        texts = [doc["content"] for doc in documents]
        metadatas = [doc.get("metadata", {}) for doc in documents]
        
        self.collection.add(
            documents=texts,
            metadatas=metadatas,
            ids=ids
        )
    
    def search(self, query: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """Search for similar documents."""
        results = self.collection.query(
            query_texts=[query],
            n_results=n_results
        )
        
        return [
            {
                "content": doc,
                "metadata": meta,
                "distance": dist
            }
            for doc, meta, dist in zip(
                results["documents"][0],
                results["metadatas"][0],
                results["distances"][0]
            )
        ]
    
    def persist(self) -> None:
        """Persist the vector store to disk."""
        self.client.persist()
```

## Generating Embeddings

Let's create a service to generate embeddings using OpenAI:

```python
# src/vector/embedding_service.py
import openai
from typing import List, Dict, Any
import os

class EmbeddingService:
    def __init__(self, model: str = "text-embedding-3-small"):
        self.model = model
    
    def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for a list of texts."""
        response = openai.Embedding.create(
            input=texts,
            model=self.model
        )
        return [item["embedding"] for item in response["data"]]
    
    def get_embedding(self, text: str) -> List[float]:
        """Generate embedding for a single text."""
        return self.get_embeddings([text])[0]
```

## Document Chunking

For large documents, we need to split them into smaller chunks:

```python
# src/utils/chunking.py
from typing import List, Dict, Any
import re

def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[Dict[str, Any]]:
    """Split text into overlapping chunks."""
    words = text.split()
    chunks = []
    
    for i in range(0, len(words), chunk_size - overlap):
        chunk = ' '.join(words[i:i + chunk_size])
        chunks.append({
            "content": chunk,
            "metadata": {
                "chunk_index": len(chunks),
                "total_chunks": -1  # Will be updated later
            }
        })
    
    # Update total chunks count
    for chunk in chunks:
        chunk["metadata"]["total_chunks"] = len(chunks)
    
    return chunks

def chunk_document(document: Dict[str, Any]) -> List[Dict[str, Any]]:
    """Chunk a document into smaller pieces with metadata."""
    content = document.get("content", "")
    metadata = document.get("metadata", {})
    
    chunks = chunk_text(content)
    
    # Add document metadata to each chunk
    for chunk in chunks:
        chunk["metadata"].update(metadata)
    
    return chunks
```

## Integrating with Database Documentation

Now, let's create a service to handle our documentation vectors:

```python
# src/services/document_service.py
from typing import List, Dict, Any
from src.vector.chroma_manager import ChromaManager
from src.vector.embedding_service import EmbeddingService
from src.utils.chunking import chunk_document

class DocumentService:
    def __init__(self):
        self.vector_store = ChromaManager()
        self.embedding_service = EmbeddingService()
    
    def index_document(self, document: Dict[str, Any]) -> None:
        """Index a single document."""
        chunks = chunk_document(document)
        self.vector_store.add_documents(chunks)
    
    def index_documents(self, documents: List[Dict[str, Any]]) -> None:
        """Index multiple documents."""
        for doc in documents:
            self.index_document(doc)
        self.vector_store.persist()
    
    def search_documents(self, query: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """Search for documents similar to the query."""
        return self.vector_store.search(query, n_results)
    
    def get_relevant_documentation(self, question: str, context: str = "") -> str:
        """Get relevant documentation for a question with optional context."""
        # Enhance query with context if provided
        if context:
            enhanced_query = f"{question} (Context: {context})"
        else:
            enhanced_query = question
        
        results = self.search_documents(enhanced_query)
        
        # Format results
        formatted_results = []
        for i, result in enumerate(results, 1):
            formatted_results.append(
                f"--- Result {i} ---\n"
                f"Content: {result['content']}\n"
                f"Source: {result['metadata'].get('source', 'Unknown')}\n"
                f"Relevance: {1 - result['distance']:.2f}"
            )
        
        return "\n\n".join(formatted_results) if formatted_results \
               else "No relevant documentation found."
```

## Testing the Implementation

Create a test script to verify everything works:

```python
# examples/test_vector_search.py
from src.services.document_service import DocumentService

def main():
    # Initialize document service
    doc_service = DocumentService()
    
    # Sample documentation
    documents = [
        {
            "content": "The users table stores all registered user information including email and hashed passwords.",
            "metadata": {"source": "database_schema", "table": "users"}
        },
        {
            "content": "The orders table contains all purchase orders with references to users and payment status.",
            "metadata": {"source": "database_schema", "table": "orders"}
        },
        {
            "content": "API endpoint: POST /api/orders - Creates a new order with the provided items.",
            "metadata": {"source": "api_docs", "endpoint": "create_order"}
        }
    ]
    
    # Index documents
    print("Indexing documents...")
    doc_service.index_documents(documents)
    
    # Test search
    queries = [
        "How do I create a new order?",
        "Where is user data stored?",
        "Show me order-related tables"
    ]
    
    for query in queries:
        print(f"\nQuery: {query}")
        print("-" * 50)
        results = doc_service.search_documents(query)
        for i, result in enumerate(results, 1):
            print(f"\nResult {i}:")
            print(f"Content: {result['content']}")
            print(f"Source: {result['metadata']}")
            print(f"Relevance: {1 - result['distance']:.2f}")

if __name__ == "__main__":
    main()
```

## Next Steps

In the next chapter, we'll build AI agents that leverage this vector store to answer natural language questions about the database schema.



================================================
FILE: tutorial/05_ai_agents.md
================================================
# Chapter 5: Building AI Agents for Database Documentation

In this chapter, we'll create intelligent agents that can understand and respond to natural language queries about the database using the vector store we set up in the previous chapter.

## Understanding AI Agents

AI agents are autonomous systems that can perceive their environment, process information, and take actions to achieve specific goals. In our case, we'll build agents that can:

1. Understand natural language questions about the database
2. Retrieve relevant documentation using semantic search
3. Generate human-readable responses
4. Chain multiple operations together for complex queries

## The Agent Architecture

We'll implement a multi-agent system with the following components:

1. **Query Understanding Agent**: Interprets the user's intent
2. **Documentation Retrieval Agent**: Finds relevant documentation
3. **Response Generation Agent**: Formulates a natural language response
4. **Orchestrator**: Coordinates between agents

## Implementing the Base Agent

Let's start by creating a base agent class:

```python
# src/agents/base_agent.py
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional
import openai
import json

class BaseAgent(ABC):
    def __init__(self, model: str = "gpt-4"):
        self.model = model
    
    @abstractmethod
    async def process(self, input_data: Any, **kwargs) -> Any:
        """Process the input and return the result."""
        pass
    
    async def generate_completion(
        self,
        system_prompt: str,
        user_prompt: str,
        temperature: float = 0.3,
        max_tokens: int = 1000,
        **kwargs
    ) -> str:
        """Generate a completion using the OpenAI API."""
        try:
            response = await openai.ChatCompletion.acreate(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"Error generating completion: {str(e)}"
    
    def parse_json_response(self, response: str) -> Dict:
        """Parse a JSON response from the model."""
        try:
            # Extract JSON from markdown code block if present
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            else:
                json_str = response.strip()
            
            return json.loads(json_str)
        except json.JSONDecodeError:
            # If parsing fails, try to extract just the JSON part
            try:
                start = response.find('{')
                end = response.rfind('}') + 1
                return json.loads(response[start:end])
            except:
                return {"error": f"Failed to parse JSON response: {response}"}
```

## Implementing the Query Understanding Agent

This agent will analyze the user's question to determine the intent and extract relevant entities.

```python
# src/agents/query_understanding_agent.py
from typing import Dict, Any
from .base_agent import BaseAgent

class QueryUnderstandingAgent(BaseAgent):
    """Agent that understands the intent behind a user's query."""
    
    async def process(self, user_query: str, **kwargs) -> Dict[str, Any]:
        system_prompt = """You are a database expert analyzing user questions. 
        Extract the intent and entities from the following question about a database.
        
        Respond in JSON format with:
        {
            "intent": "search_schema" | "explain_relationship" | "get_example_query" | "other",
            "entities": ["table1", "table2", ...],
            "action_required": "search" | "explain" | "generate_code" | "clarify",
            "certainty_score": 0.0-1.0
        }
        """
        
        response = await self.generate_completion(
            system_prompt=system_prompt,
            user_prompt=user_query,
            temperature=0.1  # Keep it deterministic
        )
        
        result = self.parse_json_response(response)
        result["original_query"] = user_query
        return result
```

## Implementing the Documentation Retrieval Agent

This agent will find the most relevant documentation based on the user's query.

```python
# src/agents/retrieval_agent.py
from typing import List, Dict, Any
from .base_agent import BaseAgent
from src.services.document_service import DocumentService

class RetrievalAgent(BaseAgent):
    """Agent that retrieves relevant documentation."""
    
    def __init__(self, document_service: DocumentService, **kwargs):
        super().__init__(**kwargs)
        self.document_service = document_service
    
    async def process(self, query_analysis: Dict[str, Any], **kwargs) -> Dict[str, Any]:
        """Retrieve relevant documentation based on the query analysis."""
        query = query_analysis.get("original_query", "")
        entities = query_analysis.get("entities", [])
        
        # Enhance query with entities if available
        if entities:
            enhanced_query = f"{query} (Related to: {', '.join(entities)})"
        else:
            enhanced_query = query
        
        # Get relevant documents
        results = self.document_service.search_documents(
            enhanced_query,
            n_results=5
        )
        
        return {
            "query_analysis": query_analysis,
            "retrieved_docs": results,
            "search_query": enhanced_query
        }
```

## Implementing the Response Generation Agent

This agent will generate a natural language response based on the retrieved documentation.

```python
# src/agents/response_agent.py
from typing import Dict, Any, List
from .base_agent import BaseAgent

class ResponseAgent(BaseAgent):
    """Agent that generates natural language responses."""
    
    async def process(self, retrieval_result: Dict[str, Any], **kwargs) -> str:
        """Generate a response based on the retrieved documentation."""
        query = retrieval_result["query_analysis"]["original_query"]
        docs = retrieval_result.get("retrieved_docs", [])
        
        if not docs:
            return "I couldn't find any relevant documentation for your query."
        
        # Format the context
        context = "\n\n".join([
            f"Document {i+1}:\n{doc['content']}\n"
            f"Source: {doc['metadata'].get('source', 'Unknown')}\n"
            f"Relevance: {1 - doc['distance']:.2f}"
            for i, doc in enumerate(docs)
        ])
        
        system_prompt = """You are a helpful database documentation assistant. 
        Answer the user's question based on the provided context.
        Be concise but thorough in your response.
        If you don't know the answer, say so.
        """
        
        user_prompt = f"""Question: {query}
        
        Context:
        {context}
        
        Please provide a clear and helpful response."""
        
        return await self.generate_completion(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            temperature=0.3
        )
```

## The Orchestrator

Now, let's create an orchestrator to manage the flow between agents:

```python
# src/agents/orchestrator.py
from typing import Dict, Any, Optional
from .query_understanding_agent import QueryUnderstandingAgent
from .retrieval_agent import RetrievalAgent
from .response_agent import ResponseAgent
from src.services.document_service import DocumentService

class AgentOrchestrator:
    """Orchestrates the flow between different agents."""
    
    def __init__(self):
        self.document_service = DocumentService()
        self.query_agent = QueryUnderstandingAgent()
        self.retrieval_agent = RetrievalAgent(self.document_service)
        self.response_agent = ResponseAgent()
    
    async def process_query(self, user_query: str) -> Dict[str, Any]:
        """Process a user query through the agent pipeline."""
        # Step 1: Understand the query
        query_analysis = await self.query_agent.process(user_query)
        
        # Step 2: Retrieve relevant documentation
        retrieval_result = await self.retrieval_agent.process(query_analysis)
        
        # Step 3: Generate a response
        response = await self.response_agent.process(retrieval_result)
        
        return {
            "query": user_query,
            "query_analysis": query_analysis,
            "response": response,
            "sources": [
                {
                    "content": doc["content"],
                    "metadata": doc["metadata"],
                    "relevance": 1 - doc["distance"]
                }
                for doc in retrieval_result.get("retrieved_docs", [])
            ]
        }
```

## Testing the Agent System

Let's create a test script to see our agents in action:

```python
# examples/test_agents.py
import asyncio
from src.agents.orchestrator import AgentOrchestrator

async def main():
    # Initialize the orchestrator
    orchestrator = AgentOrchestrator()
    
    # Test queries
    test_queries = [
        "How do I find users who made purchases?",
        "Show me the relationship between orders and products",
        "What tables store customer information?",
        "Generate a SQL query to find top customers"
    ]
    
    for query in test_queries:
        print(f"\n{'='*80}")
        print(f"QUERY: {query}")
        print("-" * 80)
        
        try:
            result = await orchestrator.process_query(query)
            print(f"\nRESPONSE:")
            print(result["response"])
            
            print("\nSOURCES:")
            for i, source in enumerate(result["sources"][:3], 1):
                print(f"\nSource {i} (Relevance: {source['relevance']:.2f}):")
                print(f"Content: {source['content'][:200]}...")
                print(f"Metadata: {source['metadata']}")
                
        except Exception as e:
            print(f"Error processing query: {str(e)}")

if __name__ == "__main__":
    asyncio.run(main())
```

## Next Steps

In the next chapter, we'll enhance our agents with the ability to:

1. Generate and execute SQL queries
2. Handle multi-turn conversations
3. Learn from user feedback
4. Integrate with a web interface

Our agents are now capable of understanding natural language queries and retrieving relevant documentation. The system can be extended with additional agents for specific tasks like query generation, schema modification suggestions, or data analysis.



================================================
FILE: tutorial/06_prompt_engineering.md
================================================
# Chapter 6: Project Prompts Reference

This chapter documents the actual prompts used in the SQL Documentation Agents project. Each section describes a specific prompt, its purpose, and how it's used within the system.

## 1. NL2SQL Query Generation Prompt

**Location** `src/agents/nl2sql.py`  
**Purpose** Generates SQL queries from natural language requests

```python
def _build_query_prompt(self, user_query: str, business_context: Dict, entity_context: Dict) -> str:
    """Build query prompt."""
    schema_info = self._format_schema_info(entity_context.get("table_schemas", {}))
    business_instructions = business_context.get("business_instructions", [])
    
    business_context_str = ""
    if business_instructions:
        business_context_str = "Business context:\n"
        for instruction in business_instructions[:3]:  # Limit to top 3
            business_context_str += f"- {instruction.get('instructions', '')}\n"
    
    return f"""
    Generate T-SQL for the following request: {user_query}
    
    Available schema information:
    {schema_info}
    
    {business_context_str}
    
    Instructions:
    1. Use get_table_schema_unified_tool() to verify column names and table structure
    2. Test your query using execute_query_and_return_results() 
    3. Return the final SQL using final_answer()
    
    Generate clean, efficient T-SQL that answers the user's request.
    """
```

### Key Components

- **User Query** The natural language request from the user
- **Schema Information** Formatted table and column information
- **Business Context** Optional business rules or instructions
- **Tool Integration** Instructions for using available tools

### Usage Flow

1. The system receives a natural language query
2. Relevant schema information is gathered
3. Business context is applied if available
4. The prompt is constructed and sent to the LLM
5. The model generates SQL using the provided tools

## 2. Schema Information Formatter

**Location** `src/agents/nl2sql.py`  
**Purpose** Formats database schema information for use in prompts

```python
def _format_schema_info(self, table_schemas: Dict) -> str:
    """Format schema information for prompt."""
    if not table_schemas:
        return "No schema information available"
    
    schema_lines = []
    for table_name, schema in table_schemas.items():
        columns = schema.get("columns", [])
        column_names = [col.get("name", "") for col in columns if col.get("name")]
        if column_names:
            schema_lines.append(f"{table_name}: {', '.join(column_names)}")
    
    return "\n".join(schema_lines) if schema_lines else "No valid schema information"
```

### Format

```text
table1: column1, column2, column3
table2: columnA, columnB, columnC
```

## 3. SQL Response Extraction

**Location** `src/agents/nl2sql.py`  
**Purpose** Extracts SQL from various response formats

```python
def _extract_sql_from_response(self, response) -> Optional[str]:
    """Extract SQL from agent response."""
    if hasattr(response, 'text'):
        response = response.text
    
    # Handle dictionary response (from final_answer tool)
    if isinstance(response, dict):
        if 'final_sql' in response:
            return response['final_sql']
        elif 'sql' in response:
            return response['sql']
        elif 'query' in response:
            return response['query']
    
    if isinstance(response, str):
        # Extract SQL from code blocks
        import re
        sql_pattern = r'```sql\s*(.*?)\s*```'
        match = re.search(sql_pattern, response, re.DOTALL)
        if match:
            return match.group(1).strip()
        
        # Look for final_answer in response
        final_answer_pattern = r'final_answer\s*\(\s*["\']([^"\']*)["\']'
        match = re.search(final_answer_pattern, response)
        if match:
            return match.group(1).strip()
    
    return None
```

### Supported Formats

1. Dictionary with `final_sql`, `sql`, or `query` keys
2. SQL code blocks (```sql ... ```)
3. `final_answer()` function calls

## 4. Validation Response Format

**Location** `src/agents/nl2sql.py`  
**Purpose** Standardizes validation results

```python
{
    "success": bool,
    "generated_sql": str,
    "validation": {
        "syntax_valid": bool,
        "business_compliant": bool,
        "security_valid": bool,
        "performance_issues": List[str]
    },
    "query_execution": Dict,
    "is_valid": bool
}
```

## 5. Business Compliance Check

**Location** `src/agents/nl2sql.py`  
**Purpose** Validates SQL against business rules

```python
def _check_business_compliance(self, query: str, business_context: Dict) -> Dict:
    """Check business compliance of query."""
    if not business_context:
        return {"valid": True, "message": "No business context provided"}
    
    try:
        result = self.business_validator.validate(query, business_context)
        return {
            "valid": result.get("valid", False),
            "message": result.get("message", ""),
            "issues": result.get("issues", [])
        }
    except Exception as e:
        logger.error(f"Business validation failed: {e}")
        return {"valid": False, "error": str(e)}
```

## 6. Query Execution

**Location** `src/agents/nl2sql.py`  
**Purpose** Executes and formats query results

```python
def _execute_query_impl(self, query: str, max_rows: int = 100) -> Dict:
    """Implementation of query execution."""
    try:
        result = self.database_tools.execute_query_safe(query, max_rows)
        
        if not result.get("success"):
            return {
                "success": False,
                "error": result.get("error", "Unknown error during query execution")
            }
        
        rows = result.get("rows", [])
        columns = result.get("columns", [])
        
        return {
            "success": True,
            "total_rows": len(rows),
            "returned_rows": len(rows),
            "truncated": len(rows) >= max_rows,
            "sample_data": self._create_sample_summary(rows, columns)
        }
    except Exception as e:
        logger.error(f"Query execution failed: {e}")
        return {"success": False, "error": str(e)}
```

## 7. Sample Data Summary

**Location** `src/agents/nl2sql.py`  
**Purpose** Creates a summary of query results for inclusion in responses

```python
def _create_sample_summary(self, rows: List[Dict], columns: List[str]) -> Dict:
    """Create summary of sample data."""
    if not rows or not columns:
        return {"message": "No data returned"}
    
    # Get first few rows as samples
    sample_rows = rows[:5]  # Limit to 5 rows for summary
    
    # Get column statistics
    column_stats = {}
    for col in columns:
        values = [str(row.get(col, "")) for row in rows if col in row]
        col_type = "string"  # Simplified type detection
        if values:
            try:
                _ = float(values[0])
                col_type = "numeric"
            except (ValueError, TypeError):
                pass
        
        column_stats[col] = {
            "type": col_type,
            "non_null_count": len([v for v in values if v is not None and v != ""]),
            "sample_values": list(set(values[:3]))  # Show up to 3 unique values
        }
    
    return {
        "row_count": len(rows),
        "column_count": len(columns),
        "sample_rows": sample_rows,
        "column_stats": column_stats
    }
```

## Next Steps

In the next chapter, we'll explore how these prompts are integrated into the SQL generation workflow and how they interact with the database and business rules.



================================================
FILE: tutorial/06_sql_generation.md
================================================
# Chapter 6: SQL Generation and Execution

In this chapter, we'll enhance our AI agents with the ability to generate, validate, and execute SQL queries based on natural language questions. This will transform our documentation system into an interactive query assistant.

## SQL Generation Architecture

We'll implement a two-phase approach:
1. **Query Planning**: Analyze the question and generate a query plan
2. **Query Generation & Execution**: Convert the plan into executable SQL and run it

## Implementing the SQL Generation Agent

First, let's create a new agent for SQL generation:

```python
# src/agents/sql_generation_agent.py
from typing import Dict, Any, List, Optional
from .base_agent import BaseAgent
import re
import json

class SQLGenerationAgent(BaseAgent):
    """Agent that generates SQL queries from natural language questions."""
    
    async def generate_sql(
        self,
        question: str,
        schema_info: Dict[str, Any],
        examples: List[Dict] = None
    ) -> Dict[str, Any]:
        """Generate SQL query based on the question and schema."""
        # Prepare schema context
        schema_context = "\n".join([
            f"Table: {table_name}\n"
            f"Columns: {', '.join([col['name'] for col in table_info['columns']])}"
            for table_name, table_info in schema_info.items()
        ])
        
        # Prepare examples if provided
        examples_context = ""
        if examples:
            examples_context = "\n\nExamples:\n" + "\n".join([
                f"Q: {ex['question']}\nA: {ex['sql']}"
                for ex in examples
            ])
        
        system_prompt = """You are a SQL expert. Generate a SQL query that answers the user's question.
        
        Follow these rules:
        1. Only generate SELECT queries unless explicitly asked to modify data
        2. Always use table aliases for clarity
        3. Include all necessary JOINs based on foreign key relationships
        4. Use proper SQL syntax for the database type
        5. If the question is ambiguous, make reasonable assumptions and note them
        """
        
        user_prompt = f"""Database Schema:
        {schema_context}
        
        {examples_context}
        
        Question: {question}
        
        Please provide the SQL query in this JSON format:
        {{
            "query": "SELECT ...",
            "tables_used": ["table1", "table2"],
            "assumptions": ["Assumption 1", "Assumption 2"],
            "description": "Brief description of what the query does"
        }}"""
        
        response = await self.generate_completion(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            temperature=0.1  # Keep it deterministic
        )
        
        try:
            result = self.parse_json_response(response)
            # Basic validation
            if not isinstance(result, dict) or 'query' not in result:
                raise ValueError("Invalid response format")
            return result
        except Exception as e:
            return {
                "error": f"Failed to generate SQL: {str(e)}",
                "raw_response": response
            }
    
    def validate_sql(self, sql: str) -> Dict[str, Any]:
        """Basic SQL validation."""
        # Check for common SQL injection patterns
        sql_upper = sql.upper()
        forbidden_patterns = [
            r'\b(DROP|TRUNCATE|DELETE\s+FROM|UPDATE\s+\w+\s+SET|INSERT\s+INTO)\b',
            r';\s*--',
            r'\b(EXEC\s*\(|EXECUTE\s+\w+\s*\()',
            r'\b(UNION\s+SELECT|SELECT\s+.*\bFROM\b.*\bWHERE\s+\d+\s*=\s*\d+)\b'
        ]
        
        for pattern in forbidden_patterns:
            if re.search(pattern, sql_upper, re.IGNORECASE):
                return {
                    "valid": False,
                    "error": f"Query contains potentially dangerous pattern: {pattern}"
                }
        
        return {"valid": True}
```

## Implementing the Query Execution Service

Let's create a service to safely execute SQL queries:

```python
# src/services/query_execution_service.py
from typing import Dict, Any, List, Optional
from sqlalchemy import create_engine, text, exc
from sqlalchemy.engine import Engine
import pandas as pd
import os
import logging

logger = logging.getLogger(__name__)

class QueryExecutionService:
    """Service for safely executing SQL queries."""
    
    def __init__(self, engine: Engine = None):
        self.engine = engine or create_engine(os.getenv("DATABASE_URL"))
        self.query_timeout = int(os.getenv("QUERY_TIMEOUT", "30"))  # seconds
    
    async def execute_query(
        self,
        sql: str,
        params: Optional[Dict] = None,
        limit: int = 100
    ) -> Dict[str, Any]:
        """Execute a SQL query and return the results."""
        if not sql.strip().upper().startswith("SELECT"):
            return {
                "success": False,
                "error": "Only SELECT queries are allowed"
            }
        
        # Add LIMIT if not present
        if "LIMIT" not in sql.upper() and limit > 0:
            sql = f"{sql.rstrip(';')} LIMIT {limit}"
        
        try:
            with self.engine.connect() as connection:
                # Set statement timeout
                connection.execute(text(f"SET statement_timeout = {self.query_timeout * 1000}"))
                
                # Execute query
                result = connection.execute(text(sql), params or {})
                
                # Convert to list of dicts
                columns = list(result.keys())
                rows = [dict(zip(columns, row)) for row in result.fetchall()]
                
                # Get row count without LIMIT (approximate for performance)
                count = len(rows)
                if count == limit:
                    count = connection.execute(
                        text(f"SELECT COUNT(*) FROM ({sql.split('LIMIT')[0]}) as subq")
                    ).scalar()
                
                return {
                    "success": True,
                    "data": rows,
                    "columns": columns,
                    "row_count": count,
                    "limited": len(rows) == limit
                }
                
        except exc.SQLAlchemyError as e:
            logger.error(f"Query execution failed: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "query": sql
            }
    
    async def explain_query(self, sql: str) -> Dict[str, Any]:
        """Get query execution plan."""
        explain_sql = f"EXPLAIN ANALYZE {sql}"
        try:
            with self.engine.connect() as connection:
                result = connection.execute(text(explain_sql))
                plan = "\n".join([row[0] for row in result])
                return {
                    "success": True,
                    "plan": plan
                }
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
```

## Updating the Orchestrator

Let's update our orchestrator to include SQL generation and execution:

```python
# src/agents/orchestrator.py (updated)
from typing import Dict, Any, Optional, List
from .query_understanding_agent import QueryUnderstandingAgent
from .retrieval_agent import RetrievalAgent
from .response_agent import ResponseAgent
from .sql_generation_agent import SQLGenerationAgent
from src.services.document_service import DocumentService
from src.services.query_execution_service import QueryExecutionService
from src.db.schema import get_schema_info

class AgentOrchestrator:
    """Orchestrates the flow between different agents with SQL capabilities."""
    
    def __init__(self):
        self.document_service = DocumentService()
        self.query_agent = QueryUnderstandingAgent()
        self.retrieval_agent = RetrievalAgent(self.document_service)
        self.response_agent = ResponseAgent()
        self.sql_agent = SQLGenerationAgent()
        self.query_service = QueryExecutionService()
        self.schema_info = None
    
    async def get_schema_info(self) -> Dict[str, Any]:
        """Get and cache schema information."""
        if self.schema_info is None:
            # This is a simplified example - in practice, you'd want to format
            # the schema info in a way that's useful for SQL generation
            engine = create_database_engine()
            self.schema_info = get_schema_info(engine)
        return self.schema_info
    
    async def process_query(self, user_query: str, execute_sql: bool = False) -> Dict[str, Any]:
        """Process a user query with optional SQL execution."""
        # Step 1: Understand the query
        query_analysis = await self.query_agent.process(user_query)
        
        # Step 2: Generate SQL if it's a data question
        sql_result = None
        if query_analysis.get("action_required") == "generate_code":
            schema_info = await self.get_schema_info()
            sql_result = await self.sql_agent.generate_sql(
                question=user_query,
                schema_info=schema_info
            )
            
            # If SQL was generated and execution is requested
            if execute_sql and "query" in sql_result:
                execution_result = await self.query_service.execute_query(
                    sql_result["query"]
                )
                sql_result["execution_result"] = execution_result
        
        # Step 3: Retrieve relevant documentation
        retrieval_result = await self.retrieval_agent.process(query_analysis)
        
        # Step 4: Generate a response
        response = await self.response_agent.process({
            "query_analysis": query_analysis,
            "sql_generation": sql_result,
            "retrieval_result": retrieval_result
        })
        
        # Prepare the full result
        result = {
            "query": user_query,
            "query_analysis": query_analysis,
            "response": response,
            "sources": [
                {
                    "content": doc["content"],
                    "metadata": doc["metadata"],
                    "relevance": 1 - doc["distance"]
                }
                for doc in retrieval_result.get("retrieved_docs", [])
            ]
        }
        
        # Add SQL generation results if available
        if sql_result:
            result["sql_generation"] = {
                "query": sql_result.get("query"),
                "tables_used": sql_result.get("tables_used", []),
                "assumptions": sql_result.get("assumptions", []),
                "description": sql_result.get("description", "")
            }
            
            if "execution_result" in sql_result:
                result["execution_result"] = {
                    "success": sql_result["execution_result"]["success"],
                    "row_count": sql_result["execution_result"].get("row_count", 0),
                    "limited": sql_result["execution_result"].get("limited", False)
                }
        
        return result
```

## Testing SQL Generation and Execution

Let's create a test script to see our enhanced system in action:

```python
# examples/test_sql_generation.py
import asyncio
from src.agents.orchestrator import AgentOrchestrator

async def main():
    # Initialize the orchestrator
    orchestrator = AgentOrchestrator()
    
    # Test queries
    test_queries = [
        "Show me the top 5 customers by total spending",
        "List all products that are out of stock",
        "Find orders placed in the last 7 days",
        "What's the average order value by month?"
    ]
    
    for query in test_queries:
        print(f"\n{'='*80}")
        print(f"QUERY: {query}")
        print("-" * 80)
        
        try:
            # First, get the SQL without executing
            result = await orchestrator.process_query(query, execute_sql=False)
            
            print("\nGENERATED SQL:")
            print(result["sql_generation"]["query"])
            
            print("\nASSUMPTIONS:")
            for assumption in result["sql_generation"].get("assumptions", []):
                print(f"- {assumption}")
            
            # Ask user if they want to execute the query
            if input("\nExecute this query? (y/n): ").lower() == 'y':
                result = await orchestrator.process_query(query, execute_sql=True)
                
                if "execution_result" in result and result["execution_result"]["success"]:
                    print("\nQUERY EXECUTED SUCCESSFULLY")
                    print(f"Rows returned: {result['execution_result']['row_count']}")
                    # In a real app, you'd want to display the results in a nice format
                else:
                    print("\nQUERY EXECUTION FAILED")
                    print(result.get("sql_generation", {}).get("error", "Unknown error"))
            
            print("\nRESPONSE:")
            print(result["response"])
            
        except Exception as e:
            print(f"Error processing query: {str(e)}")

if __name__ == "__main__":
    asyncio.run(main())
```

## Security Considerations

When implementing SQL generation and execution, security is paramount:

1. **Query Validation**: Always validate generated SQL before execution
2. **Read-Only Access**: Use a database user with read-only permissions
3. **Query Timeouts**: Set reasonable timeouts to prevent long-running queries
4. **Result Limiting**: Always limit the number of rows returned
5. **Input Sanitization**: Never directly interpolate user input into SQL
6. **Error Handling**: Don't expose database errors directly to users

## Next Steps

In the next chapter, we'll enhance our system with:
1. Multi-turn conversation support
2. Query result visualization
3. User feedback collection
4. Performance optimization techniques

Our system can now understand natural language questions, generate SQL queries, and execute them safely. The next step is to make the interaction more conversational and provide better visualization of the results.



================================================
FILE: web-ui/README.md
================================================
# Flask + React Application

This is a simple full-stack application with a Python Flask backend and React frontend.

## Project Structure

```
web-ui/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ app.py              # Flask API server
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt    # Python dependencies
‚îÇ   ‚îî‚îÄ‚îÄ venv/              # Python virtual environment
‚îî‚îÄ‚îÄ frontend/
    ‚îú‚îÄ‚îÄ src/
    ‚îÇ   ‚îú‚îÄ‚îÄ App.js         # React component that fetches from API
    ‚îÇ   ‚îî‚îÄ‚îÄ ...
    ‚îî‚îÄ‚îÄ package.json       # Node.js dependencies
```

## Setup Instructions

### 1. Backend Setup

1. Navigate to the backend directory:
   ```bash
   cd backend
   ```

2. Activate the virtual environment (if using Windows):
   ```bash
   venv\Scripts\activate
   ```

3. Install Python dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Start the Flask server:
   ```bash
   python app.py
   ```

   The backend will be running at `http://127.0.0.1:5000`

### 2. Frontend Setup

1. Open a new terminal and navigate to the frontend directory:
   ```bash
   cd frontend
   ```

2. Install Node.js dependencies:
   ```bash
   npm install
   ```

3. Start the React development server:
   ```bash
   npm start
   ```

   The frontend will be running at `http://localhost:3000`

## How It Works

- The Flask backend provides an API endpoint at `/api/message` that returns a JSON response
- The React frontend fetches this message when the component mounts and displays it
- CORS is enabled on the backend to allow cross-origin requests from the frontend

## API Endpoints

- `GET /api/message` - Returns a simple greeting message

## Technologies Used

- **Backend**: Python, Flask, Flask-CORS
- **Frontend**: React, JavaScript, HTML, CSS 


================================================
FILE: web-ui/frontend/README.md
================================================
# SQL Agent Frontend

A React-based web interface for the Smol-SQL Agents Suite, providing an intuitive way to interact with AI-powered SQL agents for database documentation, natural language query processing, and SQL generation.

## üöÄ Features

### Core Functionality
- **Natural Language Query Processing**: Convert plain English to SQL using AI agents
- **Database Documentation Explorer**: Browse and search database schema and relationships
- **SQL Generation & Validation**: Generate, validate, and execute SQL queries
- **Entity Recognition**: Identify relevant database entities for your queries
- **Business Context Analysis**: Understand business logic and domain concepts
- **Query Optimization**: Get performance suggestions and optimization tips

### User Interface
- **Splash Screen**: Application initialization with SQL Agent status checking
- **Top Navigation**: Switch between different application pages
- **Documentation Sidebar**: Explore database structure and relationships
- **Query Interface**: Natural language input with comprehensive results display
- **Modal Details**: Detailed information for selected database items

## üìÅ Project Structure

```
src/
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ ui/                    # Basic UI components (SplashScreen, TopNavigation)
‚îÇ   ‚îú‚îÄ‚îÄ query/                 # Query functionality components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ QueryPage.js       # Main query interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SQLAgentStatus.js  # Agent status display
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ QueryInput.js      # Natural language input
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ EntityRecognitionResults.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BusinessContext.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SQLGeneration.js   # Generated SQL display
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ QueryResults.js    # Query execution results
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ OptimizationSuggestions.js
‚îÇ   ‚îú‚îÄ‚îÄ documentation/         # Documentation components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DocumentationSidebar.js
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ItemDetailsModal.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DocumentationExplorer.js
‚îÇ   ‚îú‚îÄ‚îÄ pages/                # Page components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ relationships/     # Relationships page
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ documentation/    # Documentation page
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schema/          # Schema page
‚îÇ   ‚îî‚îÄ‚îÄ documentation-explorer/ # Legacy folder (to be consolidated)
‚îú‚îÄ‚îÄ App.js                    # Main application component
‚îú‚îÄ‚îÄ App.css                   # Application styles
‚îî‚îÄ‚îÄ index.js                  # Application entry point
```

## üõ†Ô∏è Technology Stack

- **React 19.1.1**: Modern React with hooks and functional components
- **Bootstrap 5**: Responsive UI framework with Bootstrap Icons
- **Fetch API**: HTTP requests to backend SQL Agent services
- **CSS3**: Custom styling with responsive design

## üöÄ Getting Started

### Prerequisites

- Node.js (v16 or higher)
- npm or yarn
- Backend SQL Agent services running on `http://127.0.0.1:5000`

### Installation

1. **Install Dependencies**
   ```bash
   npm install
   ```

2. **Start Development Server**
   ```bash
   npm start
   ```

3. **Open Application**
   Navigate to [http://localhost:3000](http://localhost:3000) in your browser

### Available Scripts

- **`npm start`**: Runs the app in development mode
- **`npm test`**: Launches the test runner
- **`npm run build`**: Builds the app for production
- **`npm run eject`**: Ejects from Create React App (one-way operation)

## üîß Configuration

### Backend Connection

The frontend connects to the SQL Agent backend services. Ensure the backend is running and accessible at:

```
http://127.0.0.1:5000/api
```

### Environment Variables

Create a `.env` file in the project root for environment-specific configuration:

```env
REACT_APP_API_BASE_URL=http://127.0.0.1:5000/api
REACT_APP_POLLING_INTERVAL=1000
REACT_APP_MAX_POLLING_ATTEMPTS=30
```

## üì± Application Flow

### 1. Initialization
- **Splash Screen**: Checks SQL Agent availability and initialization status
- **Status Polling**: Monitors backend connectivity with progress indicators
- **Agent Status**: Displays current SQL Agent status and capabilities

### 2. Main Interface
- **Top Navigation**: Switch between Query, Relationships, Schema, and Documentation pages
- **Documentation Sidebar**: Browse database structure with search capabilities
- **Query Interface**: Natural language input with comprehensive results

### 3. Query Processing
- **Natural Language Input**: Type queries in plain English
- **Entity Recognition**: Automatic identification of relevant database entities
- **Business Context**: Domain-specific analysis and concept matching
- **SQL Generation**: AI-powered SQL generation with validation
- **Query Execution**: Test generated SQL with sample data
- **Optimization**: Performance suggestions and improvements

## üîç API Integration

### Backend Endpoints

The frontend integrates with these backend endpoints:

- **`/api/status`**: Check SQL Agent availability and status
- **`/api/query`**: Process natural language queries and generate SQL
- **`/api/schema`**: Retrieve database schema information
- **`/api/documentation/summaries`**: Get documentation summaries
- **`/api/documentation/tables/{table}`**: Get detailed table documentation
- **`/api/documentation/relationships/{id}`**: Get relationship documentation
- **`/api/search`**: Search documentation using semantic search

### Error Handling

- **Connection Errors**: Graceful handling of backend connectivity issues
- **API Errors**: User-friendly error messages for failed requests
- **Timeout Handling**: Automatic retry mechanisms for transient failures
- **Fallback UI**: Degraded functionality when services are unavailable

## üé® UI Components

### Core Components

- **`SplashScreen`**: Application initialization with progress indicators
- **`TopNavigation`**: Main navigation between application pages
- **`DocumentationSidebar`**: Database structure browser with search
- **`QueryPage`**: Main query interface with comprehensive results
- **`ItemDetailsModal`**: Detailed information display for database items

### Query Components

- **`SQLAgentStatus`**: Real-time agent status and capabilities
- **`QueryInput`**: Natural language input with suggestions
- **`EntityRecognitionResults`**: Display of identified database entities
- **`BusinessContext`**: Business logic and domain analysis
- **`SQLGeneration`**: Generated SQL with validation status
- **`QueryResults`**: Query execution results and sample data
- **`OptimizationSuggestions`**: Performance and optimization tips

## üîß Development

### Component Architecture

The application follows a modular component architecture:

- **UI Components**: Reusable interface elements
- **Query Components**: Specialized components for query processing
- **Documentation Components**: Database exploration and documentation
- **Page Components**: Full-page layouts for different sections

### State Management

- **React Hooks**: `useState`, `useEffect`, `useCallback` for state management
- **Component Props**: Data flow through component hierarchy
- **Local Storage**: Persistence of user preferences and settings

### Styling

- **Bootstrap 5**: Responsive grid system and components
- **Custom CSS**: Application-specific styling and animations
- **Bootstrap Icons**: Consistent iconography throughout the interface

## üöÄ Deployment

### Production Build

```bash
npm run build
```

This creates a `build` folder with optimized production files.

### Deployment Options

- **Static Hosting**: Deploy to Netlify, Vercel, or similar services
- **Docker**: Containerize the application for consistent deployment
- **CDN**: Serve static assets through a content delivery network

### Environment Configuration

Configure the backend API URL for production:

```env
REACT_APP_API_BASE_URL=https://your-backend-domain.com/api
```

## ü§ù Contributing

### Development Setup

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

### Code Style

- Follow React best practices
- Use functional components with hooks
- Maintain consistent component structure
- Add appropriate error handling
- Include meaningful comments

## üìö Related Documentation

- [Backend API Documentation](../API_DOCUMENTATION.md)
- [SQL Agent Architecture](../../docs/concepts/architecture.md)
- [Agent Documentation](../../docs/agents/)
- [Database Integration](../../docs/database/)

## üÜò Troubleshooting

### Common Issues

1. **Backend Connection Failed**
   - Ensure the SQL Agent backend is running
   - Check the API base URL configuration
   - Verify network connectivity

2. **Component Loading Issues**
   - Clear browser cache
   - Restart the development server
   - Check for JavaScript console errors

3. **Query Processing Errors**
   - Verify backend agent availability
   - Check query syntax and format
   - Review backend logs for detailed errors

### Debug Mode

Enable debug logging by setting:

```env
REACT_APP_DEBUG=true
```

This will provide detailed console output for troubleshooting.

---

**Built with ‚ù§Ô∏è for the Smol-SQL Agents Suite**



================================================
FILE: web-ui/frontend/package.json
================================================
{
  "name": "frontend",
  "version": "0.1.0",
  "private": true,
  "dependencies": {
    "@testing-library/dom": "^10.4.1",
    "@testing-library/jest-dom": "^6.6.4",
    "@testing-library/react": "^16.3.0",
    "@testing-library/user-event": "^13.5.0",
    "react": "^19.1.1",
    "react-dom": "^19.1.1",
    "react-scripts": "5.0.1",
    "web-vitals": "^2.1.4"
  },
  "scripts": {
    "start": "react-scripts start",
    "build": "react-scripts build",
    "test": "react-scripts test",
    "eject": "react-scripts eject"
  },
  "eslintConfig": {
    "extends": [
      "react-app",
      "react-app/jest"
    ]
  },
  "browserslist": {
    "production": [
      ">0.2%",
      "not dead",
      "not op_mini all"
    ],
    "development": [
      "last 1 chrome version",
      "last 1 firefox version",
      "last 1 safari version"
    ]
  }
}



================================================
FILE: web-ui/frontend/public/index.html
================================================
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" href="%PUBLIC_URL%/favicon.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <meta
      name="description"
      content="SQL Agent Interface"
    />
    <link rel="apple-touch-icon" href="%PUBLIC_URL%/logo192.png" />
    <!--
      manifest.json provides metadata used when your web app is installed on a
      user's mobile device or desktop. See https://developers.google.com/web/fundamentals/web-app-manifest/
    -->
    <link rel="manifest" href="%PUBLIC_URL%/manifest.json" />
    <!--
      Notice the use of %PUBLIC_URL% in the tags above.
      It will be replaced with the URL of the `public` folder during the build.
      Only files inside the `public` folder can be referenced from the HTML.

      Unlike "/favicon.ico" or "favicon.ico", "%PUBLIC_URL%/favicon.ico" will
      work correctly both with client-side routing and a non-root public URL.
      Learn how to configure a non-root public URL by running `npm run build`.
    -->
    <title>SQL Agent Interface</title>
    <!-- Bootstrap 5 CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <!-- Bootstrap Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.0/font/bootstrap-icons.css">
  </head>
  <body>
    <noscript>You need to enable JavaScript to run this app.</noscript>
    <div id="root"></div>
    <!--
      This HTML file is a template.
      If you open it directly in the browser, you will see an empty page.

      You can add webfonts, meta tags, or analytics to this file.
      The build step will place the bundled scripts into the <body> tag.

      To begin the development, run `npm start` or `yarn start`.
      To create a production bundle, use `npm run build` or `yarn build`.
    -->
    <!-- Bootstrap JS Bundle with Popper -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
  </body>
</html>



================================================
FILE: web-ui/frontend/public/manifest.json
================================================
{
  "short_name": "React App",
  "name": "Create React App Sample",
  "icons": [
    {
      "src": "favicon.ico",
      "sizes": "64x64 32x32 24x24 16x16",
      "type": "image/x-icon"
    },
    {
      "src": "logo192.png",
      "type": "image/png",
      "sizes": "192x192"
    },
    {
      "src": "logo512.png",
      "type": "image/png",
      "sizes": "512x512"
    }
  ],
  "start_url": ".",
  "display": "standalone",
  "theme_color": "#000000",
  "background_color": "#ffffff"
}



================================================
FILE: web-ui/frontend/public/robots.txt
================================================
# https://www.robotstxt.org/robotstxt.html
User-agent: *
Disallow:



================================================
FILE: web-ui/frontend/src/App.css
================================================
/* Splash Screen Styles */
.splash-screen {
  position: fixed;
  top: 0;
  left: 0;
  width: 100vw;
  height: 100vh;
  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
  display: flex;
  align-items: center;
  justify-content: center;
  z-index: 9999;
  color: white;
}

.splash-content {
  text-align: center;
  max-width: 500px;
  padding: 2rem;
}

.splash-logo i {
  font-size: 4rem;
  margin-bottom: 1rem;
  display: block;
}

.splash-logo h1 {
  font-size: 2.5rem;
  font-weight: 700;
  margin-bottom: 0.5rem;
}

.splash-logo p {
  font-size: 1.1rem;
  opacity: 0.9;
  margin-bottom: 2rem;
}

.splash-status {
  margin: 2rem 0;
}

.splash-status h3 {
  font-size: 1.2rem;
  margin-bottom: 1rem;
}

.splash-footer {
  margin-top: 2rem;
  opacity: 0.7;
}

/* Additional CSS for improved Documentation Explorer */

/* Query Input Button Improvements */
.btn-info {
  background-color: #17a2b8;
  border-color: #17a2b8;
  color: white;
}

.btn-info:hover:not(:disabled) {
  background-color: #138496;
  border-color: #117a8b;
  color: white;
}

.btn-info:disabled {
  background-color: #6c757d;
  border-color: #6c757d;
  opacity: 0.8;
}

/* Spinner improvements */
.spinner-border-sm {
  border-width: 0.125em;
}

/* Input group button styling */
.input-group .btn {
  border-left: none;
  box-shadow: none;
}

.input-group .btn:focus {
  box-shadow: 0 0 0 0.2rem rgba(23, 162, 184, 0.25);
}

/* Documentation tree improvements */
.list-group-item {
  transition: background-color 0.2s ease;
}

.list-group-item:hover {
  background-color: #f8f9fa !important;
}

/* Folder icons with better styling */
.bi-folder2,
.bi-folder2-open {
  color: #6c757d;
  transition: color 0.2s ease;
}

.list-group-item:hover .bi-folder2,
.list-group-item:hover .bi-folder2-open {
  color: #495057;
}

/* Search results styling */
.documentation-search-result {
  border-left: 3px solid transparent;
  transition: border-left-color 0.2s ease;
}

.documentation-search-result:hover {
  border-left-color: #007bff;
}

/* Table and relationship items */
.documentation-item {
  padding: 0.375rem 0.75rem;
  border-radius: 0.25rem;
  margin: 0.125rem 0;
}

.documentation-item:hover {
  background-color: #e9ecef;
  cursor: pointer;
}

/* Badge improvements */
.badge.bg-light {
  border: 1px solid #dee2e6;
}

/* Modal improvements for item details */
.modal-body .table {
  font-size: 0.875rem;
}

.modal-body .table th {
  background-color: #f8f9fa;
  font-weight: 600;
  border-top: none;
}

.modal-body .table code {
  background-color: #f8f9fa;
  padding: 0.125rem 0.25rem;
  border-radius: 0.25rem;
  font-size: 0.8rem;
}

/* Key indicators */
.bi-key-fill {
  font-size: 0.75rem;
}

/* Quick actions button group */
.modal-body .d-grid .btn {
  text-align: left;
}

.modal-body .d-grid .btn i {
  width: 1rem;
}

/* Metadata list styling */
.modal-body .list-unstyled li {
  padding: 0.125rem 0;
  border-bottom: 1px solid #f8f9fa;
}

.modal-body .list-unstyled li:last-child {
  border-bottom: none;
}

/* Loading states */
.documentation-loading {
  text-align: center;
  padding: 2rem;
  color: #6c757d;
}

.documentation-loading .spinner-border {
  margin-bottom: 1rem;
}

/* Error states */
.documentation-error {
  color: #dc3545;
  background-color: #f8d7da;
  border: 1px solid #f5c6cb;
  padding: 0.75rem;
  border-radius: 0.25rem;
  margin: 0.5rem 0;
}

.documentation-error i {
  margin-right: 0.5rem;
}

/* Empty states */
.documentation-empty {
  text-align: center;
  padding: 1.5rem;
  color: #6c757d;
  font-style: italic;
}

.documentation-empty i {
  font-size: 2rem;
  margin-bottom: 0.5rem;
  display: block;
  opacity: 0.5;
}

/* Search input improvements */
.documentation-search {
  position: relative;
}

.documentation-search .form-control:focus {
  border-color: #007bff;
  box-shadow: 0 0 0 0.2rem rgba(0, 123, 255, 0.25);
}

/* Tab improvements */
.nav-tabs .nav-link {
  color: #6c757d;
  font-size: 0.875rem;
  padding: 0.5rem 0.75rem;
}

.nav-tabs .nav-link.active {
  color: #007bff;
  font-weight: 600;
}

/* Relationship type badges */
.relationship-type-badge {
  font-size: 0.75rem;
  padding: 0.25rem 0.5rem;
}

/* Column type badges */
.column-type-badge {
  font-size: 0.7rem;
  font-family: 'Courier New', monospace;
}

/* Relevance score styling */
.relevance-score {
  font-size: 0.7rem;
  font-weight: 600;
  min-width: 35px;
  text-align: center;
}

/* Improved responsiveness */
@media (max-width: 768px) {
  .documentation-tree {
    font-size: 0.875rem;
  }
  
  .modal-dialog {
    margin: 0.5rem;
    max-width: calc(100% - 1rem);
  }
  
  .modal-body .row {
    flex-direction: column;
  }
  
  .modal-body .col-md-4 {
    margin-top: 1rem;
    border-top: 1px solid #dee2e6;
    padding-top: 1rem;
  }
}

/* Animation for expanding/collapsing folders */
.folder-content {
  transition: max-height 0.3s ease;
  overflow: hidden;
}

.folder-content.collapsed {
  max-height: 0;
}

.folder-content.expanded {
  max-height: 1000px; /* Adjust as needed */
}

/* Highlight effect for selected items */
.documentation-item.selected {
  background-color: #cce5ff;
  border-left: 3px solid #007bff;
}

/* Better scrolling for long lists */
.documentation-tree {
  max-height: 600px;
  overflow-y: auto;
}

/* Custom scrollbar for documentation tree */
.documentation-tree::-webkit-scrollbar {
  width: 6px;
}

.documentation-tree::-webkit-scrollbar-track {
  background: #f8f9fa;
}

.documentation-tree::-webkit-scrollbar-thumb {
  background: #dee2e6;
  border-radius: 3px;
}

.documentation-tree::-webkit-scrollbar-thumb:hover {
  background: #adb5bd;
}

/* Fix for icon alignment */
.list-group-item i {
  vertical-align: middle;
}


================================================
FILE: web-ui/frontend/src/App.js
================================================
import React, { useState, useEffect, useCallback } from 'react';
import './App.css';
import DocumentationExplorer from './components/documentation-explorer/DocumentationExplorer';
import RelationshipsPage from './components/pages/relationships';
import DocumentationPage from './components/pages/documentation';
import SchemaPage from './components/pages/schema';
import {
  SplashScreen,
  TopNavigation,
  DocumentationSidebar,
  QueryPage,
  ItemDetailsModal
} from './components';

function App() {
  const [message, setMessage] = useState('');
  const [query, setQuery] = useState('');
  const [generatedSql, setGeneratedSql] = useState('-- Your generated SQL will appear here');
  const [results, setResults] = useState([]);
  const [isLoading, setIsLoading] = useState(false);
  const [activeTab, setActiveTab] = useState('text-search');
  const [searchQuery, setSearchQuery] = useState('');

  // New state for comprehensive SQL Agent results
  const [sqlAgentStatus, setSqlAgentStatus] = useState(null);
  const [pipelineResults, setPipelineResults] = useState(null);
  const [entityRecognition, setEntityRecognition] = useState(null);
  const [businessContext, setBusinessContext] = useState(null);
  const [sqlValidation, setSqlValidation] = useState(null);
  const [optimizationSuggestions, setOptimizationSuggestions] = useState([]);
  const [queryExecution, setQueryExecution] = useState(null);

  // Splash screen state
  const [showSplash, setShowSplash] = useState(true);
  const [splashStatus, setSplashStatus] = useState('checking');
  const [splashMessage, setSplashMessage] = useState('Initializing SQL Agents...');
  const [splashProgress, setSplashProgress] = useState(0);
  const [pollingAttempts, setPollingAttempts] = useState(0);

  // Documentation explorer state
  const [documentationData, setDocumentationData] = useState({
    tables: [],
    relationships: [],
    searchResults: [],
    expandedFolders: new Set(['tables', 'relationships']),
    isLoading: false
  });
  const [searchResults, setSearchResults] = useState([]);
  const [isSearching, setIsSearching] = useState(false);
  const [currentPage, setCurrentPage] = useState('query'); // Add navigation state

  // Modal state for item details
  const [showItemDetails, setShowItemDetails] = useState(false);
  const [selectedItem, setSelectedItem] = useState(null);

  const initializeApp = useCallback(async () => {
    setSplashStatus('checking');
    setSplashMessage('Checking SQL Agent availability...');
    setSplashProgress(20);

    // Start polling for connection
    const pollForConnection = async () => {
      let attempts = 0;
      const maxAttempts = 30; // 30 seconds max
      const pollInterval = 1000; // 1 second intervals

      while (attempts < maxAttempts) {
        setPollingAttempts(attempts + 1);

        try {
          const response = await fetch('http://127.0.0.1:5000/api/status');
          if (response.ok) {
            const status = await response.json();
            setSqlAgentStatus(status);
            setSplashProgress(50);
            setSplashMessage('SQL Agent status received...');

            // Check if SQL Agents are available and initialized
            if (status.sql_agents_available && status.initialized) {
              setSplashProgress(80);
              setSplashMessage('SQL Agents ready! Loading application...');

              // Simulate loading time for better UX
              await new Promise(resolve => setTimeout(resolve, 1000));

              setSplashProgress(100);
              setSplashMessage('Application ready!');

              // Hide splash screen
              setTimeout(() => {
                setShowSplash(false);
              }, 500);
              return; // Success - exit polling
            } else {
              setSplashMessage(`SQL Agents not ready (attempt ${attempts + 1}/${maxAttempts})...`);
            }
          } else {
            setSplashMessage(`Backend responding but not ready (attempt ${attempts + 1}/${maxAttempts})...`);
          }
        } catch (error) {
          console.error(`Connection attempt ${attempts + 1} failed:`, error);
          setSplashMessage(`Connecting to backend... (attempt ${attempts + 1}/${maxAttempts})`);
        }

        attempts++;
        setSplashProgress(20 + (attempts / maxAttempts) * 30); // Progress from 20% to 50%

        // Wait before next attempt
        await new Promise(resolve => setTimeout(resolve, pollInterval));
      }

      // If we get here, max attempts reached
      setSplashStatus('error');
      setSplashMessage('Cannot connect to backend server after multiple attempts');
      setSplashProgress(100);
    };

    // Start polling
    pollForConnection();
  }, []);

  const loadDocumentationData = useCallback(async () => {
    setDocumentationData(prev => ({ ...prev, isLoading: true }));
    try {
      console.log('Loading documentation data...');

      // Load summaries first
      const summariesResponse = await fetch('http://127.0.0.1:5000/api/documentation/summaries');
      let hasSummaries = false;

      if (summariesResponse.ok) {
        const summariesData = await summariesResponse.json();
        console.log('Summaries API response:', summariesData);
        if (summariesData.success && summariesData.summaries && Object.keys(summariesData.summaries).length > 0) {
          // Extract tables and relationships from summaries
          const tables = [];
          const relationships = [];

          Object.values(summariesData.summaries).forEach(item => {
            if (item.type === 'table') {
              tables.push({
                name: item.name,
                business_purpose: item.business_purpose,
                documentation: item.documentation,
                status: item.status
              });
            } else if (item.type === 'relationship') {
              relationships.push({
                id: item.id.replace('relationship_', ''),
                constrained_table: item.constrained_table,
                referred_table: item.referred_table,
                relationship_type: item.relationship_type,
                documentation: item.documentation,
                status: item.status
              });
            }
          });

          if (tables.length > 0 || relationships.length > 0) {
            hasSummaries = true;
            setDocumentationData(prev => ({
              ...prev,
              tables: tables,
              relationships: relationships,
              isLoading: false
            }));
          }
        }
      }

      // If no summaries or summaries are empty, fallback to schema endpoint
      if (!hasSummaries) {
        console.log('No summaries available, falling back to schema endpoint...');
        const schemaResponse = await fetch('http://127.0.0.1:5000/api/schema');
        if (schemaResponse.ok) {
          const schemaData = await schemaResponse.json();
          console.log('Schema API response:', schemaData);
          if (schemaData.success) {
            console.log('Tables received:', schemaData.tables);
            setDocumentationData(prev => ({
              ...prev,
              tables: schemaData.tables || [],
              isLoading: false
            }));
          } else {
            console.error('Schema API returned error:', schemaData.error);
            setDocumentationData(prev => ({ ...prev, isLoading: false }));
          }
        } else {
          throw new Error(`HTTP ${schemaResponse.status}: ${schemaResponse.statusText}`);
        }

        // Load relationships
        const relationshipsResponse = await fetch('http://127.0.0.1:5000/api/documentation/relationships');
        if (relationshipsResponse.ok) {
          const relationshipsData = await relationshipsResponse.json();
          if (relationshipsData.success) {
            setDocumentationData(prev => ({
              ...prev,
              relationships: relationshipsData.relationships ? Object.values(relationshipsData.relationships) : []
            }));
          }
        }
      }
    } catch (error) {
      console.error('Error loading documentation data:', error);
      setDocumentationData(prev => ({ ...prev, isLoading: false, tables: [] }));
    }
  }, []);

  // Initialize app on mount
  useEffect(() => {
    initializeApp();
  }, [initializeApp]);

  // Load documentation data when app is ready
  useEffect(() => {
    if (!showSplash && sqlAgentStatus?.sql_agents_available) {
      loadDocumentationData();
    }
  }, [showSplash, sqlAgentStatus, loadDocumentationData]);

  // Remove the keyboard event listener that was interfering with modal handling
  // The DocumentationExplorer component handles its own modal interactions

  const executeQuery = useCallback(async () => {
    console.log('executeQuery called with query:', query);
    if (!query.trim()) return;

    setIsLoading(true);

    // Reset all results
    setPipelineResults(null);
    setEntityRecognition(null);
    setBusinessContext(null);
    setSqlValidation(null);
    setOptimizationSuggestions([]);
    setQueryExecution(null);

    try {
      console.log('Making API call to /api/query with query:', query.trim());
      const response = await fetch('http://127.0.0.1:5000/api/query', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ query: query.trim() })
      });

      console.log('API response status:', response.status);
      if (!response.ok) {
        throw new Error('Network response was not ok');
      }

      const data = await response.json();
      
      console.log('API Response - data:', data);
      console.log('API Response - data.results:', data.results);
      console.log('API Response - data.pipeline_results:', data.pipeline_results);
      console.log('API Response - data.pipeline_results.sql_generation:', data.pipeline_results?.sql_generation);

      // Always set the SQL and results, regardless of success status
      setGeneratedSql(data.sql);
      
      // Extract results from the correct location in the response
      let extractedResults = data.results;
      if (data.pipeline_results?.sql_generation?.validation?.execution?.sample_data?.sample_rows) {
        extractedResults = data.pipeline_results.sql_generation.validation.execution.sample_data.sample_rows;
      }
      
      setResults(extractedResults);
      
      console.log('Setting results to:', extractedResults);

      // Extract comprehensive pipeline results if available
      if (data.pipeline_results) {
        setPipelineResults(data.pipeline_results);

        // Extract entity recognition results
        const entityRecognition = data.pipeline_results.entity_recognition;
        if (entityRecognition) {
          setEntityRecognition(entityRecognition);
        }

        // Extract business context results
        const businessContext = data.pipeline_results.business_context;
        if (businessContext) {
          setBusinessContext(businessContext);
        }

        // Extract SQL generation results
        const sqlGeneration = data.pipeline_results.sql_generation;
        if (sqlGeneration) {
          setSqlValidation(sqlGeneration.validation);
          setOptimizationSuggestions(sqlGeneration.optimization_suggestions || []);
          
          // Extract query execution data from the correct location
          const queryExecData = sqlGeneration.validation?.execution || sqlGeneration.query_execution;
          setQueryExecution(queryExecData);
        }
      }

      // Show appropriate message based on success status
      if (data.success) {
        console.log('‚úÖ SQL Agent pipeline completed successfully');
        console.log('Pipeline Results:', data.pipeline_results);
      } else {
        console.log('‚ÑπÔ∏è SQL Agent pipeline returned error');
        console.log('Error:', data.error);
      }
    } catch (error) {
      console.error('Error executing query:', error);
      // Set empty results on error
      setGeneratedSql('-- Error: Could not execute query');
      setResults([]);
    } finally {
      setIsLoading(false);
    }
  }, [query, setGeneratedSql, setResults, setPipelineResults, setEntityRecognition, setBusinessContext, setSqlValidation, setOptimizationSuggestions, setQueryExecution, setIsLoading]);

  const copySqlToClipboard = async () => {
    try {
      await navigator.clipboard.writeText(generatedSql);
      // You could add a toast notification here
    } catch (err) {
      console.error('Failed to copy text: ', err);
    }
  };

  const handleKeyPress = (e) => {
    if (e.key === 'Enter') {
      executeQuery();
    }
  };

  const refreshDocumentation = () => {
    loadDocumentationData();
  };

  const handleItemSelect = (item) => {
    setSelectedItem(item);
    setShowItemDetails(true);
  };

  const checkSQLAgentStatus = useCallback(async () => {
    try {
      const response = await fetch('http://127.0.0.1:5000/api/status');
      if (response.ok) {
        const status = await response.json();
        setSqlAgentStatus(status);
        console.log('SQL Agent Status:', status);

        // Show status in the UI
        if (status.sql_agents_available) {
          console.log('‚úÖ SQL Agents are available!');
          if (status.initialized) {
            console.log('‚úÖ SQL Agents are initialized!');
            console.log('Features available:', status.features);
            console.log('Agents status:', status.agents);
          } else {
            console.log('‚ö†Ô∏è SQL Agents are available but not initialized');
          }
        } else {
          console.log('‚ùå SQL Agents are not available');
        }

        return status;
      }
    } catch (error) {
      console.error('Error checking SQL Agent status:', error);
    }
    return null;
  }, []);

  // Check SQL Agent status on mount
  useEffect(() => {
    checkSQLAgentStatus();
  }, [checkSQLAgentStatus]);

  // Debug: Monitor results state changes
  useEffect(() => {
    console.log('App - results state changed:', results);
  }, [results]);

  // Fetch initial message
  useEffect(() => {
    // Fetch the message from the backend API
    fetch('http://127.0.0.1:5000/api/message')
      .then(res => res.json())
      .then(data => setMessage(data.message))
      .catch(err => console.error('Error fetching message:', err));
  }, [setMessage]);

  const retryInitialization = useCallback(() => {
    setShowSplash(true);
    setSplashStatus('checking');
    setSplashProgress(0);
    setPollingAttempts(0);
    setSplashMessage('Retrying connection...');
    initializeApp();
  }, [initializeApp]);



  // Show splash screen if not ready
  if (showSplash) {
    return (
      <SplashScreen
        splashStatus={splashStatus}
        splashMessage={splashMessage}
        splashProgress={splashProgress}
        pollingAttempts={pollingAttempts}
        retryInitialization={retryInitialization}
      />
    );
  }

  // Main application (existing code)
  return (
    <div className="App">
      {/* Top Navigation */}
      <TopNavigation currentPage={currentPage} setCurrentPage={setCurrentPage} />

      <div className="container-fluid mt-3">
        <div className="row g-3">
          {/* Left Sidebar */}
          <DocumentationSidebar
            documentationData={documentationData}
            setDocumentationData={setDocumentationData}
            searchQuery={searchQuery}
            setSearchQuery={setSearchQuery}
            activeTab={activeTab}
            setActiveTab={setActiveTab}
            searchResults={searchResults}
            setSearchResults={setSearchResults}
            isSearching={isSearching}
            setIsSearching={setIsSearching}
            refreshDocumentation={refreshDocumentation}
            onItemSelect={handleItemSelect}
          />

          {/* Main Content */}
          <div className="col-md-9">
            {/* Page-specific content based on currentPage */}
            {currentPage === 'query' && (
              <QueryPage
                sqlAgentStatus={sqlAgentStatus}
                query={query}
                setQuery={setQuery}
                executeQuery={executeQuery}
                isLoading={isLoading}
                handleKeyPress={handleKeyPress}
                entityRecognition={entityRecognition}
                businessContext={businessContext}
                sqlValidation={sqlValidation}
                generatedSql={generatedSql}
                copySqlToClipboard={copySqlToClipboard}
                queryExecution={queryExecution}
                optimizationSuggestions={optimizationSuggestions}
                results={results}
              />
            )}

            {/* Relationships Page */}
            {currentPage === 'relationships' && (
              <RelationshipsPage />
            )}

            {/* Schema Page */}
            {currentPage === 'schema' && (
              <SchemaPage />
            )}

            {/* Documentation Page */}
            {currentPage === 'documentation' && (
              <DocumentationPage />
            )}
          </div>
        </div>
      </div>

      {/* Item Details Modal - Rendered in main view */}
      <ItemDetailsModal
        showItemDetails={showItemDetails}
        setShowItemDetails={setShowItemDetails}
        selectedItem={selectedItem}
      />
    </div>
  );
}

export default App;


================================================
FILE: web-ui/frontend/src/App.test.js
================================================
import { render, screen } from '@testing-library/react';
import App from './App';

test('renders learn react link', () => {
  render(<App />);
  const linkElement = screen.getByText(/learn react/i);
  expect(linkElement).toBeInTheDocument();
});



================================================
FILE: web-ui/frontend/src/index.css
================================================
body {
  margin: 0;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',
    'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',
    sans-serif;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

code {
  font-family: source-code-pro, Menlo, Monaco, Consolas, 'Courier New',
    monospace;
}



================================================
FILE: web-ui/frontend/src/index.js
================================================
import React from 'react';
import ReactDOM from 'react-dom/client';
import './index.css';
import App from './App';
import reportWebVitals from './reportWebVitals';

const root = ReactDOM.createRoot(document.getElementById('root'));
root.render(
  <React.StrictMode>
    <App />
  </React.StrictMode>
);

// If you want to start measuring performance in your app, pass a function
// to log results (for example: reportWebVitals(console.log))
// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals
reportWebVitals();



================================================
FILE: web-ui/frontend/src/reportWebVitals.js
================================================
const reportWebVitals = onPerfEntry => {
  if (onPerfEntry && onPerfEntry instanceof Function) {
    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {
      getCLS(onPerfEntry);
      getFID(onPerfEntry);
      getFCP(onPerfEntry);
      getLCP(onPerfEntry);
      getTTFB(onPerfEntry);
    });
  }
};

export default reportWebVitals;



================================================
FILE: web-ui/frontend/src/setupTests.js
================================================
// jest-dom adds custom jest matchers for asserting on DOM nodes.
// allows you to do things like:
// expect(element).toHaveTextContent(/react/i)
// learn more: https://github.com/testing-library/jest-dom
import '@testing-library/jest-dom';



================================================
FILE: web-ui/frontend/src/components/index.js
================================================
// UI components
export { SplashScreen, TopNavigation } from './ui';

// Query-related components
export { 
  QueryPage,
  SQLAgentStatus,
  QueryInput,
  EntityRecognitionResults,
  BusinessContext,
  SQLGeneration,
  QueryResults,
  OptimizationSuggestions
} from './query';

// Documentation components
export { DocumentationSidebar, ItemDetailsModal } from './documentation'; 


================================================
FILE: web-ui/frontend/src/components/documentation/DocumentationSidebar.js
================================================
import React from 'react';
import DocumentationExplorer from '../documentation-explorer/DocumentationExplorer';

const DocumentationSidebar = ({
  documentationData,
  setDocumentationData,
  searchQuery,
  setSearchQuery,
  activeTab,
  setActiveTab,
  searchResults,
  setSearchResults,
  isSearching,
  setIsSearching,
  refreshDocumentation,
  onItemSelect
}) => {
  return (
    <div className="col-md-3">
      <div className="card h-100">
        <div className="card-header bg-white d-flex justify-content-between align-items-center">
          <h5 className="mb-0">Documentation Explorer</h5>
          <div>
            <button
              className="btn btn-sm btn-outline-secondary"
              title="Refresh"
              onClick={refreshDocumentation}
              disabled={documentationData.isLoading}
            >
              <i className={`bi bi-arrow-clockwise ${documentationData.isLoading ? 'spinner-border spinner-border-sm' : ''}`}></i>
            </button>
          </div>
        </div>
        <div className="card-body p-0">
          <DocumentationExplorer
            documentationData={documentationData}
            setDocumentationData={setDocumentationData}
            searchQuery={searchQuery}
            setSearchQuery={setSearchQuery}
            activeTab={activeTab}
            setActiveTab={setActiveTab}
            searchResults={searchResults}
            setSearchResults={setSearchResults}
            isSearching={isSearching}
            setIsSearching={setIsSearching}
            onRefresh={refreshDocumentation}
            onItemSelect={onItemSelect}
          />
        </div>
      </div>
    </div>
  );
};

export default DocumentationSidebar; 


================================================
FILE: web-ui/frontend/src/components/documentation/index.js
================================================
export { default as DocumentationSidebar } from './DocumentationSidebar';
export { default as ItemDetailsModal } from './ItemDetailsModal'; 


================================================
FILE: web-ui/frontend/src/components/documentation/ItemDetailsModal.css
================================================
.modal {
  z-index: 1050;
  position: fixed;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
}

.modal-backdrop {
  z-index: 1040;
  position: fixed;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  background-color: rgba(0, 0, 0, 0.5);
}

.modal-dialog {
  max-width: 1200px;
  max-height: 90vh;
  margin: 1.75rem auto;
  position: relative;
  pointer-events: auto;
}

.modal-content {
  border-radius: 0.5rem;
  box-shadow: 0 0.5rem 1rem rgba(0, 0, 0, 0.15);
  position: relative;
  background-color: #fff;
  border: 1px solid rgba(0, 0, 0, 0.2);
}

.modal-header {
  border-bottom: 1px solid #dee2e6;
  background-color: #f8f9fa;
  padding: 1rem 1.5rem;
  border-radius: 0.5rem 0.5rem 0 0;
}

.modal-title {
  font-weight: 600;
  color: #495057;
  margin: 0;
  line-height: 1.5;
}

.modal-body {
  max-height: 75vh;
  overflow-y: auto;
  padding: 1.5rem;
  position: relative;
}

.modal-footer {
  border-top: 1px solid #dee2e6;
  background-color: #f8f9fa;
  padding: 1rem 1.5rem;
  border-radius: 0 0 0.5rem 0.5rem;
}

.table-responsive {
  border-radius: 0.375rem;
  overflow: hidden;
  border: 1px solid #dee2e6;
}

.table {
  margin-bottom: 0;
  width: 100%;
}

.table th {
  background-color: #f8f9fa;
  border-bottom: 2px solid #dee2e6;
  font-weight: 600;
  font-size: 0.875rem;
  text-transform: uppercase;
  letter-spacing: 0.5px;
  font-size: 0.75rem;
  padding: 0.75rem 0.5rem;
  vertical-align: middle;
}

.table td {
  vertical-align: middle;
  font-size: 0.875rem;
  padding: 0.75rem 0.5rem;
  border-bottom: 1px solid #dee2e6;
}

.table-striped > tbody > tr:nth-of-type(odd) > td {
  background-color: rgba(0, 0, 0, 0.02);
}

.table-warning {
  background-color: rgba(255, 193, 7, 0.1) !important;
}

.badge {
  font-size: 0.75em;
  font-weight: 500;
  padding: 0.375rem 0.75rem;
}

.text-center .h4 {
  font-weight: 600;
  margin-bottom: 0.25rem;
}

.text-center small {
  font-size: 0.75rem;
}

/* Scrollbar styling for modal body */
.modal-body::-webkit-scrollbar {
  width: 6px;
}

.modal-body::-webkit-scrollbar-track {
  background: #f1f1f1;
  border-radius: 3px;
}

.modal-body::-webkit-scrollbar-thumb {
  background: #c1c1c1;
  border-radius: 3px;
}

.modal-body::-webkit-scrollbar-thumb:hover {
  background: #a8a8a8;
}

/* Prevent body scroll when modal is open */
body.modal-open {
  overflow: hidden;
  padding-right: 0;
}

/* Animation improvements */
.modal.fade {
  transition: opacity 0.15s linear;
}

.modal.fade.show {
  opacity: 1;
}

.modal-backdrop.fade {
  transition: opacity 0.15s linear;
}

.modal-backdrop.fade.show {
  opacity: 1;
}

/* Focus management */
.modal:focus {
  outline: none;
}

.modal-dialog:focus {
  outline: none;
}

/* Alert styling */
.alert {
  border-radius: 0.375rem;
  padding: 1rem;
  margin-bottom: 1rem;
  border: 1px solid transparent;
}

.alert-warning {
  color: #664d03;
  background-color: #fff3cd;
  border-color: #ffecb5;
}

.alert-danger {
  color: #721c24;
  background-color: #f8d7da;
  border-color: #f5c6cb;
}

/* Button improvements */
.btn-close {
  background: transparent;
  border: 0;
  font-size: 1.5rem;
  font-weight: 700;
  line-height: 1;
  color: #000;
  opacity: 0.5;
  padding: 0;
  margin: 0;
  cursor: pointer;
}

.btn-close:hover {
  opacity: 0.75;
}

/* Responsive improvements */
@media (max-width: 768px) {
  .modal-dialog {
    margin: 0.5rem;
    max-width: calc(100% - 1rem);
  }
  
  .modal-body {
    padding: 1rem;
  }
  
  .modal-header,
  .modal-footer {
    padding: 0.75rem 1rem;
  }
}

/* Prevent text selection during modal interactions */
.modal * {
  user-select: none;
}

.modal input,
.modal textarea,
.modal pre,
.modal code {
  user-select: text;
} 


================================================
FILE: web-ui/frontend/src/components/documentation/ItemDetailsModal.js
================================================
import React, { useEffect, useCallback } from 'react';
import './ItemDetailsModal.css';

const ItemDetailsModal = ({
  showItemDetails,
  setShowItemDetails,
  selectedItem
}) => {
  // Prevent body scroll when modal is open
  useEffect(() => {
    if (showItemDetails) {
      document.body.style.overflow = 'hidden';
    } else {
      document.body.style.overflow = 'unset';
    }

    return () => {
      document.body.style.overflow = 'unset';
    };
  }, [showItemDetails]);

  // Memoized close handler to prevent unnecessary re-renders
  const handleClose = useCallback(() => {
    setShowItemDetails(false);
  }, [setShowItemDetails]);

  // Memoized backdrop click handler
  const handleBackdropClick = useCallback((e) => {
    if (e.target === e.currentTarget) {
      handleClose();
    }
  }, [handleClose]);

  // Early return if modal should not be shown
  if (!showItemDetails || !selectedItem) {
    return null;
  }

  const renderTableDetails = () => {
    try {
      const table = selectedItem.table;
      if (!table) {
        return (
          <div className="alert alert-warning">
            <i className="bi bi-exclamation-triangle me-2"></i>
            <strong>No table data available</strong>
          </div>
        );
      }

      // Safely extract schema data with proper structure handling
      let columns = [];
      
      // Handle the actual API response structure
      if (table.columns && Array.isArray(table.columns)) {
        // Primary structure: table.columns (direct array)
        columns = table.columns.filter(col => col != null);
      } else if (table.schema && table.schema.columns && Array.isArray(table.schema.columns)) {
        // Secondary structure: table.schema.columns
        columns = table.schema.columns.filter(col => col != null);
      } else if (table.schema_data && table.schema_data.columns && table.schema_data.columns.columns && Array.isArray(table.schema_data.columns.columns)) {
        // Nested structure: table.schema_data.columns.columns (double nested)
        columns = table.schema_data.columns.columns.filter(col => col != null);
      } else if (table.schema_data && table.schema_data.columns && Array.isArray(table.schema_data.columns)) {
        // Alternative structure: table.schema_data.columns
        columns = table.schema_data.columns.filter(col => col != null);
      } else if (Array.isArray(table.schema)) {
        // Direct array structure: table.schema
        columns = table.schema.filter(col => col != null);
      }
      
      // Ensure columns is always an array and filter out null/undefined values
      if (!Array.isArray(columns)) {
        columns = [];
      }
      
      // Calculate statistics with safe property access
      const columnCount = columns.length;
      const primaryKeys = columns.filter(col => col && (col.primary_key || col.is_primary_key)).length;
      const foreignKeys = columns.filter(col => col && (col.foreign_key || col.is_foreign_key)).length;
      const notNullColumns = columns.filter(col => col && col.nullable === false).length;

      return (
        <div>
          <h5 className="mb-3">Table Details</h5>
          
          {/* Business Purpose */}
          {table.business_purpose && (
            <div className="mb-3">
              <h6>Business Purpose</h6>
              <p className="text-muted">{table.business_purpose}</p>
            </div>
          )}

          {/* Documentation */}
          {table.documentation && (
            <div className="mb-3">
              <h6>Documentation</h6>
              <div className="bg-light p-3 rounded">
                <pre className="mb-0" style={{ whiteSpace: 'pre-wrap', fontSize: '0.875rem' }}>
                  {table.documentation}
                </pre>
              </div>
            </div>
          )}

          {/* Statistics Cards */}
          <div className="row mb-4">
            <div className="col-md-3">
              <div className="text-center p-3 bg-primary bg-opacity-10 rounded">
                <div className="h3 text-primary">{columnCount}</div>
                <small className="text-muted">Columns</small>
              </div>
            </div>
            <div className="col-md-3">
              <div className="text-center p-3 bg-warning bg-opacity-10 rounded">
                <div className="h3 text-warning">{primaryKeys}</div>
                <small className="text-muted">Primary Keys</small>
              </div>
            </div>
            <div className="col-md-3">
              <div className="text-center p-3 bg-info bg-opacity-10 rounded">
                <div className="h3 text-info">{foreignKeys}</div>
                <small className="text-muted">Foreign Keys</small>
              </div>
            </div>
            <div className="col-md-3">
              <div className="text-center p-3 bg-success bg-opacity-10 rounded">
                <div className="h3 text-success">{notNullColumns}</div>
                <small className="text-muted">Not Null</small>
              </div>
            </div>
          </div>

          {/* Schema Information - Database IDE Style */}
          {columns.length > 0 && (
            <div className="mb-3">
              <h6>Schema Definition</h6>
              <div className="table-responsive">
                <table className="table table-sm table-striped">
                  <thead className="table-dark">
                    <tr>
                      <th style={{ width: '25%' }}>Column Name</th>
                      <th style={{ width: '20%' }}>Data Type</th>
                      <th style={{ width: '15%' }}>Nullable</th>
                      <th style={{ width: '15%' }}>Default</th>
                      <th style={{ width: '25%' }}>Constraints</th>
                    </tr>
                  </thead>
                  <tbody>
                    {columns.map((column, index) => {
                      if (!column) return null;
                      
                      const isPrimaryKey = column.primary_key || column.is_primary_key;
                      const isForeignKey = column.foreign_key || column.is_foreign_key;
                      
                      return (
                        <tr key={`${column.name || index}-${index}`} className={isPrimaryKey ? 'table-warning' : ''}>
                          <td>
                            <strong>{column.name || 'Unknown'}</strong>
                            {isPrimaryKey && (
                              <span className="badge bg-warning ms-1">PK</span>
                            )}
                            {isForeignKey && (
                              <span className="badge bg-info ms-1">FK</span>
                            )}
                          </td>
                          <td>
                            <code className="text-primary">{column.type || 'Unknown'}</code>
                          </td>
                          <td>
                            {column.nullable ? (
                              <span className="badge bg-success">NULL</span>
                            ) : (
                              <span className="badge bg-danger">NOT NULL</span>
                            )}
                          </td>
                          <td>
                            {column.default ? (
                              <code className="text-muted">{column.default}</code>
                            ) : (
                              <span className="text-muted">-</span>
                            )}
                          </td>
                          <td>
                            <div className="d-flex flex-wrap gap-1">
                              {isPrimaryKey && <span className="badge bg-warning">Primary Key</span>}
                              {isForeignKey && <span className="badge bg-info">Foreign Key</span>}
                              {column.unique && <span className="badge bg-secondary">Unique</span>}
                              {column.auto_increment && <span className="badge bg-dark">Auto Increment</span>}
                            </div>
                          </td>
                        </tr>
                      );
                    })}
                  </tbody>
                </table>
              </div>
            </div>
          )}

          {/* No Schema Data Message */}
          {columns.length === 0 && (
            <div className="mb-3">
              <div className="alert alert-warning">
                <i className="bi bi-exclamation-triangle me-2"></i>
                <strong>No schema data available</strong>
                <br />
                <small className="text-muted">
                  The table schema information could not be loaded. This might be because:
                  <ul className="mt-2 mb-0">
                    <li>The table hasn't been processed by the documentation system</li>
                    <li>The schema data is stored in a different format</li>
                    <li>There was an error loading the schema information</li>
                  </ul>
                </small>
              </div>
            </div>
          )}

          {/* Foreign Key Relationships */}
          {foreignKeys > 0 && (
            <div className="mb-3">
              <h6>Foreign Key Relationships</h6>
              <div className="table-responsive">
                <table className="table table-sm">
                  <thead className="table-light">
                    <tr>
                      <th>Column</th>
                      <th>References</th>
                      <th>On Delete</th>
                      <th>On Update</th>
                    </tr>
                  </thead>
                  <tbody>
                    {columns
                      .filter(col => col && (col.foreign_key || col.is_foreign_key))
                      .map((column, index) => (
                        <tr key={`fk-${column.name || index}-${index}`}>
                          <td><code>{column.name || 'Unknown'}</code></td>
                          <td>
                            {column.referenced_table && column.referenced_column ? (
                              <span>
                                <code>{column.referenced_table}</code>.<code>{column.referenced_column}</code>
                              </span>
                            ) : (
                              <span className="text-muted">Unknown</span>
                            )}
                          </td>
                          <td>
                            <span className="badge bg-secondary">
                              {column.on_delete || 'RESTRICT'}
                            </span>
                          </td>
                          <td>
                            <span className="badge bg-secondary">
                              {column.on_update || 'RESTRICT'}
                            </span>
                          </td>
                        </tr>
                      ))}
                  </tbody>
                </table>
              </div>
            </div>
          )}

          {/* Indexes */}
          {table.schema && table.schema.indexes && Array.isArray(table.schema.indexes) && table.schema.indexes.length > 0 && (
            <div className="mb-3">
              <h6>Indexes</h6>
              <div className="table-responsive">
                <table className="table table-sm">
                  <thead className="table-light">
                    <tr>
                      <th>Index Name</th>
                      <th>Columns</th>
                      <th>Type</th>
                      <th>Unique</th>
                    </tr>
                  </thead>
                  <tbody>
                    {table.schema.indexes.map((index, indexIndex) => (
                      <tr key={`index-${index.name || indexIndex}-${indexIndex}`}>
                        <td><code>{index.name || 'Unknown'}</code></td>
                        <td>
                          {index.columns && Array.isArray(index.columns) ? (
                            index.columns.map(col => (
                              <span key={col} className="badge bg-light text-dark me-1">
                                {col}
                              </span>
                            ))
                          ) : (
                            <span className="text-muted">No columns</span>
                          )}
                        </td>
                        <td>
                          <span className="badge bg-info">{index.type || 'BTREE'}</span>
                        </td>
                        <td>
                          {index.unique ? (
                            <span className="badge bg-success">Yes</span>
                          ) : (
                            <span className="badge bg-secondary">No</span>
                          )}
                        </td>
                      </tr>
                    ))}
                  </tbody>
                </table>
              </div>
            </div>
          )}
        </div>
      );
    } catch (error) {
      console.error('Error rendering table details:', error);
      return (
        <div className="alert alert-danger">
          <i className="bi bi-exclamation-triangle me-2"></i>
          <strong>Error loading table details</strong>
          <br />
          <small className="text-muted">An error occurred while rendering the table information.</small>
        </div>
      );
    }
  };

  const renderRelationshipDetails = () => {
    try {
      const relationship = selectedItem.relationship;
      if (!relationship) {
        return (
          <div className="alert alert-warning">
            <i className="bi bi-exclamation-triangle me-2"></i>
            <strong>No relationship data available</strong>
          </div>
        );
      }

      return (
        <div>
          <h5 className="mb-3">Relationship Details</h5>
          
          {/* Relationship Type */}
          <div className="mb-3">
            <h6>Type</h6>
            <span className="badge bg-primary">{relationship.relationship_type || 'Unknown'}</span>
          </div>

          {/* Constrained Table */}
          <div className="mb-3">
            <h6>Constrained Table</h6>
            <p className="mb-1">
              <strong>{relationship.constrained_table}</strong>
            </p>
            {relationship.constrained_columns && (
              <div>
                <small className="text-muted">Columns: {relationship.constrained_columns.join(', ')}</small>
              </div>
            )}
          </div>

          {/* Referred Table */}
          <div className="mb-3">
            <h6>Referred Table</h6>
            <p className="mb-1">
              <strong>{relationship.referred_table}</strong>
            </p>
            {relationship.referred_columns && (
              <div>
                <small className="text-muted">Columns: {relationship.referred_columns.join(', ')}</small>
              </div>
            )}
          </div>

          {/* Documentation */}
          {relationship.documentation && (
            <div className="mb-3">
              <h6>Documentation</h6>
              <p className="text-muted">{relationship.documentation}</p>
            </div>
          )}
        </div>
      );
    } catch (error) {
      console.error('Error rendering relationship details:', error);
      return (
        <div className="alert alert-danger">
          <i className="bi bi-exclamation-triangle me-2"></i>
          <strong>Error loading relationship details</strong>
          <br />
          <small className="text-muted">An error occurred while rendering the relationship information.</small>
        </div>
      );
    }
  };

  return (
    <>
      <div className="modal fade show" style={{ display: 'block' }} tabIndex="-1">
        <div className="modal-dialog modal-xl">
          <div className="modal-content">
            <div className="modal-header">
              <h5 className="modal-title">
                <i className={`bi ${selectedItem.type === 'table' ? 'bi-table' : 'bi-arrow-right'} me-2`}></i>
                {selectedItem.name || 'Unknown Item'}
              </h5>
              <button
                type="button"
                className="btn-close"
                onClick={handleClose}
                aria-label="Close"
              ></button>
            </div>
            <div className="modal-body">
              {selectedItem.description && (
                <div className="mb-3">
                  <h6>Description</h6>
                  <p className="text-muted">{selectedItem.description}</p>
                </div>
              )}
              
              {selectedItem.type === 'table' ? renderTableDetails() : renderRelationshipDetails()}
            </div>
            <div className="modal-footer">
              <button type="button" className="btn btn-secondary" onClick={handleClose}>
                Close
              </button>
            </div>
          </div>
        </div>
      </div>
      <div className="modal-backdrop fade show" onClick={handleBackdropClick}></div>
    </>
  );
};

export default ItemDetailsModal; 


================================================
FILE: web-ui/frontend/src/components/documentation-explorer/DocumentationExplorer.css
================================================
.documentation-explorer {
  height: 100%;
  display: flex;
  flex-direction: column;
  background-color: #ffffff;
}

.explorer-header {
  background-color: #f8f9fa;
  border-bottom: 1px solid #dee2e6;
}

.search-container {
  background-color: #f8f9fa;
  border-bottom: 1px solid #dee2e6;
}

.statistics-panel {
  background-color: #ffffff;
  border-bottom: 1px solid #dee2e6;
}

.stat-item {
  text-align: center;
  padding: 0.5rem;
  background-color: #f8f9fa;
  border-radius: 0.375rem;
  border: 1px solid #e9ecef;
}

.stat-number {
  font-size: 1.5rem;
  font-weight: 700;
  color: #0d6efd;
  line-height: 1;
}

.stat-label {
  font-size: 0.75rem;
  color: #6c757d;
  text-transform: uppercase;
  letter-spacing: 0.5px;
  margin-top: 0.25rem;
}

.search-results {
  background-color: #fff3cd;
  border-bottom: 1px solid #ffeaa7;
}

.documentation-tree {
  flex: 1;
  overflow-y: auto;
  min-height: 300px;
}

.quick-actions-panel {
  background-color: #f8f9fa;
  border-top: 1px solid #dee2e6;
  margin-top: auto;
}

.folder-section {
  border-bottom: 1px solid #dee2e6;
}

.folder-section:last-child {
  border-bottom: none;
}

.folder-content {
  background-color: #f8f9fa;
  border-top: 1px solid #dee2e6;
}

.list-group-item {
  border: none;
  border-radius: 0;
  transition: background-color 0.2s;
}

.list-group-item:hover {
  background-color: #e9ecef;
}

.list-group-item:active {
  background-color: #dee2e6;
}

.folder-section .list-group-item {
  background-color: #ffffff;
  border-bottom: 1px solid #dee2e6;
}

.folder-content .list-group-item {
  background-color: #f8f9fa;
  border-bottom: 1px solid #e9ecef;
}

.folder-content .list-group-item:hover {
  background-color: #e9ecef;
}

.badge {
  font-size: 0.75em;
}

.text-end .badge {
  margin-left: 0.25rem;
}

/* Quick Actions Styling */
.quick-actions-panel .btn {
  font-size: 0.875rem;
  padding: 0.375rem 0.75rem;
}

.quick-actions-panel .btn i {
  font-size: 0.875rem;
}

/* Statistics Panel Styling */
.statistics-panel .row {
  margin: 0;
}

.statistics-panel .col-6 {
  padding: 0 0.25rem;
}

/* Header Styling */
.explorer-header h6 {
  color: #495057;
  font-weight: 600;
}

.explorer-header .btn {
  padding: 0.25rem 0.5rem;
  font-size: 0.875rem;
}

/* Scrollbar styling */
.documentation-tree::-webkit-scrollbar {
  width: 6px;
}

.documentation-tree::-webkit-scrollbar-track {
  background: #f1f1f1;
}

.documentation-tree::-webkit-scrollbar-thumb {
  background: #c1c1c1;
  border-radius: 3px;
}

.documentation-tree::-webkit-scrollbar-thumb:hover {
  background: #a8a8a8;
}

/* Responsive Design */
@media (max-width: 768px) {
  .statistics-panel .row {
    margin: 0 -0.25rem;
  }
  
  .statistics-panel .col-6 {
    padding: 0 0.25rem;
  }
  
  .stat-number {
    font-size: 1.25rem;
  }
  
  .stat-label {
    font-size: 0.7rem;
  }
}

/* Animation for collapsible panels */
.statistics-panel,
.quick-actions-panel {
  transition: all 0.3s ease-in-out;
}

/* Hover effects for stat items */
.stat-item:hover {
  background-color: #e9ecef;
  transform: translateY(-1px);
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

/* Button hover effects */
.quick-actions-panel .btn:hover {
  transform: translateY(-1px);
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

/* Focus states */
.explorer-header .btn:focus,
.quick-actions-panel .btn:focus {
  box-shadow: 0 0 0 0.2rem rgba(13, 110, 253, 0.25);
}

/* Empty state styling */
.documentation-tree:empty::after {
  content: "No documentation available";
  display: block;
  text-align: center;
  color: #6c757d;
  padding: 2rem;
  font-style: italic;
}

/* Loading state */
.documentation-explorer.loading {
  opacity: 0.7;
  pointer-events: none;
}

/* Search results styling */
.search-results .list-group-item {
  border-left: 3px solid #ffc107;
  margin-left: 0;
  margin-right: 0;
}

.search-results .list-group-item:hover {
  border-left-color: #ffca2c;
  background-color: #fff8e1;
} 


================================================
FILE: web-ui/frontend/src/components/documentation-explorer/DocumentationExplorer.js
================================================
import React, { useState } from 'react';
import './DocumentationExplorer.css';

const DocumentationExplorer = ({
  documentationData,
  setDocumentationData,
  searchQuery,
  setSearchQuery,
  activeTab,
  setActiveTab,
  searchResults,
  setSearchResults,
  isSearching,
  setIsSearching,
  onRefresh,
  onItemSelect
}) => {
  const [showStatistics, setShowStatistics] = useState(true);
  const [showQuickActions, setShowQuickActions] = useState(true);

  const handleSearch = async () => {
    if (!searchQuery.trim()) {
      setSearchResults([]);
      return;
    }

    setIsSearching(true);
    try {
      const response = await fetch('http://127.0.0.1:5000/api/search', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          query: searchQuery,
          type: 'text'
        })
      });

      if (response.ok) {
        const data = await response.json();
        setSearchResults(data.results || []);
      } else {
        console.error('Search failed:', response.statusText);
        setSearchResults([]);
      }
    } catch (error) {
      console.error('Error during search:', error);
      setSearchResults([]);
    } finally {
      setIsSearching(false);
    }
  };

  const handleSearchKeyPress = (e) => {
    if (e.key === 'Enter') {
      handleSearch();
    }
  };

  const handleItemSelect = async (item) => {
    try {
      let detailedItem = { ...item };
      
      // Fetch detailed information based on item type
      if (item.type === 'table') {
        // First try to get documentation data
        let hasDocumentationData = false;
        try {
          const response = await fetch(`http://127.0.0.1:5000/api/documentation/tables/${item.name}`);
          if (response.ok) {
            const data = await response.json();
            if (data.success && data.table) {
              detailedItem = {
                ...item,
                table: data.table
              };
              hasDocumentationData = true;
            }
          }
        } catch (error) {
          // Documentation endpoint failed, will fall back to schema endpoint
        }
        
        // Always fetch schema data as fallback or supplement
        if (!hasDocumentationData || !detailedItem.table.schema_data || Object.keys(detailedItem.table.schema_data).length === 0) {
          const schemaResponse = await fetch('http://127.0.0.1:5000/api/schema');
          if (schemaResponse.ok) {
            const schemaData = await schemaResponse.json();
            if (schemaData.success && schemaData.tables) {
              const tableSchema = schemaData.tables.find(t => t.name === item.name);
              if (tableSchema) {
                if (!hasDocumentationData) {
                  // Create a basic table structure if no documentation data exists
                  detailedItem = {
                    ...item,
                    table: {
                      table_name: item.name,
                      name: item.name,
                      business_purpose: '',
                      documentation: '',
                      schema_data: tableSchema,
                      schema: tableSchema,
                      columns: tableSchema.columns || []
                    }
                  };
                } else {
                  // Supplement existing documentation data with schema
                  detailedItem.table = {
                    ...detailedItem.table,
                    schema_data: tableSchema,
                    schema: tableSchema,
                    columns: tableSchema.columns || []
                  };
                }
              }
            }
          }
        }
      } else if (item.type === 'relationship') {
        const relationshipId = item.id.replace('relationship_', '');
        const response = await fetch(`http://127.0.0.1:5000/api/documentation/relationships/${relationshipId}`);
        if (response.ok) {
          const data = await response.json();
          if (data.success && data.relationship) {
            detailedItem = {
              ...item,
              relationship: data.relationship
            };
          }
        }
      }
      
      onItemSelect(detailedItem);
    } catch (error) {
      console.error('Error fetching detailed item information:', error);
      // Fallback to basic item
      onItemSelect(item);
    }
  };

  const toggleFolder = (folderName) => {
    const newExpanded = new Set(documentationData.expandedFolders);
    if (newExpanded.has(folderName)) {
      newExpanded.delete(folderName);
    } else {
      newExpanded.add(folderName);
    }
    setDocumentationData(prev => ({
      ...prev,
      expandedFolders: newExpanded
    }));
  };

  const refreshDocumentation = () => {
    if (onRefresh) {
      onRefresh();
    }
  };

  // Calculate statistics
  const totalTables = documentationData.tables.length;
  const totalRelationships = documentationData.relationships.length;
  const tablesWithDocumentation = documentationData.tables.filter(t => t.business_purpose).length;
  const relationshipsWithDocumentation = documentationData.relationships.filter(r => r.documentation).length;
  const totalColumns = documentationData.tables.reduce((sum, table) => sum + (table.columns?.length || 0), 0);

  return (
    <div className="documentation-explorer">
      {/* Header */}
      <div className="explorer-header p-3 border-bottom">
        <div className="d-flex justify-content-between align-items-center">
          <h6 className="mb-0 fw-bold">
            <i className="bi bi-journal-text me-2"></i>
            Documentation Explorer
          </h6>
          <button
            className="btn btn-sm btn-outline-secondary"
            onClick={refreshDocumentation}
            title="Refresh documentation"
          >
            <i className="bi bi-arrow-clockwise"></i>
          </button>
        </div>
      </div>

      {/* Search Bar */}
      <div className="search-container p-3 border-bottom">
        <div className="input-group">
          <input
            type="text"
            className="form-control"
            placeholder="Search documentation..."
            value={searchQuery}
            onChange={(e) => setSearchQuery(e.target.value)}
            onKeyPress={handleSearchKeyPress} />
          <button
            className="btn btn-outline-secondary"
            type="button"
            onClick={handleSearch}
            disabled={isSearching}
          >
            {isSearching ? (
              <i className="bi bi-arrow-clockwise spinner-border spinner-border-sm"></i>
            ) : (
              <i className="bi bi-search"></i>
            )}
          </button>
        </div>
      </div>

      {/* Statistics Panel */}
      {showStatistics && (
        <div className="statistics-panel p-3 border-bottom">
          <div className="d-flex justify-content-between align-items-center mb-2">
            <h6 className="mb-0 fw-bold">
              <i className="bi bi-graph-up me-1"></i>
              Overview
            </h6>
            <button
              className="btn btn-sm btn-outline-secondary"
              onClick={() => setShowStatistics(!showStatistics)}
            >
              <i className={`bi ${showStatistics ? 'bi-chevron-up' : 'bi-chevron-down'}`}></i>
            </button>
          </div>
          <div className="row g-2">
            <div className="col-6">
              <div className="stat-item">
                <div className="stat-number">{totalTables}</div>
                <div className="stat-label">Tables</div>
              </div>
            </div>
            <div className="col-6">
              <div className="stat-item">
                <div className="stat-number">{totalRelationships}</div>
                <div className="stat-label">Relationships</div>
              </div>
            </div>
            <div className="col-6">
              <div className="stat-item">
                <div className="stat-number">{totalColumns}</div>
                <div className="stat-label">Columns</div>
              </div>
            </div>
            <div className="col-6">
              <div className="stat-item">
                <div className="stat-number">{tablesWithDocumentation}</div>
                <div className="stat-label">Documented</div>
              </div>
            </div>
          </div>
        </div>
      )}

      {/* Search Results */}
      {searchResults.length > 0 && (
        <div className="search-results p-3 border-bottom">
          <h6 className="mb-2">
            <i className="bi bi-search me-1"></i>
            Search Results ({searchResults.length})
          </h6>
          {searchResults.map((result, index) => (
            <div
              key={index}
              className="list-group-item py-1 px-3 border-0"
              style={{ cursor: 'pointer' }}
              onClick={() => handleItemSelect(result)}
            >
              <div className="d-flex align-items-center">
                <i className="bi bi-file-text me-2 text-muted"></i>
                <div className="flex-grow-1">
                  <span className="small fw-medium">{result.name || result.table_name}</span>
                  <div className="small text-muted">{result.description || result.business_purpose}</div>
                </div>
                <span className="badge bg-primary">{result.score?.toFixed(2) || 'N/A'}</span>
              </div>
            </div>
          ))}
        </div>
      )}

      {/* Documentation Tree */}
      <div className="documentation-tree">
        {/* Tables Section */}
        <div className="folder-section">
          <div
            className="list-group-item py-2"
            style={{ cursor: 'pointer' }}
            onClick={() => toggleFolder('tables')}
          >
            <div className="d-flex justify-content-between align-items-center">
              <span>
                <i className={`bi ${documentationData.expandedFolders.has('tables') ? 'bi-folder-fill' : 'bi-folder'} me-2`}></i>
                Tables
              </span>
              <span className="badge bg-light text-dark">{documentationData.tables.length}</span>
            </div>
          </div>
          
          {documentationData.expandedFolders.has('tables') && (
            <div className="folder-content">
              {documentationData.tables.length > 0 ? (
                documentationData.tables.map((table, index) => {
                  return (
                    <div
                      key={table.name || `table-${index}`}
                      className="list-group-item py-1 px-3 border-0"
                      style={{ cursor: 'pointer' }}
                      onClick={() => handleItemSelect({
                        id: `table-${table.name || index}`,
                        name: table.name || `Table ${index + 1}`,
                        type: 'table',
                        description: table.business_purpose || `Table containing ${table.columns?.length || 0} columns`,
                        table: table
                      })}
                    >
                      <div className="d-flex align-items-center">
                        <i className="bi bi-table me-2 text-muted"></i>
                        <div className="flex-grow-1">
                          <span className="small fw-medium">
                            {table.name || `Table ${index + 1}`}
                          </span>
                          {table.business_purpose && (
                            <div className="small text-muted" style={{ fontSize: '0.75rem', lineHeight: '1.2' }}>
                              {table.business_purpose.length > 60 
                                ? `${table.business_purpose.substring(0, 60)}...` 
                                : table.business_purpose}
                            </div>
                          )}
                          {!table.business_purpose && table.columns && (
                            <div className="small text-muted">
                              {table.columns.length} columns
                            </div>
                          )}
                        </div>
                        <div className="text-end">
                          {table.column_count && (
                            <span className="badge bg-secondary me-1">{table.column_count}</span>
                          )}
                          {table.primary_key_columns && table.primary_key_columns.length > 0 && (
                            <span className="badge bg-warning me-1">PK</span>
                          )}
                          {table.foreign_key_columns && table.foreign_key_columns.length > 0 && (
                            <span className="badge bg-info me-1">FK</span>
                          )}
                          {table.not_null_columns && table.not_null_columns.length > 0 && (
                            <span className="badge bg-success">NN</span>
                          )}
                        </div>
                      </div>
                    </div>
                  );
                })
              ) : (
                <div className="text-muted small px-3 py-1">
                  <i className="bi bi-info-circle me-1"></i>
                  No tables found
                </div>
              )}
            </div>
          )}
        </div>

        {/* Relationships Section */}
        <div className="folder-section">
          <div
            className="list-group-item py-2"
            style={{ cursor: 'pointer' }}
            onClick={() => toggleFolder('relationships')}
          >
            <div className="d-flex justify-content-between align-items-center">
              <span>
                <i className={`bi ${documentationData.expandedFolders.has('relationships') ? 'bi-folder-fill' : 'bi-folder'} me-2`}></i>
                Relationships
              </span>
              <span className="badge bg-light text-dark">{documentationData.relationships.length}</span>
            </div>
          </div>
          
          {documentationData.expandedFolders.has('relationships') && (
            <div className="folder-content">
              {documentationData.relationships.length > 0 ? (
                documentationData.relationships.map((relationship, index) => (
                  <div
                    key={relationship.id || `relationship-${index}`}
                    className="list-group-item py-1 px-3 border-0"
                    style={{ cursor: 'pointer' }}
                    onClick={() => handleItemSelect({
                      id: `relationship-${relationship.id || index}`,
                      name: `${relationship.constrained_table} ‚Üí ${relationship.referred_table}`,
                      type: 'relationship',
                      description: relationship.relationship_type || 'Relationship',
                      relationship: relationship
                    })}
                  >
                    <div className="d-flex align-items-center">
                      <i className="bi bi-arrow-right me-2 text-muted"></i>
                      <div className="flex-grow-1">
                        <span className="small fw-medium">
                          {relationship.constrained_table} ‚Üí {relationship.referred_table}
                        </span>
                        <div className="small text-muted">
                          {relationship.relationship_type || 'Relationship'}
                          {relationship.documentation && (
                            <span className="ms-2" style={{ fontSize: '0.75rem' }}>
                              ‚Ä¢ {relationship.documentation.length > 40 
                                ? `${relationship.documentation.substring(0, 40)}...` 
                                : relationship.documentation}
                            </span>
                          )}
                        </div>
                      </div>
                    </div>
                  </div>
                ))
              ) : (
                <div className="text-muted small px-3 py-1">
                  <i className="bi bi-info-circle me-1"></i>
                  No relationships found
                </div>
              )}
            </div>
          )}
        </div>
      </div>

      {/* Quick Actions Panel */}
      {showQuickActions && (
        <div className="quick-actions-panel p-3 border-top">
          <div className="d-flex justify-content-between align-items-center mb-2">
            <h6 className="mb-0 fw-bold">
              <i className="bi bi-lightning me-1"></i>
              Quick Actions
            </h6>
            <button
              className="btn btn-sm btn-outline-secondary"
              onClick={() => setShowQuickActions(!showQuickActions)}
            >
              <i className={`bi ${showQuickActions ? 'bi-chevron-down' : 'bi-chevron-up'}`}></i>
            </button>
          </div>
          <div className="d-grid gap-2">
            <button className="btn btn-sm btn-outline-primary">
              <i className="bi bi-plus-circle me-1"></i>
              Add Documentation
            </button>
            <button className="btn btn-sm btn-outline-secondary">
              <i className="bi bi-download me-1"></i>
              Export Schema
            </button>
            <button className="btn btn-sm btn-outline-info">
              <i className="bi bi-question-circle me-1"></i>
              Help
            </button>
          </div>
        </div>
      )}
    </div>
  );
};

export default DocumentationExplorer; 


================================================
FILE: web-ui/frontend/src/components/documentation-explorer/index.js
================================================
export { default } from './DocumentationExplorer'; 


================================================
FILE: web-ui/frontend/src/components/documentation-explorer/ItemDetailsModal.css
================================================
.modal {
  z-index: 1050;
  position: fixed;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
}

.modal-backdrop {
  z-index: 1040;
  position: fixed;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  background-color: rgba(0, 0, 0, 0.5);
}

.modal-dialog {
  max-width: 1000px;
  max-height: 90vh;
  margin: 1.75rem auto;
  position: relative;
  pointer-events: auto;
}

.modal-content {
  border-radius: 0.5rem;
  box-shadow: 0 0.5rem 1rem rgba(0, 0, 0, 0.15);
  position: relative;
  background-color: #fff;
  border: 1px solid rgba(0, 0, 0, 0.2);
}

.modal-header {
  border-bottom: 1px solid #dee2e6;
  background-color: #f8f9fa;
  padding: 1rem 1.5rem;
  border-radius: 0.5rem 0.5rem 0 0;
}

.modal-title {
  font-weight: 600;
  color: #495057;
  margin: 0;
  line-height: 1.5;
}

.modal-body {
  max-height: 75vh;
  overflow-y: auto;
  padding: 1.5rem;
  position: relative;
}

.modal-footer {
  border-top: 1px solid #dee2e6;
  background-color: #f8f9fa;
  padding: 1rem 1.5rem;
  border-radius: 0 0 0.5rem 0.5rem;
}

.table-responsive {
  border-radius: 0.375rem;
  overflow: hidden;
  border: 1px solid #dee2e6;
}

.table {
  margin-bottom: 0;
  width: 100%;
}

.table th {
  background-color: #f8f9fa;
  border-bottom: 2px solid #dee2e6;
  font-weight: 600;
  font-size: 0.875rem;
  text-transform: uppercase;
  letter-spacing: 0.5px;
  font-size: 0.75rem;
  padding: 0.75rem 0.5rem;
  vertical-align: middle;
}

.table td {
  vertical-align: middle;
  font-size: 0.875rem;
  padding: 0.75rem 0.5rem;
  border-bottom: 1px solid #dee2e6;
}

.table-striped > tbody > tr:nth-of-type(odd) > td {
  background-color: rgba(0, 0, 0, 0.02);
}

.table-warning {
  background-color: rgba(255, 193, 7, 0.1) !important;
}

.badge {
  font-size: 0.75em;
  font-weight: 500;
  padding: 0.375rem 0.75rem;
}

.text-center .h4 {
  font-weight: 600;
  margin-bottom: 0.25rem;
}

.text-center small {
  font-size: 0.75rem;
}

/* Scrollbar styling for modal body */
.modal-body::-webkit-scrollbar {
  width: 6px;
}

.modal-body::-webkit-scrollbar-track {
  background: #f1f1f1;
  border-radius: 3px;
}

.modal-body::-webkit-scrollbar-thumb {
  background: #c1c1c1;
  border-radius: 3px;
}

.modal-body::-webkit-scrollbar-thumb:hover {
  background: #a8a8a8;
}

/* Prevent body scroll when modal is open */
body.modal-open {
  overflow: hidden;
  padding-right: 0;
}

/* Animation improvements */
.modal.fade {
  transition: opacity 0.15s linear;
}

.modal.fade.show {
  opacity: 1;
}

.modal-backdrop.fade {
  transition: opacity 0.15s linear;
}

.modal-backdrop.fade.show {
  opacity: 1;
}

/* Focus management */
.modal:focus {
  outline: none;
}

.modal-dialog:focus {
  outline: none;
}

/* Alert styling */
.alert {
  border-radius: 0.375rem;
  padding: 1rem;
  margin-bottom: 1rem;
  border: 1px solid transparent;
}

.alert-warning {
  color: #664d03;
  background-color: #fff3cd;
  border-color: #ffecb5;
}

.alert-danger {
  color: #721c24;
  background-color: #f8d7da;
  border-color: #f5c6cb;
}

/* Button improvements */
.btn-close {
  background: transparent;
  border: 0;
  font-size: 1.5rem;
  font-weight: 700;
  line-height: 1;
  color: #000;
  opacity: 0.5;
  padding: 0;
  margin: 0;
  cursor: pointer;
}

.btn-close:hover {
  opacity: 0.75;
}

/* Responsive improvements */
@media (max-width: 768px) {
  .modal-dialog {
    margin: 0.5rem;
    max-width: calc(100% - 1rem);
  }
  
  .modal-body {
    padding: 1rem;
  }
  
  .modal-header,
  .modal-footer {
    padding: 0.75rem 1rem;
  }
}

/* Prevent text selection during modal interactions */
.modal * {
  user-select: none;
}

.modal input,
.modal textarea,
.modal pre,
.modal code {
  user-select: text;
} 


================================================
FILE: web-ui/frontend/src/components/documentation-explorer/ItemDetailsModal.js
================================================
import React, { useEffect, useCallback } from 'react';
import './ItemDetailsModal.css';

const ItemDetailsModal = ({
  showItemDetails,
  setShowItemDetails,
  selectedItem
}) => {
  // Prevent body scroll when modal is open
  useEffect(() => {
    if (showItemDetails) {
      document.body.style.overflow = 'hidden';
    } else {
      document.body.style.overflow = 'unset';
    }

    return () => {
      document.body.style.overflow = 'unset';
    };
  }, [showItemDetails]);

  // Memoized close handler to prevent unnecessary re-renders
  const handleClose = useCallback(() => {
    setShowItemDetails(false);
  }, [setShowItemDetails]);

  // Memoized backdrop click handler
  const handleBackdropClick = useCallback((e) => {
    if (e.target === e.currentTarget) {
      handleClose();
    }
  }, [handleClose]);

  // Early return if modal should not be shown
  if (!showItemDetails || !selectedItem) {
    return null;
  }

  const renderTableDetails = () => {
    try {
      const table = selectedItem.table;
      if (!table) {
        return (
          <div className="alert alert-warning">
            <i className="bi bi-exclamation-triangle me-2"></i>
            <strong>No table data available</strong>
          </div>
        );
      }

      // Safely extract schema data with fallbacks
      const schemaData = table.schema || table.schema_data || {};
      let columns = [];
      
      // Handle different possible column data structures with proper null checks
      if (schemaData && schemaData.columns) {
        if (Array.isArray(schemaData.columns)) {
          columns = schemaData.columns.filter(col => col != null);
        } else if (typeof schemaData.columns === 'object') {
          columns = Object.values(schemaData.columns).filter(col => col != null);
        }
      } else if (Array.isArray(schemaData)) {
        columns = schemaData.filter(col => col != null);
      } else if (table.columns && Array.isArray(table.columns)) {
        columns = table.columns.filter(col => col != null);
      }
      
      // Ensure columns is always an array and filter out null/undefined values
      if (!Array.isArray(columns)) {
        columns = [];
      }
      
      // Calculate statistics with safe property access
      const columnCount = columns.length;
      const primaryKeys = columns.filter(col => col && (col.primary_key || col.is_primary_key)).length;
      const foreignKeys = columns.filter(col => col && (col.foreign_key || col.is_foreign_key)).length;
      const notNullColumns = columns.filter(col => col && col.nullable === false).length;

      return (
        <div>
          <h5 className="mb-3">Table Details</h5>
          
          {/* Business Purpose */}
          {table.business_purpose && (
            <div className="mb-3">
              <h6>Business Purpose</h6>
              <p className="text-muted">{table.business_purpose}</p>
            </div>
          )}

          {/* Documentation */}
          {table.documentation && (
            <div className="mb-3">
              <h6>Documentation</h6>
              <div className="bg-light p-3 rounded">
                <pre className="mb-0" style={{ whiteSpace: 'pre-wrap', fontSize: '0.875rem' }}>
                  {table.documentation}
                </pre>
              </div>
            </div>
          )}

          {/* Statistics Cards */}
          <div className="row mb-4">
            <div className="col-md-3">
              <div className="text-center p-3 bg-primary bg-opacity-10 rounded">
                <div className="h3 text-primary">{columnCount}</div>
                <small className="text-muted">Columns</small>
              </div>
            </div>
            <div className="col-md-3">
              <div className="text-center p-3 bg-warning bg-opacity-10 rounded">
                <div className="h3 text-warning">{primaryKeys}</div>
                <small className="text-muted">Primary Keys</small>
              </div>
            </div>
            <div className="col-md-3">
              <div className="text-center p-3 bg-info bg-opacity-10 rounded">
                <div className="h3 text-info">{foreignKeys}</div>
                <small className="text-muted">Foreign Keys</small>
              </div>
            </div>
            <div className="col-md-3">
              <div className="text-center p-3 bg-success bg-opacity-10 rounded">
                <div className="h3 text-success">{notNullColumns}</div>
                <small className="text-muted">Not Null</small>
              </div>
            </div>
          </div>

          {/* Schema Information - Database IDE Style */}
          {columns.length > 0 && (
            <div className="mb-3">
              <h6>Schema Definition</h6>
              <div className="table-responsive">
                <table className="table table-sm table-striped">
                  <thead className="table-dark">
                    <tr>
                      <th style={{ width: '25%' }}>Column Name</th>
                      <th style={{ width: '20%' }}>Data Type</th>
                      <th style={{ width: '15%' }}>Nullable</th>
                      <th style={{ width: '15%' }}>Default</th>
                      <th style={{ width: '25%' }}>Constraints</th>
                    </tr>
                  </thead>
                  <tbody>
                    {columns.map((column, index) => {
                      if (!column) return null;
                      
                      const isPrimaryKey = column.primary_key || column.is_primary_key;
                      const isForeignKey = column.foreign_key || column.is_foreign_key;
                      
                      return (
                        <tr key={`${column.name || index}-${index}`} className={isPrimaryKey ? 'table-warning' : ''}>
                          <td>
                            <strong>{column.name || 'Unknown'}</strong>
                            {isPrimaryKey && (
                              <span className="badge bg-warning ms-1">PK</span>
                            )}
                            {isForeignKey && (
                              <span className="badge bg-info ms-1">FK</span>
                            )}
                          </td>
                          <td>
                            <code className="text-primary">{column.type || 'Unknown'}</code>
                          </td>
                          <td>
                            {column.nullable ? (
                              <span className="badge bg-success">NULL</span>
                            ) : (
                              <span className="badge bg-danger">NOT NULL</span>
                            )}
                          </td>
                          <td>
                            {column.default ? (
                              <code className="text-muted">{column.default}</code>
                            ) : (
                              <span className="text-muted">-</span>
                            )}
                          </td>
                          <td>
                            <div className="d-flex flex-wrap gap-1">
                              {isPrimaryKey && <span className="badge bg-warning">Primary Key</span>}
                              {isForeignKey && <span className="badge bg-info">Foreign Key</span>}
                              {column.unique && <span className="badge bg-secondary">Unique</span>}
                              {column.auto_increment && <span className="badge bg-dark">Auto Increment</span>}
                            </div>
                          </td>
                        </tr>
                      );
                    })}
                  </tbody>
                </table>
              </div>
            </div>
          )}

          {/* No Schema Data Message */}
          {columns.length === 0 && (
            <div className="mb-3">
              <div className="alert alert-warning">
                <i className="bi bi-exclamation-triangle me-2"></i>
                <strong>No schema data available</strong>
                <br />
                <small className="text-muted">
                  The table schema information could not be loaded. This might be because:
                  <ul className="mt-2 mb-0">
                    <li>The table hasn't been processed by the documentation system</li>
                    <li>The schema data is stored in a different format</li>
                    <li>There was an error loading the schema information</li>
                  </ul>
                </small>
              </div>
            </div>
          )}

          {/* Foreign Key Relationships */}
          {foreignKeys > 0 && (
            <div className="mb-3">
              <h6>Foreign Key Relationships</h6>
              <div className="table-responsive">
                <table className="table table-sm">
                  <thead className="table-light">
                    <tr>
                      <th>Column</th>
                      <th>References</th>
                      <th>On Delete</th>
                      <th>On Update</th>
                    </tr>
                  </thead>
                  <tbody>
                    {columns
                      .filter(col => col && (col.foreign_key || col.is_foreign_key))
                      .map((column, index) => (
                        <tr key={`fk-${column.name || index}-${index}`}>
                          <td><code>{column.name || 'Unknown'}</code></td>
                          <td>
                            {column.referenced_table && column.referenced_column ? (
                              <span>
                                <code>{column.referenced_table}</code>.<code>{column.referenced_column}</code>
                              </span>
                            ) : (
                              <span className="text-muted">Unknown</span>
                            )}
                          </td>
                          <td>
                            <span className="badge bg-secondary">
                              {column.on_delete || 'RESTRICT'}
                            </span>
                          </td>
                          <td>
                            <span className="badge bg-secondary">
                              {column.on_update || 'RESTRICT'}
                            </span>
                          </td>
                        </tr>
                      ))}
                  </tbody>
                </table>
              </div>
            </div>
          )}

          {/* Indexes */}
          {schemaData.indexes && Array.isArray(schemaData.indexes) && schemaData.indexes.length > 0 && (
            <div className="mb-3">
              <h6>Indexes</h6>
              <div className="table-responsive">
                <table className="table table-sm">
                  <thead className="table-light">
                    <tr>
                      <th>Index Name</th>
                      <th>Columns</th>
                      <th>Type</th>
                      <th>Unique</th>
                    </tr>
                  </thead>
                  <tbody>
                    {schemaData.indexes.map((index, indexIndex) => (
                      <tr key={`index-${index.name || indexIndex}-${indexIndex}`}>
                        <td><code>{index.name || 'Unknown'}</code></td>
                        <td>
                          {index.columns && Array.isArray(index.columns) ? (
                            index.columns.map(col => (
                              <span key={col} className="badge bg-light text-dark me-1">
                                {col}
                              </span>
                            ))
                          ) : (
                            <span className="text-muted">No columns</span>
                          )}
                        </td>
                        <td>
                          <span className="badge bg-info">{index.type || 'BTREE'}</span>
                        </td>
                        <td>
                          {index.unique ? (
                            <span className="badge bg-success">Yes</span>
                          ) : (
                            <span className="badge bg-secondary">No</span>
                          )}
                        </td>
                      </tr>
                    ))}
                  </tbody>
                </table>
              </div>
            </div>
          )}
        </div>
      );
    } catch (error) {
      console.error('Error rendering table details:', error);
      return (
        <div className="alert alert-danger">
          <i className="bi bi-exclamation-triangle me-2"></i>
          <strong>Error loading table details</strong>
          <br />
          <small className="text-muted">An error occurred while rendering the table information.</small>
        </div>
      );
    }
  };

  const renderRelationshipDetails = () => {
    try {
      const relationship = selectedItem.relationship;
      if (!relationship) {
        return (
          <div className="alert alert-warning">
            <i className="bi bi-exclamation-triangle me-2"></i>
            <strong>No relationship data available</strong>
          </div>
        );
      }

      return (
        <div>
          <h5 className="mb-3">Relationship Details</h5>
          
          {/* Relationship Type */}
          <div className="mb-3">
            <h6>Type</h6>
            <span className="badge bg-primary">{relationship.relationship_type || 'Unknown'}</span>
          </div>

          {/* Constrained Table */}
          <div className="mb-3">
            <h6>Constrained Table</h6>
            <p className="mb-1">
              <strong>{relationship.constrained_table}</strong>
            </p>
            {relationship.constrained_columns && (
              <div>
                <small className="text-muted">Columns: {relationship.constrained_columns.join(', ')}</small>
              </div>
            )}
          </div>

          {/* Referred Table */}
          <div className="mb-3">
            <h6>Referred Table</h6>
            <p className="mb-1">
              <strong>{relationship.referred_table}</strong>
            </p>
            {relationship.referred_columns && (
              <div>
                <small className="text-muted">Columns: {relationship.referred_columns.join(', ')}</small>
              </div>
            )}
          </div>

          {/* Documentation */}
          {relationship.documentation && (
            <div className="mb-3">
              <h6>Documentation</h6>
              <p className="text-muted">{relationship.documentation}</p>
            </div>
          )}
        </div>
      );
    } catch (error) {
      console.error('Error rendering relationship details:', error);
      return (
        <div className="alert alert-danger">
          <i className="bi bi-exclamation-triangle me-2"></i>
          <strong>Error loading relationship details</strong>
          <br />
          <small className="text-muted">An error occurred while rendering the relationship information.</small>
        </div>
      );
    }
  };

  return (
    <>
      <div className="modal fade show" style={{ display: 'block' }} tabIndex="-1">
        <div className="modal-dialog modal-lg">
          <div className="modal-content">
            <div className="modal-header">
              <h5 className="modal-title">
                <i className={`bi ${selectedItem.type === 'table' ? 'bi-table' : 'bi-arrow-right'} me-2`}></i>
                {selectedItem.name || 'Unknown Item'}
              </h5>
              <button
                type="button"
                className="btn-close"
                onClick={handleClose}
                aria-label="Close"
              ></button>
            </div>
            <div className="modal-body">
              {selectedItem.description && (
                <div className="mb-3">
                  <h6>Description</h6>
                  <p className="text-muted">{selectedItem.description}</p>
                </div>
              )}
              
              {selectedItem.type === 'table' ? renderTableDetails() : renderRelationshipDetails()}
            </div>
            <div className="modal-footer">
              <button type="button" className="btn btn-secondary" onClick={handleClose}>
                Close
              </button>
            </div>
          </div>
        </div>
      </div>
      <div className="modal-backdrop fade show" onClick={handleBackdropClick}></div>
    </>
  );
};

export default ItemDetailsModal; 


================================================
FILE: web-ui/frontend/src/components/pages/documentation/README.md
================================================
# Documentation Page

A comprehensive database documentation viewer that displays all documented tables, relationships, and business purposes in an organized, searchable format with tabbed navigation and detailed modal views.

## Features

### üéØ Core Functionality
- **Tabbed Navigation**: Separate tabs for Tables, Relationships, and All Documentation
- **Search & Filter**: Real-time search across all documentation content
- **Category Filtering**: Filter by tables, relationships, or view all documentation
- **Detailed Modal View**: Click any item to see comprehensive documentation details
- **Status Tracking**: Visual status badges for documentation completion
- **Responsive Design**: Works on desktop, tablet, and mobile devices

### üîç Search & Filtering
- **Text Search**: Search across business purposes, documentation text, table names, and relationship types
- **Category Filter**: Filter by type (Tables, Relationships, or All)
- **Clear Filters**: Reset all filters with one click
- **Real-time Filtering**: Results update as you type or change filters

### üìä Data Sources
The page automatically combines multiple data sources:
1. **Documentation API**: `/api/documentation/summaries` - For documented items with business purposes
2. **Schema API**: `/api/schema` - For raw schema data as fallback
3. **Combined View**: Merges both sources for comprehensive coverage

### üé® Visual Design
- **Card-based Layout**: Each documented item displayed as an interactive card
- **Status Badges**: Color-coded status indicators (Complete, Pending, Failed)
- **Tab Navigation**: Clean tab interface for different content types
- **Hover Effects**: Cards lift and highlight on interaction
- **Modern UI**: Professional Bootstrap-based design with custom styling

### üì± Responsive Features
- **Mobile-friendly**: Cards stack vertically on small screens
- **Touch-friendly**: Large touch targets for mobile interaction
- **Adaptive Layout**: Tabs and filters adapt to screen size
- **Optimized Typography**: Readable text at all screen sizes

## Usage

### Basic Navigation
1. **View Documentation**: All documented items are displayed in organized tabs
2. **Search**: Use the search box to find specific documentation
3. **Filter**: Use the category dropdown to filter by type
4. **Switch Tabs**: Click between Tables, Relationships, and All Documentation
5. **Details**: Click any documentation card to view detailed information
6. **Refresh**: Use the refresh button to reload documentation data

### Tab Navigation
- **Tables Tab**: Shows all documented tables with business purposes and schema info
- **Relationships Tab**: Shows all documented relationships with types and details
- **All Documentation Tab**: Shows everything in a unified view

### Documentation Cards
Each card shows:
- **Type Badge**: Indicates if it's a Table or Relationship
- **Status Badge**: Shows documentation completion status
- **Title**: Table name or relationship description
- **Description**: Business purpose or documentation preview
- **Metadata**: Column count, relationship type, or processing date

### Detailed Modal
Clicking a documentation card opens a detailed modal showing:
- **Business Purpose**: Full business purpose text
- **Documentation**: Complete documentation content
- **Schema Information**: For tables, shows column details
- **Relationship Details**: For relationships, shows source and target tables
- **Metadata**: Processing status, dates, and other information

## Data Structure

### Documentation Item Object
```javascript
{
  id: "unique_identifier",
  name: "table_name",
  type: "table" | "relationship",
  business_purpose: "Business purpose description",
  documentation: "Detailed documentation text",
  status: "completed" | "pending" | "failed",
  processed_at: "2024-01-01T00:00:00Z",
  column_count: 5,
  columns: [...],
  relationship_type: "Foreign Key",
  constrained_table: "source_table",
  referred_table: "target_table"
}
```

### API Endpoints
- **GET** `/api/documentation/summaries` - Get all documented items
- **GET** `/api/schema` - Get raw schema data as fallback

## Styling

The page uses custom CSS classes:
- `.documentation-page` - Main container
- `.documentation-card` - Individual documentation cards
- `.content-tabs` - Tab navigation container
- `.content-area` - Main content display area
- `.filters-section` - Search and filter controls

## Browser Support

- **Modern Browsers**: Chrome, Firefox, Safari, Edge
- **Mobile Browsers**: iOS Safari, Chrome Mobile
- **Responsive**: Works on all screen sizes
- **Accessibility**: Keyboard navigation and screen reader support

## Performance

- **Lazy Loading**: Data loaded on component mount
- **Efficient Filtering**: Real-time filtering without API calls
- **Optimized Rendering**: Only re-renders when data changes
- **Memory Efficient**: Minimal state management

## Error Handling

- **Network Errors**: Graceful fallback to empty state
- **Data Errors**: Handles malformed documentation data
- **Loading States**: Shows spinner during data loading
- **Empty States**: Helpful messages when no data is available

## Status Indicators

### Status Badges
- **Complete** (Green): Documentation has been fully processed
- **Pending** (Yellow): Documentation is queued for processing
- **Failed** (Red): Documentation processing encountered an error
- **Unknown** (Gray): Status information not available

### Type Badges
- **Table** (Blue): Database table documentation
- **Relationship** (Cyan): Foreign key relationship documentation

## Content Organization

### Tables Tab
- Shows all documented tables
- Displays business purposes and schema information
- Shows column counts and processing status
- Includes schema details in modal view

### Relationships Tab
- Shows all documented relationships
- Displays relationship types and descriptions
- Shows source and target table information
- Includes detailed relationship mapping

### All Documentation Tab
- Unified view of all documented items
- Mixed display of tables and relationships
- Shows processing dates and status
- Comprehensive search across all content 


================================================
FILE: web-ui/frontend/src/components/pages/documentation/DocumentationPage.css
================================================
.documentation-page {
  padding: 2rem;
  background-color: #f8f9fa;
  min-height: 100vh;
}

.page-header {
  background-color: white;
  padding: 1.5rem;
  border-radius: 0.5rem;
  box-shadow: 0 0.125rem 0.25rem rgba(0, 0, 0, 0.075);
  margin-bottom: 1.5rem;
}

.filters-section {
  background-color: white;
  padding: 1.5rem;
  border-radius: 0.5rem;
  box-shadow: 0 0.125rem 0.25rem rgba(0, 0, 0, 0.075);
  margin-bottom: 1.5rem;
}

.content-tabs {
  background-color: white;
  border-radius: 0.5rem 0.5rem 0 0;
  box-shadow: 0 0.125rem 0.25rem rgba(0, 0, 0, 0.075);
  margin-bottom: 0;
}

.content-tabs .nav-tabs {
  border-bottom: none;
  padding: 0 1.5rem;
}

.content-tabs .nav-link {
  border: none;
  border-radius: 0;
  color: #6c757d;
  font-weight: 500;
  padding: 1rem 1.5rem;
  transition: all 0.2s ease-in-out;
}

.content-tabs .nav-link:hover {
  color: #495057;
  background-color: #f8f9fa;
  border-color: transparent;
}

.content-tabs .nav-link.active {
  color: #0d6efd;
  background-color: white;
  border-bottom: 3px solid #0d6efd;
  font-weight: 600;
}

.content-area {
  background-color: white;
  border-radius: 0 0 0.5rem 0.5rem;
  box-shadow: 0 0.125rem 0.25rem rgba(0, 0, 0, 0.075);
  padding: 1.5rem;
  min-height: 400px;
}

.items-grid {
  margin-top: 0;
}

.documentation-card {
  cursor: pointer;
  transition: all 0.2s ease-in-out;
}

.documentation-card:hover {
  transform: translateY(-2px);
  box-shadow: 0 0.5rem 1rem rgba(0, 0, 0, 0.15);
}

.documentation-card .card {
  border: 1px solid #dee2e6;
  border-radius: 0.5rem;
  transition: all 0.2s ease-in-out;
  background-color: white;
}

.documentation-card:hover .card {
  border-color: #0d6efd;
  box-shadow: 0 0.5rem 1rem rgba(0, 0, 0, 0.15);
}

.documentation-card .card-header {
  background-color: #f8f9fa;
  border-bottom: 1px solid #dee2e6;
  padding: 0.75rem 1rem;
}

.documentation-card .card-title {
  color: #495057;
  font-weight: 600;
  margin-bottom: 0.5rem;
}

.documentation-card .card-text {
  color: #6c757d;
  font-size: 0.875rem;
  line-height: 1.4;
}

.empty-state {
  background-color: white;
  border-radius: 0.5rem;
  box-shadow: 0 0.125rem 0.25rem rgba(0, 0, 0, 0.075);
}

.empty-state .display-1 {
  font-size: 4rem;
  opacity: 0.5;
}

/* Modal Styles */
.modal {
  z-index: 1050;
}

.modal-backdrop {
  z-index: 1040;
}

.modal-dialog {
  max-width: 900px;
}

.modal-content {
  border-radius: 0.5rem;
  box-shadow: 0 0.5rem 1rem rgba(0, 0, 0, 0.15);
}

.modal-header {
  border-bottom: 1px solid #dee2e6;
  background-color: #f8f9fa;
}

.modal-title {
  font-weight: 600;
  color: #495057;
}

.modal-body {
  padding: 1.5rem;
}

.modal-footer {
  border-top: 1px solid #dee2e6;
  background-color: #f8f9fa;
}

/* Badge Colors */
.badge.bg-primary {
  background-color: #0d6efd !important;
}

.badge.bg-info {
  background-color: #0dcaf0 !important;
}

.badge.bg-success {
  background-color: #198754 !important;
}

.badge.bg-warning {
  background-color: #ffc107 !important;
  color: #000 !important;
}

.badge.bg-danger {
  background-color: #dc3545 !important;
}

.badge.bg-secondary {
  background-color: #6c757d !important;
}

/* Table Styles */
.table-responsive {
  border-radius: 0.375rem;
  overflow: hidden;
}

.table {
  margin-bottom: 0;
}

.table th {
  background-color: #f8f9fa;
  border-bottom: 2px solid #dee2e6;
  font-weight: 600;
  font-size: 0.875rem;
  text-transform: uppercase;
  letter-spacing: 0.5px;
  font-size: 0.75rem;
}

.table td {
  vertical-align: middle;
  font-size: 0.875rem;
  padding: 0.75rem 0.5rem;
}

.table-striped > tbody > tr:nth-of-type(odd) > td {
  background-color: rgba(0, 0, 0, 0.02);
}

/* Documentation Content */
.bg-light {
  background-color: #f8f9fa !important;
  border: 1px solid #e9ecef;
  border-radius: 0.375rem;
}

.bg-light p {
  margin-bottom: 0;
  line-height: 1.5;
}

/* Responsive Design */
@media (max-width: 768px) {
  .documentation-page {
    padding: 1rem;
  }
  
  .page-header {
    padding: 1rem;
  }
  
  .filters-section {
    padding: 1rem;
  }
  
  .content-area {
    padding: 1rem;
  }
  
  .content-tabs .nav-tabs {
    padding: 0 1rem;
  }
  
  .content-tabs .nav-link {
    padding: 0.75rem 1rem;
    font-size: 0.875rem;
  }
}

@media (max-width: 576px) {
  .page-header .d-flex {
    flex-direction: column;
    gap: 1rem;
    align-items: flex-start !important;
  }
  
  .filters-section .row {
    margin: 0;
  }
  
  .filters-section .col-md-6,
  .filters-section .col-md-3 {
    padding: 0;
    margin-bottom: 0.5rem;
  }
  
  .content-tabs .nav-tabs {
    flex-direction: column;
  }
  
  .content-tabs .nav-item {
    width: 100%;
  }
  
  .content-tabs .nav-link {
    text-align: center;
    border-radius: 0;
  }
}

/* Loading Animation */
.spinner-border {
  width: 3rem;
  height: 3rem;
}

/* Card Hover Effects */
.card {
  transition: all 0.2s ease-in-out;
}

.card:hover {
  transform: translateY(-2px);
  box-shadow: 0 0.5rem 1rem rgba(0, 0, 0, 0.15);
}

/* Input Group Styling */
.input-group-text {
  background-color: #f8f9fa;
  border-color: #dee2e6;
  color: #6c757d;
}

.form-control:focus,
.form-select:focus {
  border-color: #0d6efd;
  box-shadow: 0 0 0 0.2rem rgba(13, 110, 253, 0.25);
}

/* Button Styling */
.btn-outline-primary:hover {
  background-color: #0d6efd;
  border-color: #0d6efd;
  color: white;
}

.btn-outline-secondary:hover {
  background-color: #6c757d;
  border-color: #6c757d;
  color: white;
}

/* Tab Content Animation */
.content-area {
  animation: fadeIn 0.3s ease-in-out;
}

@keyframes fadeIn {
  from {
    opacity: 0;
    transform: translateY(10px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

/* Status Badge Styling */
.badge {
  font-size: 0.75em;
  font-weight: 500;
  padding: 0.375rem 0.75rem;
}

/* Metadata List Styling */
.list-unstyled li {
  padding: 0.25rem 0;
  border-bottom: 1px solid #f8f9fa;
}

.list-unstyled li:last-child {
  border-bottom: none;
}

.list-unstyled strong {
  color: #495057;
  font-weight: 600;
}

/* Code Styling */
code {
  background-color: #f8f9fa;
  color: #e83e8c;
  padding: 0.125rem 0.25rem;
  border-radius: 0.25rem;
  font-size: 0.875em;
}

/* Documentation Text Styling */
.text-muted {
  color: #6c757d !important;
}

/* Modal Content Styling */
.modal-body h6 {
  color: #495057;
  font-weight: 600;
  margin-bottom: 0.5rem;
  border-bottom: 2px solid #e9ecef;
  padding-bottom: 0.25rem;
}

.modal-body p {
  margin-bottom: 1rem;
  line-height: 1.6;
} 


================================================
FILE: web-ui/frontend/src/components/pages/documentation/index.js
================================================
import React, { useState, useEffect } from 'react';
import './DocumentationPage.css';

const DocumentationPage = () => {
  const [documentationData, setDocumentationData] = useState({
    tables: [],
    relationships: [],
    summaries: []
  });
  const [filteredData, setFilteredData] = useState({
    tables: [],
    relationships: [],
    summaries: []
  });
  const [searchQuery, setSearchQuery] = useState('');
  const [selectedCategory, setSelectedCategory] = useState('all');
  const [selectedItem, setSelectedItem] = useState(null);
  const [showDetails, setShowDetails] = useState(false);
  const [isLoading, setIsLoading] = useState(true);
  const [activeTab, setActiveTab] = useState('tables');

  useEffect(() => {
    loadDocumentationData();
  }, []);

  useEffect(() => {
    filterData();
  }, [documentationData, searchQuery, selectedCategory]);

  const loadDocumentationData = async () => {
    setIsLoading(true);
    try {
      // Load summaries (documented items)
      const summariesResponse = await fetch('http://127.0.0.1:5000/api/documentation/summaries');
      let summaries = [];
      
      if (summariesResponse.ok) {
        const summariesData = await summariesResponse.json();
        if (summariesData.success && summariesData.summaries) {
          summaries = Object.values(summariesData.summaries);
        }
      }

      // Load schema data as fallback
      const schemaResponse = await fetch('http://127.0.0.1:5000/api/schema');
      let tables = [];
      let relationships = [];
      
      if (schemaResponse.ok) {
        const schemaData = await schemaResponse.json();
        if (schemaData.success) {
          tables = schemaData.tables || [];
          relationships = schemaData.relationships || [];
        }
      }

      // Combine and organize data
      const organizedData = {
        tables: [...summaries.filter(item => item.type === 'table'), ...tables],
        relationships: [...summaries.filter(item => item.type === 'relationship'), ...relationships],
        summaries: summaries
      };

      setDocumentationData(organizedData);
    } catch (error) {
      console.error('Error loading documentation data:', error);
    } finally {
      setIsLoading(false);
    }
  };

  const filterData = () => {
    const query = searchQuery.toLowerCase();
    const category = selectedCategory;

    const filterItems = (items) => {
      return items.filter(item => {
        const matchesQuery = !query || 
          item.name?.toLowerCase().includes(query) ||
          item.table_name?.toLowerCase().includes(query) ||
          item.business_purpose?.toLowerCase().includes(query) ||
          item.documentation?.toLowerCase().includes(query) ||
          item.relationship_type?.toLowerCase().includes(query) ||
          item.constrained_table?.toLowerCase().includes(query) ||
          item.referred_table?.toLowerCase().includes(query);

        const matchesCategory = category === 'all' || 
          (category === 'tables' && (item.type === 'table' || item.columns)) ||
          (category === 'relationships' && (item.type === 'relationship' || item.constrained_table));

        return matchesQuery && matchesCategory;
      });
    };

    setFilteredData({
      tables: filterItems(documentationData.tables),
      relationships: filterItems(documentationData.relationships),
      summaries: filterItems(documentationData.summaries)
    });
  };

  const handleItemClick = (item) => {
    setSelectedItem(item);
    setShowDetails(true);
  };

  const closeDetails = () => {
    setShowDetails(false);
    setSelectedItem(null);
  };

  const getItemType = (item) => {
    if (item.type === 'table' || item.columns) return 'table';
    if (item.type === 'relationship' || item.constrained_table) return 'relationship';
    return 'summary';
  };

  const getItemIcon = (item) => {
    const type = getItemType(item);
    switch (type) {
      case 'table':
        return 'bi-table';
      case 'relationship':
        return 'bi-arrow-right';
      default:
        return 'bi-file-text';
    }
  };

  const getItemTitle = (item) => {
    return item.name || item.table_name || `${item.constrained_table} ‚Üí ${item.referred_table}` || 'Unknown';
  };

  const getItemDescription = (item) => {
    return item.business_purpose || item.documentation || item.relationship_type || 'No description available';
  };

  const getStatusBadge = (item) => {
    if (item.status === 'completed') {
      return <span className="badge bg-success">Complete</span>;
    } else if (item.status === 'pending') {
      return <span className="badge bg-warning">Pending</span>;
    } else if (item.status === 'failed') {
      return <span className="badge bg-danger">Failed</span>;
    }
    return <span className="badge bg-secondary">Unknown</span>;
  };

  if (isLoading) {
    return (
      <div className="documentation-page">
        <div className="d-flex justify-content-center align-items-center" style={{ height: '400px' }}>
          <div className="spinner-border text-primary" role="status">
            <span className="visually-hidden">Loading...</span>
          </div>
        </div>
      </div>
    );
  }

  return (
    <div className="documentation-page">
      {/* Header */}
      <div className="page-header">
        <div className="d-flex justify-content-between align-items-center">
          <div>
            <h2 className="mb-1">
              <i className="bi bi-journal-text me-2"></i>
              Database Documentation
            </h2>
            <p className="text-muted mb-0">
              {filteredData.tables.length + filteredData.relationships.length} documented items
            </p>
          </div>
          <button 
            className="btn btn-outline-primary"
            onClick={loadDocumentationData}
          >
            <i className="bi bi-arrow-clockwise me-1"></i>
            Refresh
          </button>
        </div>
      </div>

      {/* Filters */}
      <div className="filters-section">
        <div className="row g-3">
          <div className="col-md-6">
            <div className="input-group">
              <span className="input-group-text">
                <i className="bi bi-search"></i>
              </span>
              <input
                type="text"
                className="form-control"
                placeholder="Search documentation..."
                value={searchQuery}
                onChange={(e) => setSearchQuery(e.target.value)}
              />
            </div>
          </div>
          <div className="col-md-3">
            <select
              className="form-select"
              value={selectedCategory}
              onChange={(e) => setSelectedCategory(e.target.value)}
            >
              <option value="all">All Categories</option>
              <option value="tables">Tables</option>
              <option value="relationships">Relationships</option>
            </select>
          </div>
          <div className="col-md-3">
            <button 
              className="btn btn-outline-secondary w-100"
              onClick={() => {
                setSearchQuery('');
                setSelectedCategory('all');
              }}
            >
              Clear Filters
            </button>
          </div>
        </div>
      </div>

      {/* Content Tabs */}
      <div className="content-tabs">
        <ul className="nav nav-tabs" role="tablist">
          <li className="nav-item" role="presentation">
            <button
              className={`nav-link ${activeTab === 'tables' ? 'active' : ''}`}
              onClick={() => setActiveTab('tables')}
            >
              <i className="bi bi-table me-1"></i>
              Tables ({filteredData.tables.length})
            </button>
          </li>
          <li className="nav-item" role="presentation">
            <button
              className={`nav-link ${activeTab === 'relationships' ? 'active' : ''}`}
              onClick={() => setActiveTab('relationships')}
            >
              <i className="bi bi-arrow-right me-1"></i>
              Relationships ({filteredData.relationships.length})
            </button>
          </li>
          <li className="nav-item" role="presentation">
            <button
              className={`nav-link ${activeTab === 'summaries' ? 'active' : ''}`}
              onClick={() => setActiveTab('summaries')}
            >
              <i className="bi bi-file-text me-1"></i>
              All Documentation ({filteredData.summaries.length})
            </button>
          </li>
        </ul>
      </div>

      {/* Content Area */}
      <div className="content-area">
        {activeTab === 'tables' && (
          <div className="items-grid">
            {filteredData.tables.length > 0 ? (
              <div className="row g-4">
                {filteredData.tables.map((item, index) => (
                  <div key={index} className="col-lg-6 col-xl-4">
                    <div 
                      className="documentation-card"
                      onClick={() => handleItemClick(item)}
                    >
                      <div className="card h-100">
                        <div className="card-header d-flex justify-content-between align-items-center">
                          <span className="badge bg-primary">
                            <i className={`bi ${getItemIcon(item)} me-1`}></i>
                            Table
                          </span>
                          {getStatusBadge(item)}
                        </div>
                        <div className="card-body">
                          <h6 className="card-title">{getItemTitle(item)}</h6>
                          <p className="card-text text-muted">
                            {getItemDescription(item).length > 100 
                              ? `${getItemDescription(item).substring(0, 100)}...`
                              : getItemDescription(item)}
                          </p>
                          {item.column_count && (
                            <div className="mt-2">
                              <small className="text-muted">
                                {item.column_count} columns
                              </small>
                            </div>
                          )}
                        </div>
                      </div>
                    </div>
                  </div>
                ))}
              </div>
            ) : (
              <div className="empty-state">
                <div className="text-center py-5">
                  <i className="bi bi-table display-1 text-muted"></i>
                  <h4 className="mt-3">No tables found</h4>
                  <p className="text-muted">
                    {searchQuery || selectedCategory !== 'all' 
                      ? 'Try adjusting your search criteria or filters.'
                      : 'No table documentation is currently available.'}
                  </p>
                </div>
              </div>
            )}
          </div>
        )}

        {activeTab === 'relationships' && (
          <div className="items-grid">
            {filteredData.relationships.length > 0 ? (
              <div className="row g-4">
                {filteredData.relationships.map((item, index) => (
                  <div key={index} className="col-lg-6 col-xl-4">
                    <div 
                      className="documentation-card"
                      onClick={() => handleItemClick(item)}
                    >
                      <div className="card h-100">
                        <div className="card-header d-flex justify-content-between align-items-center">
                          <span className="badge bg-info">
                            <i className={`bi ${getItemIcon(item)} me-1`}></i>
                            Relationship
                          </span>
                          {getStatusBadge(item)}
                        </div>
                        <div className="card-body">
                          <h6 className="card-title">{getItemTitle(item)}</h6>
                          <p className="card-text text-muted">
                            {getItemDescription(item).length > 100 
                              ? `${getItemDescription(item).substring(0, 100)}...`
                              : getItemDescription(item)}
                          </p>
                          {item.relationship_type && (
                            <div className="mt-2">
                              <small className="text-muted">
                                Type: {item.relationship_type}
                              </small>
                            </div>
                          )}
                        </div>
                      </div>
                    </div>
                  </div>
                ))}
              </div>
            ) : (
              <div className="empty-state">
                <div className="text-center py-5">
                  <i className="bi bi-arrow-right display-1 text-muted"></i>
                  <h4 className="mt-3">No relationships found</h4>
                  <p className="text-muted">
                    {searchQuery || selectedCategory !== 'all' 
                      ? 'Try adjusting your search criteria or filters.'
                      : 'No relationship documentation is currently available.'}
                  </p>
                </div>
              </div>
            )}
          </div>
        )}

        {activeTab === 'summaries' && (
          <div className="items-grid">
            {filteredData.summaries.length > 0 ? (
              <div className="row g-4">
                {filteredData.summaries.map((item, index) => (
                  <div key={index} className="col-lg-6 col-xl-4">
                    <div 
                      className="documentation-card"
                      onClick={() => handleItemClick(item)}
                    >
                      <div className="card h-100">
                        <div className="card-header d-flex justify-content-between align-items-center">
                          <span className={`badge bg-${getItemType(item) === 'table' ? 'primary' : 'info'}`}>
                            <i className={`bi ${getItemIcon(item)} me-1`}></i>
                            {getItemType(item) === 'table' ? 'Table' : 'Relationship'}
                          </span>
                          {getStatusBadge(item)}
                        </div>
                        <div className="card-body">
                          <h6 className="card-title">{getItemTitle(item)}</h6>
                          <p className="card-text text-muted">
                            {getItemDescription(item).length > 100 
                              ? `${getItemDescription(item).substring(0, 100)}...`
                              : getItemDescription(item)}
                          </p>
                          {item.processed_at && (
                            <div className="mt-2">
                              <small className="text-muted">
                                Processed: {new Date(item.processed_at).toLocaleDateString()}
                              </small>
                            </div>
                          )}
                        </div>
                      </div>
                    </div>
                  </div>
                ))}
              </div>
            ) : (
              <div className="empty-state">
                <div className="text-center py-5">
                  <i className="bi bi-file-text display-1 text-muted"></i>
                  <h4 className="mt-3">No documentation found</h4>
                  <p className="text-muted">
                    {searchQuery || selectedCategory !== 'all' 
                      ? 'Try adjusting your search criteria or filters.'
                      : 'No documentation is currently available.'}
                  </p>
                </div>
              </div>
            )}
          </div>
        )}
      </div>

      {/* Item Details Modal */}
      {showDetails && selectedItem && (
        <div className="modal fade show" style={{ display: 'block' }} tabIndex="-1">
          <div className="modal-dialog modal-lg">
            <div className="modal-content">
              <div className="modal-header">
                <h5 className="modal-title">
                  <i className={`bi ${getItemIcon(selectedItem)} me-2`}></i>
                  {getItemTitle(selectedItem)}
                </h5>
                <button
                  type="button"
                  className="btn-close"
                  onClick={closeDetails}
                  aria-label="Close"
                ></button>
              </div>
              <div className="modal-body">
                <div className="row">
                  <div className="col-md-8">
                    <h6>Business Purpose</h6>
                    <p className="text-muted">
                      {selectedItem.business_purpose || 'No business purpose documented.'}
                    </p>

                    <h6 className="mt-4">Documentation</h6>
                    <div className="bg-light p-3 rounded">
                      <p className="mb-0">
                        {selectedItem.documentation || 'No detailed documentation available.'}
                      </p>
                    </div>

                    {getItemType(selectedItem) === 'table' && selectedItem.columns && (
                      <>
                        <h6 className="mt-4">Schema Information</h6>
                        <div className="table-responsive">
                          <table className="table table-sm">
                            <thead>
                              <tr>
                                <th>Column</th>
                                <th>Type</th>
                                <th>Nullable</th>
                                <th>Primary Key</th>
                              </tr>
                            </thead>
                            <tbody>
                              {selectedItem.columns.map((col, index) => (
                                <tr key={index}>
                                  <td><code>{col.name}</code></td>
                                  <td><code>{col.type}</code></td>
                                  <td>
                                    <span className={`badge ${col.nullable ? 'bg-success' : 'bg-danger'}`}>
                                      {col.nullable ? 'NULL' : 'NOT NULL'}
                                    </span>
                                  </td>
                                  <td>
                                    {col.primary_key && <span className="badge bg-warning">PK</span>}
                                  </td>
                                </tr>
                              ))}
                            </tbody>
                          </table>
                        </div>
                      </>
                    )}

                    {getItemType(selectedItem) === 'relationship' && (
                      <>
                        <h6 className="mt-4">Relationship Details</h6>
                        <div className="row">
                          <div className="col-md-6">
                            <strong>From:</strong> {selectedItem.constrained_table}
                            {selectedItem.constrained_columns && (
                              <div className="small text-muted">
                                Columns: {selectedItem.constrained_columns.join(', ')}
                              </div>
                            )}
                          </div>
                          <div className="col-md-6">
                            <strong>To:</strong> {selectedItem.referred_table}
                            {selectedItem.referred_columns && (
                              <div className="small text-muted">
                                Columns: {selectedItem.referred_columns.join(', ')}
                              </div>
                            )}
                          </div>
                        </div>
                      </>
                    )}
                  </div>
                  <div className="col-md-4">
                    <h6>Metadata</h6>
                    <ul className="list-unstyled">
                      <li><strong>Type:</strong> {getItemType(selectedItem)}</li>
                      <li><strong>Status:</strong> {getStatusBadge(selectedItem)}</li>
                      {selectedItem.processed_at && (
                        <li><strong>Processed:</strong> {new Date(selectedItem.processed_at).toLocaleString()}</li>
                      )}
                      {selectedItem.column_count && (
                        <li><strong>Columns:</strong> {selectedItem.column_count}</li>
                      )}
                      {selectedItem.relationship_type && (
                        <li><strong>Relationship Type:</strong> {selectedItem.relationship_type}</li>
                      )}
                    </ul>
                  </div>
                </div>
              </div>
              <div className="modal-footer">
                <button type="button" className="btn btn-secondary" onClick={closeDetails}>
                  Close
                </button>
              </div>
            </div>
          </div>
        </div>
      )}
      
      {/* Modal Backdrop */}
      {showDetails && (
        <div className="modal-backdrop fade show" onClick={closeDetails}></div>
      )}
    </div>
  );
};

export default DocumentationPage; 


================================================
FILE: web-ui/frontend/src/components/pages/relationships/README.md
================================================
# Relationships Page

A comprehensive database relationships viewer that displays all foreign key relationships in the database with filtering, search, and detailed view capabilities.

## Features

### üéØ Core Functionality
- **Visual Relationship Display**: Shows relationships as cards with clear table-to-table flow
- **Search & Filter**: Search by table names, relationship types, or documentation
- **Type Filtering**: Filter relationships by type (One-to-Many, Many-to-One, etc.)
- **Detailed Modal View**: Click any relationship to see detailed information
- **Responsive Design**: Works on desktop, tablet, and mobile devices

### üîç Search & Filtering
- **Text Search**: Search across table names, relationship types, and documentation
- **Type Filter**: Filter by relationship type (Foreign Key, One-to-Many, etc.)
- **Clear Filters**: Reset all filters with one click
- **Real-time Filtering**: Results update as you type or change filters

### üìä Data Sources
The page automatically tries multiple data sources:
1. **Documentation API**: `/api/documentation/relationships` - For documented relationships
2. **Schema API**: `/api/schema` - For raw foreign key relationships
3. **Fallback**: Creates basic relationship objects if no data is available

### üé® Visual Design
- **Card-based Layout**: Each relationship displayed as an interactive card
- **Flow Visualization**: Clear arrows showing relationship direction
- **Hover Effects**: Cards lift and highlight on hover
- **Color-coded Types**: Different colors for different relationship types
- **Modern UI**: Clean, professional design with Bootstrap components

### üì± Responsive Features
- **Mobile-friendly**: Cards stack vertically on small screens
- **Touch-friendly**: Large touch targets for mobile interaction
- **Adaptive Layout**: Filters and headers adapt to screen size
- **Optimized Typography**: Readable text at all screen sizes

## Usage

### Basic Navigation
1. **View Relationships**: All relationships are displayed in a grid layout
2. **Search**: Use the search box to find specific relationships
3. **Filter**: Use the type dropdown to filter by relationship type
4. **Details**: Click any relationship card to view detailed information
5. **Refresh**: Use the refresh button to reload relationship data

### Relationship Cards
Each card shows:
- **Relationship Type**: Badge showing the type of relationship
- **Source Table**: The table that contains the foreign key
- **Target Table**: The table being referenced
- **Columns**: The specific columns involved in the relationship
- **Documentation**: Brief description if available

### Detailed Modal
Clicking a relationship card opens a detailed modal showing:
- **Constrained Table**: Full details of the source table
- **Referred Table**: Full details of the target table
- **Relationship Type**: The type of relationship with color coding
- **Documentation**: Full documentation text if available

## Data Structure

### Relationship Object
```javascript
{
  id: "unique_identifier",
  constrained_table: "source_table_name",
  referred_table: "target_table_name",
  constrained_columns: ["column1", "column2"],
  referred_columns: ["column1", "column2"],
  relationship_type: "Foreign Key" | "One-to-Many" | "Many-to-One" | etc.,
  documentation: "Optional documentation text"
}
```

### API Endpoints
- **GET** `/api/documentation/relationships` - Get documented relationships
- **GET** `/api/schema` - Get raw schema data including relationships

## Styling

The page uses custom CSS classes:
- `.relationships-page` - Main container
- `.relationship-card` - Individual relationship cards
- `.relationship-flow` - Visual flow between tables
- `.table-info` - Table information display
- `.arrow-container` - Direction indicator

## Browser Support

- **Modern Browsers**: Chrome, Firefox, Safari, Edge
- **Mobile Browsers**: iOS Safari, Chrome Mobile
- **Responsive**: Works on all screen sizes
- **Accessibility**: Keyboard navigation and screen reader support

## Performance

- **Lazy Loading**: Data loaded on component mount
- **Efficient Filtering**: Real-time filtering without API calls
- **Optimized Rendering**: Only re-renders when data changes
- **Memory Efficient**: Minimal state management

## Error Handling

- **Network Errors**: Graceful fallback to empty state
- **Data Errors**: Handles malformed relationship data
- **Loading States**: Shows spinner during data loading
- **Empty States**: Helpful messages when no data is available 


================================================
FILE: web-ui/frontend/src/components/pages/relationships/index.js
================================================
import React, { useState, useEffect } from 'react';
import './RelationshipsPage.css';

const RelationshipsPage = () => {
  const [relationships, setRelationships] = useState([]);
  const [filteredRelationships, setFilteredRelationships] = useState([]);
  const [searchQuery, setSearchQuery] = useState('');
  const [selectedType, setSelectedType] = useState('all');
  const [isLoading, setIsLoading] = useState(true);
  const [selectedRelationship, setSelectedRelationship] = useState(null);
  const [showDetails, setShowDetails] = useState(false);

  useEffect(() => {
    loadRelationships();
  }, []);

  useEffect(() => {
    filterRelationships();
  }, [relationships, searchQuery, selectedType]);

  const loadRelationships = async () => {
    setIsLoading(true);
    try {
      // Try to get relationships from documentation endpoint first
      const response = await fetch('http://127.0.0.1:5000/api/documentation/relationships');
      let relationshipsData = [];
      
      if (response.ok) {
        const data = await response.json();
        if (data.success && data.relationships) {
          relationshipsData = Object.values(data.relationships);
        }
      }
      
      // If no documentation relationships, get from schema endpoint
      if (relationshipsData.length === 0) {
        const schemaResponse = await fetch('http://127.0.0.1:5000/api/schema');
        if (schemaResponse.ok) {
          const schemaData = await schemaResponse.json();
          if (schemaData.success && schemaData.relationships) {
            relationshipsData = schemaData.relationships.map(rel => ({
              id: `${rel.constrained_table}_${rel.referred_table}`,
              constrained_table: rel.constrained_table,
              referred_table: rel.referred_table,
              constrained_columns: rel.constrained_columns,
              referred_columns: rel.referred_columns,
              relationship_type: 'Foreign Key',
              documentation: `Foreign key relationship from ${rel.constrained_table} to ${rel.referred_table}`
            }));
          }
        }
      }
      
      setRelationships(relationshipsData);
    } catch (error) {
      console.error('Error loading relationships:', error);
    } finally {
      setIsLoading(false);
    }
  };

  const filterRelationships = () => {
    let filtered = relationships;

    // Filter by search query
    if (searchQuery) {
      const query = searchQuery.toLowerCase();
      filtered = filtered.filter(rel => 
        rel.constrained_table.toLowerCase().includes(query) ||
        rel.referred_table.toLowerCase().includes(query) ||
        rel.relationship_type?.toLowerCase().includes(query) ||
        rel.documentation?.toLowerCase().includes(query)
      );
    }

    // Filter by type
    if (selectedType !== 'all') {
      filtered = filtered.filter(rel => rel.relationship_type === selectedType);
    }

    setFilteredRelationships(filtered);
  };

  const getRelationshipTypeColor = (type) => {
    switch (type?.toLowerCase()) {
      case 'one-to-many':
        return 'primary';
      case 'many-to-one':
        return 'info';
      case 'one-to-one':
        return 'success';
      case 'many-to-many':
        return 'warning';
      case 'foreign key':
        return 'secondary';
      default:
        return 'dark';
    }
  };

  const handleRelationshipClick = (relationship) => {
    setSelectedRelationship(relationship);
    setShowDetails(true);
  };

  const closeDetails = () => {
    setShowDetails(false);
    setSelectedRelationship(null);
  };

  const getUniqueTypes = () => {
    const types = relationships.map(rel => rel.relationship_type).filter(Boolean);
    return ['all', ...new Set(types)];
  };

  if (isLoading) {
    return (
      <div className="relationships-page">
        <div className="d-flex justify-content-center align-items-center" style={{ height: '400px' }}>
          <div className="spinner-border text-primary" role="status">
            <span className="visually-hidden">Loading...</span>
          </div>
        </div>
      </div>
    );
  }

  return (
    <div className="relationships-page">
      {/* Header */}
      <div className="page-header">
        <div className="d-flex justify-content-between align-items-center">
          <div>
            <h2 className="mb-1">
              <i className="bi bi-arrow-left-right me-2"></i>
              Database Relationships
            </h2>
            <p className="text-muted mb-0">
              {filteredRelationships.length} of {relationships.length} relationships
            </p>
          </div>
          <button 
            className="btn btn-outline-primary"
            onClick={loadRelationships}
          >
            <i className="bi bi-arrow-clockwise me-1"></i>
            Refresh
          </button>
        </div>
      </div>

      {/* Filters */}
      <div className="filters-section">
        <div className="row g-3">
          <div className="col-md-6">
            <div className="input-group">
              <span className="input-group-text">
                <i className="bi bi-search"></i>
              </span>
              <input
                type="text"
                className="form-control"
                placeholder="Search relationships..."
                value={searchQuery}
                onChange={(e) => setSearchQuery(e.target.value)}
              />
            </div>
          </div>
          <div className="col-md-3">
            <select
              className="form-select"
              value={selectedType}
              onChange={(e) => setSelectedType(e.target.value)}
            >
              {getUniqueTypes().map(type => (
                <option key={type} value={type}>
                  {type === 'all' ? 'All Types' : type}
                </option>
              ))}
            </select>
          </div>
          <div className="col-md-3">
            <button 
              className="btn btn-outline-secondary w-100"
              onClick={() => {
                setSearchQuery('');
                setSelectedType('all');
              }}
            >
              Clear Filters
            </button>
          </div>
        </div>
      </div>

      {/* Relationships Grid */}
      <div className="relationships-grid">
        {filteredRelationships.length > 0 ? (
          <div className="row g-4">
            {filteredRelationships.map((relationship, index) => (
              <div key={relationship.id || index} className="col-lg-6 col-xl-4">
                <div 
                  className="relationship-card"
                  onClick={() => handleRelationshipClick(relationship)}
                >
                  <div className="card h-100">
                    <div className="card-header d-flex justify-content-between align-items-center">
                      <span className="badge bg-primary">
                        {relationship.relationship_type || 'Foreign Key'}
                      </span>
                      <i className="bi bi-arrow-right text-muted"></i>
                    </div>
                    <div className="card-body">
                      <div className="relationship-flow">
                        <div className="table-info">
                          <h6 className="mb-1">{relationship.constrained_table}</h6>
                          <small className="text-muted">
                            {relationship.constrained_columns?.join(', ') || 'N/A'}
                          </small>
                        </div>
                        <div className="arrow-container">
                          <i className="bi bi-arrow-right"></i>
                        </div>
                        <div className="table-info">
                          <h6 className="mb-1">{relationship.referred_table}</h6>
                          <small className="text-muted">
                            {relationship.referred_columns?.join(', ') || 'N/A'}
                          </small>
                        </div>
                      </div>
                      {relationship.documentation && (
                        <div className="mt-3">
                          <small className="text-muted">
                            {relationship.documentation.length > 100 
                              ? `${relationship.documentation.substring(0, 100)}...`
                              : relationship.documentation}
                          </small>
                        </div>
                      )}
                    </div>
                  </div>
                </div>
              </div>
            ))}
          </div>
        ) : (
          <div className="empty-state">
            <div className="text-center py-5">
              <i className="bi bi-arrow-left-right display-1 text-muted"></i>
              <h4 className="mt-3">No relationships found</h4>
              <p className="text-muted">
                {searchQuery || selectedType !== 'all' 
                  ? 'Try adjusting your search criteria or filters.'
                  : 'No relationships are currently defined in the database.'}
              </p>
            </div>
          </div>
        )}
      </div>

      {/* Relationship Details Modal */}
      {showDetails && selectedRelationship && (
        <div className="modal fade show" style={{ display: 'block' }} tabIndex="-1">
          <div className="modal-dialog modal-lg">
            <div className="modal-content">
              <div className="modal-header">
                <h5 className="modal-title">
                  <i className="bi bi-arrow-right me-2"></i>
                  Relationship Details
                </h5>
                <button
                  type="button"
                  className="btn-close"
                  onClick={closeDetails}
                  aria-label="Close"
                ></button>
              </div>
              <div className="modal-body">
                <div className="row">
                  <div className="col-md-6">
                    <h6>Constrained Table</h6>
                    <div className="card">
                      <div className="card-body">
                        <h5 className="card-title">{selectedRelationship.constrained_table}</h5>
                        <p className="card-text">
                          <strong>Columns:</strong> {selectedRelationship.constrained_columns?.join(', ') || 'N/A'}
                        </p>
                      </div>
                    </div>
                  </div>
                  <div className="col-md-6">
                    <h6>Referred Table</h6>
                    <div className="card">
                      <div className="card-body">
                        <h5 className="card-title">{selectedRelationship.referred_table}</h5>
                        <p className="card-text">
                          <strong>Columns:</strong> {selectedRelationship.referred_columns?.join(', ') || 'N/A'}
                        </p>
                      </div>
                    </div>
                  </div>
                </div>
                
                <div className="mt-4">
                  <h6>Relationship Type</h6>
                  <span className={`badge bg-${getRelationshipTypeColor(selectedRelationship.relationship_type)}`}>
                    {selectedRelationship.relationship_type || 'Foreign Key'}
                  </span>
                </div>

                {selectedRelationship.documentation && (
                  <div className="mt-4">
                    <h6>Documentation</h6>
                    <div className="bg-light p-3 rounded">
                      <p className="mb-0">{selectedRelationship.documentation}</p>
                    </div>
                  </div>
                )}
              </div>
              <div className="modal-footer">
                <button type="button" className="btn btn-secondary" onClick={closeDetails}>
                  Close
                </button>
              </div>
            </div>
          </div>
        </div>
      )}
      
      {/* Modal Backdrop */}
      {showDetails && (
        <div className="modal-backdrop fade show" onClick={closeDetails}></div>
      )}
    </div>
  );
};

export default RelationshipsPage; 


================================================
FILE: web-ui/frontend/src/components/pages/relationships/RelationshipsPage.css
================================================
.relationships-page {
  padding: 2rem;
  background-color: #f8f9fa;
  min-height: 100vh;
}

.page-header {
  background-color: white;
  padding: 1.5rem;
  border-radius: 0.5rem;
  box-shadow: 0 0.125rem 0.25rem rgba(0, 0, 0, 0.075);
  margin-bottom: 1.5rem;
}

.filters-section {
  background-color: white;
  padding: 1.5rem;
  border-radius: 0.5rem;
  box-shadow: 0 0.125rem 0.25rem rgba(0, 0, 0, 0.075);
  margin-bottom: 1.5rem;
}

.relationships-grid {
  margin-top: 1.5rem;
}

.relationship-card {
  cursor: pointer;
  transition: all 0.2s ease-in-out;
}

.relationship-card:hover {
  transform: translateY(-2px);
  box-shadow: 0 0.5rem 1rem rgba(0, 0, 0, 0.15);
}

.relationship-card .card {
  border: 1px solid #dee2e6;
  border-radius: 0.5rem;
  transition: all 0.2s ease-in-out;
  background-color: white;
}

.relationship-card:hover .card {
  border-color: #0d6efd;
  box-shadow: 0 0.5rem 1rem rgba(0, 0, 0, 0.15);
}

.relationship-card .card-header {
  background-color: #f8f9fa;
  border-bottom: 1px solid #dee2e6;
  padding: 0.75rem 1rem;
}

.relationship-flow {
  display: flex;
  align-items: center;
  justify-content: space-between;
  gap: 1rem;
}

.table-info {
  flex: 1;
  text-align: center;
  padding: 0.5rem;
  background-color: #f8f9fa;
  border-radius: 0.375rem;
  border: 1px solid #e9ecef;
}

.table-info h6 {
  margin-bottom: 0.25rem;
  color: #495057;
  font-weight: 600;
}

.table-info small {
  color: #6c757d;
  font-size: 0.75rem;
}

.arrow-container {
  display: flex;
  align-items: center;
  justify-content: center;
  color: #6c757d;
  font-size: 1.25rem;
  padding: 0.5rem;
}

.arrow-container i {
  transition: transform 0.2s ease-in-out;
}

.relationship-card:hover .arrow-container i {
  transform: scale(1.2);
  color: #0d6efd;
}

.empty-state {
  background-color: white;
  border-radius: 0.5rem;
  box-shadow: 0 0.125rem 0.25rem rgba(0, 0, 0, 0.075);
}

.empty-state .display-1 {
  font-size: 4rem;
  opacity: 0.5;
}

/* Modal Styles */
.modal {
  z-index: 1050;
}

.modal-backdrop {
  z-index: 1040;
}

.modal-dialog {
  max-width: 800px;
}

.modal-content {
  border-radius: 0.5rem;
  box-shadow: 0 0.5rem 1rem rgba(0, 0, 0, 0.15);
}

.modal-header {
  border-bottom: 1px solid #dee2e6;
  background-color: #f8f9fa;
}

.modal-title {
  font-weight: 600;
  color: #495057;
}

.modal-body {
  padding: 1.5rem;
}

.modal-footer {
  border-top: 1px solid #dee2e6;
  background-color: #f8f9fa;
}

/* Badge Colors */
.badge.bg-primary {
  background-color: #0d6efd !important;
}

.badge.bg-info {
  background-color: #0dcaf0 !important;
}

.badge.bg-success {
  background-color: #198754 !important;
}

.badge.bg-warning {
  background-color: #ffc107 !important;
  color: #000 !important;
}

.badge.bg-secondary {
  background-color: #6c757d !important;
}

.badge.bg-dark {
  background-color: #212529 !important;
}

/* Responsive Design */
@media (max-width: 768px) {
  .relationships-page {
    padding: 1rem;
  }
  
  .page-header {
    padding: 1rem;
  }
  
  .filters-section {
    padding: 1rem;
  }
  
  .relationship-flow {
    flex-direction: column;
    gap: 0.5rem;
  }
  
  .arrow-container {
    transform: rotate(90deg);
  }
  
  .relationship-card:hover .arrow-container {
    transform: rotate(90deg) scale(1.2);
  }
}

@media (max-width: 576px) {
  .page-header .d-flex {
    flex-direction: column;
    gap: 1rem;
    align-items: flex-start !important;
  }
  
  .filters-section .row {
    margin: 0;
  }
  
  .filters-section .col-md-6,
  .filters-section .col-md-3 {
    padding: 0;
    margin-bottom: 0.5rem;
  }
}

/* Loading Animation */
.spinner-border {
  width: 3rem;
  height: 3rem;
}

/* Card Hover Effects */
.card {
  transition: all 0.2s ease-in-out;
}

.card:hover {
  transform: translateY(-2px);
  box-shadow: 0 0.5rem 1rem rgba(0, 0, 0, 0.15);
}

/* Input Group Styling */
.input-group-text {
  background-color: #f8f9fa;
  border-color: #dee2e6;
  color: #6c757d;
}

.form-control:focus,
.form-select:focus {
  border-color: #0d6efd;
  box-shadow: 0 0 0 0.2rem rgba(13, 110, 253, 0.25);
}

/* Button Styling */
.btn-outline-primary:hover {
  background-color: #0d6efd;
  border-color: #0d6efd;
  color: white;
}

.btn-outline-secondary:hover {
  background-color: #6c757d;
  border-color: #6c757d;
  color: white;
}

/* Table Info Cards in Modal */
.modal-body .card {
  border: 1px solid #dee2e6;
  border-radius: 0.375rem;
}

.modal-body .card-title {
  color: #495057;
  font-weight: 600;
  margin-bottom: 0.5rem;
}

.modal-body .card-text {
  color: #6c757d;
  margin-bottom: 0;
}

/* Documentation Section */
.modal-body .bg-light {
  background-color: #f8f9fa !important;
  border: 1px solid #e9ecef;
  border-radius: 0.375rem;
}

.modal-body .bg-light p {
  margin-bottom: 0;
  line-height: 1.5;
} 


================================================
FILE: web-ui/frontend/src/components/pages/schema/README.md
================================================
# Schema Page

A comprehensive database schema viewer that displays all database tables, columns, data types, constraints, and relationships in an organized and visual format with statistics, search capabilities, and detailed modal views.

## Features

### üéØ Core Functionality
- **Schema Overview**: Complete database schema visualization with statistics
- **Dual View Modes**: Card view and table view for different preferences
- **Search & Filter**: Real-time search across tables, columns, and data types
- **Sorting Options**: Sort by name, column count, or relationship count
- **Detailed Modal View**: Click any table to see comprehensive schema details
- **Statistics Dashboard**: Visual statistics cards showing database metrics
- **Responsive Design**: Works on desktop, tablet, and mobile devices

### üìä Statistics Dashboard
- **Total Tables**: Count of all database tables
- **Total Columns**: Count of all columns across all tables
- **Total Relationships**: Count of foreign key relationships
- **Primary Keys**: Count of primary key constraints
- **Visual Cards**: Color-coded statistics with icons and hover effects

### üîç Search & Filtering
- **Text Search**: Search across table names, column names, and data types
- **Sort Options**: Sort by table name, column count, or relationship count
- **View Modes**: Switch between card view and table view
- **Real-time Filtering**: Results update as you type or change options

### üé® Visual Design
- **Card-based Layout**: Each table displayed as an interactive card
- **Data Type Colors**: Color-coded badges for different data types
- **Constraint Badges**: Visual indicators for PK, FK, UQ, NN constraints
- **Statistics Cards**: Hover effects and modern styling
- **Professional UI**: Clean Bootstrap-based design with custom styling

### üì± Responsive Features
- **Mobile-friendly**: Cards stack vertically on small screens
- **Touch-friendly**: Large touch targets for mobile interaction
- **Adaptive Layout**: Statistics and controls adapt to screen size
- **Optimized Typography**: Readable text at all screen sizes

## Usage

### Basic Navigation
1. **View Schema**: All database tables are displayed with statistics
2. **Search**: Use the search box to find specific tables or columns
3. **Sort**: Use the dropdown to sort tables by different criteria
4. **Switch Views**: Toggle between card view and table view
5. **Details**: Click any table card or row to view detailed schema
6. **Refresh**: Use the refresh button to reload schema data

### View Modes
- **Card View**: Visual cards showing table information with column previews
- **Table View**: Compact table format with all tables listed in rows

### Schema Cards
Each card shows:
- **Table Name**: Primary identifier for the table
- **Column Count**: Number of columns in the table
- **Column Preview**: First 3 columns with data types and constraints
- **Relationship Count**: Number of relationships involving this table
- **Interactive**: Click to view detailed schema information

### Detailed Modal
Clicking a table opens a detailed modal showing:
- **Complete Column List**: All columns with data types and constraints
- **Table Statistics**: Summary of primary keys, foreign keys, etc.
- **Relationships**: All relationships involving this table
- **Metadata**: Detailed information about the table structure

## Data Structure

### Schema Data Object
```javascript
{
  tables: [
    {
      name: "table_name",
      columns: [
        {
          name: "column_name",
          type: "VARCHAR(255)",
          nullable: true,
          primary_key: false,
          foreign_key: false,
          unique: false,
          default: null
        }
      ]
    }
  ],
  relationships: [
    {
      constrained_table: "source_table",
      referred_table: "target_table",
      constrained_columns: ["column1"],
      referred_columns: ["column2"],
      relationship_type: "Foreign Key"
    }
  ],
  statistics: {
    totalTables: 10,
    totalColumns: 50,
    totalRelationships: 5,
    constraints: {
      primaryKeys: 10,
      foreignKeys: 5,
      uniqueConstraints: 3,
      notNullColumns: 25
    }
  }
}
```

### API Endpoints
- **GET** `/api/schema` - Get complete database schema information

## Styling

The page uses custom CSS classes:
- `.schema-page` - Main container
- `.stat-card` - Statistics dashboard cards
- `.schema-card` - Individual table schema cards
- `.controls-section` - Search and filter controls
- `.schema-content` - Main content display area

## Browser Support

- **Modern Browsers**: Chrome, Firefox, Safari, Edge
- **Mobile Browsers**: iOS Safari, Chrome Mobile
- **Responsive**: Works on all screen sizes
- **Accessibility**: Keyboard navigation and screen reader support

## Performance

- **Lazy Loading**: Data loaded on component mount
- **Efficient Filtering**: Real-time filtering without API calls
- **Optimized Rendering**: Only re-renders when data changes
- **Memory Efficient**: Minimal state management

## Error Handling

- **Network Errors**: Graceful fallback to empty state
- **Data Errors**: Handles malformed schema data
- **Loading States**: Shows spinner during data loading
- **Empty States**: Helpful messages when no data is available

## Data Type Colors

### Color Coding
- **INTEGER**: Blue (Primary)
- **VARCHAR**: Cyan (Info)
- **DECIMAL**: Green (Success)
- **DATETIME**: Yellow (Warning)
- **DATE**: Gray (Secondary)
- **BOOLEAN**: Dark (Dark)

### Constraint Badges
- **PK**: Primary Key (Yellow)
- **FK**: Foreign Key (Cyan)
- **UQ**: Unique Constraint (Green)
- **NN**: Not Null (Red)

## Content Organization

### Statistics Dashboard
- **Visual Cards**: Color-coded statistics with icons
- **Hover Effects**: Cards lift and highlight on interaction
- **Real-time Updates**: Statistics update when data changes

### Card View
- **Table Cards**: Each table as an interactive card
- **Column Preview**: Shows first 3 columns with types
- **Relationship Info**: Shows relationship count
- **Hover Effects**: Cards lift and highlight on interaction

### Table View
- **Compact Format**: All tables in a single table
- **Sortable Columns**: Click headers to sort
- **Action Buttons**: View details for each table
- **Responsive**: Adapts to screen size

### Detailed Modal
- **Complete Schema**: All columns with full details
- **Statistics Panel**: Table-level statistics
- **Relationships Panel**: All related tables
- **Responsive Layout**: Adapts to modal size

## Search Capabilities

### Search Targets
- **Table Names**: Search by table name
- **Column Names**: Search by column name
- **Data Types**: Search by data type
- **Real-time**: Results update as you type

### Sort Options
- **Name**: Alphabetical by table name
- **Columns**: By number of columns (descending)
- **Relationships**: By number of relationships (descending)

## View Modes

### Card View
- **Visual**: Each table as a card with preview
- **Interactive**: Click cards for details
- **Responsive**: Cards stack on mobile
- **Preview**: Shows column types and constraints

### Table View
- **Compact**: All tables in tabular format
- **Sortable**: Click headers to sort
- **Actions**: View button for each table
- **Overview**: Quick comparison of tables

## Statistics Calculation

### Real-time Statistics
- **Table Count**: Total number of tables
- **Column Count**: Total number of columns
- **Relationship Count**: Total foreign key relationships
- **Constraint Counts**: Primary keys, foreign keys, etc.

### Data Type Analysis
- **Type Distribution**: Count of each data type
- **Constraint Analysis**: Count of each constraint type
- **Relationship Mapping**: Count of relationships per table 


================================================
FILE: web-ui/frontend/src/components/pages/schema/index.js
================================================
import React, { useState, useEffect } from 'react';
import './SchemaPage.css';

const SchemaPage = () => {
  const [schemaData, setSchemaData] = useState({
    tables: [],
    relationships: [],
    statistics: {
      totalTables: 0,
      totalColumns: 0,
      totalRelationships: 0,
      dataTypes: {},
      constraints: {}
    }
  });
  const [filteredTables, setFilteredTables] = useState([]);
  const [searchQuery, setSearchQuery] = useState('');
  const [selectedTable, setSelectedTable] = useState(null);
  const [showTableDetails, setShowTableDetails] = useState(false);
  const [isLoading, setIsLoading] = useState(true);
  const [viewMode, setViewMode] = useState('cards'); // 'cards' or 'table'
  const [sortBy, setSortBy] = useState('name'); // 'name', 'columns', 'relationships'

  useEffect(() => {
    loadSchemaData();
  }, []);

  useEffect(() => {
    filterAndSortTables();
  }, [schemaData, searchQuery, sortBy]);

  const loadSchemaData = async () => {
    setIsLoading(true);
    try {
      const response = await fetch('http://127.0.0.1:5000/api/schema');
      if (response.ok) {
        const data = await response.json();
        if (data.success) {
          const tables = data.tables || [];
          const relationships = data.relationships || [];
          
          // Calculate statistics
          const statistics = calculateStatistics(tables, relationships);
          
          setSchemaData({
            tables,
            relationships,
            statistics
          });
        }
      }
    } catch (error) {
      console.error('Error loading schema data:', error);
    } finally {
      setIsLoading(false);
    }
  };

  const calculateStatistics = (tables, relationships) => {
    const dataTypes = {};
    const constraints = {
      primaryKeys: 0,
      foreignKeys: 0,
      uniqueConstraints: 0,
      notNullColumns: 0
    };

    let totalColumns = 0;

    tables.forEach(table => {
      if (table.columns) {
        totalColumns += table.columns.length;
        
        table.columns.forEach(column => {
          // Count data types
          const type = column.type?.split('(')[0] || 'UNKNOWN';
          dataTypes[type] = (dataTypes[type] || 0) + 1;
          
          // Count constraints
          if (column.primary_key) constraints.primaryKeys++;
          if (column.foreign_key) constraints.foreignKeys++;
          if (column.unique) constraints.uniqueConstraints++;
          if (!column.nullable) constraints.notNullColumns++;
        });
      }
    });

    return {
      totalTables: tables.length,
      totalColumns,
      totalRelationships: relationships.length,
      dataTypes,
      constraints
    };
  };

  const filterAndSortTables = () => {
    let filtered = schemaData.tables;

    // Filter by search query
    if (searchQuery) {
      const query = searchQuery.toLowerCase();
      filtered = filtered.filter(table => 
        table.name?.toLowerCase().includes(query) ||
        table.columns?.some(col => col.name?.toLowerCase().includes(query)) ||
        table.columns?.some(col => col.type?.toLowerCase().includes(query))
      );
    }

    // Sort tables
    filtered.sort((a, b) => {
      switch (sortBy) {
        case 'name':
          return (a.name || '').localeCompare(b.name || '');
        case 'columns':
          return (b.columns?.length || 0) - (a.columns?.length || 0);
        case 'relationships':
          const aRels = schemaData.relationships.filter(r => 
            r.constrained_table === a.name || r.referred_table === a.name
          ).length;
          const bRels = schemaData.relationships.filter(r => 
            r.constrained_table === b.name || r.referred_table === b.name
          ).length;
          return bRels - aRels;
        default:
          return 0;
      }
    });

    setFilteredTables(filtered);
  };

  const handleTableClick = async (table) => {
    setSelectedTable(table);
    setShowTableDetails(true);
    
    // Fetch documentation data for this table
    try {
      const docResponse = await fetch(`http://127.0.0.1:5000/api/documentation/tables/${table.name}`);
      if (docResponse.ok) {
        const docData = await docResponse.json();
        if (docData.success) {
          setSelectedTable({
            ...table,
            documentation: docData.documentation,
            business_purpose: docData.business_purpose,
            status: docData.status,
            processed_at: docData.processed_at
          });
        }
      }
    } catch (error) {
      console.error('Error fetching documentation:', error);
    }
  };

  const closeTableDetails = () => {
    setShowTableDetails(false);
    setSelectedTable(null);
  };

  const getTableRelationships = (tableName) => {
    return schemaData.relationships.filter(rel => 
      rel.constrained_table === tableName || rel.referred_table === tableName
    );
  };

  const getDataTypeColor = (type) => {
    const typeMap = {
      'INTEGER': 'primary',
      'VARCHAR': 'info',
      'DECIMAL': 'success',
      'DATETIME': 'warning',
      'DATE': 'secondary',
      'BOOLEAN': 'dark'
    };
    return typeMap[type] || 'secondary';
  };

  const getConstraintBadges = (column) => {
    const badges = [];
    if (column.primary_key) badges.push({ text: 'PK', color: 'warning' });
    if (column.foreign_key) badges.push({ text: 'FK', color: 'info' });
    if (column.unique) badges.push({ text: 'UQ', color: 'success' });
    if (!column.nullable) badges.push({ text: 'NN', color: 'danger' });
    return badges;
  };

  if (isLoading) {
    return (
      <div className="schema-page">
        <div className="d-flex justify-content-center align-items-center" style={{ height: '400px' }}>
          <div className="spinner-border text-primary" role="status">
            <span className="visually-hidden">Loading...</span>
          </div>
        </div>
      </div>
    );
  }

  return (
    <div className="schema-page">
      {/* Header */}
      <div className="page-header">
        <div className="d-flex justify-content-between align-items-center">
          <div>
            <h2 className="mb-1">
              <i className="bi bi-diagram-3 me-2"></i>
              Database Schema
            </h2>
            <p className="text-muted mb-0">
              {schemaData.statistics.totalTables} tables, {schemaData.statistics.totalColumns} columns, {schemaData.statistics.totalRelationships} relationships
            </p>
          </div>
          <button 
            className="btn btn-outline-primary"
            onClick={loadSchemaData}
          >
            <i className="bi bi-arrow-clockwise me-1"></i>
            Refresh
          </button>
        </div>
      </div>

      {/* Statistics Cards */}
      <div className="statistics-section">
        <div className="row g-3">
          <div className="col-md-3">
            <div className="stat-card">
              <div className="stat-icon bg-primary">
                <i className="bi bi-table"></i>
              </div>
              <div className="stat-content">
                <h3>{schemaData.statistics.totalTables}</h3>
                <p>Tables</p>
              </div>
            </div>
          </div>
          <div className="col-md-3">
            <div className="stat-card">
              <div className="stat-icon bg-info">
                <i className="bi bi-list-ul"></i>
              </div>
              <div className="stat-content">
                <h3>{schemaData.statistics.totalColumns}</h3>
                <p>Columns</p>
              </div>
            </div>
          </div>
          <div className="col-md-3">
            <div className="stat-card">
              <div className="stat-icon bg-success">
                <i className="bi bi-arrow-right"></i>
              </div>
              <div className="stat-content">
                <h3>{schemaData.statistics.totalRelationships}</h3>
                <p>Relationships</p>
              </div>
            </div>
          </div>
          <div className="col-md-3">
            <div className="stat-card">
              <div className="stat-icon bg-warning">
                <i className="bi bi-key"></i>
              </div>
              <div className="stat-content">
                <h3>{schemaData.statistics.constraints.primaryKeys}</h3>
                <p>Primary Keys</p>
              </div>
            </div>
          </div>
        </div>
      </div>

      {/* Filters and Controls */}
      <div className="controls-section">
        <div className="row g-3">
          <div className="col-md-6">
            <div className="input-group">
              <span className="input-group-text">
                <i className="bi bi-search"></i>
              </span>
              <input
                type="text"
                className="form-control"
                placeholder="Search tables, columns, or data types..."
                value={searchQuery}
                onChange={(e) => setSearchQuery(e.target.value)}
              />
            </div>
          </div>
          <div className="col-md-3">
            <select
              className="form-select"
              value={sortBy}
              onChange={(e) => setSortBy(e.target.value)}
            >
              <option value="name">Sort by Name</option>
              <option value="columns">Sort by Column Count</option>
              <option value="relationships">Sort by Relationships</option>
            </select>
          </div>
          <div className="col-md-3">
            <div className="btn-group w-100" role="group">
              <button
                type="button"
                className={`btn btn-outline-secondary ${viewMode === 'cards' ? 'active' : ''}`}
                onClick={() => setViewMode('cards')}
              >
                <i className="bi bi-grid-3x3-gap"></i>
              </button>
              <button
                type="button"
                className={`btn btn-outline-secondary ${viewMode === 'table' ? 'active' : ''}`}
                onClick={() => setViewMode('table')}
              >
                <i className="bi bi-table"></i>
              </button>
            </div>
          </div>
        </div>
      </div>

      {/* Schema Content */}
      <div className="schema-content">
        {viewMode === 'cards' ? (
          <div className="cards-view">
            {filteredTables.length > 0 ? (
              <div className="row g-4">
                {filteredTables.map((table, index) => (
                  <div key={index} className="col-lg-6 col-xl-4">
                    <div 
                      className="schema-card"
                      onClick={() => handleTableClick(table)}
                    >
                      <div className="card h-100">
                        <div className="card-header d-flex justify-content-between align-items-center">
                          <span className="badge bg-primary">
                            <i className="bi bi-table me-1"></i>
                            Table
                          </span>
                          <div className="table-stats">
                            <small className="text-muted">
                              {table.columns?.length || 0} columns
                            </small>
                          </div>
                        </div>
                        <div className="card-body">
                          <h6 className="card-title">{table.name}</h6>
                          
                          {/* Column Preview */}
                          <div className="columns-preview">
                            {table.columns?.slice(0, 3).map((col, colIndex) => (
                              <div key={colIndex} className="column-item">
                                <code className="column-name">{col.name}</code>
                                <span className={`badge bg-${getDataTypeColor(col.type)}`}>
                                  {col.type?.split('(')[0]}
                                </span>
                                {getConstraintBadges(col).map((badge, badgeIndex) => (
                                  <span key={badgeIndex} className={`badge bg-${badge.color}`}>
                                    {badge.text}
                                  </span>
                                ))}
                              </div>
                            ))}
                            {table.columns?.length > 3 && (
                              <div className="more-columns">
                                <small className="text-muted">
                                  +{table.columns.length - 3} more columns
                                </small>
                              </div>
                            )}
                          </div>

                          {/* Relationships */}
                          {getTableRelationships(table.name).length > 0 && (
                            <div className="relationships-preview mt-2">
                              <small className="text-muted">
                                {getTableRelationships(table.name).length} relationships
                              </small>
                            </div>
                          )}
                        </div>
                      </div>
                    </div>
                  </div>
                ))}
              </div>
            ) : (
              <div className="empty-state">
                <div className="text-center py-5">
                  <i className="bi bi-table display-1 text-muted"></i>
                  <h4 className="mt-3">No tables found</h4>
                  <p className="text-muted">
                    {searchQuery 
                      ? 'Try adjusting your search criteria.'
                      : 'No tables are currently available in the database.'}
                  </p>
                </div>
              </div>
            )}
          </div>
        ) : (
          <div className="table-view">
            <div className="table-responsive">
              <table className="table table-hover">
                <thead>
                  <tr>
                    <th>Table Name</th>
                    <th>Columns</th>
                    <th>Primary Keys</th>
                    <th>Foreign Keys</th>
                    <th>Relationships</th>
                    <th>Actions</th>
                  </tr>
                </thead>
                <tbody>
                  {filteredTables.map((table, index) => (
                    <tr key={index}>
                      <td>
                        <strong>{table.name}</strong>
                      </td>
                      <td>
                        <span className="badge bg-secondary">
                          {table.columns?.length || 0}
                        </span>
                      </td>
                      <td>
                        {table.columns?.filter(col => col.primary_key).length || 0}
                      </td>
                      <td>
                        {table.columns?.filter(col => col.foreign_key).length || 0}
                      </td>
                      <td>
                        <span className="badge bg-info">
                          {getTableRelationships(table.name).length}
                        </span>
                      </td>
                      <td>
                        <button
                          className="btn btn-sm btn-outline-primary"
                          onClick={() => handleTableClick(table)}
                        >
                          <i className="bi bi-eye"></i>
                        </button>
                      </td>
                    </tr>
                  ))}
                </tbody>
              </table>
            </div>
          </div>
        )}
      </div>

      {/* Table Details Modal */}
      {showTableDetails && selectedTable && (
        <div className="modal fade show" style={{ display: 'block' }} tabIndex="-1">
          <div className="modal-dialog modal-xl">
            <div className="modal-content">
              <div className="modal-header">
                <h5 className="modal-title">
                  <i className="bi bi-table me-2"></i>
                  {selectedTable.name}
                </h5>
                <button
                  type="button"
                  className="btn-close"
                  onClick={closeTableDetails}
                  aria-label="Close"
                ></button>
              </div>
              <div className="modal-body">
                <div className="row">
                  <div className="col-md-8">
                    {/* Documentation Section */}
                    {(selectedTable.business_purpose || selectedTable.documentation) && (
                      <>
                        <h6>Documentation</h6>
                        {selectedTable.business_purpose && (
                          <div className="mb-3">
                            <strong>Business Purpose:</strong>
                            <p className="text-muted mb-0 mt-1">
                              {selectedTable.business_purpose}
                            </p>
                          </div>
                        )}
                        {selectedTable.documentation && (
                          <div className="mb-4">
                            <strong>Detailed Documentation:</strong>
                            <div className="bg-light p-3 rounded mt-1">
                              <p className="mb-0">
                                {selectedTable.documentation}
                              </p>
                            </div>
                          </div>
                        )}
                      </>
                    )}

                    <h6>Schema Information</h6>
                    <div className="table-responsive">
                      <table className="table table-sm">
                        <thead>
                          <tr>
                            <th>Column</th>
                            <th>Data Type</th>
                            <th>Nullable</th>
                            <th>Default</th>
                            <th>Constraints</th>
                          </tr>
                        </thead>
                        <tbody>
                          {selectedTable.columns?.map((column, index) => (
                            <tr key={index}>
                              <td>
                                <code>{column.name}</code>
                              </td>
                              <td>
                                <span className={`badge bg-${getDataTypeColor(column.type)}`}>
                                  {column.type}
                                </span>
                              </td>
                              <td>
                                <span className={`badge ${column.nullable ? 'bg-success' : 'bg-danger'}`}>
                                  {column.nullable ? 'NULL' : 'NOT NULL'}
                                </span>
                              </td>
                              <td>
                                {column.default ? (
                                  <code className="text-muted">{column.default}</code>
                                ) : (
                                  <span className="text-muted">-</span>
                                )}
                              </td>
                              <td>
                                <div className="d-flex flex-wrap gap-1">
                                  {getConstraintBadges(column).map((badge, badgeIndex) => (
                                    <span key={badgeIndex} className={`badge bg-${badge.color}`}>
                                      {badge.text}
                                    </span>
                                  ))}
                                </div>
                              </td>
                            </tr>
                          ))}
                        </tbody>
                      </table>
                    </div>
                  </div>
                  <div className="col-md-4">
                    <h6>Table Statistics</h6>
                    <ul className="list-unstyled">
                      <li><strong>Total Columns:</strong> {selectedTable.columns?.length || 0}</li>
                      <li><strong>Primary Keys:</strong> {selectedTable.columns?.filter(col => col.primary_key).length || 0}</li>
                      <li><strong>Foreign Keys:</strong> {selectedTable.columns?.filter(col => col.foreign_key).length || 0}</li>
                      <li><strong>Nullable Columns:</strong> {selectedTable.columns?.filter(col => col.nullable).length || 0}</li>
                      <li><strong>Not Null Columns:</strong> {selectedTable.columns?.filter(col => !col.nullable).length || 0}</li>
                    </ul>

                    {/* Documentation Status */}
                    {selectedTable.status && (
                      <>
                        <h6 className="mt-4">Documentation Status</h6>
                        <ul className="list-unstyled">
                          <li>
                            <strong>Status:</strong> 
                            <span className={`badge ms-2 ${selectedTable.status === 'completed' ? 'bg-success' : selectedTable.status === 'pending' ? 'bg-warning' : 'bg-danger'}`}>
                              {selectedTable.status}
                            </span>
                          </li>
                          {selectedTable.processed_at && (
                            <li><strong>Processed:</strong> {new Date(selectedTable.processed_at).toLocaleString()}</li>
                          )}
                          <li><strong>Has Business Purpose:</strong> {selectedTable.business_purpose ? 'Yes' : 'No'}</li>
                          <li><strong>Has Documentation:</strong> {selectedTable.documentation ? 'Yes' : 'No'}</li>
                        </ul>
                      </>
                    )}

                    <h6 className="mt-4">Relationships</h6>
                    {getTableRelationships(selectedTable.name).length > 0 ? (
                      <div className="relationships-list">
                        {getTableRelationships(selectedTable.name).map((rel, index) => (
                          <div key={index} className="relationship-item">
                            <small>
                              <strong>{rel.constrained_table}</strong> ‚Üí <strong>{rel.referred_table}</strong>
                            </small>
                            <br />
                            <small className="text-muted">
                              {rel.constrained_columns?.join(', ')} ‚Üí {rel.referred_columns?.join(', ')}
                            </small>
                          </div>
                        ))}
                      </div>
                    ) : (
                      <p className="text-muted small">No relationships found</p>
                    )}
                  </div>
                </div>
              </div>
              <div className="modal-footer">
                <button type="button" className="btn btn-secondary" onClick={closeTableDetails}>
                  Close
                </button>
              </div>
            </div>
          </div>
        </div>
      )}
      
      {/* Modal Backdrop */}
      {showTableDetails && (
        <div className="modal-backdrop fade show" onClick={closeTableDetails}></div>
      )}
    </div>
  );
};

export default SchemaPage; 


================================================
FILE: web-ui/frontend/src/components/pages/schema/SchemaPage.css
================================================
.schema-page {
  padding: 2rem;
  background-color: #f8f9fa;
  min-height: 100vh;
}

.page-header {
  background-color: white;
  padding: 1.5rem;
  border-radius: 0.5rem;
  box-shadow: 0 0.125rem 0.25rem rgba(0, 0, 0, 0.075);
  margin-bottom: 1.5rem;
}

.statistics-section {
  margin-bottom: 1.5rem;
}

.stat-card {
  background-color: white;
  border-radius: 0.5rem;
  box-shadow: 0 0.125rem 0.25rem rgba(0, 0, 0, 0.075);
  padding: 1.5rem;
  display: flex;
  align-items: center;
  gap: 1rem;
  transition: all 0.2s ease-in-out;
}

.stat-card:hover {
  transform: translateY(-2px);
  box-shadow: 0 0.5rem 1rem rgba(0, 0, 0, 0.15);
}

.stat-icon {
  width: 60px;
  height: 60px;
  border-radius: 50%;
  display: flex;
  align-items: center;
  justify-content: center;
  color: white;
  font-size: 1.5rem;
}

.stat-content h3 {
  margin: 0;
  font-size: 2rem;
  font-weight: 700;
  color: #495057;
}

.stat-content p {
  margin: 0;
  color: #6c757d;
  font-size: 0.875rem;
  text-transform: uppercase;
  letter-spacing: 0.5px;
}

.controls-section {
  background-color: white;
  padding: 1.5rem;
  border-radius: 0.5rem;
  box-shadow: 0 0.125rem 0.25rem rgba(0, 0, 0, 0.075);
  margin-bottom: 1.5rem;
}

.schema-content {
  background-color: white;
  border-radius: 0.5rem;
  box-shadow: 0 0.125rem 0.25rem rgba(0, 0, 0, 0.075);
  padding: 1.5rem;
  min-height: 400px;
}

.schema-card {
  cursor: pointer;
  transition: all 0.2s ease-in-out;
}

.schema-card:hover {
  transform: translateY(-2px);
  box-shadow: 0 0.5rem 1rem rgba(0, 0, 0, 0.15);
}

.schema-card .card {
  border: 1px solid #dee2e6;
  border-radius: 0.5rem;
  transition: all 0.2s ease-in-out;
  background-color: white;
}

.schema-card:hover .card {
  border-color: #0d6efd;
  box-shadow: 0 0.5rem 1rem rgba(0, 0, 0, 0.15);
}

.schema-card .card-header {
  background-color: #f8f9fa;
  border-bottom: 1px solid #dee2e6;
  padding: 0.75rem 1rem;
}

.schema-card .card-title {
  color: #495057;
  font-weight: 600;
  margin-bottom: 0.75rem;
}

.columns-preview {
  margin-bottom: 1rem;
}

.column-item {
  display: flex;
  align-items: center;
  gap: 0.5rem;
  margin-bottom: 0.5rem;
  padding: 0.25rem 0;
}

.column-name {
  font-size: 0.875rem;
  background-color: #f8f9fa;
  color: #495057;
  padding: 0.125rem 0.25rem;
  border-radius: 0.25rem;
  font-weight: 500;
}

.more-columns {
  margin-top: 0.5rem;
  padding-top: 0.5rem;
  border-top: 1px solid #e9ecef;
}

.relationships-preview {
  padding-top: 0.5rem;
  border-top: 1px solid #e9ecef;
}

.empty-state {
  background-color: white;
  border-radius: 0.5rem;
  box-shadow: 0 0.125rem 0.25rem rgba(0, 0, 0, 0.075);
}

.empty-state .display-1 {
  font-size: 4rem;
  opacity: 0.5;
}

/* Table View Styles */
.table-view {
  background-color: white;
  border-radius: 0.5rem;
  overflow: hidden;
}

.table-view .table {
  margin-bottom: 0;
}

.table-view .table th {
  background-color: #f8f9fa;
  border-bottom: 2px solid #dee2e6;
  font-weight: 600;
  font-size: 0.875rem;
  text-transform: uppercase;
  letter-spacing: 0.5px;
  font-size: 0.75rem;
}

.table-view .table td {
  vertical-align: middle;
  font-size: 0.875rem;
  padding: 0.75rem 0.5rem;
}

.table-view .table-hover tbody tr:hover {
  background-color: rgba(13, 110, 253, 0.05);
}

/* Modal Styles */
.modal {
  z-index: 1050;
}

.modal-backdrop {
  z-index: 1040;
}

.modal-dialog {
  max-width: 1200px;
}

.modal-content {
  border-radius: 0.5rem;
  box-shadow: 0 0.5rem 1rem rgba(0, 0, 0, 0.15);
}

.modal-header {
  border-bottom: 1px solid #dee2e6;
  background-color: #f8f9fa;
}

.modal-title {
  font-weight: 600;
  color: #495057;
}

.modal-body {
  padding: 1.5rem;
}

.modal-footer {
  border-top: 1px solid #dee2e6;
  background-color: #f8f9fa;
}

/* Badge Colors */
.badge.bg-primary {
  background-color: #0d6efd !important;
}

.badge.bg-info {
  background-color: #0dcaf0 !important;
}

.badge.bg-success {
  background-color: #198754 !important;
}

.badge.bg-warning {
  background-color: #ffc107 !important;
  color: #000 !important;
}

.badge.bg-danger {
  background-color: #dc3545 !important;
}

.badge.bg-secondary {
  background-color: #6c757d !important;
}

.badge.bg-dark {
  background-color: #212529 !important;
}

/* Data Type Colors */
.badge.bg-primary {
  background-color: #0d6efd !important;
}

.badge.bg-info {
  background-color: #0dcaf0 !important;
}

.badge.bg-success {
  background-color: #198754 !important;
}

.badge.bg-warning {
  background-color: #ffc107 !important;
  color: #000 !important;
}

.badge.bg-secondary {
  background-color: #6c757d !important;
}

.badge.bg-dark {
  background-color: #212529 !important;
}

/* Relationships List */
.relationships-list {
  max-height: 200px;
  overflow-y: auto;
}

.relationship-item {
  padding: 0.5rem;
  border-bottom: 1px solid #f8f9fa;
  border-radius: 0.25rem;
  margin-bottom: 0.5rem;
  background-color: #f8f9fa;
}

.relationship-item:last-child {
  border-bottom: none;
  margin-bottom: 0;
}

/* Responsive Design */
@media (max-width: 768px) {
  .schema-page {
    padding: 1rem;
  }
  
  .page-header {
    padding: 1rem;
  }
  
  .controls-section {
    padding: 1rem;
  }
  
  .schema-content {
    padding: 1rem;
  }
  
  .stat-card {
    padding: 1rem;
  }
  
  .stat-icon {
    width: 50px;
    height: 50px;
    font-size: 1.25rem;
  }
  
  .stat-content h3 {
    font-size: 1.5rem;
  }
}

@media (max-width: 576px) {
  .page-header .d-flex {
    flex-direction: column;
    gap: 1rem;
    align-items: flex-start !important;
  }
  
  .controls-section .row {
    margin: 0;
  }
  
  .controls-section .col-md-6,
  .controls-section .col-md-3 {
    padding: 0;
    margin-bottom: 0.5rem;
  }
  
  .stat-card {
    flex-direction: column;
    text-align: center;
  }
  
  .stat-icon {
    margin: 0 auto;
  }
}

/* Loading Animation */
.spinner-border {
  width: 3rem;
  height: 3rem;
}

/* Card Hover Effects */
.card {
  transition: all 0.2s ease-in-out;
}

.card:hover {
  transform: translateY(-2px);
  box-shadow: 0 0.5rem 1rem rgba(0, 0, 0, 0.15);
}

/* Input Group Styling */
.input-group-text {
  background-color: #f8f9fa;
  border-color: #dee2e6;
  color: #6c757d;
}

.form-control:focus,
.form-select:focus {
  border-color: #0d6efd;
  box-shadow: 0 0 0 0.2rem rgba(13, 110, 253, 0.25);
}

/* Button Styling */
.btn-outline-primary:hover {
  background-color: #0d6efd;
  border-color: #0d6efd;
  color: white;
}

.btn-outline-secondary:hover {
  background-color: #6c757d;
  border-color: #6c757d;
  color: white;
}

.btn-outline-secondary.active {
  background-color: #6c757d;
  border-color: #6c757d;
  color: white;
}

/* Table Stats */
.table-stats {
  font-size: 0.75rem;
}

/* Code Styling */
code {
  background-color: #f8f9fa;
  color: #e83e8c;
  padding: 0.125rem 0.25rem;
  border-radius: 0.25rem;
  font-size: 0.875em;
}

/* List Styling */
.list-unstyled li {
  padding: 0.25rem 0;
  border-bottom: 1px solid #f8f9fa;
}

.list-unstyled li:last-child {
  border-bottom: none;
}

.list-unstyled strong {
  color: #495057;
  font-weight: 600;
}

/* Content Animation */
.schema-content {
  animation: fadeIn 0.3s ease-in-out;
}

@keyframes fadeIn {
  from {
    opacity: 0;
    transform: translateY(10px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

/* Status Badge Styling */
.badge {
  font-size: 0.75em;
  font-weight: 500;
  padding: 0.375rem 0.75rem;
}

/* Documentation Text Styling */
.text-muted {
  color: #6c757d !important;
}

/* Modal Content Styling */
.modal-body h6 {
  color: #495057;
  font-weight: 600;
  margin-bottom: 0.5rem;
  border-bottom: 2px solid #e9ecef;
  padding-bottom: 0.25rem;
}

.modal-body p {
  margin-bottom: 1rem;
  line-height: 1.6;
}

/* Documentation Section Styling */
.modal-body .bg-light {
  background-color: #f8f9fa !important;
  border: 1px solid #e9ecef;
  border-radius: 0.375rem;
}

.modal-body .bg-light p {
  margin-bottom: 0;
  line-height: 1.5;
}

.modal-body strong {
  color: #495057;
  font-weight: 600;
}

.modal-body .text-muted {
  color: #6c757d !important;
}

/* Documentation Status Badges */
.modal-body .badge.bg-success {
  background-color: #198754 !important;
}

.modal-body .badge.bg-warning {
  background-color: #ffc107 !important;
  color: #000 !important;
}

.modal-body .badge.bg-danger {
  background-color: #dc3545 !important;
}

/* Modal Layout Improvements */
.modal-body .row {
  margin: 0;
}

.modal-body .col-md-8,
.modal-body .col-md-4 {
  padding: 0 1rem;
}

.modal-body .col-md-8 {
  border-right: 1px solid #e9ecef;
}

/* Documentation Section Spacing */
.modal-body .mb-3 {
  margin-bottom: 1rem !important;
}

.modal-body .mb-4 {
  margin-bottom: 1.5rem !important;
}

.modal-body .mt-1 {
  margin-top: 0.25rem !important;
}

.modal-body .mt-4 {
  margin-top: 1.5rem !important;
}

.modal-body .ms-2 {
  margin-left: 0.5rem !important;
}

/* List Styling for Documentation Status */
.modal-body .list-unstyled li {
  padding: 0.25rem 0;
  border-bottom: 1px solid #f8f9fa;
}

.modal-body .list-unstyled li:last-child {
  border-bottom: none;
}

.modal-body .list-unstyled strong {
  color: #495057;
  font-weight: 600;
}

/* Table Responsive */
.table-responsive {
  border-radius: 0.375rem;
  overflow: hidden;
}

.table {
  margin-bottom: 0;
}

.table th {
  background-color: #f8f9fa;
  border-bottom: 2px solid #dee2e6;
  font-weight: 600;
  font-size: 0.875rem;
  text-transform: uppercase;
  letter-spacing: 0.5px;
  font-size: 0.75rem;
}

.table td {
  vertical-align: middle;
  font-size: 0.875rem;
  padding: 0.75rem 0.5rem;
}

.table-striped > tbody > tr:nth-of-type(odd) > td {
  background-color: rgba(0, 0, 0, 0.02);
} 


================================================
FILE: web-ui/frontend/src/components/query/BusinessContext.js
================================================
import React from 'react';

const BusinessContext = ({ businessContext }) => {
  if (!businessContext) return null;

  return (
    <div className="card mb-3">
      <div className="card-header bg-white">
        <h5 className="mb-0">
          <i className="bi bi-lightbulb me-2"></i>Business Context
        </h5>
      </div>
      <div className="card-body">
        <p className="mb-2">{businessContext.context}</p>
        <div className="d-flex flex-wrap gap-2">
          {businessContext.relevant_tables?.map((table, index) => (
            <span key={index} className="badge bg-secondary">{table}</span>
          ))}
        </div>
      </div>
    </div>
  );
};

export default BusinessContext; 


================================================
FILE: web-ui/frontend/src/components/query/EntityRecognitionResults.js
================================================
import React from 'react';

const EntityRecognitionResults = ({ entityRecognition }) => {
  if (!entityRecognition) return null;

  return (
    <div className="card mb-3">
      <div className="card-header bg-white">
        <h5 className="mb-0">
          <i className="bi bi-diagram-3 me-2"></i>Entity Recognition
        </h5>
      </div>
      <div className="card-body">
        <div className="row">
          <div className="col-md-6">
            <h6>Recognized Entities</h6>
            <div className="d-flex flex-wrap gap-2">
              {entityRecognition.entities?.map((entity, index) => (
                <span key={index} className="badge bg-info">
                  {typeof entity === 'string' ? entity : `${entity.type}: ${entity.value}`}
                </span>
              ))}
            </div>
          </div>
          <div className="col-md-6">
            <h6>Confidence</h6>
            <div className="progress">
              <div
                className="progress-bar"
                style={{ width: `${(entityRecognition.confidence || 0) * 100}%` }}
              >
                {(entityRecognition.confidence || 0) * 100}%
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  );
};

export default EntityRecognitionResults; 


================================================
FILE: web-ui/frontend/src/components/query/index.js
================================================
export { default as QueryPage } from './QueryPage';
export { default as SQLAgentStatus } from './SQLAgentStatus';
export { default as QueryInput } from './QueryInput';
export { default as EntityRecognitionResults } from './EntityRecognitionResults';
export { default as BusinessContext } from './BusinessContext';
export { default as SQLGeneration } from './SQLGeneration';
export { default as QueryResults } from './QueryResults';
export { default as OptimizationSuggestions } from './OptimizationSuggestions'; 


================================================
FILE: web-ui/frontend/src/components/query/OptimizationSuggestions.js
================================================
import React from 'react';

const OptimizationSuggestions = ({ optimizationSuggestions }) => {
  if (!optimizationSuggestions || optimizationSuggestions.length === 0) return null;

  return (
    <div className="card mb-3">
      <div className="card-header bg-white">
        <h5 className="mb-0">
          <i className="bi bi-speedometer2 me-2"></i>Optimization Suggestions
        </h5>
      </div>
      <div className="card-body">
        <div className="row">
          {optimizationSuggestions.map((suggestion, index) => (
            <div key={index} className="col-md-6 mb-3">
              <div className="card border-warning">
                <div className="card-body">
                  <h6 className="card-title">
                    <i className="bi bi-lightbulb text-warning me-2"></i>
                    {suggestion.type}
                  </h6>
                  <p className="card-text">{suggestion.description}</p>
                  {suggestion.sql && (
                    <pre className="bg-light p-2 rounded small">
                      <code>{suggestion.sql}</code>
                    </pre>
                  )}
                </div>
              </div>
            </div>
          ))}
        </div>
      </div>
    </div>
  );
};

export default OptimizationSuggestions; 


================================================
FILE: web-ui/frontend/src/components/query/QueryInput.js
================================================
import React from 'react';

const QueryInput = ({ 
  query, 
  setQuery, 
  executeQuery, 
  isLoading, 
  handleKeyPress 
}) => {
  return (
    <div className="card mb-3">
      <div className="card-header bg-white">
        <h5 className="mb-0">Natural Language Query</h5>
      </div>
      <div className="card-body">
        <div className="input-group">
          <input
            type="text"
            className="form-control"
            id="queryInput"
            placeholder="Ask a question about your data..."
            value={query}
            onChange={(e) => setQuery(e.target.value)}
            onKeyPress={handleKeyPress}
          />
          <button
            className={`btn ${isLoading ? 'btn-info' : 'btn-primary'} position-relative`}
            type="button"
            onClick={executeQuery}
            disabled={isLoading || !query.trim()}
            style={{
              minWidth: '120px',
              transition: 'all 0.3s ease',
              borderRadius: '0 0.375rem 0.375rem 0'
            }}
          >
            {isLoading ? (
              <>
                <span 
                  className="spinner-border spinner-border-sm me-2" 
                  role="status" 
                  aria-hidden="true"
                  style={{ width: '1rem', height: '1rem' }}
                ></span>
                <span style={{ fontWeight: '500' }}>Running...</span>
              </>
            ) : (
              <>
                <i className="bi bi-play-fill me-2"></i>
                <span style={{ fontWeight: '500' }}>Run Query</span>
              </>
            )}
          </button>
        </div>
        <div className="mt-2 text-muted small">
          Example: "Show me the top 10 customers by total spending"
        </div>
      </div>
    </div>
  );
};

export default QueryInput; 


================================================
FILE: web-ui/frontend/src/components/query/QueryPage.js
================================================
import React from 'react';
import SQLAgentStatus from './SQLAgentStatus';
import QueryInput from './QueryInput';
import EntityRecognitionResults from './EntityRecognitionResults';
import BusinessContext from './BusinessContext';
import SQLGeneration from './SQLGeneration';
import QueryResults from './QueryResults';
import OptimizationSuggestions from './OptimizationSuggestions';

const QueryPage = ({
  sqlAgentStatus,
  query,
  setQuery,
  executeQuery,
  isLoading,
  handleKeyPress,
  entityRecognition,
  businessContext,
  sqlValidation,
  generatedSql,
  copySqlToClipboard,
  queryExecution,
  optimizationSuggestions,
  results
}) => {
  console.log('QueryPage - results prop:', results);
  console.log('QueryPage - queryExecution prop:', queryExecution);
  return (
    <>
      {/* SQL Agent Status */}
      <SQLAgentStatus sqlAgentStatus={sqlAgentStatus} />

      {/* Query Input */}
      <QueryInput
        query={query}
        setQuery={setQuery}
        executeQuery={executeQuery}
        isLoading={isLoading}
        handleKeyPress={handleKeyPress}
      />

      {/* Entity Recognition Results */}
      <EntityRecognitionResults entityRecognition={entityRecognition} />

      {/* Business Context */}
      <BusinessContext businessContext={businessContext} />

      {/* SQL Generation */}
      <SQLGeneration
        sqlValidation={sqlValidation}
        generatedSql={generatedSql}
        copySqlToClipboard={copySqlToClipboard}
      />

      {/* Query Results */}
      <QueryResults queryExecution={queryExecution} results={results} />

      {/* Optimization Suggestions */}
      <OptimizationSuggestions optimizationSuggestions={optimizationSuggestions} />
    </>
  );
};

export default QueryPage; 


================================================
FILE: web-ui/frontend/src/components/query/QueryResults.js
================================================
import React from 'react';

const QueryResults = ({ queryExecution, results }) => {
  // Debug logging
  console.log('QueryResults - queryExecution:', queryExecution);
  console.log('QueryResults - results:', results);
  
  // Use results prop if available, otherwise fall back to queryExecution
  const displayResults = results || queryExecution?.sample_data?.sample_rows || [];
  const columns = queryExecution?.sample_data?.columns || [];
  
  console.log('QueryResults - displayResults:', displayResults);
  console.log('QueryResults - columns:', columns);
  
  if (!displayResults.length && !queryExecution) {
    console.log('QueryResults - returning null, no data to display');
    return null;
  }

  return (
    <div className="card mb-3">
      <div className="card-header bg-white">
        <h5 className="mb-0">
          <i className="bi bi-table me-2"></i>Query Results
        </h5>
      </div>
      <div className="card-body">
        <div className="table-responsive">
          <table className="table table-striped table-hover">
            <thead>
              <tr>
                {columns.map((column, index) => (
                  <th key={index}>{column}</th>
                ))}
              </tr>
            </thead>
            <tbody>
              {displayResults.map((row, rowIndex) => (
                <tr key={rowIndex}>
                  {Object.values(row).map((cell, cellIndex) => (
                    <td key={cellIndex}>{cell}</td>
                  ))}
                </tr>
              ))}
            </tbody>
          </table>
        </div>
        <div className="mt-3 text-muted small">
          {displayResults.length} rows returned
          {queryExecution?.total_rows && queryExecution.total_rows !== displayResults.length && (
            <span className="ms-2 text-info">
              (showing {displayResults.length} of {queryExecution.total_rows} total rows)
            </span>
          )}
        </div>
      </div>
    </div>
  );
};

export default QueryResults; 


================================================
FILE: web-ui/frontend/src/components/query/SQLAgentStatus.js
================================================
import React from 'react';

const SQLAgentStatus = ({ sqlAgentStatus }) => {
  if (!sqlAgentStatus) return null;

  return (
    <div className="card mb-3">
      <div className="card-header bg-white">
        <h5 className="mb-0">
          <i className="bi bi-robot me-2"></i>SQL Agent Status
        </h5>
      </div>
      <div className="card-body">
        <div className="row">
          <div className="col-md-6">
            <h6>Features</h6>
            <div className="d-flex flex-wrap gap-2">
              {Object.entries(sqlAgentStatus.features || {}).map(([feature, available]) => (
                <span key={feature} className={`badge ${available ? 'bg-success' : 'bg-secondary'}`}>
                  {available ? '‚úÖ' : '‚ùå'} {feature.replace('_', ' ')}
                </span>
              ))}
            </div>
          </div>
          <div className="col-md-6">
            <h6>Agents</h6>
            <div className="d-flex flex-wrap gap-2">
              {Object.entries(sqlAgentStatus.agents || {}).map(([agent, available]) => (
                <span key={agent} className={`badge ${available ? 'bg-success' : 'bg-secondary'}`}>
                  {available ? '‚úÖ' : '‚ùå'} {agent.replace('_', ' ')}
                </span>
              ))}
            </div>
          </div>
        </div>

        {/* Show informative message when SQL Agents are not available */}
        {!sqlAgentStatus.sql_agents_available && (
          <div className="mt-3 p-3 bg-light rounded">
            <div className="d-flex align-items-center">
              <i className="bi bi-exclamation-triangle text-warning me-2"></i>
              <div>
                <strong>SQL Agents Not Available</strong>
                <br />
                <small className="text-muted">
                  SQL Agents are not available. Please ensure the backend server is running and SQL Agents are properly initialized.
                </small>
              </div>
            </div>
          </div>
        )}
      </div>
    </div>
  );
};

export default SQLAgentStatus; 


================================================
FILE: web-ui/frontend/src/components/query/SQLGeneration.js
================================================
import React from 'react';

const SQLGeneration = ({ sqlValidation, generatedSql, copySqlToClipboard }) => {
  if (!sqlValidation) return null;

  return (
    <div className="card mb-3">
      <div className="card-header bg-white d-flex justify-content-between align-items-center">
        <h5 className="mb-0">
          <i className="bi bi-code-slash me-2"></i>Generated SQL
        </h5>
        <div>
          <button
            className="btn btn-sm btn-outline-secondary me-2"
            onClick={copySqlToClipboard}
          >
            <i className="bi bi-clipboard me-1"></i>Copy
          </button>
          <span className={`badge ${sqlValidation.is_valid ? 'bg-success' : 'bg-danger'}`}>
            {sqlValidation.is_valid ? 'Valid' : 'Invalid'}
          </span>
        </div>
      </div>
      <div className="card-body">
        <pre className="bg-light p-3 rounded">
          <code>{generatedSql}</code>
        </pre>
        {sqlValidation.errors && sqlValidation.errors.length > 0 && (
          <div className="mt-3">
            <h6>Validation Errors:</h6>
            <ul className="list-unstyled">
              {sqlValidation.errors.map((error, index) => (
                <li key={index} className="text-danger">
                  <i className="bi bi-exclamation-triangle me-1"></i>
                  {error}
                </li>
              ))}
            </ul>
          </div>
        )}
      </div>
    </div>
  );
};

export default SQLGeneration; 


================================================
FILE: web-ui/frontend/src/components/ui/index.js
================================================
export { default as SplashScreen } from './SplashScreen';
export { default as TopNavigation } from './TopNavigation'; 


================================================
FILE: web-ui/frontend/src/components/ui/SplashScreen.js
================================================
import React from 'react';

const SplashScreen = ({ 
  splashStatus, 
  splashMessage, 
  splashProgress, 
  pollingAttempts, 
  retryInitialization 
}) => {
  return (
    <div className="splash-screen">
      <div className="splash-content">
        <div className="splash-logo">
          <i className="bi bi-database-gear"></i>
          <h1>SQL Agent</h1>
          <p>Natural Language to SQL Pipeline</p>
        </div>

        <div className="splash-status">
          <div className="status-indicator">
            {splashStatus === 'checking' && (
              <div className="spinner-border text-primary" role="status">
                <span className="visually-hidden">Loading...</span>
              </div>
            )}
            {splashStatus === 'error' && (
              <i className="bi bi-exclamation-triangle text-danger"></i>
            )}
          </div>

          <h3>{splashMessage}</h3>

          <div className="progress mt-3" style={{ height: '4px' }}>
            <div
              className="progress-bar"
              role="progressbar"
              style={{ width: `${splashProgress}%` }}
              aria-valuenow={splashProgress}
              aria-valuemin="0"
              aria-valuemax="100"
            ></div>
          </div>

          {splashStatus === 'checking' && (
            <div className="mt-3">
              <small className="text-muted">
                <i className="bi bi-info-circle me-1"></i>
                Automatically retrying connection... (attempt {pollingAttempts}/30)
              </small>
            </div>
          )}

          {splashStatus === 'error' && (
            <div className="mt-4">
              <button
                className="btn btn-primary"
                onClick={retryInitialization}
              >
                <i className="bi bi-arrow-clockwise me-2"></i>
                Retry Connection
              </button>
              <div className="mt-3">
                <small className="text-muted">
                  <i className="bi bi-lightbulb me-1"></i>
                  Make sure the backend server is running at http://127.0.0.1:5000
                </small>
                <br />
                <small className="text-muted">
                  <i className="bi bi-clock me-1"></i>
                  The app will automatically retry for 30 seconds
                </small>
              </div>
            </div>
          )}
        </div>

        <div className="splash-footer">
          <small className="text-muted">
            Powered by smol-sql-agents
          </small>
        </div>
      </div>
    </div>
  );
};

export default SplashScreen; 


================================================
FILE: web-ui/frontend/src/components/ui/TopNavigation.js
================================================
import React from 'react';

const TopNavigation = ({ currentPage, setCurrentPage }) => {
  return (
    <nav className="navbar navbar-expand-lg navbar-dark bg-primary">
      <div className="container-fluid" style={{ color: 'white' }}>
        <i className="bi bi-database-gear me-2"></i>SQL Agent
        <button className="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav">
          <span className="navbar-toggler-icon"></span>
        </button>
        <div className="collapse navbar-collapse" id="navbarNav">
          <ul className="navbar-nav me-auto">
            <li className="nav-item">
              <button
                className={`nav-link ${currentPage === 'query' ? 'active' : ''}`}
                onClick={() => setCurrentPage('query')}
              >
                <i className="bi bi-search me-1"></i>Query
              </button>
            </li>
            <li className="nav-item">
              <button
                className={`nav-link ${currentPage === 'schema' ? 'active' : ''}`}
                onClick={() => setCurrentPage('schema')}
              >
                <i className="bi bi-diagram-3 me-1"></i>Schema
              </button>
            </li>
            <li className="nav-item">
              <button
                className={`nav-link ${currentPage === 'relationships' ? 'active' : ''}`}
                onClick={() => setCurrentPage('relationships')}
              >
                <i className="bi bi-diagram-3-fill me-1"></i>Relationships
              </button>
            </li>
            <li className="nav-item">
              <button
                className={`nav-link ${currentPage === 'documentation' ? 'active' : ''}`}
                onClick={() => setCurrentPage('documentation')}
              >
                <i className="bi bi-journal-text me-1"></i>Documentation
              </button>
            </li>
          </ul>
          <div className="d-flex">
            <button className="btn btn-outline-light me-2" id="connectBtn">
              <i className="bi bi-plug-fill me-1"></i> Connect to Database
            </button>
            <div className="dropdown">
              <button className="btn btn-light dropdown-toggle" type="button" id="userMenu" data-bs-toggle="dropdown">
                <i className="bi bi-person-circle me-1"></i> User
              </button>
              <ul className="dropdown-menu dropdown-menu-end">
                <li><button className="dropdown-item">Profile</button></li>
                <li><button className="dropdown-item">Settings</button></li>
                <li><hr className="dropdown-divider" /></li>
                <li><button className="dropdown-item">Logout</button></li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </nav>
  );
};

export default TopNavigation; 

